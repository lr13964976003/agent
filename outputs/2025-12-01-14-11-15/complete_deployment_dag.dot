// Complete LLM Deployment DAG - 64 GPUs with Hybrid Parallelism
digraph {
	dpi=300 rankdir=TB size="100,100"
	node [fontname=Arial fontsize=8]
	edge [fontname=Arial fontsize=6]
	input [label="Model Input\n\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=white fontname="Arial Bold" fontsize=12 shape=box style=filled]
	subgraph cluster_pipeline_stage_0 {
		fillcolor=orange fontname="Arial Bold" fontsize=10 label="Pipeline Stage 0 (Layers 0-3)" style="rounded,filled"
		subgraph cluster_layer_0 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 0" style="rounded,filled"
			subgraph cluster_layer0_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 0)" style="rounded,filled"
				layer0_expert0_route [label="Expert Routing\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert0_gate [label="Expert Gate\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert0_a2a [label="All-to-All Comm\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert0_attn_split [label="Attention Head Split\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert0_q_proj [label="Q Projection (Col-Par)\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_k_proj [label="K Projection (Col-Par)\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_v_proj [label="V Projection (Col-Par)\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_attn [label="Multi-Head Attention\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_attn_out [label="Attention Output (Row-Par)\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_attn_ar [label="Attention All-Reduce\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert0_attn_res [label="Attention + Residual\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert0_ln1 [label="Layer Norm 1\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_gelu [label="GELU Activation\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert0_mlp_ar [label="MLP All-Reduce\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert0_mlp_res [label="MLP + Residual\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert0_ln2 [label="Layer Norm 2\nL0_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer0_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 2)" style="rounded,filled"
				layer0_expert1_route [label="Expert Routing\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert1_gate [label="Expert Gate\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert1_a2a [label="All-to-All Comm\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert1_attn_split [label="Attention Head Split\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert1_q_proj [label="Q Projection (Col-Par)\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_k_proj [label="K Projection (Col-Par)\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_v_proj [label="V Projection (Col-Par)\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_attn [label="Multi-Head Attention\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_attn_out [label="Attention Output (Row-Par)\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_attn_ar [label="Attention All-Reduce\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert1_attn_res [label="Attention + Residual\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert1_ln1 [label="Layer Norm 1\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_gelu [label="GELU Activation\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert1_mlp_ar [label="MLP All-Reduce\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert1_mlp_res [label="MLP + Residual\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert1_ln2 [label="Layer Norm 2\nL0_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer0_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 4)" style="rounded,filled"
				layer0_expert2_route [label="Expert Routing\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert2_gate [label="Expert Gate\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert2_a2a [label="All-to-All Comm\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert2_attn_split [label="Attention Head Split\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert2_q_proj [label="Q Projection (Col-Par)\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_k_proj [label="K Projection (Col-Par)\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_v_proj [label="V Projection (Col-Par)\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_attn [label="Multi-Head Attention\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_attn_out [label="Attention Output (Row-Par)\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_attn_ar [label="Attention All-Reduce\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert2_attn_res [label="Attention + Residual\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert2_ln1 [label="Layer Norm 1\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_gelu [label="GELU Activation\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert2_mlp_ar [label="MLP All-Reduce\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert2_mlp_res [label="MLP + Residual\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert2_ln2 [label="Layer Norm 2\nL0_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer0_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 6)" style="rounded,filled"
				layer0_expert3_route [label="Expert Routing\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert3_gate [label="Expert Gate\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert3_a2a [label="All-to-All Comm\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert3_attn_split [label="Attention Head Split\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert3_q_proj [label="Q Projection (Col-Par)\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_k_proj [label="K Projection (Col-Par)\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_v_proj [label="V Projection (Col-Par)\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_attn [label="Multi-Head Attention\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_attn_out [label="Attention Output (Row-Par)\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_attn_ar [label="Attention All-Reduce\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert3_attn_res [label="Attention + Residual\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert3_ln1 [label="Layer Norm 1\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_gelu [label="GELU Activation\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert3_mlp_ar [label="MLP All-Reduce\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert3_mlp_res [label="MLP + Residual\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert3_ln2 [label="Layer Norm 2\nL0_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer0_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 8)" style="rounded,filled"
				layer0_expert4_route [label="Expert Routing\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert4_gate [label="Expert Gate\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert4_a2a [label="All-to-All Comm\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert4_attn_split [label="Attention Head Split\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert4_q_proj [label="Q Projection (Col-Par)\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_k_proj [label="K Projection (Col-Par)\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_v_proj [label="V Projection (Col-Par)\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_attn [label="Multi-Head Attention\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_attn_out [label="Attention Output (Row-Par)\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_attn_ar [label="Attention All-Reduce\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert4_attn_res [label="Attention + Residual\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert4_ln1 [label="Layer Norm 1\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_gelu [label="GELU Activation\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert4_mlp_ar [label="MLP All-Reduce\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert4_mlp_res [label="MLP + Residual\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert4_ln2 [label="Layer Norm 2\nL0_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer0_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 10)" style="rounded,filled"
				layer0_expert5_route [label="Expert Routing\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert5_gate [label="Expert Gate\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert5_a2a [label="All-to-All Comm\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert5_attn_split [label="Attention Head Split\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert5_q_proj [label="Q Projection (Col-Par)\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_k_proj [label="K Projection (Col-Par)\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_v_proj [label="V Projection (Col-Par)\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_attn [label="Multi-Head Attention\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_attn_out [label="Attention Output (Row-Par)\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_attn_ar [label="Attention All-Reduce\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert5_attn_res [label="Attention + Residual\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert5_ln1 [label="Layer Norm 1\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_gelu [label="GELU Activation\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert5_mlp_ar [label="MLP All-Reduce\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert5_mlp_res [label="MLP + Residual\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert5_ln2 [label="Layer Norm 2\nL0_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer0_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 12)" style="rounded,filled"
				layer0_expert6_route [label="Expert Routing\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert6_gate [label="Expert Gate\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert6_a2a [label="All-to-All Comm\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert6_attn_split [label="Attention Head Split\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert6_q_proj [label="Q Projection (Col-Par)\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_k_proj [label="K Projection (Col-Par)\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_v_proj [label="V Projection (Col-Par)\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_attn [label="Multi-Head Attention\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_attn_out [label="Attention Output (Row-Par)\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_attn_ar [label="Attention All-Reduce\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert6_attn_res [label="Attention + Residual\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert6_ln1 [label="Layer Norm 1\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_gelu [label="GELU Activation\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert6_mlp_ar [label="MLP All-Reduce\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert6_mlp_res [label="MLP + Residual\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert6_ln2 [label="Layer Norm 2\nL0_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer0_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 14)" style="rounded,filled"
				layer0_expert7_route [label="Expert Routing\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer0_expert7_gate [label="Expert Gate\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert7_a2a [label="All-to-All Comm\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert7_attn_split [label="Attention Head Split\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert7_q_proj [label="Q Projection (Col-Par)\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_k_proj [label="K Projection (Col-Par)\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_v_proj [label="V Projection (Col-Par)\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_attn [label="Multi-Head Attention\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_attn_out [label="Attention Output (Row-Par)\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_attn_ar [label="Attention All-Reduce\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert7_attn_res [label="Attention + Residual\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert7_ln1 [label="Layer Norm 1\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_gelu [label="GELU Activation\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer0_expert7_mlp_ar [label="MLP All-Reduce\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer0_expert7_mlp_res [label="MLP + Residual\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer0_expert7_ln2 [label="Layer Norm 2\nL0_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_1 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 1" style="rounded,filled"
			subgraph cluster_layer1_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 0)" style="rounded,filled"
				layer1_expert0_route [label="Expert Routing\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert0_gate [label="Expert Gate\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert0_a2a [label="All-to-All Comm\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert0_attn_split [label="Attention Head Split\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert0_q_proj [label="Q Projection (Col-Par)\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_k_proj [label="K Projection (Col-Par)\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_v_proj [label="V Projection (Col-Par)\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_attn [label="Multi-Head Attention\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_attn_out [label="Attention Output (Row-Par)\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_attn_ar [label="Attention All-Reduce\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert0_attn_res [label="Attention + Residual\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert0_ln1 [label="Layer Norm 1\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_gelu [label="GELU Activation\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert0_mlp_ar [label="MLP All-Reduce\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert0_mlp_res [label="MLP + Residual\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert0_ln2 [label="Layer Norm 2\nL1_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer1_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 2)" style="rounded,filled"
				layer1_expert1_route [label="Expert Routing\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert1_gate [label="Expert Gate\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert1_a2a [label="All-to-All Comm\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert1_attn_split [label="Attention Head Split\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert1_q_proj [label="Q Projection (Col-Par)\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_k_proj [label="K Projection (Col-Par)\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_v_proj [label="V Projection (Col-Par)\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_attn [label="Multi-Head Attention\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_attn_out [label="Attention Output (Row-Par)\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_attn_ar [label="Attention All-Reduce\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert1_attn_res [label="Attention + Residual\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert1_ln1 [label="Layer Norm 1\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_gelu [label="GELU Activation\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert1_mlp_ar [label="MLP All-Reduce\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert1_mlp_res [label="MLP + Residual\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert1_ln2 [label="Layer Norm 2\nL1_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer1_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 4)" style="rounded,filled"
				layer1_expert2_route [label="Expert Routing\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert2_gate [label="Expert Gate\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert2_a2a [label="All-to-All Comm\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert2_attn_split [label="Attention Head Split\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert2_q_proj [label="Q Projection (Col-Par)\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_k_proj [label="K Projection (Col-Par)\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_v_proj [label="V Projection (Col-Par)\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_attn [label="Multi-Head Attention\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_attn_out [label="Attention Output (Row-Par)\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_attn_ar [label="Attention All-Reduce\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert2_attn_res [label="Attention + Residual\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert2_ln1 [label="Layer Norm 1\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_gelu [label="GELU Activation\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert2_mlp_ar [label="MLP All-Reduce\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert2_mlp_res [label="MLP + Residual\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert2_ln2 [label="Layer Norm 2\nL1_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer1_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 6)" style="rounded,filled"
				layer1_expert3_route [label="Expert Routing\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert3_gate [label="Expert Gate\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert3_a2a [label="All-to-All Comm\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert3_attn_split [label="Attention Head Split\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert3_q_proj [label="Q Projection (Col-Par)\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_k_proj [label="K Projection (Col-Par)\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_v_proj [label="V Projection (Col-Par)\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_attn [label="Multi-Head Attention\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_attn_out [label="Attention Output (Row-Par)\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_attn_ar [label="Attention All-Reduce\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert3_attn_res [label="Attention + Residual\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert3_ln1 [label="Layer Norm 1\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_gelu [label="GELU Activation\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert3_mlp_ar [label="MLP All-Reduce\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert3_mlp_res [label="MLP + Residual\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert3_ln2 [label="Layer Norm 2\nL1_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer1_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 8)" style="rounded,filled"
				layer1_expert4_route [label="Expert Routing\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert4_gate [label="Expert Gate\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert4_a2a [label="All-to-All Comm\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert4_attn_split [label="Attention Head Split\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert4_q_proj [label="Q Projection (Col-Par)\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_k_proj [label="K Projection (Col-Par)\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_v_proj [label="V Projection (Col-Par)\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_attn [label="Multi-Head Attention\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_attn_out [label="Attention Output (Row-Par)\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_attn_ar [label="Attention All-Reduce\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert4_attn_res [label="Attention + Residual\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert4_ln1 [label="Layer Norm 1\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_gelu [label="GELU Activation\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert4_mlp_ar [label="MLP All-Reduce\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert4_mlp_res [label="MLP + Residual\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert4_ln2 [label="Layer Norm 2\nL1_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer1_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 10)" style="rounded,filled"
				layer1_expert5_route [label="Expert Routing\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert5_gate [label="Expert Gate\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert5_a2a [label="All-to-All Comm\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert5_attn_split [label="Attention Head Split\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert5_q_proj [label="Q Projection (Col-Par)\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_k_proj [label="K Projection (Col-Par)\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_v_proj [label="V Projection (Col-Par)\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_attn [label="Multi-Head Attention\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_attn_out [label="Attention Output (Row-Par)\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_attn_ar [label="Attention All-Reduce\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert5_attn_res [label="Attention + Residual\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert5_ln1 [label="Layer Norm 1\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_gelu [label="GELU Activation\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert5_mlp_ar [label="MLP All-Reduce\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert5_mlp_res [label="MLP + Residual\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert5_ln2 [label="Layer Norm 2\nL1_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer1_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 12)" style="rounded,filled"
				layer1_expert6_route [label="Expert Routing\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert6_gate [label="Expert Gate\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert6_a2a [label="All-to-All Comm\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert6_attn_split [label="Attention Head Split\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert6_q_proj [label="Q Projection (Col-Par)\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_k_proj [label="K Projection (Col-Par)\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_v_proj [label="V Projection (Col-Par)\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_attn [label="Multi-Head Attention\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_attn_out [label="Attention Output (Row-Par)\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_attn_ar [label="Attention All-Reduce\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert6_attn_res [label="Attention + Residual\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert6_ln1 [label="Layer Norm 1\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_gelu [label="GELU Activation\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert6_mlp_ar [label="MLP All-Reduce\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert6_mlp_res [label="MLP + Residual\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert6_ln2 [label="Layer Norm 2\nL1_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer1_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 14)" style="rounded,filled"
				layer1_expert7_route [label="Expert Routing\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer1_expert7_gate [label="Expert Gate\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert7_a2a [label="All-to-All Comm\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert7_attn_split [label="Attention Head Split\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert7_q_proj [label="Q Projection (Col-Par)\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_k_proj [label="K Projection (Col-Par)\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_v_proj [label="V Projection (Col-Par)\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_attn [label="Multi-Head Attention\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_attn_out [label="Attention Output (Row-Par)\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_attn_ar [label="Attention All-Reduce\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert7_attn_res [label="Attention + Residual\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert7_ln1 [label="Layer Norm 1\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_gelu [label="GELU Activation\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer1_expert7_mlp_ar [label="MLP All-Reduce\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer1_expert7_mlp_res [label="MLP + Residual\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer1_expert7_ln2 [label="Layer Norm 2\nL1_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_2 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 2" style="rounded,filled"
			subgraph cluster_layer2_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 0)" style="rounded,filled"
				layer2_expert0_route [label="Expert Routing\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert0_gate [label="Expert Gate\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert0_a2a [label="All-to-All Comm\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert0_attn_split [label="Attention Head Split\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert0_q_proj [label="Q Projection (Col-Par)\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_k_proj [label="K Projection (Col-Par)\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_v_proj [label="V Projection (Col-Par)\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_attn [label="Multi-Head Attention\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_attn_out [label="Attention Output (Row-Par)\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_attn_ar [label="Attention All-Reduce\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert0_attn_res [label="Attention + Residual\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert0_ln1 [label="Layer Norm 1\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_gelu [label="GELU Activation\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert0_mlp_ar [label="MLP All-Reduce\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert0_mlp_res [label="MLP + Residual\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert0_ln2 [label="Layer Norm 2\nL2_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer2_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 2)" style="rounded,filled"
				layer2_expert1_route [label="Expert Routing\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert1_gate [label="Expert Gate\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert1_a2a [label="All-to-All Comm\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert1_attn_split [label="Attention Head Split\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert1_q_proj [label="Q Projection (Col-Par)\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_k_proj [label="K Projection (Col-Par)\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_v_proj [label="V Projection (Col-Par)\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_attn [label="Multi-Head Attention\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_attn_out [label="Attention Output (Row-Par)\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_attn_ar [label="Attention All-Reduce\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert1_attn_res [label="Attention + Residual\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert1_ln1 [label="Layer Norm 1\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_gelu [label="GELU Activation\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert1_mlp_ar [label="MLP All-Reduce\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert1_mlp_res [label="MLP + Residual\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert1_ln2 [label="Layer Norm 2\nL2_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer2_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 4)" style="rounded,filled"
				layer2_expert2_route [label="Expert Routing\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert2_gate [label="Expert Gate\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert2_a2a [label="All-to-All Comm\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert2_attn_split [label="Attention Head Split\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert2_q_proj [label="Q Projection (Col-Par)\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_k_proj [label="K Projection (Col-Par)\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_v_proj [label="V Projection (Col-Par)\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_attn [label="Multi-Head Attention\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_attn_out [label="Attention Output (Row-Par)\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_attn_ar [label="Attention All-Reduce\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert2_attn_res [label="Attention + Residual\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert2_ln1 [label="Layer Norm 1\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_gelu [label="GELU Activation\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert2_mlp_ar [label="MLP All-Reduce\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert2_mlp_res [label="MLP + Residual\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert2_ln2 [label="Layer Norm 2\nL2_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer2_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 6)" style="rounded,filled"
				layer2_expert3_route [label="Expert Routing\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert3_gate [label="Expert Gate\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert3_a2a [label="All-to-All Comm\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert3_attn_split [label="Attention Head Split\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert3_q_proj [label="Q Projection (Col-Par)\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_k_proj [label="K Projection (Col-Par)\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_v_proj [label="V Projection (Col-Par)\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_attn [label="Multi-Head Attention\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_attn_out [label="Attention Output (Row-Par)\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_attn_ar [label="Attention All-Reduce\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert3_attn_res [label="Attention + Residual\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert3_ln1 [label="Layer Norm 1\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_gelu [label="GELU Activation\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert3_mlp_ar [label="MLP All-Reduce\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert3_mlp_res [label="MLP + Residual\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert3_ln2 [label="Layer Norm 2\nL2_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer2_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 8)" style="rounded,filled"
				layer2_expert4_route [label="Expert Routing\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert4_gate [label="Expert Gate\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert4_a2a [label="All-to-All Comm\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert4_attn_split [label="Attention Head Split\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert4_q_proj [label="Q Projection (Col-Par)\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_k_proj [label="K Projection (Col-Par)\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_v_proj [label="V Projection (Col-Par)\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_attn [label="Multi-Head Attention\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_attn_out [label="Attention Output (Row-Par)\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_attn_ar [label="Attention All-Reduce\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert4_attn_res [label="Attention + Residual\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert4_ln1 [label="Layer Norm 1\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_gelu [label="GELU Activation\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert4_mlp_ar [label="MLP All-Reduce\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert4_mlp_res [label="MLP + Residual\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert4_ln2 [label="Layer Norm 2\nL2_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer2_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 10)" style="rounded,filled"
				layer2_expert5_route [label="Expert Routing\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert5_gate [label="Expert Gate\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert5_a2a [label="All-to-All Comm\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert5_attn_split [label="Attention Head Split\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert5_q_proj [label="Q Projection (Col-Par)\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_k_proj [label="K Projection (Col-Par)\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_v_proj [label="V Projection (Col-Par)\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_attn [label="Multi-Head Attention\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_attn_out [label="Attention Output (Row-Par)\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_attn_ar [label="Attention All-Reduce\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert5_attn_res [label="Attention + Residual\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert5_ln1 [label="Layer Norm 1\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_gelu [label="GELU Activation\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert5_mlp_ar [label="MLP All-Reduce\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert5_mlp_res [label="MLP + Residual\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert5_ln2 [label="Layer Norm 2\nL2_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer2_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 12)" style="rounded,filled"
				layer2_expert6_route [label="Expert Routing\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert6_gate [label="Expert Gate\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert6_a2a [label="All-to-All Comm\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert6_attn_split [label="Attention Head Split\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert6_q_proj [label="Q Projection (Col-Par)\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_k_proj [label="K Projection (Col-Par)\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_v_proj [label="V Projection (Col-Par)\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_attn [label="Multi-Head Attention\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_attn_out [label="Attention Output (Row-Par)\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_attn_ar [label="Attention All-Reduce\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert6_attn_res [label="Attention + Residual\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert6_ln1 [label="Layer Norm 1\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_gelu [label="GELU Activation\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert6_mlp_ar [label="MLP All-Reduce\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert6_mlp_res [label="MLP + Residual\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert6_ln2 [label="Layer Norm 2\nL2_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer2_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 14)" style="rounded,filled"
				layer2_expert7_route [label="Expert Routing\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer2_expert7_gate [label="Expert Gate\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert7_a2a [label="All-to-All Comm\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert7_attn_split [label="Attention Head Split\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert7_q_proj [label="Q Projection (Col-Par)\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_k_proj [label="K Projection (Col-Par)\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_v_proj [label="V Projection (Col-Par)\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_attn [label="Multi-Head Attention\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_attn_out [label="Attention Output (Row-Par)\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_attn_ar [label="Attention All-Reduce\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert7_attn_res [label="Attention + Residual\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert7_ln1 [label="Layer Norm 1\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_gelu [label="GELU Activation\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer2_expert7_mlp_ar [label="MLP All-Reduce\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer2_expert7_mlp_res [label="MLP + Residual\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer2_expert7_ln2 [label="Layer Norm 2\nL2_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_3 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 3" style="rounded,filled"
			subgraph cluster_layer3_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 0)" style="rounded,filled"
				layer3_expert0_route [label="Expert Routing\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert0_gate [label="Expert Gate\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert0_a2a [label="All-to-All Comm\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert0_attn_split [label="Attention Head Split\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert0_q_proj [label="Q Projection (Col-Par)\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_k_proj [label="K Projection (Col-Par)\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_v_proj [label="V Projection (Col-Par)\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_attn [label="Multi-Head Attention\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_attn_out [label="Attention Output (Row-Par)\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_attn_ar [label="Attention All-Reduce\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert0_attn_res [label="Attention + Residual\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert0_ln1 [label="Layer Norm 1\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_gelu [label="GELU Activation\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert0_mlp_ar [label="MLP All-Reduce\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert0_mlp_res [label="MLP + Residual\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert0_ln2 [label="Layer Norm 2\nL3_E0\nGPU:0\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer3_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 2)" style="rounded,filled"
				layer3_expert1_route [label="Expert Routing\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert1_gate [label="Expert Gate\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert1_a2a [label="All-to-All Comm\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert1_attn_split [label="Attention Head Split\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert1_q_proj [label="Q Projection (Col-Par)\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_k_proj [label="K Projection (Col-Par)\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_v_proj [label="V Projection (Col-Par)\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_attn [label="Multi-Head Attention\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_attn_out [label="Attention Output (Row-Par)\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_attn_ar [label="Attention All-Reduce\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert1_attn_res [label="Attention + Residual\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert1_ln1 [label="Layer Norm 1\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_gelu [label="GELU Activation\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert1_mlp_ar [label="MLP All-Reduce\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert1_mlp_res [label="MLP + Residual\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert1_ln2 [label="Layer Norm 2\nL3_E1\nGPU:2\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer3_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 4)" style="rounded,filled"
				layer3_expert2_route [label="Expert Routing\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert2_gate [label="Expert Gate\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert2_a2a [label="All-to-All Comm\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert2_attn_split [label="Attention Head Split\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert2_q_proj [label="Q Projection (Col-Par)\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_k_proj [label="K Projection (Col-Par)\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_v_proj [label="V Projection (Col-Par)\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_attn [label="Multi-Head Attention\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_attn_out [label="Attention Output (Row-Par)\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_attn_ar [label="Attention All-Reduce\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert2_attn_res [label="Attention + Residual\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert2_ln1 [label="Layer Norm 1\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_gelu [label="GELU Activation\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert2_mlp_ar [label="MLP All-Reduce\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert2_mlp_res [label="MLP + Residual\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert2_ln2 [label="Layer Norm 2\nL3_E2\nGPU:4\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer3_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 6)" style="rounded,filled"
				layer3_expert3_route [label="Expert Routing\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert3_gate [label="Expert Gate\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert3_a2a [label="All-to-All Comm\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert3_attn_split [label="Attention Head Split\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert3_q_proj [label="Q Projection (Col-Par)\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_k_proj [label="K Projection (Col-Par)\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_v_proj [label="V Projection (Col-Par)\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_attn [label="Multi-Head Attention\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_attn_out [label="Attention Output (Row-Par)\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_attn_ar [label="Attention All-Reduce\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert3_attn_res [label="Attention + Residual\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert3_ln1 [label="Layer Norm 1\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_gelu [label="GELU Activation\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert3_mlp_ar [label="MLP All-Reduce\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert3_mlp_res [label="MLP + Residual\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert3_ln2 [label="Layer Norm 2\nL3_E3\nGPU:6\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer3_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 8)" style="rounded,filled"
				layer3_expert4_route [label="Expert Routing\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert4_gate [label="Expert Gate\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert4_a2a [label="All-to-All Comm\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert4_attn_split [label="Attention Head Split\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert4_q_proj [label="Q Projection (Col-Par)\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_k_proj [label="K Projection (Col-Par)\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_v_proj [label="V Projection (Col-Par)\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_attn [label="Multi-Head Attention\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_attn_out [label="Attention Output (Row-Par)\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_attn_ar [label="Attention All-Reduce\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert4_attn_res [label="Attention + Residual\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert4_ln1 [label="Layer Norm 1\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_gelu [label="GELU Activation\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert4_mlp_ar [label="MLP All-Reduce\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert4_mlp_res [label="MLP + Residual\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert4_ln2 [label="Layer Norm 2\nL3_E4\nGPU:8\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer3_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 10)" style="rounded,filled"
				layer3_expert5_route [label="Expert Routing\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert5_gate [label="Expert Gate\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert5_a2a [label="All-to-All Comm\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert5_attn_split [label="Attention Head Split\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert5_q_proj [label="Q Projection (Col-Par)\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_k_proj [label="K Projection (Col-Par)\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_v_proj [label="V Projection (Col-Par)\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_attn [label="Multi-Head Attention\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_attn_out [label="Attention Output (Row-Par)\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_attn_ar [label="Attention All-Reduce\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert5_attn_res [label="Attention + Residual\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert5_ln1 [label="Layer Norm 1\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_gelu [label="GELU Activation\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert5_mlp_ar [label="MLP All-Reduce\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert5_mlp_res [label="MLP + Residual\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert5_ln2 [label="Layer Norm 2\nL3_E5\nGPU:10\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer3_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 12)" style="rounded,filled"
				layer3_expert6_route [label="Expert Routing\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert6_gate [label="Expert Gate\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert6_a2a [label="All-to-All Comm\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert6_attn_split [label="Attention Head Split\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert6_q_proj [label="Q Projection (Col-Par)\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_k_proj [label="K Projection (Col-Par)\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_v_proj [label="V Projection (Col-Par)\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_attn [label="Multi-Head Attention\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_attn_out [label="Attention Output (Row-Par)\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_attn_ar [label="Attention All-Reduce\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert6_attn_res [label="Attention + Residual\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert6_ln1 [label="Layer Norm 1\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_gelu [label="GELU Activation\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert6_mlp_ar [label="MLP All-Reduce\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert6_mlp_res [label="MLP + Residual\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert6_ln2 [label="Layer Norm 2\nL3_E6\nGPU:12\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer3_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 14)" style="rounded,filled"
				layer3_expert7_route [label="Expert Routing\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer3_expert7_gate [label="Expert Gate\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert7_a2a [label="All-to-All Comm\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert7_attn_split [label="Attention Head Split\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert7_q_proj [label="Q Projection (Col-Par)\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_k_proj [label="K Projection (Col-Par)\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_v_proj [label="V Projection (Col-Par)\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_attn [label="Multi-Head Attention\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_attn_out [label="Attention Output (Row-Par)\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_attn_ar [label="Attention All-Reduce\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert7_attn_res [label="Attention + Residual\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert7_ln1 [label="Layer Norm 1\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_gelu [label="GELU Activation\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer3_expert7_mlp_ar [label="MLP All-Reduce\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer3_expert7_mlp_res [label="MLP + Residual\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer3_expert7_ln2 [label="Layer Norm 2\nL3_E7\nGPU:14\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
	}
	subgraph cluster_pipeline_stage_1 {
		fillcolor=orange fontname="Arial Bold" fontsize=10 label="Pipeline Stage 1 (Layers 4-7)" style="rounded,filled"
		subgraph cluster_layer_4 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 4" style="rounded,filled"
			subgraph cluster_layer4_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 16)" style="rounded,filled"
				layer4_expert0_route [label="Expert Routing\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert0_gate [label="Expert Gate\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert0_a2a [label="All-to-All Comm\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert0_attn_split [label="Attention Head Split\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert0_q_proj [label="Q Projection (Col-Par)\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_k_proj [label="K Projection (Col-Par)\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_v_proj [label="V Projection (Col-Par)\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_attn [label="Multi-Head Attention\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_attn_out [label="Attention Output (Row-Par)\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_attn_ar [label="Attention All-Reduce\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert0_attn_res [label="Attention + Residual\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert0_ln1 [label="Layer Norm 1\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_gelu [label="GELU Activation\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert0_mlp_ar [label="MLP All-Reduce\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert0_mlp_res [label="MLP + Residual\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert0_ln2 [label="Layer Norm 2\nL4_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer4_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 18)" style="rounded,filled"
				layer4_expert1_route [label="Expert Routing\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert1_gate [label="Expert Gate\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert1_a2a [label="All-to-All Comm\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert1_attn_split [label="Attention Head Split\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert1_q_proj [label="Q Projection (Col-Par)\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_k_proj [label="K Projection (Col-Par)\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_v_proj [label="V Projection (Col-Par)\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_attn [label="Multi-Head Attention\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_attn_out [label="Attention Output (Row-Par)\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_attn_ar [label="Attention All-Reduce\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert1_attn_res [label="Attention + Residual\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert1_ln1 [label="Layer Norm 1\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_gelu [label="GELU Activation\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert1_mlp_ar [label="MLP All-Reduce\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert1_mlp_res [label="MLP + Residual\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert1_ln2 [label="Layer Norm 2\nL4_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer4_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 20)" style="rounded,filled"
				layer4_expert2_route [label="Expert Routing\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert2_gate [label="Expert Gate\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert2_a2a [label="All-to-All Comm\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert2_attn_split [label="Attention Head Split\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert2_q_proj [label="Q Projection (Col-Par)\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_k_proj [label="K Projection (Col-Par)\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_v_proj [label="V Projection (Col-Par)\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_attn [label="Multi-Head Attention\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_attn_out [label="Attention Output (Row-Par)\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_attn_ar [label="Attention All-Reduce\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert2_attn_res [label="Attention + Residual\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert2_ln1 [label="Layer Norm 1\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_gelu [label="GELU Activation\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert2_mlp_ar [label="MLP All-Reduce\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert2_mlp_res [label="MLP + Residual\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert2_ln2 [label="Layer Norm 2\nL4_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer4_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 22)" style="rounded,filled"
				layer4_expert3_route [label="Expert Routing\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert3_gate [label="Expert Gate\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert3_a2a [label="All-to-All Comm\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert3_attn_split [label="Attention Head Split\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert3_q_proj [label="Q Projection (Col-Par)\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_k_proj [label="K Projection (Col-Par)\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_v_proj [label="V Projection (Col-Par)\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_attn [label="Multi-Head Attention\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_attn_out [label="Attention Output (Row-Par)\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_attn_ar [label="Attention All-Reduce\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert3_attn_res [label="Attention + Residual\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert3_ln1 [label="Layer Norm 1\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_gelu [label="GELU Activation\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert3_mlp_ar [label="MLP All-Reduce\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert3_mlp_res [label="MLP + Residual\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert3_ln2 [label="Layer Norm 2\nL4_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer4_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 24)" style="rounded,filled"
				layer4_expert4_route [label="Expert Routing\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert4_gate [label="Expert Gate\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert4_a2a [label="All-to-All Comm\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert4_attn_split [label="Attention Head Split\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert4_q_proj [label="Q Projection (Col-Par)\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_k_proj [label="K Projection (Col-Par)\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_v_proj [label="V Projection (Col-Par)\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_attn [label="Multi-Head Attention\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_attn_out [label="Attention Output (Row-Par)\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_attn_ar [label="Attention All-Reduce\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert4_attn_res [label="Attention + Residual\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert4_ln1 [label="Layer Norm 1\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_gelu [label="GELU Activation\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert4_mlp_ar [label="MLP All-Reduce\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert4_mlp_res [label="MLP + Residual\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert4_ln2 [label="Layer Norm 2\nL4_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer4_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 26)" style="rounded,filled"
				layer4_expert5_route [label="Expert Routing\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert5_gate [label="Expert Gate\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert5_a2a [label="All-to-All Comm\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert5_attn_split [label="Attention Head Split\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert5_q_proj [label="Q Projection (Col-Par)\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_k_proj [label="K Projection (Col-Par)\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_v_proj [label="V Projection (Col-Par)\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_attn [label="Multi-Head Attention\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_attn_out [label="Attention Output (Row-Par)\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_attn_ar [label="Attention All-Reduce\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert5_attn_res [label="Attention + Residual\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert5_ln1 [label="Layer Norm 1\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_gelu [label="GELU Activation\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert5_mlp_ar [label="MLP All-Reduce\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert5_mlp_res [label="MLP + Residual\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert5_ln2 [label="Layer Norm 2\nL4_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer4_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 28)" style="rounded,filled"
				layer4_expert6_route [label="Expert Routing\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert6_gate [label="Expert Gate\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert6_a2a [label="All-to-All Comm\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert6_attn_split [label="Attention Head Split\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert6_q_proj [label="Q Projection (Col-Par)\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_k_proj [label="K Projection (Col-Par)\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_v_proj [label="V Projection (Col-Par)\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_attn [label="Multi-Head Attention\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_attn_out [label="Attention Output (Row-Par)\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_attn_ar [label="Attention All-Reduce\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert6_attn_res [label="Attention + Residual\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert6_ln1 [label="Layer Norm 1\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_gelu [label="GELU Activation\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert6_mlp_ar [label="MLP All-Reduce\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert6_mlp_res [label="MLP + Residual\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert6_ln2 [label="Layer Norm 2\nL4_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer4_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 30)" style="rounded,filled"
				layer4_expert7_route [label="Expert Routing\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer4_expert7_gate [label="Expert Gate\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert7_a2a [label="All-to-All Comm\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert7_attn_split [label="Attention Head Split\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert7_q_proj [label="Q Projection (Col-Par)\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_k_proj [label="K Projection (Col-Par)\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_v_proj [label="V Projection (Col-Par)\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_attn [label="Multi-Head Attention\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_attn_out [label="Attention Output (Row-Par)\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_attn_ar [label="Attention All-Reduce\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert7_attn_res [label="Attention + Residual\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert7_ln1 [label="Layer Norm 1\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_gelu [label="GELU Activation\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer4_expert7_mlp_ar [label="MLP All-Reduce\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer4_expert7_mlp_res [label="MLP + Residual\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer4_expert7_ln2 [label="Layer Norm 2\nL4_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_5 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 5" style="rounded,filled"
			subgraph cluster_layer5_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 16)" style="rounded,filled"
				layer5_expert0_route [label="Expert Routing\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert0_gate [label="Expert Gate\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert0_a2a [label="All-to-All Comm\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert0_attn_split [label="Attention Head Split\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert0_q_proj [label="Q Projection (Col-Par)\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_k_proj [label="K Projection (Col-Par)\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_v_proj [label="V Projection (Col-Par)\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_attn [label="Multi-Head Attention\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_attn_out [label="Attention Output (Row-Par)\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_attn_ar [label="Attention All-Reduce\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert0_attn_res [label="Attention + Residual\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert0_ln1 [label="Layer Norm 1\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_gelu [label="GELU Activation\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert0_mlp_ar [label="MLP All-Reduce\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert0_mlp_res [label="MLP + Residual\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert0_ln2 [label="Layer Norm 2\nL5_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer5_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 18)" style="rounded,filled"
				layer5_expert1_route [label="Expert Routing\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert1_gate [label="Expert Gate\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert1_a2a [label="All-to-All Comm\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert1_attn_split [label="Attention Head Split\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert1_q_proj [label="Q Projection (Col-Par)\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_k_proj [label="K Projection (Col-Par)\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_v_proj [label="V Projection (Col-Par)\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_attn [label="Multi-Head Attention\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_attn_out [label="Attention Output (Row-Par)\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_attn_ar [label="Attention All-Reduce\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert1_attn_res [label="Attention + Residual\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert1_ln1 [label="Layer Norm 1\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_gelu [label="GELU Activation\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert1_mlp_ar [label="MLP All-Reduce\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert1_mlp_res [label="MLP + Residual\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert1_ln2 [label="Layer Norm 2\nL5_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer5_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 20)" style="rounded,filled"
				layer5_expert2_route [label="Expert Routing\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert2_gate [label="Expert Gate\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert2_a2a [label="All-to-All Comm\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert2_attn_split [label="Attention Head Split\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert2_q_proj [label="Q Projection (Col-Par)\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_k_proj [label="K Projection (Col-Par)\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_v_proj [label="V Projection (Col-Par)\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_attn [label="Multi-Head Attention\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_attn_out [label="Attention Output (Row-Par)\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_attn_ar [label="Attention All-Reduce\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert2_attn_res [label="Attention + Residual\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert2_ln1 [label="Layer Norm 1\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_gelu [label="GELU Activation\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert2_mlp_ar [label="MLP All-Reduce\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert2_mlp_res [label="MLP + Residual\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert2_ln2 [label="Layer Norm 2\nL5_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer5_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 22)" style="rounded,filled"
				layer5_expert3_route [label="Expert Routing\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert3_gate [label="Expert Gate\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert3_a2a [label="All-to-All Comm\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert3_attn_split [label="Attention Head Split\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert3_q_proj [label="Q Projection (Col-Par)\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_k_proj [label="K Projection (Col-Par)\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_v_proj [label="V Projection (Col-Par)\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_attn [label="Multi-Head Attention\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_attn_out [label="Attention Output (Row-Par)\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_attn_ar [label="Attention All-Reduce\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert3_attn_res [label="Attention + Residual\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert3_ln1 [label="Layer Norm 1\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_gelu [label="GELU Activation\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert3_mlp_ar [label="MLP All-Reduce\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert3_mlp_res [label="MLP + Residual\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert3_ln2 [label="Layer Norm 2\nL5_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer5_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 24)" style="rounded,filled"
				layer5_expert4_route [label="Expert Routing\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert4_gate [label="Expert Gate\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert4_a2a [label="All-to-All Comm\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert4_attn_split [label="Attention Head Split\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert4_q_proj [label="Q Projection (Col-Par)\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_k_proj [label="K Projection (Col-Par)\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_v_proj [label="V Projection (Col-Par)\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_attn [label="Multi-Head Attention\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_attn_out [label="Attention Output (Row-Par)\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_attn_ar [label="Attention All-Reduce\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert4_attn_res [label="Attention + Residual\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert4_ln1 [label="Layer Norm 1\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_gelu [label="GELU Activation\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert4_mlp_ar [label="MLP All-Reduce\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert4_mlp_res [label="MLP + Residual\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert4_ln2 [label="Layer Norm 2\nL5_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer5_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 26)" style="rounded,filled"
				layer5_expert5_route [label="Expert Routing\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert5_gate [label="Expert Gate\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert5_a2a [label="All-to-All Comm\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert5_attn_split [label="Attention Head Split\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert5_q_proj [label="Q Projection (Col-Par)\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_k_proj [label="K Projection (Col-Par)\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_v_proj [label="V Projection (Col-Par)\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_attn [label="Multi-Head Attention\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_attn_out [label="Attention Output (Row-Par)\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_attn_ar [label="Attention All-Reduce\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert5_attn_res [label="Attention + Residual\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert5_ln1 [label="Layer Norm 1\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_gelu [label="GELU Activation\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert5_mlp_ar [label="MLP All-Reduce\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert5_mlp_res [label="MLP + Residual\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert5_ln2 [label="Layer Norm 2\nL5_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer5_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 28)" style="rounded,filled"
				layer5_expert6_route [label="Expert Routing\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert6_gate [label="Expert Gate\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert6_a2a [label="All-to-All Comm\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert6_attn_split [label="Attention Head Split\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert6_q_proj [label="Q Projection (Col-Par)\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_k_proj [label="K Projection (Col-Par)\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_v_proj [label="V Projection (Col-Par)\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_attn [label="Multi-Head Attention\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_attn_out [label="Attention Output (Row-Par)\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_attn_ar [label="Attention All-Reduce\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert6_attn_res [label="Attention + Residual\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert6_ln1 [label="Layer Norm 1\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_gelu [label="GELU Activation\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert6_mlp_ar [label="MLP All-Reduce\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert6_mlp_res [label="MLP + Residual\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert6_ln2 [label="Layer Norm 2\nL5_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer5_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 30)" style="rounded,filled"
				layer5_expert7_route [label="Expert Routing\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer5_expert7_gate [label="Expert Gate\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert7_a2a [label="All-to-All Comm\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert7_attn_split [label="Attention Head Split\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert7_q_proj [label="Q Projection (Col-Par)\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_k_proj [label="K Projection (Col-Par)\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_v_proj [label="V Projection (Col-Par)\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_attn [label="Multi-Head Attention\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_attn_out [label="Attention Output (Row-Par)\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_attn_ar [label="Attention All-Reduce\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert7_attn_res [label="Attention + Residual\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert7_ln1 [label="Layer Norm 1\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_gelu [label="GELU Activation\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer5_expert7_mlp_ar [label="MLP All-Reduce\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer5_expert7_mlp_res [label="MLP + Residual\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer5_expert7_ln2 [label="Layer Norm 2\nL5_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_6 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 6" style="rounded,filled"
			subgraph cluster_layer6_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 16)" style="rounded,filled"
				layer6_expert0_route [label="Expert Routing\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert0_gate [label="Expert Gate\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert0_a2a [label="All-to-All Comm\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert0_attn_split [label="Attention Head Split\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert0_q_proj [label="Q Projection (Col-Par)\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_k_proj [label="K Projection (Col-Par)\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_v_proj [label="V Projection (Col-Par)\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_attn [label="Multi-Head Attention\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_attn_out [label="Attention Output (Row-Par)\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_attn_ar [label="Attention All-Reduce\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert0_attn_res [label="Attention + Residual\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert0_ln1 [label="Layer Norm 1\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_gelu [label="GELU Activation\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert0_mlp_ar [label="MLP All-Reduce\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert0_mlp_res [label="MLP + Residual\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert0_ln2 [label="Layer Norm 2\nL6_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer6_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 18)" style="rounded,filled"
				layer6_expert1_route [label="Expert Routing\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert1_gate [label="Expert Gate\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert1_a2a [label="All-to-All Comm\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert1_attn_split [label="Attention Head Split\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert1_q_proj [label="Q Projection (Col-Par)\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_k_proj [label="K Projection (Col-Par)\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_v_proj [label="V Projection (Col-Par)\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_attn [label="Multi-Head Attention\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_attn_out [label="Attention Output (Row-Par)\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_attn_ar [label="Attention All-Reduce\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert1_attn_res [label="Attention + Residual\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert1_ln1 [label="Layer Norm 1\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_gelu [label="GELU Activation\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert1_mlp_ar [label="MLP All-Reduce\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert1_mlp_res [label="MLP + Residual\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert1_ln2 [label="Layer Norm 2\nL6_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer6_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 20)" style="rounded,filled"
				layer6_expert2_route [label="Expert Routing\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert2_gate [label="Expert Gate\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert2_a2a [label="All-to-All Comm\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert2_attn_split [label="Attention Head Split\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert2_q_proj [label="Q Projection (Col-Par)\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_k_proj [label="K Projection (Col-Par)\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_v_proj [label="V Projection (Col-Par)\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_attn [label="Multi-Head Attention\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_attn_out [label="Attention Output (Row-Par)\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_attn_ar [label="Attention All-Reduce\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert2_attn_res [label="Attention + Residual\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert2_ln1 [label="Layer Norm 1\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_gelu [label="GELU Activation\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert2_mlp_ar [label="MLP All-Reduce\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert2_mlp_res [label="MLP + Residual\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert2_ln2 [label="Layer Norm 2\nL6_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer6_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 22)" style="rounded,filled"
				layer6_expert3_route [label="Expert Routing\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert3_gate [label="Expert Gate\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert3_a2a [label="All-to-All Comm\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert3_attn_split [label="Attention Head Split\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert3_q_proj [label="Q Projection (Col-Par)\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_k_proj [label="K Projection (Col-Par)\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_v_proj [label="V Projection (Col-Par)\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_attn [label="Multi-Head Attention\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_attn_out [label="Attention Output (Row-Par)\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_attn_ar [label="Attention All-Reduce\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert3_attn_res [label="Attention + Residual\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert3_ln1 [label="Layer Norm 1\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_gelu [label="GELU Activation\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert3_mlp_ar [label="MLP All-Reduce\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert3_mlp_res [label="MLP + Residual\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert3_ln2 [label="Layer Norm 2\nL6_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer6_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 24)" style="rounded,filled"
				layer6_expert4_route [label="Expert Routing\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert4_gate [label="Expert Gate\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert4_a2a [label="All-to-All Comm\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert4_attn_split [label="Attention Head Split\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert4_q_proj [label="Q Projection (Col-Par)\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_k_proj [label="K Projection (Col-Par)\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_v_proj [label="V Projection (Col-Par)\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_attn [label="Multi-Head Attention\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_attn_out [label="Attention Output (Row-Par)\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_attn_ar [label="Attention All-Reduce\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert4_attn_res [label="Attention + Residual\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert4_ln1 [label="Layer Norm 1\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_gelu [label="GELU Activation\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert4_mlp_ar [label="MLP All-Reduce\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert4_mlp_res [label="MLP + Residual\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert4_ln2 [label="Layer Norm 2\nL6_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer6_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 26)" style="rounded,filled"
				layer6_expert5_route [label="Expert Routing\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert5_gate [label="Expert Gate\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert5_a2a [label="All-to-All Comm\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert5_attn_split [label="Attention Head Split\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert5_q_proj [label="Q Projection (Col-Par)\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_k_proj [label="K Projection (Col-Par)\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_v_proj [label="V Projection (Col-Par)\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_attn [label="Multi-Head Attention\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_attn_out [label="Attention Output (Row-Par)\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_attn_ar [label="Attention All-Reduce\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert5_attn_res [label="Attention + Residual\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert5_ln1 [label="Layer Norm 1\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_gelu [label="GELU Activation\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert5_mlp_ar [label="MLP All-Reduce\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert5_mlp_res [label="MLP + Residual\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert5_ln2 [label="Layer Norm 2\nL6_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer6_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 28)" style="rounded,filled"
				layer6_expert6_route [label="Expert Routing\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert6_gate [label="Expert Gate\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert6_a2a [label="All-to-All Comm\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert6_attn_split [label="Attention Head Split\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert6_q_proj [label="Q Projection (Col-Par)\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_k_proj [label="K Projection (Col-Par)\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_v_proj [label="V Projection (Col-Par)\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_attn [label="Multi-Head Attention\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_attn_out [label="Attention Output (Row-Par)\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_attn_ar [label="Attention All-Reduce\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert6_attn_res [label="Attention + Residual\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert6_ln1 [label="Layer Norm 1\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_gelu [label="GELU Activation\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert6_mlp_ar [label="MLP All-Reduce\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert6_mlp_res [label="MLP + Residual\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert6_ln2 [label="Layer Norm 2\nL6_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer6_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 30)" style="rounded,filled"
				layer6_expert7_route [label="Expert Routing\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer6_expert7_gate [label="Expert Gate\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert7_a2a [label="All-to-All Comm\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert7_attn_split [label="Attention Head Split\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert7_q_proj [label="Q Projection (Col-Par)\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_k_proj [label="K Projection (Col-Par)\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_v_proj [label="V Projection (Col-Par)\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_attn [label="Multi-Head Attention\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_attn_out [label="Attention Output (Row-Par)\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_attn_ar [label="Attention All-Reduce\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert7_attn_res [label="Attention + Residual\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert7_ln1 [label="Layer Norm 1\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_gelu [label="GELU Activation\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer6_expert7_mlp_ar [label="MLP All-Reduce\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer6_expert7_mlp_res [label="MLP + Residual\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer6_expert7_ln2 [label="Layer Norm 2\nL6_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_7 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 7" style="rounded,filled"
			subgraph cluster_layer7_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 16)" style="rounded,filled"
				layer7_expert0_route [label="Expert Routing\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert0_gate [label="Expert Gate\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert0_a2a [label="All-to-All Comm\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert0_attn_split [label="Attention Head Split\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert0_q_proj [label="Q Projection (Col-Par)\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_k_proj [label="K Projection (Col-Par)\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_v_proj [label="V Projection (Col-Par)\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_attn [label="Multi-Head Attention\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_attn_out [label="Attention Output (Row-Par)\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_attn_ar [label="Attention All-Reduce\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert0_attn_res [label="Attention + Residual\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert0_ln1 [label="Layer Norm 1\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_gelu [label="GELU Activation\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert0_mlp_ar [label="MLP All-Reduce\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert0_mlp_res [label="MLP + Residual\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert0_ln2 [label="Layer Norm 2\nL7_E0\nGPU:16\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer7_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 18)" style="rounded,filled"
				layer7_expert1_route [label="Expert Routing\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert1_gate [label="Expert Gate\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert1_a2a [label="All-to-All Comm\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert1_attn_split [label="Attention Head Split\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert1_q_proj [label="Q Projection (Col-Par)\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_k_proj [label="K Projection (Col-Par)\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_v_proj [label="V Projection (Col-Par)\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_attn [label="Multi-Head Attention\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_attn_out [label="Attention Output (Row-Par)\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_attn_ar [label="Attention All-Reduce\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert1_attn_res [label="Attention + Residual\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert1_ln1 [label="Layer Norm 1\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_gelu [label="GELU Activation\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert1_mlp_ar [label="MLP All-Reduce\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert1_mlp_res [label="MLP + Residual\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert1_ln2 [label="Layer Norm 2\nL7_E1\nGPU:18\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer7_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 20)" style="rounded,filled"
				layer7_expert2_route [label="Expert Routing\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert2_gate [label="Expert Gate\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert2_a2a [label="All-to-All Comm\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert2_attn_split [label="Attention Head Split\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert2_q_proj [label="Q Projection (Col-Par)\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_k_proj [label="K Projection (Col-Par)\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_v_proj [label="V Projection (Col-Par)\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_attn [label="Multi-Head Attention\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_attn_out [label="Attention Output (Row-Par)\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_attn_ar [label="Attention All-Reduce\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert2_attn_res [label="Attention + Residual\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert2_ln1 [label="Layer Norm 1\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_gelu [label="GELU Activation\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert2_mlp_ar [label="MLP All-Reduce\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert2_mlp_res [label="MLP + Residual\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert2_ln2 [label="Layer Norm 2\nL7_E2\nGPU:20\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer7_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 22)" style="rounded,filled"
				layer7_expert3_route [label="Expert Routing\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert3_gate [label="Expert Gate\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert3_a2a [label="All-to-All Comm\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert3_attn_split [label="Attention Head Split\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert3_q_proj [label="Q Projection (Col-Par)\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_k_proj [label="K Projection (Col-Par)\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_v_proj [label="V Projection (Col-Par)\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_attn [label="Multi-Head Attention\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_attn_out [label="Attention Output (Row-Par)\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_attn_ar [label="Attention All-Reduce\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert3_attn_res [label="Attention + Residual\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert3_ln1 [label="Layer Norm 1\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_gelu [label="GELU Activation\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert3_mlp_ar [label="MLP All-Reduce\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert3_mlp_res [label="MLP + Residual\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert3_ln2 [label="Layer Norm 2\nL7_E3\nGPU:22\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer7_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 24)" style="rounded,filled"
				layer7_expert4_route [label="Expert Routing\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert4_gate [label="Expert Gate\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert4_a2a [label="All-to-All Comm\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert4_attn_split [label="Attention Head Split\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert4_q_proj [label="Q Projection (Col-Par)\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_k_proj [label="K Projection (Col-Par)\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_v_proj [label="V Projection (Col-Par)\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_attn [label="Multi-Head Attention\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_attn_out [label="Attention Output (Row-Par)\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_attn_ar [label="Attention All-Reduce\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert4_attn_res [label="Attention + Residual\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert4_ln1 [label="Layer Norm 1\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_gelu [label="GELU Activation\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert4_mlp_ar [label="MLP All-Reduce\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert4_mlp_res [label="MLP + Residual\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert4_ln2 [label="Layer Norm 2\nL7_E4\nGPU:24\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer7_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 26)" style="rounded,filled"
				layer7_expert5_route [label="Expert Routing\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert5_gate [label="Expert Gate\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert5_a2a [label="All-to-All Comm\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert5_attn_split [label="Attention Head Split\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert5_q_proj [label="Q Projection (Col-Par)\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_k_proj [label="K Projection (Col-Par)\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_v_proj [label="V Projection (Col-Par)\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_attn [label="Multi-Head Attention\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_attn_out [label="Attention Output (Row-Par)\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_attn_ar [label="Attention All-Reduce\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert5_attn_res [label="Attention + Residual\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert5_ln1 [label="Layer Norm 1\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_gelu [label="GELU Activation\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert5_mlp_ar [label="MLP All-Reduce\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert5_mlp_res [label="MLP + Residual\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert5_ln2 [label="Layer Norm 2\nL7_E5\nGPU:26\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer7_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 28)" style="rounded,filled"
				layer7_expert6_route [label="Expert Routing\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert6_gate [label="Expert Gate\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert6_a2a [label="All-to-All Comm\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert6_attn_split [label="Attention Head Split\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert6_q_proj [label="Q Projection (Col-Par)\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_k_proj [label="K Projection (Col-Par)\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_v_proj [label="V Projection (Col-Par)\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_attn [label="Multi-Head Attention\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_attn_out [label="Attention Output (Row-Par)\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_attn_ar [label="Attention All-Reduce\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert6_attn_res [label="Attention + Residual\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert6_ln1 [label="Layer Norm 1\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_gelu [label="GELU Activation\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert6_mlp_ar [label="MLP All-Reduce\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert6_mlp_res [label="MLP + Residual\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert6_ln2 [label="Layer Norm 2\nL7_E6\nGPU:28\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer7_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 30)" style="rounded,filled"
				layer7_expert7_route [label="Expert Routing\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer7_expert7_gate [label="Expert Gate\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert7_a2a [label="All-to-All Comm\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert7_attn_split [label="Attention Head Split\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert7_q_proj [label="Q Projection (Col-Par)\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_k_proj [label="K Projection (Col-Par)\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_v_proj [label="V Projection (Col-Par)\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_attn [label="Multi-Head Attention\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_attn_out [label="Attention Output (Row-Par)\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_attn_ar [label="Attention All-Reduce\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert7_attn_res [label="Attention + Residual\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert7_ln1 [label="Layer Norm 1\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_gelu [label="GELU Activation\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer7_expert7_mlp_ar [label="MLP All-Reduce\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer7_expert7_mlp_res [label="MLP + Residual\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer7_expert7_ln2 [label="Layer Norm 2\nL7_E7\nGPU:30\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
	}
	subgraph cluster_pipeline_stage_2 {
		fillcolor=orange fontname="Arial Bold" fontsize=10 label="Pipeline Stage 2 (Layers 8-11)" style="rounded,filled"
		subgraph cluster_layer_8 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 8" style="rounded,filled"
			subgraph cluster_layer8_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 32)" style="rounded,filled"
				layer8_expert0_route [label="Expert Routing\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert0_gate [label="Expert Gate\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert0_a2a [label="All-to-All Comm\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert0_attn_split [label="Attention Head Split\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert0_q_proj [label="Q Projection (Col-Par)\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_k_proj [label="K Projection (Col-Par)\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_v_proj [label="V Projection (Col-Par)\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_attn [label="Multi-Head Attention\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_attn_out [label="Attention Output (Row-Par)\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_attn_ar [label="Attention All-Reduce\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert0_attn_res [label="Attention + Residual\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert0_ln1 [label="Layer Norm 1\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_gelu [label="GELU Activation\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert0_mlp_ar [label="MLP All-Reduce\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert0_mlp_res [label="MLP + Residual\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert0_ln2 [label="Layer Norm 2\nL8_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer8_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 34)" style="rounded,filled"
				layer8_expert1_route [label="Expert Routing\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert1_gate [label="Expert Gate\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert1_a2a [label="All-to-All Comm\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert1_attn_split [label="Attention Head Split\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert1_q_proj [label="Q Projection (Col-Par)\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_k_proj [label="K Projection (Col-Par)\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_v_proj [label="V Projection (Col-Par)\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_attn [label="Multi-Head Attention\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_attn_out [label="Attention Output (Row-Par)\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_attn_ar [label="Attention All-Reduce\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert1_attn_res [label="Attention + Residual\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert1_ln1 [label="Layer Norm 1\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_gelu [label="GELU Activation\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert1_mlp_ar [label="MLP All-Reduce\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert1_mlp_res [label="MLP + Residual\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert1_ln2 [label="Layer Norm 2\nL8_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer8_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 36)" style="rounded,filled"
				layer8_expert2_route [label="Expert Routing\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert2_gate [label="Expert Gate\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert2_a2a [label="All-to-All Comm\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert2_attn_split [label="Attention Head Split\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert2_q_proj [label="Q Projection (Col-Par)\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_k_proj [label="K Projection (Col-Par)\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_v_proj [label="V Projection (Col-Par)\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_attn [label="Multi-Head Attention\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_attn_out [label="Attention Output (Row-Par)\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_attn_ar [label="Attention All-Reduce\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert2_attn_res [label="Attention + Residual\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert2_ln1 [label="Layer Norm 1\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_gelu [label="GELU Activation\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert2_mlp_ar [label="MLP All-Reduce\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert2_mlp_res [label="MLP + Residual\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert2_ln2 [label="Layer Norm 2\nL8_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer8_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 38)" style="rounded,filled"
				layer8_expert3_route [label="Expert Routing\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert3_gate [label="Expert Gate\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert3_a2a [label="All-to-All Comm\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert3_attn_split [label="Attention Head Split\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert3_q_proj [label="Q Projection (Col-Par)\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_k_proj [label="K Projection (Col-Par)\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_v_proj [label="V Projection (Col-Par)\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_attn [label="Multi-Head Attention\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_attn_out [label="Attention Output (Row-Par)\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_attn_ar [label="Attention All-Reduce\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert3_attn_res [label="Attention + Residual\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert3_ln1 [label="Layer Norm 1\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_gelu [label="GELU Activation\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert3_mlp_ar [label="MLP All-Reduce\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert3_mlp_res [label="MLP + Residual\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert3_ln2 [label="Layer Norm 2\nL8_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer8_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 40)" style="rounded,filled"
				layer8_expert4_route [label="Expert Routing\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert4_gate [label="Expert Gate\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert4_a2a [label="All-to-All Comm\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert4_attn_split [label="Attention Head Split\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert4_q_proj [label="Q Projection (Col-Par)\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_k_proj [label="K Projection (Col-Par)\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_v_proj [label="V Projection (Col-Par)\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_attn [label="Multi-Head Attention\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_attn_out [label="Attention Output (Row-Par)\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_attn_ar [label="Attention All-Reduce\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert4_attn_res [label="Attention + Residual\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert4_ln1 [label="Layer Norm 1\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_gelu [label="GELU Activation\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert4_mlp_ar [label="MLP All-Reduce\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert4_mlp_res [label="MLP + Residual\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert4_ln2 [label="Layer Norm 2\nL8_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer8_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 42)" style="rounded,filled"
				layer8_expert5_route [label="Expert Routing\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert5_gate [label="Expert Gate\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert5_a2a [label="All-to-All Comm\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert5_attn_split [label="Attention Head Split\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert5_q_proj [label="Q Projection (Col-Par)\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_k_proj [label="K Projection (Col-Par)\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_v_proj [label="V Projection (Col-Par)\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_attn [label="Multi-Head Attention\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_attn_out [label="Attention Output (Row-Par)\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_attn_ar [label="Attention All-Reduce\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert5_attn_res [label="Attention + Residual\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert5_ln1 [label="Layer Norm 1\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_gelu [label="GELU Activation\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert5_mlp_ar [label="MLP All-Reduce\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert5_mlp_res [label="MLP + Residual\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert5_ln2 [label="Layer Norm 2\nL8_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer8_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 44)" style="rounded,filled"
				layer8_expert6_route [label="Expert Routing\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert6_gate [label="Expert Gate\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert6_a2a [label="All-to-All Comm\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert6_attn_split [label="Attention Head Split\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert6_q_proj [label="Q Projection (Col-Par)\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_k_proj [label="K Projection (Col-Par)\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_v_proj [label="V Projection (Col-Par)\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_attn [label="Multi-Head Attention\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_attn_out [label="Attention Output (Row-Par)\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_attn_ar [label="Attention All-Reduce\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert6_attn_res [label="Attention + Residual\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert6_ln1 [label="Layer Norm 1\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_gelu [label="GELU Activation\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert6_mlp_ar [label="MLP All-Reduce\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert6_mlp_res [label="MLP + Residual\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert6_ln2 [label="Layer Norm 2\nL8_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer8_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 46)" style="rounded,filled"
				layer8_expert7_route [label="Expert Routing\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer8_expert7_gate [label="Expert Gate\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert7_a2a [label="All-to-All Comm\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert7_attn_split [label="Attention Head Split\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert7_q_proj [label="Q Projection (Col-Par)\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_k_proj [label="K Projection (Col-Par)\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_v_proj [label="V Projection (Col-Par)\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_attn [label="Multi-Head Attention\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_attn_out [label="Attention Output (Row-Par)\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_attn_ar [label="Attention All-Reduce\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert7_attn_res [label="Attention + Residual\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert7_ln1 [label="Layer Norm 1\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_gelu [label="GELU Activation\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer8_expert7_mlp_ar [label="MLP All-Reduce\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer8_expert7_mlp_res [label="MLP + Residual\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer8_expert7_ln2 [label="Layer Norm 2\nL8_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_9 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 9" style="rounded,filled"
			subgraph cluster_layer9_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 32)" style="rounded,filled"
				layer9_expert0_route [label="Expert Routing\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert0_gate [label="Expert Gate\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert0_a2a [label="All-to-All Comm\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert0_attn_split [label="Attention Head Split\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert0_q_proj [label="Q Projection (Col-Par)\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_k_proj [label="K Projection (Col-Par)\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_v_proj [label="V Projection (Col-Par)\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_attn [label="Multi-Head Attention\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_attn_out [label="Attention Output (Row-Par)\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_attn_ar [label="Attention All-Reduce\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert0_attn_res [label="Attention + Residual\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert0_ln1 [label="Layer Norm 1\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_gelu [label="GELU Activation\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert0_mlp_ar [label="MLP All-Reduce\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert0_mlp_res [label="MLP + Residual\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert0_ln2 [label="Layer Norm 2\nL9_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer9_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 34)" style="rounded,filled"
				layer9_expert1_route [label="Expert Routing\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert1_gate [label="Expert Gate\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert1_a2a [label="All-to-All Comm\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert1_attn_split [label="Attention Head Split\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert1_q_proj [label="Q Projection (Col-Par)\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_k_proj [label="K Projection (Col-Par)\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_v_proj [label="V Projection (Col-Par)\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_attn [label="Multi-Head Attention\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_attn_out [label="Attention Output (Row-Par)\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_attn_ar [label="Attention All-Reduce\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert1_attn_res [label="Attention + Residual\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert1_ln1 [label="Layer Norm 1\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_gelu [label="GELU Activation\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert1_mlp_ar [label="MLP All-Reduce\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert1_mlp_res [label="MLP + Residual\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert1_ln2 [label="Layer Norm 2\nL9_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer9_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 36)" style="rounded,filled"
				layer9_expert2_route [label="Expert Routing\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert2_gate [label="Expert Gate\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert2_a2a [label="All-to-All Comm\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert2_attn_split [label="Attention Head Split\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert2_q_proj [label="Q Projection (Col-Par)\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_k_proj [label="K Projection (Col-Par)\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_v_proj [label="V Projection (Col-Par)\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_attn [label="Multi-Head Attention\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_attn_out [label="Attention Output (Row-Par)\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_attn_ar [label="Attention All-Reduce\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert2_attn_res [label="Attention + Residual\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert2_ln1 [label="Layer Norm 1\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_gelu [label="GELU Activation\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert2_mlp_ar [label="MLP All-Reduce\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert2_mlp_res [label="MLP + Residual\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert2_ln2 [label="Layer Norm 2\nL9_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer9_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 38)" style="rounded,filled"
				layer9_expert3_route [label="Expert Routing\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert3_gate [label="Expert Gate\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert3_a2a [label="All-to-All Comm\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert3_attn_split [label="Attention Head Split\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert3_q_proj [label="Q Projection (Col-Par)\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_k_proj [label="K Projection (Col-Par)\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_v_proj [label="V Projection (Col-Par)\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_attn [label="Multi-Head Attention\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_attn_out [label="Attention Output (Row-Par)\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_attn_ar [label="Attention All-Reduce\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert3_attn_res [label="Attention + Residual\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert3_ln1 [label="Layer Norm 1\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_gelu [label="GELU Activation\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert3_mlp_ar [label="MLP All-Reduce\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert3_mlp_res [label="MLP + Residual\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert3_ln2 [label="Layer Norm 2\nL9_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer9_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 40)" style="rounded,filled"
				layer9_expert4_route [label="Expert Routing\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert4_gate [label="Expert Gate\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert4_a2a [label="All-to-All Comm\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert4_attn_split [label="Attention Head Split\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert4_q_proj [label="Q Projection (Col-Par)\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_k_proj [label="K Projection (Col-Par)\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_v_proj [label="V Projection (Col-Par)\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_attn [label="Multi-Head Attention\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_attn_out [label="Attention Output (Row-Par)\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_attn_ar [label="Attention All-Reduce\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert4_attn_res [label="Attention + Residual\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert4_ln1 [label="Layer Norm 1\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_gelu [label="GELU Activation\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert4_mlp_ar [label="MLP All-Reduce\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert4_mlp_res [label="MLP + Residual\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert4_ln2 [label="Layer Norm 2\nL9_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer9_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 42)" style="rounded,filled"
				layer9_expert5_route [label="Expert Routing\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert5_gate [label="Expert Gate\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert5_a2a [label="All-to-All Comm\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert5_attn_split [label="Attention Head Split\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert5_q_proj [label="Q Projection (Col-Par)\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_k_proj [label="K Projection (Col-Par)\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_v_proj [label="V Projection (Col-Par)\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_attn [label="Multi-Head Attention\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_attn_out [label="Attention Output (Row-Par)\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_attn_ar [label="Attention All-Reduce\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert5_attn_res [label="Attention + Residual\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert5_ln1 [label="Layer Norm 1\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_gelu [label="GELU Activation\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert5_mlp_ar [label="MLP All-Reduce\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert5_mlp_res [label="MLP + Residual\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert5_ln2 [label="Layer Norm 2\nL9_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer9_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 44)" style="rounded,filled"
				layer9_expert6_route [label="Expert Routing\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert6_gate [label="Expert Gate\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert6_a2a [label="All-to-All Comm\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert6_attn_split [label="Attention Head Split\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert6_q_proj [label="Q Projection (Col-Par)\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_k_proj [label="K Projection (Col-Par)\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_v_proj [label="V Projection (Col-Par)\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_attn [label="Multi-Head Attention\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_attn_out [label="Attention Output (Row-Par)\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_attn_ar [label="Attention All-Reduce\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert6_attn_res [label="Attention + Residual\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert6_ln1 [label="Layer Norm 1\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_gelu [label="GELU Activation\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert6_mlp_ar [label="MLP All-Reduce\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert6_mlp_res [label="MLP + Residual\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert6_ln2 [label="Layer Norm 2\nL9_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer9_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 46)" style="rounded,filled"
				layer9_expert7_route [label="Expert Routing\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer9_expert7_gate [label="Expert Gate\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert7_a2a [label="All-to-All Comm\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert7_attn_split [label="Attention Head Split\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert7_q_proj [label="Q Projection (Col-Par)\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_k_proj [label="K Projection (Col-Par)\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_v_proj [label="V Projection (Col-Par)\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_attn [label="Multi-Head Attention\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_attn_out [label="Attention Output (Row-Par)\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_attn_ar [label="Attention All-Reduce\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert7_attn_res [label="Attention + Residual\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert7_ln1 [label="Layer Norm 1\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_gelu [label="GELU Activation\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer9_expert7_mlp_ar [label="MLP All-Reduce\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer9_expert7_mlp_res [label="MLP + Residual\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer9_expert7_ln2 [label="Layer Norm 2\nL9_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_10 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 10" style="rounded,filled"
			subgraph cluster_layer10_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 32)" style="rounded,filled"
				layer10_expert0_route [label="Expert Routing\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert0_gate [label="Expert Gate\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert0_a2a [label="All-to-All Comm\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert0_attn_split [label="Attention Head Split\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert0_q_proj [label="Q Projection (Col-Par)\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_k_proj [label="K Projection (Col-Par)\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_v_proj [label="V Projection (Col-Par)\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_attn [label="Multi-Head Attention\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_attn_out [label="Attention Output (Row-Par)\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_attn_ar [label="Attention All-Reduce\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert0_attn_res [label="Attention + Residual\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert0_ln1 [label="Layer Norm 1\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_gelu [label="GELU Activation\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert0_mlp_ar [label="MLP All-Reduce\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert0_mlp_res [label="MLP + Residual\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert0_ln2 [label="Layer Norm 2\nL10_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer10_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 34)" style="rounded,filled"
				layer10_expert1_route [label="Expert Routing\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert1_gate [label="Expert Gate\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert1_a2a [label="All-to-All Comm\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert1_attn_split [label="Attention Head Split\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert1_q_proj [label="Q Projection (Col-Par)\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_k_proj [label="K Projection (Col-Par)\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_v_proj [label="V Projection (Col-Par)\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_attn [label="Multi-Head Attention\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_attn_out [label="Attention Output (Row-Par)\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_attn_ar [label="Attention All-Reduce\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert1_attn_res [label="Attention + Residual\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert1_ln1 [label="Layer Norm 1\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_gelu [label="GELU Activation\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert1_mlp_ar [label="MLP All-Reduce\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert1_mlp_res [label="MLP + Residual\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert1_ln2 [label="Layer Norm 2\nL10_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer10_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 36)" style="rounded,filled"
				layer10_expert2_route [label="Expert Routing\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert2_gate [label="Expert Gate\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert2_a2a [label="All-to-All Comm\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert2_attn_split [label="Attention Head Split\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert2_q_proj [label="Q Projection (Col-Par)\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_k_proj [label="K Projection (Col-Par)\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_v_proj [label="V Projection (Col-Par)\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_attn [label="Multi-Head Attention\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_attn_out [label="Attention Output (Row-Par)\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_attn_ar [label="Attention All-Reduce\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert2_attn_res [label="Attention + Residual\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert2_ln1 [label="Layer Norm 1\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_gelu [label="GELU Activation\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert2_mlp_ar [label="MLP All-Reduce\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert2_mlp_res [label="MLP + Residual\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert2_ln2 [label="Layer Norm 2\nL10_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer10_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 38)" style="rounded,filled"
				layer10_expert3_route [label="Expert Routing\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert3_gate [label="Expert Gate\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert3_a2a [label="All-to-All Comm\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert3_attn_split [label="Attention Head Split\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert3_q_proj [label="Q Projection (Col-Par)\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_k_proj [label="K Projection (Col-Par)\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_v_proj [label="V Projection (Col-Par)\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_attn [label="Multi-Head Attention\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_attn_out [label="Attention Output (Row-Par)\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_attn_ar [label="Attention All-Reduce\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert3_attn_res [label="Attention + Residual\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert3_ln1 [label="Layer Norm 1\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_gelu [label="GELU Activation\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert3_mlp_ar [label="MLP All-Reduce\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert3_mlp_res [label="MLP + Residual\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert3_ln2 [label="Layer Norm 2\nL10_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer10_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 40)" style="rounded,filled"
				layer10_expert4_route [label="Expert Routing\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert4_gate [label="Expert Gate\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert4_a2a [label="All-to-All Comm\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert4_attn_split [label="Attention Head Split\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert4_q_proj [label="Q Projection (Col-Par)\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_k_proj [label="K Projection (Col-Par)\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_v_proj [label="V Projection (Col-Par)\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_attn [label="Multi-Head Attention\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_attn_out [label="Attention Output (Row-Par)\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_attn_ar [label="Attention All-Reduce\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert4_attn_res [label="Attention + Residual\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert4_ln1 [label="Layer Norm 1\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_gelu [label="GELU Activation\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert4_mlp_ar [label="MLP All-Reduce\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert4_mlp_res [label="MLP + Residual\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert4_ln2 [label="Layer Norm 2\nL10_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer10_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 42)" style="rounded,filled"
				layer10_expert5_route [label="Expert Routing\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert5_gate [label="Expert Gate\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert5_a2a [label="All-to-All Comm\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert5_attn_split [label="Attention Head Split\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert5_q_proj [label="Q Projection (Col-Par)\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_k_proj [label="K Projection (Col-Par)\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_v_proj [label="V Projection (Col-Par)\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_attn [label="Multi-Head Attention\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_attn_out [label="Attention Output (Row-Par)\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_attn_ar [label="Attention All-Reduce\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert5_attn_res [label="Attention + Residual\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert5_ln1 [label="Layer Norm 1\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_gelu [label="GELU Activation\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert5_mlp_ar [label="MLP All-Reduce\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert5_mlp_res [label="MLP + Residual\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert5_ln2 [label="Layer Norm 2\nL10_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer10_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 44)" style="rounded,filled"
				layer10_expert6_route [label="Expert Routing\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert6_gate [label="Expert Gate\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert6_a2a [label="All-to-All Comm\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert6_attn_split [label="Attention Head Split\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert6_q_proj [label="Q Projection (Col-Par)\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_k_proj [label="K Projection (Col-Par)\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_v_proj [label="V Projection (Col-Par)\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_attn [label="Multi-Head Attention\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_attn_out [label="Attention Output (Row-Par)\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_attn_ar [label="Attention All-Reduce\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert6_attn_res [label="Attention + Residual\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert6_ln1 [label="Layer Norm 1\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_gelu [label="GELU Activation\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert6_mlp_ar [label="MLP All-Reduce\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert6_mlp_res [label="MLP + Residual\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert6_ln2 [label="Layer Norm 2\nL10_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer10_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 46)" style="rounded,filled"
				layer10_expert7_route [label="Expert Routing\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer10_expert7_gate [label="Expert Gate\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert7_a2a [label="All-to-All Comm\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert7_attn_split [label="Attention Head Split\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert7_q_proj [label="Q Projection (Col-Par)\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_k_proj [label="K Projection (Col-Par)\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_v_proj [label="V Projection (Col-Par)\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_attn [label="Multi-Head Attention\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_attn_out [label="Attention Output (Row-Par)\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_attn_ar [label="Attention All-Reduce\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert7_attn_res [label="Attention + Residual\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert7_ln1 [label="Layer Norm 1\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_gelu [label="GELU Activation\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer10_expert7_mlp_ar [label="MLP All-Reduce\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer10_expert7_mlp_res [label="MLP + Residual\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer10_expert7_ln2 [label="Layer Norm 2\nL10_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_11 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 11" style="rounded,filled"
			subgraph cluster_layer11_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 32)" style="rounded,filled"
				layer11_expert0_route [label="Expert Routing\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert0_gate [label="Expert Gate\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert0_a2a [label="All-to-All Comm\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert0_attn_split [label="Attention Head Split\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert0_q_proj [label="Q Projection (Col-Par)\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_k_proj [label="K Projection (Col-Par)\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_v_proj [label="V Projection (Col-Par)\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_attn [label="Multi-Head Attention\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_attn_out [label="Attention Output (Row-Par)\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_attn_ar [label="Attention All-Reduce\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert0_attn_res [label="Attention + Residual\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert0_ln1 [label="Layer Norm 1\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_gelu [label="GELU Activation\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert0_mlp_ar [label="MLP All-Reduce\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert0_mlp_res [label="MLP + Residual\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert0_ln2 [label="Layer Norm 2\nL11_E0\nGPU:32\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer11_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 34)" style="rounded,filled"
				layer11_expert1_route [label="Expert Routing\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert1_gate [label="Expert Gate\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert1_a2a [label="All-to-All Comm\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert1_attn_split [label="Attention Head Split\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert1_q_proj [label="Q Projection (Col-Par)\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_k_proj [label="K Projection (Col-Par)\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_v_proj [label="V Projection (Col-Par)\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_attn [label="Multi-Head Attention\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_attn_out [label="Attention Output (Row-Par)\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_attn_ar [label="Attention All-Reduce\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert1_attn_res [label="Attention + Residual\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert1_ln1 [label="Layer Norm 1\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_gelu [label="GELU Activation\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert1_mlp_ar [label="MLP All-Reduce\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert1_mlp_res [label="MLP + Residual\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert1_ln2 [label="Layer Norm 2\nL11_E1\nGPU:34\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer11_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 36)" style="rounded,filled"
				layer11_expert2_route [label="Expert Routing\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert2_gate [label="Expert Gate\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert2_a2a [label="All-to-All Comm\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert2_attn_split [label="Attention Head Split\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert2_q_proj [label="Q Projection (Col-Par)\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_k_proj [label="K Projection (Col-Par)\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_v_proj [label="V Projection (Col-Par)\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_attn [label="Multi-Head Attention\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_attn_out [label="Attention Output (Row-Par)\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_attn_ar [label="Attention All-Reduce\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert2_attn_res [label="Attention + Residual\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert2_ln1 [label="Layer Norm 1\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_gelu [label="GELU Activation\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert2_mlp_ar [label="MLP All-Reduce\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert2_mlp_res [label="MLP + Residual\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert2_ln2 [label="Layer Norm 2\nL11_E2\nGPU:36\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer11_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 38)" style="rounded,filled"
				layer11_expert3_route [label="Expert Routing\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert3_gate [label="Expert Gate\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert3_a2a [label="All-to-All Comm\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert3_attn_split [label="Attention Head Split\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert3_q_proj [label="Q Projection (Col-Par)\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_k_proj [label="K Projection (Col-Par)\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_v_proj [label="V Projection (Col-Par)\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_attn [label="Multi-Head Attention\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_attn_out [label="Attention Output (Row-Par)\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_attn_ar [label="Attention All-Reduce\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert3_attn_res [label="Attention + Residual\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert3_ln1 [label="Layer Norm 1\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_gelu [label="GELU Activation\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert3_mlp_ar [label="MLP All-Reduce\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert3_mlp_res [label="MLP + Residual\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert3_ln2 [label="Layer Norm 2\nL11_E3\nGPU:38\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer11_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 40)" style="rounded,filled"
				layer11_expert4_route [label="Expert Routing\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert4_gate [label="Expert Gate\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert4_a2a [label="All-to-All Comm\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert4_attn_split [label="Attention Head Split\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert4_q_proj [label="Q Projection (Col-Par)\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_k_proj [label="K Projection (Col-Par)\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_v_proj [label="V Projection (Col-Par)\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_attn [label="Multi-Head Attention\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_attn_out [label="Attention Output (Row-Par)\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_attn_ar [label="Attention All-Reduce\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert4_attn_res [label="Attention + Residual\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert4_ln1 [label="Layer Norm 1\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_gelu [label="GELU Activation\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert4_mlp_ar [label="MLP All-Reduce\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert4_mlp_res [label="MLP + Residual\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert4_ln2 [label="Layer Norm 2\nL11_E4\nGPU:40\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer11_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 42)" style="rounded,filled"
				layer11_expert5_route [label="Expert Routing\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert5_gate [label="Expert Gate\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert5_a2a [label="All-to-All Comm\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert5_attn_split [label="Attention Head Split\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert5_q_proj [label="Q Projection (Col-Par)\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_k_proj [label="K Projection (Col-Par)\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_v_proj [label="V Projection (Col-Par)\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_attn [label="Multi-Head Attention\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_attn_out [label="Attention Output (Row-Par)\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_attn_ar [label="Attention All-Reduce\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert5_attn_res [label="Attention + Residual\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert5_ln1 [label="Layer Norm 1\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_gelu [label="GELU Activation\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert5_mlp_ar [label="MLP All-Reduce\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert5_mlp_res [label="MLP + Residual\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert5_ln2 [label="Layer Norm 2\nL11_E5\nGPU:42\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer11_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 44)" style="rounded,filled"
				layer11_expert6_route [label="Expert Routing\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert6_gate [label="Expert Gate\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert6_a2a [label="All-to-All Comm\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert6_attn_split [label="Attention Head Split\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert6_q_proj [label="Q Projection (Col-Par)\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_k_proj [label="K Projection (Col-Par)\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_v_proj [label="V Projection (Col-Par)\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_attn [label="Multi-Head Attention\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_attn_out [label="Attention Output (Row-Par)\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_attn_ar [label="Attention All-Reduce\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert6_attn_res [label="Attention + Residual\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert6_ln1 [label="Layer Norm 1\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_gelu [label="GELU Activation\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert6_mlp_ar [label="MLP All-Reduce\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert6_mlp_res [label="MLP + Residual\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert6_ln2 [label="Layer Norm 2\nL11_E6\nGPU:44\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer11_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 46)" style="rounded,filled"
				layer11_expert7_route [label="Expert Routing\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer11_expert7_gate [label="Expert Gate\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert7_a2a [label="All-to-All Comm\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert7_attn_split [label="Attention Head Split\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert7_q_proj [label="Q Projection (Col-Par)\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_k_proj [label="K Projection (Col-Par)\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_v_proj [label="V Projection (Col-Par)\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_attn [label="Multi-Head Attention\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_attn_out [label="Attention Output (Row-Par)\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_attn_ar [label="Attention All-Reduce\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert7_attn_res [label="Attention + Residual\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert7_ln1 [label="Layer Norm 1\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_gelu [label="GELU Activation\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer11_expert7_mlp_ar [label="MLP All-Reduce\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer11_expert7_mlp_res [label="MLP + Residual\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer11_expert7_ln2 [label="Layer Norm 2\nL11_E7\nGPU:46\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
	}
	subgraph cluster_pipeline_stage_3 {
		fillcolor=orange fontname="Arial Bold" fontsize=10 label="Pipeline Stage 3 (Layers 12-15)" style="rounded,filled"
		subgraph cluster_layer_12 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 12" style="rounded,filled"
			subgraph cluster_layer12_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 48)" style="rounded,filled"
				layer12_expert0_route [label="Expert Routing\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert0_gate [label="Expert Gate\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert0_a2a [label="All-to-All Comm\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert0_attn_split [label="Attention Head Split\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert0_q_proj [label="Q Projection (Col-Par)\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_k_proj [label="K Projection (Col-Par)\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_v_proj [label="V Projection (Col-Par)\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_attn [label="Multi-Head Attention\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_attn_out [label="Attention Output (Row-Par)\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_attn_ar [label="Attention All-Reduce\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert0_attn_res [label="Attention + Residual\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert0_ln1 [label="Layer Norm 1\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_gelu [label="GELU Activation\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert0_mlp_ar [label="MLP All-Reduce\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert0_mlp_res [label="MLP + Residual\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert0_ln2 [label="Layer Norm 2\nL12_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer12_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 50)" style="rounded,filled"
				layer12_expert1_route [label="Expert Routing\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert1_gate [label="Expert Gate\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert1_a2a [label="All-to-All Comm\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert1_attn_split [label="Attention Head Split\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert1_q_proj [label="Q Projection (Col-Par)\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_k_proj [label="K Projection (Col-Par)\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_v_proj [label="V Projection (Col-Par)\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_attn [label="Multi-Head Attention\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_attn_out [label="Attention Output (Row-Par)\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_attn_ar [label="Attention All-Reduce\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert1_attn_res [label="Attention + Residual\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert1_ln1 [label="Layer Norm 1\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_gelu [label="GELU Activation\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert1_mlp_ar [label="MLP All-Reduce\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert1_mlp_res [label="MLP + Residual\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert1_ln2 [label="Layer Norm 2\nL12_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer12_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 52)" style="rounded,filled"
				layer12_expert2_route [label="Expert Routing\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert2_gate [label="Expert Gate\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert2_a2a [label="All-to-All Comm\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert2_attn_split [label="Attention Head Split\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert2_q_proj [label="Q Projection (Col-Par)\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_k_proj [label="K Projection (Col-Par)\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_v_proj [label="V Projection (Col-Par)\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_attn [label="Multi-Head Attention\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_attn_out [label="Attention Output (Row-Par)\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_attn_ar [label="Attention All-Reduce\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert2_attn_res [label="Attention + Residual\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert2_ln1 [label="Layer Norm 1\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_gelu [label="GELU Activation\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert2_mlp_ar [label="MLP All-Reduce\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert2_mlp_res [label="MLP + Residual\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert2_ln2 [label="Layer Norm 2\nL12_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer12_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 54)" style="rounded,filled"
				layer12_expert3_route [label="Expert Routing\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert3_gate [label="Expert Gate\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert3_a2a [label="All-to-All Comm\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert3_attn_split [label="Attention Head Split\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert3_q_proj [label="Q Projection (Col-Par)\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_k_proj [label="K Projection (Col-Par)\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_v_proj [label="V Projection (Col-Par)\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_attn [label="Multi-Head Attention\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_attn_out [label="Attention Output (Row-Par)\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_attn_ar [label="Attention All-Reduce\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert3_attn_res [label="Attention + Residual\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert3_ln1 [label="Layer Norm 1\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_gelu [label="GELU Activation\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert3_mlp_ar [label="MLP All-Reduce\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert3_mlp_res [label="MLP + Residual\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert3_ln2 [label="Layer Norm 2\nL12_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer12_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 56)" style="rounded,filled"
				layer12_expert4_route [label="Expert Routing\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert4_gate [label="Expert Gate\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert4_a2a [label="All-to-All Comm\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert4_attn_split [label="Attention Head Split\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert4_q_proj [label="Q Projection (Col-Par)\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_k_proj [label="K Projection (Col-Par)\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_v_proj [label="V Projection (Col-Par)\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_attn [label="Multi-Head Attention\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_attn_out [label="Attention Output (Row-Par)\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_attn_ar [label="Attention All-Reduce\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert4_attn_res [label="Attention + Residual\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert4_ln1 [label="Layer Norm 1\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_gelu [label="GELU Activation\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert4_mlp_ar [label="MLP All-Reduce\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert4_mlp_res [label="MLP + Residual\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert4_ln2 [label="Layer Norm 2\nL12_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer12_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 58)" style="rounded,filled"
				layer12_expert5_route [label="Expert Routing\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert5_gate [label="Expert Gate\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert5_a2a [label="All-to-All Comm\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert5_attn_split [label="Attention Head Split\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert5_q_proj [label="Q Projection (Col-Par)\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_k_proj [label="K Projection (Col-Par)\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_v_proj [label="V Projection (Col-Par)\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_attn [label="Multi-Head Attention\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_attn_out [label="Attention Output (Row-Par)\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_attn_ar [label="Attention All-Reduce\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert5_attn_res [label="Attention + Residual\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert5_ln1 [label="Layer Norm 1\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_gelu [label="GELU Activation\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert5_mlp_ar [label="MLP All-Reduce\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert5_mlp_res [label="MLP + Residual\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert5_ln2 [label="Layer Norm 2\nL12_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer12_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 60)" style="rounded,filled"
				layer12_expert6_route [label="Expert Routing\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert6_gate [label="Expert Gate\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert6_a2a [label="All-to-All Comm\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert6_attn_split [label="Attention Head Split\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert6_q_proj [label="Q Projection (Col-Par)\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_k_proj [label="K Projection (Col-Par)\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_v_proj [label="V Projection (Col-Par)\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_attn [label="Multi-Head Attention\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_attn_out [label="Attention Output (Row-Par)\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_attn_ar [label="Attention All-Reduce\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert6_attn_res [label="Attention + Residual\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert6_ln1 [label="Layer Norm 1\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_gelu [label="GELU Activation\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert6_mlp_ar [label="MLP All-Reduce\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert6_mlp_res [label="MLP + Residual\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert6_ln2 [label="Layer Norm 2\nL12_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer12_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 62)" style="rounded,filled"
				layer12_expert7_route [label="Expert Routing\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer12_expert7_gate [label="Expert Gate\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert7_a2a [label="All-to-All Comm\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert7_attn_split [label="Attention Head Split\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert7_q_proj [label="Q Projection (Col-Par)\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_k_proj [label="K Projection (Col-Par)\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_v_proj [label="V Projection (Col-Par)\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_attn [label="Multi-Head Attention\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_attn_out [label="Attention Output (Row-Par)\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_attn_ar [label="Attention All-Reduce\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert7_attn_res [label="Attention + Residual\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert7_ln1 [label="Layer Norm 1\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_gelu [label="GELU Activation\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer12_expert7_mlp_ar [label="MLP All-Reduce\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer12_expert7_mlp_res [label="MLP + Residual\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer12_expert7_ln2 [label="Layer Norm 2\nL12_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_13 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 13" style="rounded,filled"
			subgraph cluster_layer13_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 48)" style="rounded,filled"
				layer13_expert0_route [label="Expert Routing\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert0_gate [label="Expert Gate\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert0_a2a [label="All-to-All Comm\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert0_attn_split [label="Attention Head Split\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert0_q_proj [label="Q Projection (Col-Par)\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_k_proj [label="K Projection (Col-Par)\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_v_proj [label="V Projection (Col-Par)\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_attn [label="Multi-Head Attention\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_attn_out [label="Attention Output (Row-Par)\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_attn_ar [label="Attention All-Reduce\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert0_attn_res [label="Attention + Residual\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert0_ln1 [label="Layer Norm 1\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_gelu [label="GELU Activation\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert0_mlp_ar [label="MLP All-Reduce\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert0_mlp_res [label="MLP + Residual\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert0_ln2 [label="Layer Norm 2\nL13_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer13_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 50)" style="rounded,filled"
				layer13_expert1_route [label="Expert Routing\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert1_gate [label="Expert Gate\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert1_a2a [label="All-to-All Comm\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert1_attn_split [label="Attention Head Split\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert1_q_proj [label="Q Projection (Col-Par)\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_k_proj [label="K Projection (Col-Par)\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_v_proj [label="V Projection (Col-Par)\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_attn [label="Multi-Head Attention\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_attn_out [label="Attention Output (Row-Par)\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_attn_ar [label="Attention All-Reduce\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert1_attn_res [label="Attention + Residual\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert1_ln1 [label="Layer Norm 1\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_gelu [label="GELU Activation\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert1_mlp_ar [label="MLP All-Reduce\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert1_mlp_res [label="MLP + Residual\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert1_ln2 [label="Layer Norm 2\nL13_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer13_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 52)" style="rounded,filled"
				layer13_expert2_route [label="Expert Routing\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert2_gate [label="Expert Gate\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert2_a2a [label="All-to-All Comm\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert2_attn_split [label="Attention Head Split\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert2_q_proj [label="Q Projection (Col-Par)\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_k_proj [label="K Projection (Col-Par)\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_v_proj [label="V Projection (Col-Par)\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_attn [label="Multi-Head Attention\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_attn_out [label="Attention Output (Row-Par)\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_attn_ar [label="Attention All-Reduce\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert2_attn_res [label="Attention + Residual\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert2_ln1 [label="Layer Norm 1\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_gelu [label="GELU Activation\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert2_mlp_ar [label="MLP All-Reduce\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert2_mlp_res [label="MLP + Residual\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert2_ln2 [label="Layer Norm 2\nL13_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer13_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 54)" style="rounded,filled"
				layer13_expert3_route [label="Expert Routing\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert3_gate [label="Expert Gate\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert3_a2a [label="All-to-All Comm\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert3_attn_split [label="Attention Head Split\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert3_q_proj [label="Q Projection (Col-Par)\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_k_proj [label="K Projection (Col-Par)\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_v_proj [label="V Projection (Col-Par)\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_attn [label="Multi-Head Attention\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_attn_out [label="Attention Output (Row-Par)\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_attn_ar [label="Attention All-Reduce\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert3_attn_res [label="Attention + Residual\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert3_ln1 [label="Layer Norm 1\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_gelu [label="GELU Activation\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert3_mlp_ar [label="MLP All-Reduce\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert3_mlp_res [label="MLP + Residual\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert3_ln2 [label="Layer Norm 2\nL13_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer13_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 56)" style="rounded,filled"
				layer13_expert4_route [label="Expert Routing\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert4_gate [label="Expert Gate\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert4_a2a [label="All-to-All Comm\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert4_attn_split [label="Attention Head Split\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert4_q_proj [label="Q Projection (Col-Par)\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_k_proj [label="K Projection (Col-Par)\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_v_proj [label="V Projection (Col-Par)\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_attn [label="Multi-Head Attention\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_attn_out [label="Attention Output (Row-Par)\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_attn_ar [label="Attention All-Reduce\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert4_attn_res [label="Attention + Residual\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert4_ln1 [label="Layer Norm 1\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_gelu [label="GELU Activation\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert4_mlp_ar [label="MLP All-Reduce\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert4_mlp_res [label="MLP + Residual\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert4_ln2 [label="Layer Norm 2\nL13_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer13_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 58)" style="rounded,filled"
				layer13_expert5_route [label="Expert Routing\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert5_gate [label="Expert Gate\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert5_a2a [label="All-to-All Comm\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert5_attn_split [label="Attention Head Split\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert5_q_proj [label="Q Projection (Col-Par)\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_k_proj [label="K Projection (Col-Par)\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_v_proj [label="V Projection (Col-Par)\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_attn [label="Multi-Head Attention\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_attn_out [label="Attention Output (Row-Par)\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_attn_ar [label="Attention All-Reduce\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert5_attn_res [label="Attention + Residual\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert5_ln1 [label="Layer Norm 1\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_gelu [label="GELU Activation\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert5_mlp_ar [label="MLP All-Reduce\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert5_mlp_res [label="MLP + Residual\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert5_ln2 [label="Layer Norm 2\nL13_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer13_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 60)" style="rounded,filled"
				layer13_expert6_route [label="Expert Routing\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert6_gate [label="Expert Gate\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert6_a2a [label="All-to-All Comm\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert6_attn_split [label="Attention Head Split\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert6_q_proj [label="Q Projection (Col-Par)\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_k_proj [label="K Projection (Col-Par)\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_v_proj [label="V Projection (Col-Par)\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_attn [label="Multi-Head Attention\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_attn_out [label="Attention Output (Row-Par)\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_attn_ar [label="Attention All-Reduce\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert6_attn_res [label="Attention + Residual\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert6_ln1 [label="Layer Norm 1\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_gelu [label="GELU Activation\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert6_mlp_ar [label="MLP All-Reduce\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert6_mlp_res [label="MLP + Residual\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert6_ln2 [label="Layer Norm 2\nL13_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer13_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 62)" style="rounded,filled"
				layer13_expert7_route [label="Expert Routing\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer13_expert7_gate [label="Expert Gate\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert7_a2a [label="All-to-All Comm\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert7_attn_split [label="Attention Head Split\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert7_q_proj [label="Q Projection (Col-Par)\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_k_proj [label="K Projection (Col-Par)\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_v_proj [label="V Projection (Col-Par)\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_attn [label="Multi-Head Attention\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_attn_out [label="Attention Output (Row-Par)\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_attn_ar [label="Attention All-Reduce\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert7_attn_res [label="Attention + Residual\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert7_ln1 [label="Layer Norm 1\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_gelu [label="GELU Activation\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer13_expert7_mlp_ar [label="MLP All-Reduce\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer13_expert7_mlp_res [label="MLP + Residual\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer13_expert7_ln2 [label="Layer Norm 2\nL13_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_14 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 14" style="rounded,filled"
			subgraph cluster_layer14_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 48)" style="rounded,filled"
				layer14_expert0_route [label="Expert Routing\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert0_gate [label="Expert Gate\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert0_a2a [label="All-to-All Comm\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert0_attn_split [label="Attention Head Split\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert0_q_proj [label="Q Projection (Col-Par)\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_k_proj [label="K Projection (Col-Par)\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_v_proj [label="V Projection (Col-Par)\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_attn [label="Multi-Head Attention\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_attn_out [label="Attention Output (Row-Par)\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_attn_ar [label="Attention All-Reduce\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert0_attn_res [label="Attention + Residual\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert0_ln1 [label="Layer Norm 1\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_gelu [label="GELU Activation\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert0_mlp_ar [label="MLP All-Reduce\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert0_mlp_res [label="MLP + Residual\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert0_ln2 [label="Layer Norm 2\nL14_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer14_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 50)" style="rounded,filled"
				layer14_expert1_route [label="Expert Routing\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert1_gate [label="Expert Gate\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert1_a2a [label="All-to-All Comm\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert1_attn_split [label="Attention Head Split\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert1_q_proj [label="Q Projection (Col-Par)\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_k_proj [label="K Projection (Col-Par)\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_v_proj [label="V Projection (Col-Par)\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_attn [label="Multi-Head Attention\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_attn_out [label="Attention Output (Row-Par)\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_attn_ar [label="Attention All-Reduce\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert1_attn_res [label="Attention + Residual\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert1_ln1 [label="Layer Norm 1\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_gelu [label="GELU Activation\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert1_mlp_ar [label="MLP All-Reduce\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert1_mlp_res [label="MLP + Residual\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert1_ln2 [label="Layer Norm 2\nL14_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer14_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 52)" style="rounded,filled"
				layer14_expert2_route [label="Expert Routing\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert2_gate [label="Expert Gate\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert2_a2a [label="All-to-All Comm\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert2_attn_split [label="Attention Head Split\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert2_q_proj [label="Q Projection (Col-Par)\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_k_proj [label="K Projection (Col-Par)\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_v_proj [label="V Projection (Col-Par)\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_attn [label="Multi-Head Attention\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_attn_out [label="Attention Output (Row-Par)\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_attn_ar [label="Attention All-Reduce\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert2_attn_res [label="Attention + Residual\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert2_ln1 [label="Layer Norm 1\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_gelu [label="GELU Activation\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert2_mlp_ar [label="MLP All-Reduce\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert2_mlp_res [label="MLP + Residual\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert2_ln2 [label="Layer Norm 2\nL14_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer14_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 54)" style="rounded,filled"
				layer14_expert3_route [label="Expert Routing\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert3_gate [label="Expert Gate\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert3_a2a [label="All-to-All Comm\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert3_attn_split [label="Attention Head Split\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert3_q_proj [label="Q Projection (Col-Par)\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_k_proj [label="K Projection (Col-Par)\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_v_proj [label="V Projection (Col-Par)\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_attn [label="Multi-Head Attention\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_attn_out [label="Attention Output (Row-Par)\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_attn_ar [label="Attention All-Reduce\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert3_attn_res [label="Attention + Residual\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert3_ln1 [label="Layer Norm 1\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_gelu [label="GELU Activation\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert3_mlp_ar [label="MLP All-Reduce\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert3_mlp_res [label="MLP + Residual\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert3_ln2 [label="Layer Norm 2\nL14_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer14_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 56)" style="rounded,filled"
				layer14_expert4_route [label="Expert Routing\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert4_gate [label="Expert Gate\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert4_a2a [label="All-to-All Comm\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert4_attn_split [label="Attention Head Split\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert4_q_proj [label="Q Projection (Col-Par)\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_k_proj [label="K Projection (Col-Par)\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_v_proj [label="V Projection (Col-Par)\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_attn [label="Multi-Head Attention\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_attn_out [label="Attention Output (Row-Par)\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_attn_ar [label="Attention All-Reduce\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert4_attn_res [label="Attention + Residual\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert4_ln1 [label="Layer Norm 1\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_gelu [label="GELU Activation\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert4_mlp_ar [label="MLP All-Reduce\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert4_mlp_res [label="MLP + Residual\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert4_ln2 [label="Layer Norm 2\nL14_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer14_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 58)" style="rounded,filled"
				layer14_expert5_route [label="Expert Routing\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert5_gate [label="Expert Gate\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert5_a2a [label="All-to-All Comm\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert5_attn_split [label="Attention Head Split\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert5_q_proj [label="Q Projection (Col-Par)\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_k_proj [label="K Projection (Col-Par)\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_v_proj [label="V Projection (Col-Par)\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_attn [label="Multi-Head Attention\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_attn_out [label="Attention Output (Row-Par)\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_attn_ar [label="Attention All-Reduce\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert5_attn_res [label="Attention + Residual\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert5_ln1 [label="Layer Norm 1\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_gelu [label="GELU Activation\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert5_mlp_ar [label="MLP All-Reduce\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert5_mlp_res [label="MLP + Residual\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert5_ln2 [label="Layer Norm 2\nL14_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer14_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 60)" style="rounded,filled"
				layer14_expert6_route [label="Expert Routing\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert6_gate [label="Expert Gate\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert6_a2a [label="All-to-All Comm\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert6_attn_split [label="Attention Head Split\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert6_q_proj [label="Q Projection (Col-Par)\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_k_proj [label="K Projection (Col-Par)\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_v_proj [label="V Projection (Col-Par)\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_attn [label="Multi-Head Attention\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_attn_out [label="Attention Output (Row-Par)\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_attn_ar [label="Attention All-Reduce\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert6_attn_res [label="Attention + Residual\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert6_ln1 [label="Layer Norm 1\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_gelu [label="GELU Activation\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert6_mlp_ar [label="MLP All-Reduce\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert6_mlp_res [label="MLP + Residual\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert6_ln2 [label="Layer Norm 2\nL14_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer14_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 62)" style="rounded,filled"
				layer14_expert7_route [label="Expert Routing\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer14_expert7_gate [label="Expert Gate\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert7_a2a [label="All-to-All Comm\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert7_attn_split [label="Attention Head Split\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert7_q_proj [label="Q Projection (Col-Par)\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_k_proj [label="K Projection (Col-Par)\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_v_proj [label="V Projection (Col-Par)\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_attn [label="Multi-Head Attention\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_attn_out [label="Attention Output (Row-Par)\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_attn_ar [label="Attention All-Reduce\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert7_attn_res [label="Attention + Residual\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert7_ln1 [label="Layer Norm 1\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_gelu [label="GELU Activation\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer14_expert7_mlp_ar [label="MLP All-Reduce\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer14_expert7_mlp_res [label="MLP + Residual\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer14_expert7_ln2 [label="Layer Norm 2\nL14_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
		subgraph cluster_layer_15 {
			fillcolor=lightgray fontname="Arial Bold" label="Layer 15" style="rounded,filled"
			subgraph cluster_layer15_expert0 {
				fillcolor=lightcyan label="Expert Group 0 (GPU 48)" style="rounded,filled"
				layer15_expert0_route [label="Expert Routing\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert0_gate [label="Expert Gate\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert0_a2a [label="All-to-All Comm\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert0_attn_split [label="Attention Head Split\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert0_q_proj [label="Q Projection (Col-Par)\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_k_proj [label="K Projection (Col-Par)\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_v_proj [label="V Projection (Col-Par)\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_attn [label="Multi-Head Attention\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_attn_out [label="Attention Output (Row-Par)\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_attn_ar [label="Attention All-Reduce\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert0_attn_res [label="Attention + Residual\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert0_ln1 [label="Layer Norm 1\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_gelu [label="GELU Activation\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert0_mlp_ar [label="MLP All-Reduce\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert0_mlp_res [label="MLP + Residual\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert0_ln2 [label="Layer Norm 2\nL15_E0\nGPU:48\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer15_expert1 {
				fillcolor=lightcyan label="Expert Group 1 (GPU 50)" style="rounded,filled"
				layer15_expert1_route [label="Expert Routing\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert1_gate [label="Expert Gate\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert1_a2a [label="All-to-All Comm\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert1_attn_split [label="Attention Head Split\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert1_q_proj [label="Q Projection (Col-Par)\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_k_proj [label="K Projection (Col-Par)\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_v_proj [label="V Projection (Col-Par)\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_attn [label="Multi-Head Attention\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_attn_out [label="Attention Output (Row-Par)\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_attn_ar [label="Attention All-Reduce\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert1_attn_res [label="Attention + Residual\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert1_ln1 [label="Layer Norm 1\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_gelu [label="GELU Activation\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert1_mlp_ar [label="MLP All-Reduce\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert1_mlp_res [label="MLP + Residual\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert1_ln2 [label="Layer Norm 2\nL15_E1\nGPU:50\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer15_expert2 {
				fillcolor=lightcyan label="Expert Group 2 (GPU 52)" style="rounded,filled"
				layer15_expert2_route [label="Expert Routing\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert2_gate [label="Expert Gate\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert2_a2a [label="All-to-All Comm\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert2_attn_split [label="Attention Head Split\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert2_q_proj [label="Q Projection (Col-Par)\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_k_proj [label="K Projection (Col-Par)\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_v_proj [label="V Projection (Col-Par)\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_attn [label="Multi-Head Attention\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_attn_out [label="Attention Output (Row-Par)\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_attn_ar [label="Attention All-Reduce\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert2_attn_res [label="Attention + Residual\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert2_ln1 [label="Layer Norm 1\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_gelu [label="GELU Activation\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert2_mlp_ar [label="MLP All-Reduce\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert2_mlp_res [label="MLP + Residual\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert2_ln2 [label="Layer Norm 2\nL15_E2\nGPU:52\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer15_expert3 {
				fillcolor=lightcyan label="Expert Group 3 (GPU 54)" style="rounded,filled"
				layer15_expert3_route [label="Expert Routing\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert3_gate [label="Expert Gate\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert3_a2a [label="All-to-All Comm\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert3_attn_split [label="Attention Head Split\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert3_q_proj [label="Q Projection (Col-Par)\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_k_proj [label="K Projection (Col-Par)\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_v_proj [label="V Projection (Col-Par)\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_attn [label="Multi-Head Attention\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_attn_out [label="Attention Output (Row-Par)\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_attn_ar [label="Attention All-Reduce\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert3_attn_res [label="Attention + Residual\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert3_ln1 [label="Layer Norm 1\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_gelu [label="GELU Activation\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert3_mlp_ar [label="MLP All-Reduce\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert3_mlp_res [label="MLP + Residual\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert3_ln2 [label="Layer Norm 2\nL15_E3\nGPU:54\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer15_expert4 {
				fillcolor=lightcyan label="Expert Group 4 (GPU 56)" style="rounded,filled"
				layer15_expert4_route [label="Expert Routing\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert4_gate [label="Expert Gate\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert4_a2a [label="All-to-All Comm\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert4_attn_split [label="Attention Head Split\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert4_q_proj [label="Q Projection (Col-Par)\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_k_proj [label="K Projection (Col-Par)\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_v_proj [label="V Projection (Col-Par)\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_attn [label="Multi-Head Attention\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_attn_out [label="Attention Output (Row-Par)\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_attn_ar [label="Attention All-Reduce\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert4_attn_res [label="Attention + Residual\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert4_ln1 [label="Layer Norm 1\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_gelu [label="GELU Activation\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert4_mlp_ar [label="MLP All-Reduce\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert4_mlp_res [label="MLP + Residual\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert4_ln2 [label="Layer Norm 2\nL15_E4\nGPU:56\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer15_expert5 {
				fillcolor=lightcyan label="Expert Group 5 (GPU 58)" style="rounded,filled"
				layer15_expert5_route [label="Expert Routing\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert5_gate [label="Expert Gate\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert5_a2a [label="All-to-All Comm\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert5_attn_split [label="Attention Head Split\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert5_q_proj [label="Q Projection (Col-Par)\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_k_proj [label="K Projection (Col-Par)\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_v_proj [label="V Projection (Col-Par)\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_attn [label="Multi-Head Attention\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_attn_out [label="Attention Output (Row-Par)\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_attn_ar [label="Attention All-Reduce\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert5_attn_res [label="Attention + Residual\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert5_ln1 [label="Layer Norm 1\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_gelu [label="GELU Activation\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert5_mlp_ar [label="MLP All-Reduce\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert5_mlp_res [label="MLP + Residual\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert5_ln2 [label="Layer Norm 2\nL15_E5\nGPU:58\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer15_expert6 {
				fillcolor=lightcyan label="Expert Group 6 (GPU 60)" style="rounded,filled"
				layer15_expert6_route [label="Expert Routing\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert6_gate [label="Expert Gate\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert6_a2a [label="All-to-All Comm\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert6_attn_split [label="Attention Head Split\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert6_q_proj [label="Q Projection (Col-Par)\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_k_proj [label="K Projection (Col-Par)\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_v_proj [label="V Projection (Col-Par)\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_attn [label="Multi-Head Attention\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_attn_out [label="Attention Output (Row-Par)\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_attn_ar [label="Attention All-Reduce\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert6_attn_res [label="Attention + Residual\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert6_ln1 [label="Layer Norm 1\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_gelu [label="GELU Activation\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert6_mlp_ar [label="MLP All-Reduce\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert6_mlp_res [label="MLP + Residual\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert6_ln2 [label="Layer Norm 2\nL15_E6\nGPU:60\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
			subgraph cluster_layer15_expert7 {
				fillcolor=lightcyan label="Expert Group 7 (GPU 62)" style="rounded,filled"
				layer15_expert7_route [label="Expert Routing\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=paralleram style=filled]
				layer15_expert7_gate [label="Expert Gate\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, num_experts=8]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert7_a2a [label="All-to-All Comm\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert7_attn_split [label="Attention Head Split\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert7_q_proj [label="Q Projection (Col-Par)\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_k_proj [label="K Projection (Col-Par)\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_v_proj [label="V Projection (Col-Par)\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_attn [label="Multi-Head Attention\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, heads=8, d_k=64]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_attn_out [label="Attention Output (Row-Par)\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_attn_ar [label="Attention All-Reduce\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert7_attn_res [label="Attention + Residual\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert7_ln1 [label="Layer Norm 1\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_mlp1 [label="MLP Linear 1 (Col-Par)\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_gelu [label="GELU Activation\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, ffn_hidden=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_mlp2 [label="MLP Linear 2 (Row-Par)\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
				layer15_expert7_mlp_ar [label="MLP All-Reduce\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightblue shape=ellipse style=filled]
				layer15_expert7_mlp_res [label="MLP + Residual\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
				layer15_expert7_ln2 [label="Layer Norm 2\nL15_E7\nGPU:62\n[batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=lightgreen shape=rectangle style=filled]
			}
		}
	}
	output [label="Model Output\n\nInput: [batch_size=128, seq_len=1024, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=1024, hidden_dim=1024]" fillcolor=white fontname="Arial Bold" fontsize=12 shape=box style=filled]
	input -> layer0_expert0_route
	layer0_expert0_route -> layer0_expert0_gate
	layer0_expert0_gate -> layer0_expert0_a2a
	layer0_expert0_a2a -> layer0_expert0_attn_split
	layer0_expert0_attn_split -> layer0_expert0_q_proj
	layer0_expert0_attn_split -> layer0_expert0_k_proj
	layer0_expert0_attn_split -> layer0_expert0_v_proj
	layer0_expert0_q_proj -> layer0_expert0_attn
	layer0_expert0_k_proj -> layer0_expert0_attn
	layer0_expert0_v_proj -> layer0_expert0_attn
	layer0_expert0_attn -> layer0_expert0_attn_out
	layer0_expert0_attn_out -> layer0_expert0_attn_ar
	layer0_expert0_attn_ar -> layer0_expert0_attn_res
	layer0_expert0_attn_res -> layer0_expert0_ln1
	layer0_expert0_ln1 -> layer0_expert0_mlp1
	layer0_expert0_mlp1 -> layer0_expert0_gelu
	layer0_expert0_gelu -> layer0_expert0_mlp2
	layer0_expert0_mlp2 -> layer0_expert0_mlp_ar
	layer0_expert0_mlp_ar -> layer0_expert0_mlp_res
	layer0_expert0_mlp_res -> layer0_expert0_ln2
	layer0_expert0_ln2 -> output
}
