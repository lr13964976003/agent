// Llama3_70B_Instruct Parallel Strategy DAG
digraph {
	graph [bgcolor=white fontname=Arial fontsize=12 nodesep=0.5 rankdir=TB ranksep=1.0]
	node [fontname=Arial fontsize=10 margin="0.2,0.1" shape=rectangle style="filled,rounded"]
	edge [arrowhead=normal fontname=Arial fontsize=9 penwidth=1.5]
	input [label="Input\nInput: [batch_size=?, seq_len=?]\nOutput: [batch_size=?, seq_len=?, hidden_size=8192]" fillcolor="#E8F5E8" shape=ellipse style=filled]
	embed_0 [label="Embedding GPU0\nInput: [batch_size=?, seq_len=?]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	embed_1 [label="Embedding GPU1\nInput: [batch_size=?, seq_len=?]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	input -> embed_0
	input -> embed_1
	embed_ag [label="All-Gather\nEmbedding Output\n[TP All-Gather]" fillcolor="#FFE6E6" shape=parallelogram]
	embed_0 -> embed_ag
	embed_1 -> embed_ag
	layer0_qkv_0 [label="Layer 0 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer0_qkv_1 [label="Layer 0 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	embed_ag -> layer0_qkv_0
	embed_ag -> layer0_qkv_1
	layer0_attn_0 [label="Layer 0 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer0_attn_1 [label="Layer 0 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer0_qkv_0 -> layer0_attn_0
	layer0_qkv_1 -> layer0_attn_1
	layer0_attn_out_0 [label="Layer 0 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer0_attn_out_1 [label="Layer 0 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer0_attn_0 -> layer0_attn_out_0
	layer0_attn_1 -> layer0_attn_out_1
	layer0_attn_ar [label="Layer 0 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer0_attn_out_0 -> layer0_attn_ar
	layer0_attn_out_1 -> layer0_attn_ar
	layer0_ffn1_0 [label="Layer 0 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer0_ffn1_1 [label="Layer 0 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer0_attn_ar -> layer0_ffn1_0
	layer0_attn_ar -> layer0_ffn1_1
	layer0_act_0 [label="Layer 0 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer0_act_1 [label="Layer 0 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer0_ffn1_0 -> layer0_act_0
	layer0_ffn1_1 -> layer0_act_1
	layer0_ffn2_0 [label="Layer 0 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer0_ffn2_1 [label="Layer 0 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer0_act_0 -> layer0_ffn2_0
	layer0_act_1 -> layer0_ffn2_1
	layer0_ffn_ar [label="Layer 0 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer0_ffn2_0 -> layer0_ffn_ar
	layer0_ffn2_1 -> layer0_ffn_ar
	layer1_qkv_0 [label="Layer 1 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer1_qkv_1 [label="Layer 1 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer0_ffn_ar -> layer1_qkv_0
	layer0_ffn_ar -> layer1_qkv_1
	layer1_attn_0 [label="Layer 1 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer1_attn_1 [label="Layer 1 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer1_qkv_0 -> layer1_attn_0
	layer1_qkv_1 -> layer1_attn_1
	layer1_attn_out_0 [label="Layer 1 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer1_attn_out_1 [label="Layer 1 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer1_attn_0 -> layer1_attn_out_0
	layer1_attn_1 -> layer1_attn_out_1
	layer1_attn_ar [label="Layer 1 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer1_attn_out_0 -> layer1_attn_ar
	layer1_attn_out_1 -> layer1_attn_ar
	layer1_ffn1_0 [label="Layer 1 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer1_ffn1_1 [label="Layer 1 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer1_attn_ar -> layer1_ffn1_0
	layer1_attn_ar -> layer1_ffn1_1
	layer1_act_0 [label="Layer 1 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer1_act_1 [label="Layer 1 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer1_ffn1_0 -> layer1_act_0
	layer1_ffn1_1 -> layer1_act_1
	layer1_ffn2_0 [label="Layer 1 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer1_ffn2_1 [label="Layer 1 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer1_act_0 -> layer1_ffn2_0
	layer1_act_1 -> layer1_ffn2_1
	layer1_ffn_ar [label="Layer 1 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer1_ffn2_0 -> layer1_ffn_ar
	layer1_ffn2_1 -> layer1_ffn_ar
	layer2_qkv_0 [label="Layer 2 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer2_qkv_1 [label="Layer 2 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer1_ffn_ar -> layer2_qkv_0
	layer1_ffn_ar -> layer2_qkv_1
	layer2_attn_0 [label="Layer 2 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer2_attn_1 [label="Layer 2 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer2_qkv_0 -> layer2_attn_0
	layer2_qkv_1 -> layer2_attn_1
	layer2_attn_out_0 [label="Layer 2 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer2_attn_out_1 [label="Layer 2 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer2_attn_0 -> layer2_attn_out_0
	layer2_attn_1 -> layer2_attn_out_1
	layer2_attn_ar [label="Layer 2 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer2_attn_out_0 -> layer2_attn_ar
	layer2_attn_out_1 -> layer2_attn_ar
	layer2_ffn1_0 [label="Layer 2 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer2_ffn1_1 [label="Layer 2 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer2_attn_ar -> layer2_ffn1_0
	layer2_attn_ar -> layer2_ffn1_1
	layer2_act_0 [label="Layer 2 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer2_act_1 [label="Layer 2 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer2_ffn1_0 -> layer2_act_0
	layer2_ffn1_1 -> layer2_act_1
	layer2_ffn2_0 [label="Layer 2 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer2_ffn2_1 [label="Layer 2 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer2_act_0 -> layer2_ffn2_0
	layer2_act_1 -> layer2_ffn2_1
	layer2_ffn_ar [label="Layer 2 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer2_ffn2_0 -> layer2_ffn_ar
	layer2_ffn2_1 -> layer2_ffn_ar
	layer3_qkv_0 [label="Layer 3 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer3_qkv_1 [label="Layer 3 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer2_ffn_ar -> layer3_qkv_0
	layer2_ffn_ar -> layer3_qkv_1
	layer3_attn_0 [label="Layer 3 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer3_attn_1 [label="Layer 3 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer3_qkv_0 -> layer3_attn_0
	layer3_qkv_1 -> layer3_attn_1
	layer3_attn_out_0 [label="Layer 3 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer3_attn_out_1 [label="Layer 3 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer3_attn_0 -> layer3_attn_out_0
	layer3_attn_1 -> layer3_attn_out_1
	layer3_attn_ar [label="Layer 3 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer3_attn_out_0 -> layer3_attn_ar
	layer3_attn_out_1 -> layer3_attn_ar
	layer3_ffn1_0 [label="Layer 3 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer3_ffn1_1 [label="Layer 3 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer3_attn_ar -> layer3_ffn1_0
	layer3_attn_ar -> layer3_ffn1_1
	layer3_act_0 [label="Layer 3 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer3_act_1 [label="Layer 3 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer3_ffn1_0 -> layer3_act_0
	layer3_ffn1_1 -> layer3_act_1
	layer3_ffn2_0 [label="Layer 3 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer3_ffn2_1 [label="Layer 3 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer3_act_0 -> layer3_ffn2_0
	layer3_act_1 -> layer3_ffn2_1
	layer3_ffn_ar [label="Layer 3 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer3_ffn2_0 -> layer3_ffn_ar
	layer3_ffn2_1 -> layer3_ffn_ar
	layer4_qkv_0 [label="Layer 4 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer4_qkv_1 [label="Layer 4 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer3_ffn_ar -> layer4_qkv_0
	layer3_ffn_ar -> layer4_qkv_1
	layer4_attn_0 [label="Layer 4 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer4_attn_1 [label="Layer 4 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer4_qkv_0 -> layer4_attn_0
	layer4_qkv_1 -> layer4_attn_1
	layer4_attn_out_0 [label="Layer 4 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer4_attn_out_1 [label="Layer 4 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer4_attn_0 -> layer4_attn_out_0
	layer4_attn_1 -> layer4_attn_out_1
	layer4_attn_ar [label="Layer 4 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer4_attn_out_0 -> layer4_attn_ar
	layer4_attn_out_1 -> layer4_attn_ar
	layer4_ffn1_0 [label="Layer 4 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer4_ffn1_1 [label="Layer 4 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer4_attn_ar -> layer4_ffn1_0
	layer4_attn_ar -> layer4_ffn1_1
	layer4_act_0 [label="Layer 4 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer4_act_1 [label="Layer 4 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer4_ffn1_0 -> layer4_act_0
	layer4_ffn1_1 -> layer4_act_1
	layer4_ffn2_0 [label="Layer 4 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer4_ffn2_1 [label="Layer 4 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer4_act_0 -> layer4_ffn2_0
	layer4_act_1 -> layer4_ffn2_1
	layer4_ffn_ar [label="Layer 4 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer4_ffn2_0 -> layer4_ffn_ar
	layer4_ffn2_1 -> layer4_ffn_ar
	layer5_qkv_0 [label="Layer 5 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer5_qkv_1 [label="Layer 5 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer4_ffn_ar -> layer5_qkv_0
	layer4_ffn_ar -> layer5_qkv_1
	layer5_attn_0 [label="Layer 5 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer5_attn_1 [label="Layer 5 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer5_qkv_0 -> layer5_attn_0
	layer5_qkv_1 -> layer5_attn_1
	layer5_attn_out_0 [label="Layer 5 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer5_attn_out_1 [label="Layer 5 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer5_attn_0 -> layer5_attn_out_0
	layer5_attn_1 -> layer5_attn_out_1
	layer5_attn_ar [label="Layer 5 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer5_attn_out_0 -> layer5_attn_ar
	layer5_attn_out_1 -> layer5_attn_ar
	layer5_ffn1_0 [label="Layer 5 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer5_ffn1_1 [label="Layer 5 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer5_attn_ar -> layer5_ffn1_0
	layer5_attn_ar -> layer5_ffn1_1
	layer5_act_0 [label="Layer 5 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer5_act_1 [label="Layer 5 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer5_ffn1_0 -> layer5_act_0
	layer5_ffn1_1 -> layer5_act_1
	layer5_ffn2_0 [label="Layer 5 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer5_ffn2_1 [label="Layer 5 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer5_act_0 -> layer5_ffn2_0
	layer5_act_1 -> layer5_ffn2_1
	layer5_ffn_ar [label="Layer 5 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer5_ffn2_0 -> layer5_ffn_ar
	layer5_ffn2_1 -> layer5_ffn_ar
	layer6_qkv_0 [label="Layer 6 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer6_qkv_1 [label="Layer 6 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer5_ffn_ar -> layer6_qkv_0
	layer5_ffn_ar -> layer6_qkv_1
	layer6_attn_0 [label="Layer 6 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer6_attn_1 [label="Layer 6 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer6_qkv_0 -> layer6_attn_0
	layer6_qkv_1 -> layer6_attn_1
	layer6_attn_out_0 [label="Layer 6 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer6_attn_out_1 [label="Layer 6 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer6_attn_0 -> layer6_attn_out_0
	layer6_attn_1 -> layer6_attn_out_1
	layer6_attn_ar [label="Layer 6 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer6_attn_out_0 -> layer6_attn_ar
	layer6_attn_out_1 -> layer6_attn_ar
	layer6_ffn1_0 [label="Layer 6 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer6_ffn1_1 [label="Layer 6 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer6_attn_ar -> layer6_ffn1_0
	layer6_attn_ar -> layer6_ffn1_1
	layer6_act_0 [label="Layer 6 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer6_act_1 [label="Layer 6 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer6_ffn1_0 -> layer6_act_0
	layer6_ffn1_1 -> layer6_act_1
	layer6_ffn2_0 [label="Layer 6 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer6_ffn2_1 [label="Layer 6 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer6_act_0 -> layer6_ffn2_0
	layer6_act_1 -> layer6_ffn2_1
	layer6_ffn_ar [label="Layer 6 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer6_ffn2_0 -> layer6_ffn_ar
	layer6_ffn2_1 -> layer6_ffn_ar
	layer7_qkv_0 [label="Layer 7 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer7_qkv_1 [label="Layer 7 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer6_ffn_ar -> layer7_qkv_0
	layer6_ffn_ar -> layer7_qkv_1
	layer7_attn_0 [label="Layer 7 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer7_attn_1 [label="Layer 7 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer7_qkv_0 -> layer7_attn_0
	layer7_qkv_1 -> layer7_attn_1
	layer7_attn_out_0 [label="Layer 7 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer7_attn_out_1 [label="Layer 7 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer7_attn_0 -> layer7_attn_out_0
	layer7_attn_1 -> layer7_attn_out_1
	layer7_attn_ar [label="Layer 7 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer7_attn_out_0 -> layer7_attn_ar
	layer7_attn_out_1 -> layer7_attn_ar
	layer7_ffn1_0 [label="Layer 7 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer7_ffn1_1 [label="Layer 7 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer7_attn_ar -> layer7_ffn1_0
	layer7_attn_ar -> layer7_ffn1_1
	layer7_act_0 [label="Layer 7 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer7_act_1 [label="Layer 7 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer7_ffn1_0 -> layer7_act_0
	layer7_ffn1_1 -> layer7_act_1
	layer7_ffn2_0 [label="Layer 7 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer7_ffn2_1 [label="Layer 7 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer7_act_0 -> layer7_ffn2_0
	layer7_act_1 -> layer7_ffn2_1
	layer7_ffn_ar [label="Layer 7 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer7_ffn2_0 -> layer7_ffn_ar
	layer7_ffn2_1 -> layer7_ffn_ar
	layer8_qkv_0 [label="Layer 8 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer8_qkv_1 [label="Layer 8 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer7_ffn_ar -> layer8_qkv_0
	layer7_ffn_ar -> layer8_qkv_1
	layer8_attn_0 [label="Layer 8 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer8_attn_1 [label="Layer 8 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer8_qkv_0 -> layer8_attn_0
	layer8_qkv_1 -> layer8_attn_1
	layer8_attn_out_0 [label="Layer 8 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer8_attn_out_1 [label="Layer 8 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer8_attn_0 -> layer8_attn_out_0
	layer8_attn_1 -> layer8_attn_out_1
	layer8_attn_ar [label="Layer 8 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer8_attn_out_0 -> layer8_attn_ar
	layer8_attn_out_1 -> layer8_attn_ar
	layer8_ffn1_0 [label="Layer 8 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer8_ffn1_1 [label="Layer 8 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer8_attn_ar -> layer8_ffn1_0
	layer8_attn_ar -> layer8_ffn1_1
	layer8_act_0 [label="Layer 8 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer8_act_1 [label="Layer 8 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer8_ffn1_0 -> layer8_act_0
	layer8_ffn1_1 -> layer8_act_1
	layer8_ffn2_0 [label="Layer 8 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer8_ffn2_1 [label="Layer 8 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer8_act_0 -> layer8_ffn2_0
	layer8_act_1 -> layer8_ffn2_1
	layer8_ffn_ar [label="Layer 8 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer8_ffn2_0 -> layer8_ffn_ar
	layer8_ffn2_1 -> layer8_ffn_ar
	layer9_qkv_0 [label="Layer 9 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer9_qkv_1 [label="Layer 9 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer8_ffn_ar -> layer9_qkv_0
	layer8_ffn_ar -> layer9_qkv_1
	layer9_attn_0 [label="Layer 9 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer9_attn_1 [label="Layer 9 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer9_qkv_0 -> layer9_attn_0
	layer9_qkv_1 -> layer9_attn_1
	layer9_attn_out_0 [label="Layer 9 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer9_attn_out_1 [label="Layer 9 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer9_attn_0 -> layer9_attn_out_0
	layer9_attn_1 -> layer9_attn_out_1
	layer9_attn_ar [label="Layer 9 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer9_attn_out_0 -> layer9_attn_ar
	layer9_attn_out_1 -> layer9_attn_ar
	layer9_ffn1_0 [label="Layer 9 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer9_ffn1_1 [label="Layer 9 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer9_attn_ar -> layer9_ffn1_0
	layer9_attn_ar -> layer9_ffn1_1
	layer9_act_0 [label="Layer 9 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer9_act_1 [label="Layer 9 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer9_ffn1_0 -> layer9_act_0
	layer9_ffn1_1 -> layer9_act_1
	layer9_ffn2_0 [label="Layer 9 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer9_ffn2_1 [label="Layer 9 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer9_act_0 -> layer9_ffn2_0
	layer9_act_1 -> layer9_ffn2_1
	layer9_ffn_ar [label="Layer 9 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer9_ffn2_0 -> layer9_ffn_ar
	layer9_ffn2_1 -> layer9_ffn_ar
	layer10_qkv_0 [label="Layer 10 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer10_qkv_1 [label="Layer 10 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer9_ffn_ar -> layer10_qkv_0
	layer9_ffn_ar -> layer10_qkv_1
	layer10_attn_0 [label="Layer 10 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer10_attn_1 [label="Layer 10 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer10_qkv_0 -> layer10_attn_0
	layer10_qkv_1 -> layer10_attn_1
	layer10_attn_out_0 [label="Layer 10 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer10_attn_out_1 [label="Layer 10 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer10_attn_0 -> layer10_attn_out_0
	layer10_attn_1 -> layer10_attn_out_1
	layer10_attn_ar [label="Layer 10 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer10_attn_out_0 -> layer10_attn_ar
	layer10_attn_out_1 -> layer10_attn_ar
	layer10_ffn1_0 [label="Layer 10 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer10_ffn1_1 [label="Layer 10 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer10_attn_ar -> layer10_ffn1_0
	layer10_attn_ar -> layer10_ffn1_1
	layer10_act_0 [label="Layer 10 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer10_act_1 [label="Layer 10 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer10_ffn1_0 -> layer10_act_0
	layer10_ffn1_1 -> layer10_act_1
	layer10_ffn2_0 [label="Layer 10 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer10_ffn2_1 [label="Layer 10 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer10_act_0 -> layer10_ffn2_0
	layer10_act_1 -> layer10_ffn2_1
	layer10_ffn_ar [label="Layer 10 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer10_ffn2_0 -> layer10_ffn_ar
	layer10_ffn2_1 -> layer10_ffn_ar
	layer11_qkv_0 [label="Layer 11 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer11_qkv_1 [label="Layer 11 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer10_ffn_ar -> layer11_qkv_0
	layer10_ffn_ar -> layer11_qkv_1
	layer11_attn_0 [label="Layer 11 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer11_attn_1 [label="Layer 11 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer11_qkv_0 -> layer11_attn_0
	layer11_qkv_1 -> layer11_attn_1
	layer11_attn_out_0 [label="Layer 11 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer11_attn_out_1 [label="Layer 11 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer11_attn_0 -> layer11_attn_out_0
	layer11_attn_1 -> layer11_attn_out_1
	layer11_attn_ar [label="Layer 11 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer11_attn_out_0 -> layer11_attn_ar
	layer11_attn_out_1 -> layer11_attn_ar
	layer11_ffn1_0 [label="Layer 11 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer11_ffn1_1 [label="Layer 11 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer11_attn_ar -> layer11_ffn1_0
	layer11_attn_ar -> layer11_ffn1_1
	layer11_act_0 [label="Layer 11 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer11_act_1 [label="Layer 11 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer11_ffn1_0 -> layer11_act_0
	layer11_ffn1_1 -> layer11_act_1
	layer11_ffn2_0 [label="Layer 11 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer11_ffn2_1 [label="Layer 11 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer11_act_0 -> layer11_ffn2_0
	layer11_act_1 -> layer11_ffn2_1
	layer11_ffn_ar [label="Layer 11 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer11_ffn2_0 -> layer11_ffn_ar
	layer11_ffn2_1 -> layer11_ffn_ar
	layer12_qkv_0 [label="Layer 12 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer12_qkv_1 [label="Layer 12 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer11_ffn_ar -> layer12_qkv_0
	layer11_ffn_ar -> layer12_qkv_1
	layer12_attn_0 [label="Layer 12 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer12_attn_1 [label="Layer 12 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer12_qkv_0 -> layer12_attn_0
	layer12_qkv_1 -> layer12_attn_1
	layer12_attn_out_0 [label="Layer 12 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer12_attn_out_1 [label="Layer 12 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer12_attn_0 -> layer12_attn_out_0
	layer12_attn_1 -> layer12_attn_out_1
	layer12_attn_ar [label="Layer 12 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer12_attn_out_0 -> layer12_attn_ar
	layer12_attn_out_1 -> layer12_attn_ar
	layer12_ffn1_0 [label="Layer 12 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer12_ffn1_1 [label="Layer 12 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer12_attn_ar -> layer12_ffn1_0
	layer12_attn_ar -> layer12_ffn1_1
	layer12_act_0 [label="Layer 12 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer12_act_1 [label="Layer 12 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer12_ffn1_0 -> layer12_act_0
	layer12_ffn1_1 -> layer12_act_1
	layer12_ffn2_0 [label="Layer 12 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer12_ffn2_1 [label="Layer 12 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer12_act_0 -> layer12_ffn2_0
	layer12_act_1 -> layer12_ffn2_1
	layer12_ffn_ar [label="Layer 12 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer12_ffn2_0 -> layer12_ffn_ar
	layer12_ffn2_1 -> layer12_ffn_ar
	layer13_qkv_0 [label="Layer 13 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer13_qkv_1 [label="Layer 13 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer12_ffn_ar -> layer13_qkv_0
	layer12_ffn_ar -> layer13_qkv_1
	layer13_attn_0 [label="Layer 13 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer13_attn_1 [label="Layer 13 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer13_qkv_0 -> layer13_attn_0
	layer13_qkv_1 -> layer13_attn_1
	layer13_attn_out_0 [label="Layer 13 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer13_attn_out_1 [label="Layer 13 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer13_attn_0 -> layer13_attn_out_0
	layer13_attn_1 -> layer13_attn_out_1
	layer13_attn_ar [label="Layer 13 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer13_attn_out_0 -> layer13_attn_ar
	layer13_attn_out_1 -> layer13_attn_ar
	layer13_ffn1_0 [label="Layer 13 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer13_ffn1_1 [label="Layer 13 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer13_attn_ar -> layer13_ffn1_0
	layer13_attn_ar -> layer13_ffn1_1
	layer13_act_0 [label="Layer 13 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer13_act_1 [label="Layer 13 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer13_ffn1_0 -> layer13_act_0
	layer13_ffn1_1 -> layer13_act_1
	layer13_ffn2_0 [label="Layer 13 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer13_ffn2_1 [label="Layer 13 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer13_act_0 -> layer13_ffn2_0
	layer13_act_1 -> layer13_ffn2_1
	layer13_ffn_ar [label="Layer 13 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer13_ffn2_0 -> layer13_ffn_ar
	layer13_ffn2_1 -> layer13_ffn_ar
	layer14_qkv_0 [label="Layer 14 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer14_qkv_1 [label="Layer 14 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer13_ffn_ar -> layer14_qkv_0
	layer13_ffn_ar -> layer14_qkv_1
	layer14_attn_0 [label="Layer 14 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer14_attn_1 [label="Layer 14 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer14_qkv_0 -> layer14_attn_0
	layer14_qkv_1 -> layer14_attn_1
	layer14_attn_out_0 [label="Layer 14 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer14_attn_out_1 [label="Layer 14 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer14_attn_0 -> layer14_attn_out_0
	layer14_attn_1 -> layer14_attn_out_1
	layer14_attn_ar [label="Layer 14 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer14_attn_out_0 -> layer14_attn_ar
	layer14_attn_out_1 -> layer14_attn_ar
	layer14_ffn1_0 [label="Layer 14 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer14_ffn1_1 [label="Layer 14 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer14_attn_ar -> layer14_ffn1_0
	layer14_attn_ar -> layer14_ffn1_1
	layer14_act_0 [label="Layer 14 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer14_act_1 [label="Layer 14 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer14_ffn1_0 -> layer14_act_0
	layer14_ffn1_1 -> layer14_act_1
	layer14_ffn2_0 [label="Layer 14 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer14_ffn2_1 [label="Layer 14 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer14_act_0 -> layer14_ffn2_0
	layer14_act_1 -> layer14_ffn2_1
	layer14_ffn_ar [label="Layer 14 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer14_ffn2_0 -> layer14_ffn_ar
	layer14_ffn2_1 -> layer14_ffn_ar
	layer15_qkv_0 [label="Layer 15 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer15_qkv_1 [label="Layer 15 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer14_ffn_ar -> layer15_qkv_0
	layer14_ffn_ar -> layer15_qkv_1
	layer15_attn_0 [label="Layer 15 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer15_attn_1 [label="Layer 15 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer15_qkv_0 -> layer15_attn_0
	layer15_qkv_1 -> layer15_attn_1
	layer15_attn_out_0 [label="Layer 15 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer15_attn_out_1 [label="Layer 15 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer15_attn_0 -> layer15_attn_out_0
	layer15_attn_1 -> layer15_attn_out_1
	layer15_attn_ar [label="Layer 15 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer15_attn_out_0 -> layer15_attn_ar
	layer15_attn_out_1 -> layer15_attn_ar
	layer15_ffn1_0 [label="Layer 15 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer15_ffn1_1 [label="Layer 15 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer15_attn_ar -> layer15_ffn1_0
	layer15_attn_ar -> layer15_ffn1_1
	layer15_act_0 [label="Layer 15 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer15_act_1 [label="Layer 15 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer15_ffn1_0 -> layer15_act_0
	layer15_ffn1_1 -> layer15_act_1
	layer15_ffn2_0 [label="Layer 15 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer15_ffn2_1 [label="Layer 15 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer15_act_0 -> layer15_ffn2_0
	layer15_act_1 -> layer15_ffn2_1
	layer15_ffn_ar [label="Layer 15 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer15_ffn2_0 -> layer15_ffn_ar
	layer15_ffn2_1 -> layer15_ffn_ar
	layer16_qkv_0 [label="Layer 16 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer16_qkv_1 [label="Layer 16 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer15_ffn_ar -> layer16_qkv_0
	layer15_ffn_ar -> layer16_qkv_1
	layer16_attn_0 [label="Layer 16 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer16_attn_1 [label="Layer 16 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer16_qkv_0 -> layer16_attn_0
	layer16_qkv_1 -> layer16_attn_1
	layer16_attn_out_0 [label="Layer 16 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer16_attn_out_1 [label="Layer 16 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer16_attn_0 -> layer16_attn_out_0
	layer16_attn_1 -> layer16_attn_out_1
	layer16_attn_ar [label="Layer 16 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer16_attn_out_0 -> layer16_attn_ar
	layer16_attn_out_1 -> layer16_attn_ar
	layer16_ffn1_0 [label="Layer 16 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer16_ffn1_1 [label="Layer 16 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer16_attn_ar -> layer16_ffn1_0
	layer16_attn_ar -> layer16_ffn1_1
	layer16_act_0 [label="Layer 16 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer16_act_1 [label="Layer 16 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer16_ffn1_0 -> layer16_act_0
	layer16_ffn1_1 -> layer16_act_1
	layer16_ffn2_0 [label="Layer 16 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer16_ffn2_1 [label="Layer 16 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer16_act_0 -> layer16_ffn2_0
	layer16_act_1 -> layer16_ffn2_1
	layer16_ffn_ar [label="Layer 16 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer16_ffn2_0 -> layer16_ffn_ar
	layer16_ffn2_1 -> layer16_ffn_ar
	layer17_qkv_0 [label="Layer 17 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer17_qkv_1 [label="Layer 17 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer16_ffn_ar -> layer17_qkv_0
	layer16_ffn_ar -> layer17_qkv_1
	layer17_attn_0 [label="Layer 17 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer17_attn_1 [label="Layer 17 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer17_qkv_0 -> layer17_attn_0
	layer17_qkv_1 -> layer17_attn_1
	layer17_attn_out_0 [label="Layer 17 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer17_attn_out_1 [label="Layer 17 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer17_attn_0 -> layer17_attn_out_0
	layer17_attn_1 -> layer17_attn_out_1
	layer17_attn_ar [label="Layer 17 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer17_attn_out_0 -> layer17_attn_ar
	layer17_attn_out_1 -> layer17_attn_ar
	layer17_ffn1_0 [label="Layer 17 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer17_ffn1_1 [label="Layer 17 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer17_attn_ar -> layer17_ffn1_0
	layer17_attn_ar -> layer17_ffn1_1
	layer17_act_0 [label="Layer 17 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer17_act_1 [label="Layer 17 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer17_ffn1_0 -> layer17_act_0
	layer17_ffn1_1 -> layer17_act_1
	layer17_ffn2_0 [label="Layer 17 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer17_ffn2_1 [label="Layer 17 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer17_act_0 -> layer17_ffn2_0
	layer17_act_1 -> layer17_ffn2_1
	layer17_ffn_ar [label="Layer 17 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer17_ffn2_0 -> layer17_ffn_ar
	layer17_ffn2_1 -> layer17_ffn_ar
	layer18_qkv_0 [label="Layer 18 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer18_qkv_1 [label="Layer 18 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer17_ffn_ar -> layer18_qkv_0
	layer17_ffn_ar -> layer18_qkv_1
	layer18_attn_0 [label="Layer 18 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer18_attn_1 [label="Layer 18 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer18_qkv_0 -> layer18_attn_0
	layer18_qkv_1 -> layer18_attn_1
	layer18_attn_out_0 [label="Layer 18 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer18_attn_out_1 [label="Layer 18 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer18_attn_0 -> layer18_attn_out_0
	layer18_attn_1 -> layer18_attn_out_1
	layer18_attn_ar [label="Layer 18 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer18_attn_out_0 -> layer18_attn_ar
	layer18_attn_out_1 -> layer18_attn_ar
	layer18_ffn1_0 [label="Layer 18 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer18_ffn1_1 [label="Layer 18 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer18_attn_ar -> layer18_ffn1_0
	layer18_attn_ar -> layer18_ffn1_1
	layer18_act_0 [label="Layer 18 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer18_act_1 [label="Layer 18 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer18_ffn1_0 -> layer18_act_0
	layer18_ffn1_1 -> layer18_act_1
	layer18_ffn2_0 [label="Layer 18 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer18_ffn2_1 [label="Layer 18 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer18_act_0 -> layer18_ffn2_0
	layer18_act_1 -> layer18_ffn2_1
	layer18_ffn_ar [label="Layer 18 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer18_ffn2_0 -> layer18_ffn_ar
	layer18_ffn2_1 -> layer18_ffn_ar
	layer19_qkv_0 [label="Layer 19 QKV Proj GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer19_qkv_1 [label="Layer 19 QKV Proj GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer18_ffn_ar -> layer19_qkv_0
	layer18_ffn_ar -> layer19_qkv_1
	layer19_attn_0 [label="Layer 19 Attention GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer19_attn_1 [label="Layer 19 Attention GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer19_qkv_0 -> layer19_attn_0
	layer19_qkv_1 -> layer19_attn_1
	layer19_attn_out_0 [label="Layer 19 Attn Out GPU0\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer19_attn_out_1 [label="Layer 19 Attn Out GPU1\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer19_attn_0 -> layer19_attn_out_0
	layer19_attn_1 -> layer19_attn_out_1
	layer19_attn_ar [label="Layer 19 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer19_attn_out_0 -> layer19_attn_ar
	layer19_attn_out_1 -> layer19_attn_ar
	layer19_ffn1_0 [label="Layer 19 FFN1 GPU0\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer19_ffn1_1 [label="Layer 19 FFN1 GPU1\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer19_attn_ar -> layer19_ffn1_0
	layer19_attn_ar -> layer19_ffn1_1
	layer19_act_0 [label="Layer 19 SiLU GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer19_act_1 [label="Layer 19 SiLU GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer19_ffn1_0 -> layer19_act_0
	layer19_ffn1_1 -> layer19_act_1
	layer19_ffn2_0 [label="Layer 19 FFN2 GPU0\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer19_ffn2_1 [label="Layer 19 FFN2 GPU1\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer19_act_0 -> layer19_ffn2_0
	layer19_act_1 -> layer19_ffn2_1
	layer19_ffn_ar [label="Layer 19 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer19_ffn2_0 -> layer19_ffn_ar
	layer19_ffn2_1 -> layer19_ffn_ar
	stage0_to_stage1 [label="Pipeline Forward\nStage 0  Stage 1\n[PP Forward]" fillcolor="#FFE6E6" shape=parallelogram]
	layer19_ffn_ar -> stage0_to_stage1
	layer20_qkv_2 [label="Layer 20 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer20_qkv_3 [label="Layer 20 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	stage0_to_stage1 -> layer20_qkv_2
	stage0_to_stage1 -> layer20_qkv_3
	layer20_attn_2 [label="Layer 20 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer20_attn_3 [label="Layer 20 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer20_qkv_2 -> layer20_attn_2
	layer20_qkv_3 -> layer20_attn_3
	layer20_attn_out_2 [label="Layer 20 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer20_attn_out_3 [label="Layer 20 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer20_attn_2 -> layer20_attn_out_2
	layer20_attn_3 -> layer20_attn_out_3
	layer20_attn_ar [label="Layer 20 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer20_attn_out_2 -> layer20_attn_ar
	layer20_attn_out_3 -> layer20_attn_ar
	layer20_ffn1_2 [label="Layer 20 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer20_ffn1_3 [label="Layer 20 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer20_attn_ar -> layer20_ffn1_2
	layer20_attn_ar -> layer20_ffn1_3
	layer20_act_2 [label="Layer 20 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer20_act_3 [label="Layer 20 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer20_ffn1_2 -> layer20_act_2
	layer20_ffn1_3 -> layer20_act_3
	layer20_ffn2_2 [label="Layer 20 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer20_ffn2_3 [label="Layer 20 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer20_act_2 -> layer20_ffn2_2
	layer20_act_3 -> layer20_ffn2_3
	layer20_ffn_ar [label="Layer 20 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer20_ffn2_2 -> layer20_ffn_ar
	layer20_ffn2_3 -> layer20_ffn_ar
	layer21_qkv_2 [label="Layer 21 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer21_qkv_3 [label="Layer 21 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer20_ffn_ar -> layer21_qkv_2
	layer20_ffn_ar -> layer21_qkv_3
	layer21_attn_2 [label="Layer 21 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer21_attn_3 [label="Layer 21 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer21_qkv_2 -> layer21_attn_2
	layer21_qkv_3 -> layer21_attn_3
	layer21_attn_out_2 [label="Layer 21 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer21_attn_out_3 [label="Layer 21 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer21_attn_2 -> layer21_attn_out_2
	layer21_attn_3 -> layer21_attn_out_3
	layer21_attn_ar [label="Layer 21 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer21_attn_out_2 -> layer21_attn_ar
	layer21_attn_out_3 -> layer21_attn_ar
	layer21_ffn1_2 [label="Layer 21 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer21_ffn1_3 [label="Layer 21 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer21_attn_ar -> layer21_ffn1_2
	layer21_attn_ar -> layer21_ffn1_3
	layer21_act_2 [label="Layer 21 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer21_act_3 [label="Layer 21 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer21_ffn1_2 -> layer21_act_2
	layer21_ffn1_3 -> layer21_act_3
	layer21_ffn2_2 [label="Layer 21 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer21_ffn2_3 [label="Layer 21 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer21_act_2 -> layer21_ffn2_2
	layer21_act_3 -> layer21_ffn2_3
	layer21_ffn_ar [label="Layer 21 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer21_ffn2_2 -> layer21_ffn_ar
	layer21_ffn2_3 -> layer21_ffn_ar
	layer22_qkv_2 [label="Layer 22 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer22_qkv_3 [label="Layer 22 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer21_ffn_ar -> layer22_qkv_2
	layer21_ffn_ar -> layer22_qkv_3
	layer22_attn_2 [label="Layer 22 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer22_attn_3 [label="Layer 22 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer22_qkv_2 -> layer22_attn_2
	layer22_qkv_3 -> layer22_attn_3
	layer22_attn_out_2 [label="Layer 22 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer22_attn_out_3 [label="Layer 22 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer22_attn_2 -> layer22_attn_out_2
	layer22_attn_3 -> layer22_attn_out_3
	layer22_attn_ar [label="Layer 22 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer22_attn_out_2 -> layer22_attn_ar
	layer22_attn_out_3 -> layer22_attn_ar
	layer22_ffn1_2 [label="Layer 22 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer22_ffn1_3 [label="Layer 22 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer22_attn_ar -> layer22_ffn1_2
	layer22_attn_ar -> layer22_ffn1_3
	layer22_act_2 [label="Layer 22 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer22_act_3 [label="Layer 22 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer22_ffn1_2 -> layer22_act_2
	layer22_ffn1_3 -> layer22_act_3
	layer22_ffn2_2 [label="Layer 22 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer22_ffn2_3 [label="Layer 22 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer22_act_2 -> layer22_ffn2_2
	layer22_act_3 -> layer22_ffn2_3
	layer22_ffn_ar [label="Layer 22 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer22_ffn2_2 -> layer22_ffn_ar
	layer22_ffn2_3 -> layer22_ffn_ar
	layer23_qkv_2 [label="Layer 23 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer23_qkv_3 [label="Layer 23 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer22_ffn_ar -> layer23_qkv_2
	layer22_ffn_ar -> layer23_qkv_3
	layer23_attn_2 [label="Layer 23 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer23_attn_3 [label="Layer 23 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer23_qkv_2 -> layer23_attn_2
	layer23_qkv_3 -> layer23_attn_3
	layer23_attn_out_2 [label="Layer 23 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer23_attn_out_3 [label="Layer 23 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer23_attn_2 -> layer23_attn_out_2
	layer23_attn_3 -> layer23_attn_out_3
	layer23_attn_ar [label="Layer 23 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer23_attn_out_2 -> layer23_attn_ar
	layer23_attn_out_3 -> layer23_attn_ar
	layer23_ffn1_2 [label="Layer 23 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer23_ffn1_3 [label="Layer 23 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer23_attn_ar -> layer23_ffn1_2
	layer23_attn_ar -> layer23_ffn1_3
	layer23_act_2 [label="Layer 23 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer23_act_3 [label="Layer 23 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer23_ffn1_2 -> layer23_act_2
	layer23_ffn1_3 -> layer23_act_3
	layer23_ffn2_2 [label="Layer 23 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer23_ffn2_3 [label="Layer 23 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer23_act_2 -> layer23_ffn2_2
	layer23_act_3 -> layer23_ffn2_3
	layer23_ffn_ar [label="Layer 23 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer23_ffn2_2 -> layer23_ffn_ar
	layer23_ffn2_3 -> layer23_ffn_ar
	layer24_qkv_2 [label="Layer 24 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer24_qkv_3 [label="Layer 24 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer23_ffn_ar -> layer24_qkv_2
	layer23_ffn_ar -> layer24_qkv_3
	layer24_attn_2 [label="Layer 24 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer24_attn_3 [label="Layer 24 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer24_qkv_2 -> layer24_attn_2
	layer24_qkv_3 -> layer24_attn_3
	layer24_attn_out_2 [label="Layer 24 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer24_attn_out_3 [label="Layer 24 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer24_attn_2 -> layer24_attn_out_2
	layer24_attn_3 -> layer24_attn_out_3
	layer24_attn_ar [label="Layer 24 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer24_attn_out_2 -> layer24_attn_ar
	layer24_attn_out_3 -> layer24_attn_ar
	layer24_ffn1_2 [label="Layer 24 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer24_ffn1_3 [label="Layer 24 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer24_attn_ar -> layer24_ffn1_2
	layer24_attn_ar -> layer24_ffn1_3
	layer24_act_2 [label="Layer 24 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer24_act_3 [label="Layer 24 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer24_ffn1_2 -> layer24_act_2
	layer24_ffn1_3 -> layer24_act_3
	layer24_ffn2_2 [label="Layer 24 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer24_ffn2_3 [label="Layer 24 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer24_act_2 -> layer24_ffn2_2
	layer24_act_3 -> layer24_ffn2_3
	layer24_ffn_ar [label="Layer 24 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer24_ffn2_2 -> layer24_ffn_ar
	layer24_ffn2_3 -> layer24_ffn_ar
	layer25_qkv_2 [label="Layer 25 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer25_qkv_3 [label="Layer 25 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer24_ffn_ar -> layer25_qkv_2
	layer24_ffn_ar -> layer25_qkv_3
	layer25_attn_2 [label="Layer 25 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer25_attn_3 [label="Layer 25 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer25_qkv_2 -> layer25_attn_2
	layer25_qkv_3 -> layer25_attn_3
	layer25_attn_out_2 [label="Layer 25 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer25_attn_out_3 [label="Layer 25 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer25_attn_2 -> layer25_attn_out_2
	layer25_attn_3 -> layer25_attn_out_3
	layer25_attn_ar [label="Layer 25 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer25_attn_out_2 -> layer25_attn_ar
	layer25_attn_out_3 -> layer25_attn_ar
	layer25_ffn1_2 [label="Layer 25 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer25_ffn1_3 [label="Layer 25 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer25_attn_ar -> layer25_ffn1_2
	layer25_attn_ar -> layer25_ffn1_3
	layer25_act_2 [label="Layer 25 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer25_act_3 [label="Layer 25 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer25_ffn1_2 -> layer25_act_2
	layer25_ffn1_3 -> layer25_act_3
	layer25_ffn2_2 [label="Layer 25 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer25_ffn2_3 [label="Layer 25 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer25_act_2 -> layer25_ffn2_2
	layer25_act_3 -> layer25_ffn2_3
	layer25_ffn_ar [label="Layer 25 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer25_ffn2_2 -> layer25_ffn_ar
	layer25_ffn2_3 -> layer25_ffn_ar
	layer26_qkv_2 [label="Layer 26 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer26_qkv_3 [label="Layer 26 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer25_ffn_ar -> layer26_qkv_2
	layer25_ffn_ar -> layer26_qkv_3
	layer26_attn_2 [label="Layer 26 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer26_attn_3 [label="Layer 26 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer26_qkv_2 -> layer26_attn_2
	layer26_qkv_3 -> layer26_attn_3
	layer26_attn_out_2 [label="Layer 26 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer26_attn_out_3 [label="Layer 26 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer26_attn_2 -> layer26_attn_out_2
	layer26_attn_3 -> layer26_attn_out_3
	layer26_attn_ar [label="Layer 26 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer26_attn_out_2 -> layer26_attn_ar
	layer26_attn_out_3 -> layer26_attn_ar
	layer26_ffn1_2 [label="Layer 26 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer26_ffn1_3 [label="Layer 26 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer26_attn_ar -> layer26_ffn1_2
	layer26_attn_ar -> layer26_ffn1_3
	layer26_act_2 [label="Layer 26 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer26_act_3 [label="Layer 26 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer26_ffn1_2 -> layer26_act_2
	layer26_ffn1_3 -> layer26_act_3
	layer26_ffn2_2 [label="Layer 26 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer26_ffn2_3 [label="Layer 26 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer26_act_2 -> layer26_ffn2_2
	layer26_act_3 -> layer26_ffn2_3
	layer26_ffn_ar [label="Layer 26 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer26_ffn2_2 -> layer26_ffn_ar
	layer26_ffn2_3 -> layer26_ffn_ar
	layer27_qkv_2 [label="Layer 27 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer27_qkv_3 [label="Layer 27 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer26_ffn_ar -> layer27_qkv_2
	layer26_ffn_ar -> layer27_qkv_3
	layer27_attn_2 [label="Layer 27 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer27_attn_3 [label="Layer 27 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer27_qkv_2 -> layer27_attn_2
	layer27_qkv_3 -> layer27_attn_3
	layer27_attn_out_2 [label="Layer 27 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer27_attn_out_3 [label="Layer 27 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer27_attn_2 -> layer27_attn_out_2
	layer27_attn_3 -> layer27_attn_out_3
	layer27_attn_ar [label="Layer 27 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer27_attn_out_2 -> layer27_attn_ar
	layer27_attn_out_3 -> layer27_attn_ar
	layer27_ffn1_2 [label="Layer 27 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer27_ffn1_3 [label="Layer 27 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer27_attn_ar -> layer27_ffn1_2
	layer27_attn_ar -> layer27_ffn1_3
	layer27_act_2 [label="Layer 27 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer27_act_3 [label="Layer 27 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer27_ffn1_2 -> layer27_act_2
	layer27_ffn1_3 -> layer27_act_3
	layer27_ffn2_2 [label="Layer 27 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer27_ffn2_3 [label="Layer 27 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer27_act_2 -> layer27_ffn2_2
	layer27_act_3 -> layer27_ffn2_3
	layer27_ffn_ar [label="Layer 27 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer27_ffn2_2 -> layer27_ffn_ar
	layer27_ffn2_3 -> layer27_ffn_ar
	layer28_qkv_2 [label="Layer 28 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer28_qkv_3 [label="Layer 28 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer27_ffn_ar -> layer28_qkv_2
	layer27_ffn_ar -> layer28_qkv_3
	layer28_attn_2 [label="Layer 28 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer28_attn_3 [label="Layer 28 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer28_qkv_2 -> layer28_attn_2
	layer28_qkv_3 -> layer28_attn_3
	layer28_attn_out_2 [label="Layer 28 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer28_attn_out_3 [label="Layer 28 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer28_attn_2 -> layer28_attn_out_2
	layer28_attn_3 -> layer28_attn_out_3
	layer28_attn_ar [label="Layer 28 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer28_attn_out_2 -> layer28_attn_ar
	layer28_attn_out_3 -> layer28_attn_ar
	layer28_ffn1_2 [label="Layer 28 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer28_ffn1_3 [label="Layer 28 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer28_attn_ar -> layer28_ffn1_2
	layer28_attn_ar -> layer28_ffn1_3
	layer28_act_2 [label="Layer 28 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer28_act_3 [label="Layer 28 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer28_ffn1_2 -> layer28_act_2
	layer28_ffn1_3 -> layer28_act_3
	layer28_ffn2_2 [label="Layer 28 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer28_ffn2_3 [label="Layer 28 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer28_act_2 -> layer28_ffn2_2
	layer28_act_3 -> layer28_ffn2_3
	layer28_ffn_ar [label="Layer 28 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer28_ffn2_2 -> layer28_ffn_ar
	layer28_ffn2_3 -> layer28_ffn_ar
	layer29_qkv_2 [label="Layer 29 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer29_qkv_3 [label="Layer 29 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer28_ffn_ar -> layer29_qkv_2
	layer28_ffn_ar -> layer29_qkv_3
	layer29_attn_2 [label="Layer 29 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer29_attn_3 [label="Layer 29 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer29_qkv_2 -> layer29_attn_2
	layer29_qkv_3 -> layer29_attn_3
	layer29_attn_out_2 [label="Layer 29 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer29_attn_out_3 [label="Layer 29 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer29_attn_2 -> layer29_attn_out_2
	layer29_attn_3 -> layer29_attn_out_3
	layer29_attn_ar [label="Layer 29 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer29_attn_out_2 -> layer29_attn_ar
	layer29_attn_out_3 -> layer29_attn_ar
	layer29_ffn1_2 [label="Layer 29 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer29_ffn1_3 [label="Layer 29 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer29_attn_ar -> layer29_ffn1_2
	layer29_attn_ar -> layer29_ffn1_3
	layer29_act_2 [label="Layer 29 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer29_act_3 [label="Layer 29 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer29_ffn1_2 -> layer29_act_2
	layer29_ffn1_3 -> layer29_act_3
	layer29_ffn2_2 [label="Layer 29 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer29_ffn2_3 [label="Layer 29 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer29_act_2 -> layer29_ffn2_2
	layer29_act_3 -> layer29_ffn2_3
	layer29_ffn_ar [label="Layer 29 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer29_ffn2_2 -> layer29_ffn_ar
	layer29_ffn2_3 -> layer29_ffn_ar
	layer30_qkv_2 [label="Layer 30 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer30_qkv_3 [label="Layer 30 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer29_ffn_ar -> layer30_qkv_2
	layer29_ffn_ar -> layer30_qkv_3
	layer30_attn_2 [label="Layer 30 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer30_attn_3 [label="Layer 30 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer30_qkv_2 -> layer30_attn_2
	layer30_qkv_3 -> layer30_attn_3
	layer30_attn_out_2 [label="Layer 30 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer30_attn_out_3 [label="Layer 30 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer30_attn_2 -> layer30_attn_out_2
	layer30_attn_3 -> layer30_attn_out_3
	layer30_attn_ar [label="Layer 30 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer30_attn_out_2 -> layer30_attn_ar
	layer30_attn_out_3 -> layer30_attn_ar
	layer30_ffn1_2 [label="Layer 30 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer30_ffn1_3 [label="Layer 30 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer30_attn_ar -> layer30_ffn1_2
	layer30_attn_ar -> layer30_ffn1_3
	layer30_act_2 [label="Layer 30 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer30_act_3 [label="Layer 30 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer30_ffn1_2 -> layer30_act_2
	layer30_ffn1_3 -> layer30_act_3
	layer30_ffn2_2 [label="Layer 30 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer30_ffn2_3 [label="Layer 30 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer30_act_2 -> layer30_ffn2_2
	layer30_act_3 -> layer30_ffn2_3
	layer30_ffn_ar [label="Layer 30 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer30_ffn2_2 -> layer30_ffn_ar
	layer30_ffn2_3 -> layer30_ffn_ar
	layer31_qkv_2 [label="Layer 31 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer31_qkv_3 [label="Layer 31 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer30_ffn_ar -> layer31_qkv_2
	layer30_ffn_ar -> layer31_qkv_3
	layer31_attn_2 [label="Layer 31 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer31_attn_3 [label="Layer 31 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer31_qkv_2 -> layer31_attn_2
	layer31_qkv_3 -> layer31_attn_3
	layer31_attn_out_2 [label="Layer 31 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer31_attn_out_3 [label="Layer 31 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer31_attn_2 -> layer31_attn_out_2
	layer31_attn_3 -> layer31_attn_out_3
	layer31_attn_ar [label="Layer 31 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer31_attn_out_2 -> layer31_attn_ar
	layer31_attn_out_3 -> layer31_attn_ar
	layer31_ffn1_2 [label="Layer 31 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer31_ffn1_3 [label="Layer 31 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer31_attn_ar -> layer31_ffn1_2
	layer31_attn_ar -> layer31_ffn1_3
	layer31_act_2 [label="Layer 31 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer31_act_3 [label="Layer 31 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer31_ffn1_2 -> layer31_act_2
	layer31_ffn1_3 -> layer31_act_3
	layer31_ffn2_2 [label="Layer 31 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer31_ffn2_3 [label="Layer 31 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer31_act_2 -> layer31_ffn2_2
	layer31_act_3 -> layer31_ffn2_3
	layer31_ffn_ar [label="Layer 31 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer31_ffn2_2 -> layer31_ffn_ar
	layer31_ffn2_3 -> layer31_ffn_ar
	layer32_qkv_2 [label="Layer 32 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer32_qkv_3 [label="Layer 32 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer31_ffn_ar -> layer32_qkv_2
	layer31_ffn_ar -> layer32_qkv_3
	layer32_attn_2 [label="Layer 32 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer32_attn_3 [label="Layer 32 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer32_qkv_2 -> layer32_attn_2
	layer32_qkv_3 -> layer32_attn_3
	layer32_attn_out_2 [label="Layer 32 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer32_attn_out_3 [label="Layer 32 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer32_attn_2 -> layer32_attn_out_2
	layer32_attn_3 -> layer32_attn_out_3
	layer32_attn_ar [label="Layer 32 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer32_attn_out_2 -> layer32_attn_ar
	layer32_attn_out_3 -> layer32_attn_ar
	layer32_ffn1_2 [label="Layer 32 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer32_ffn1_3 [label="Layer 32 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer32_attn_ar -> layer32_ffn1_2
	layer32_attn_ar -> layer32_ffn1_3
	layer32_act_2 [label="Layer 32 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer32_act_3 [label="Layer 32 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer32_ffn1_2 -> layer32_act_2
	layer32_ffn1_3 -> layer32_act_3
	layer32_ffn2_2 [label="Layer 32 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer32_ffn2_3 [label="Layer 32 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer32_act_2 -> layer32_ffn2_2
	layer32_act_3 -> layer32_ffn2_3
	layer32_ffn_ar [label="Layer 32 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer32_ffn2_2 -> layer32_ffn_ar
	layer32_ffn2_3 -> layer32_ffn_ar
	layer33_qkv_2 [label="Layer 33 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer33_qkv_3 [label="Layer 33 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer32_ffn_ar -> layer33_qkv_2
	layer32_ffn_ar -> layer33_qkv_3
	layer33_attn_2 [label="Layer 33 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer33_attn_3 [label="Layer 33 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer33_qkv_2 -> layer33_attn_2
	layer33_qkv_3 -> layer33_attn_3
	layer33_attn_out_2 [label="Layer 33 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer33_attn_out_3 [label="Layer 33 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer33_attn_2 -> layer33_attn_out_2
	layer33_attn_3 -> layer33_attn_out_3
	layer33_attn_ar [label="Layer 33 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer33_attn_out_2 -> layer33_attn_ar
	layer33_attn_out_3 -> layer33_attn_ar
	layer33_ffn1_2 [label="Layer 33 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer33_ffn1_3 [label="Layer 33 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer33_attn_ar -> layer33_ffn1_2
	layer33_attn_ar -> layer33_ffn1_3
	layer33_act_2 [label="Layer 33 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer33_act_3 [label="Layer 33 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer33_ffn1_2 -> layer33_act_2
	layer33_ffn1_3 -> layer33_act_3
	layer33_ffn2_2 [label="Layer 33 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer33_ffn2_3 [label="Layer 33 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer33_act_2 -> layer33_ffn2_2
	layer33_act_3 -> layer33_ffn2_3
	layer33_ffn_ar [label="Layer 33 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer33_ffn2_2 -> layer33_ffn_ar
	layer33_ffn2_3 -> layer33_ffn_ar
	layer34_qkv_2 [label="Layer 34 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer34_qkv_3 [label="Layer 34 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer33_ffn_ar -> layer34_qkv_2
	layer33_ffn_ar -> layer34_qkv_3
	layer34_attn_2 [label="Layer 34 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer34_attn_3 [label="Layer 34 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer34_qkv_2 -> layer34_attn_2
	layer34_qkv_3 -> layer34_attn_3
	layer34_attn_out_2 [label="Layer 34 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer34_attn_out_3 [label="Layer 34 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer34_attn_2 -> layer34_attn_out_2
	layer34_attn_3 -> layer34_attn_out_3
	layer34_attn_ar [label="Layer 34 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer34_attn_out_2 -> layer34_attn_ar
	layer34_attn_out_3 -> layer34_attn_ar
	layer34_ffn1_2 [label="Layer 34 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer34_ffn1_3 [label="Layer 34 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer34_attn_ar -> layer34_ffn1_2
	layer34_attn_ar -> layer34_ffn1_3
	layer34_act_2 [label="Layer 34 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer34_act_3 [label="Layer 34 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer34_ffn1_2 -> layer34_act_2
	layer34_ffn1_3 -> layer34_act_3
	layer34_ffn2_2 [label="Layer 34 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer34_ffn2_3 [label="Layer 34 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer34_act_2 -> layer34_ffn2_2
	layer34_act_3 -> layer34_ffn2_3
	layer34_ffn_ar [label="Layer 34 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer34_ffn2_2 -> layer34_ffn_ar
	layer34_ffn2_3 -> layer34_ffn_ar
	layer35_qkv_2 [label="Layer 35 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer35_qkv_3 [label="Layer 35 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer34_ffn_ar -> layer35_qkv_2
	layer34_ffn_ar -> layer35_qkv_3
	layer35_attn_2 [label="Layer 35 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer35_attn_3 [label="Layer 35 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer35_qkv_2 -> layer35_attn_2
	layer35_qkv_3 -> layer35_attn_3
	layer35_attn_out_2 [label="Layer 35 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer35_attn_out_3 [label="Layer 35 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer35_attn_2 -> layer35_attn_out_2
	layer35_attn_3 -> layer35_attn_out_3
	layer35_attn_ar [label="Layer 35 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer35_attn_out_2 -> layer35_attn_ar
	layer35_attn_out_3 -> layer35_attn_ar
	layer35_ffn1_2 [label="Layer 35 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer35_ffn1_3 [label="Layer 35 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer35_attn_ar -> layer35_ffn1_2
	layer35_attn_ar -> layer35_ffn1_3
	layer35_act_2 [label="Layer 35 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer35_act_3 [label="Layer 35 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer35_ffn1_2 -> layer35_act_2
	layer35_ffn1_3 -> layer35_act_3
	layer35_ffn2_2 [label="Layer 35 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer35_ffn2_3 [label="Layer 35 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer35_act_2 -> layer35_ffn2_2
	layer35_act_3 -> layer35_ffn2_3
	layer35_ffn_ar [label="Layer 35 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer35_ffn2_2 -> layer35_ffn_ar
	layer35_ffn2_3 -> layer35_ffn_ar
	layer36_qkv_2 [label="Layer 36 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer36_qkv_3 [label="Layer 36 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer35_ffn_ar -> layer36_qkv_2
	layer35_ffn_ar -> layer36_qkv_3
	layer36_attn_2 [label="Layer 36 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer36_attn_3 [label="Layer 36 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer36_qkv_2 -> layer36_attn_2
	layer36_qkv_3 -> layer36_attn_3
	layer36_attn_out_2 [label="Layer 36 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer36_attn_out_3 [label="Layer 36 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer36_attn_2 -> layer36_attn_out_2
	layer36_attn_3 -> layer36_attn_out_3
	layer36_attn_ar [label="Layer 36 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer36_attn_out_2 -> layer36_attn_ar
	layer36_attn_out_3 -> layer36_attn_ar
	layer36_ffn1_2 [label="Layer 36 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer36_ffn1_3 [label="Layer 36 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer36_attn_ar -> layer36_ffn1_2
	layer36_attn_ar -> layer36_ffn1_3
	layer36_act_2 [label="Layer 36 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer36_act_3 [label="Layer 36 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer36_ffn1_2 -> layer36_act_2
	layer36_ffn1_3 -> layer36_act_3
	layer36_ffn2_2 [label="Layer 36 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer36_ffn2_3 [label="Layer 36 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer36_act_2 -> layer36_ffn2_2
	layer36_act_3 -> layer36_ffn2_3
	layer36_ffn_ar [label="Layer 36 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer36_ffn2_2 -> layer36_ffn_ar
	layer36_ffn2_3 -> layer36_ffn_ar
	layer37_qkv_2 [label="Layer 37 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer37_qkv_3 [label="Layer 37 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer36_ffn_ar -> layer37_qkv_2
	layer36_ffn_ar -> layer37_qkv_3
	layer37_attn_2 [label="Layer 37 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer37_attn_3 [label="Layer 37 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer37_qkv_2 -> layer37_attn_2
	layer37_qkv_3 -> layer37_attn_3
	layer37_attn_out_2 [label="Layer 37 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer37_attn_out_3 [label="Layer 37 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer37_attn_2 -> layer37_attn_out_2
	layer37_attn_3 -> layer37_attn_out_3
	layer37_attn_ar [label="Layer 37 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer37_attn_out_2 -> layer37_attn_ar
	layer37_attn_out_3 -> layer37_attn_ar
	layer37_ffn1_2 [label="Layer 37 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer37_ffn1_3 [label="Layer 37 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer37_attn_ar -> layer37_ffn1_2
	layer37_attn_ar -> layer37_ffn1_3
	layer37_act_2 [label="Layer 37 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer37_act_3 [label="Layer 37 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer37_ffn1_2 -> layer37_act_2
	layer37_ffn1_3 -> layer37_act_3
	layer37_ffn2_2 [label="Layer 37 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer37_ffn2_3 [label="Layer 37 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer37_act_2 -> layer37_ffn2_2
	layer37_act_3 -> layer37_ffn2_3
	layer37_ffn_ar [label="Layer 37 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer37_ffn2_2 -> layer37_ffn_ar
	layer37_ffn2_3 -> layer37_ffn_ar
	layer38_qkv_2 [label="Layer 38 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer38_qkv_3 [label="Layer 38 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer37_ffn_ar -> layer38_qkv_2
	layer37_ffn_ar -> layer38_qkv_3
	layer38_attn_2 [label="Layer 38 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer38_attn_3 [label="Layer 38 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer38_qkv_2 -> layer38_attn_2
	layer38_qkv_3 -> layer38_attn_3
	layer38_attn_out_2 [label="Layer 38 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer38_attn_out_3 [label="Layer 38 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer38_attn_2 -> layer38_attn_out_2
	layer38_attn_3 -> layer38_attn_out_3
	layer38_attn_ar [label="Layer 38 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer38_attn_out_2 -> layer38_attn_ar
	layer38_attn_out_3 -> layer38_attn_ar
	layer38_ffn1_2 [label="Layer 38 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer38_ffn1_3 [label="Layer 38 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer38_attn_ar -> layer38_ffn1_2
	layer38_attn_ar -> layer38_ffn1_3
	layer38_act_2 [label="Layer 38 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer38_act_3 [label="Layer 38 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer38_ffn1_2 -> layer38_act_2
	layer38_ffn1_3 -> layer38_act_3
	layer38_ffn2_2 [label="Layer 38 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer38_ffn2_3 [label="Layer 38 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer38_act_2 -> layer38_ffn2_2
	layer38_act_3 -> layer38_ffn2_3
	layer38_ffn_ar [label="Layer 38 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer38_ffn2_2 -> layer38_ffn_ar
	layer38_ffn2_3 -> layer38_ffn_ar
	layer39_qkv_2 [label="Layer 39 QKV Proj GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer39_qkv_3 [label="Layer 39 QKV Proj GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer38_ffn_ar -> layer39_qkv_2
	layer38_ffn_ar -> layer39_qkv_3
	layer39_attn_2 [label="Layer 39 Attention GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer39_attn_3 [label="Layer 39 Attention GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer39_qkv_2 -> layer39_attn_2
	layer39_qkv_3 -> layer39_attn_3
	layer39_attn_out_2 [label="Layer 39 Attn Out GPU2\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer39_attn_out_3 [label="Layer 39 Attn Out GPU3\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer39_attn_2 -> layer39_attn_out_2
	layer39_attn_3 -> layer39_attn_out_3
	layer39_attn_ar [label="Layer 39 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer39_attn_out_2 -> layer39_attn_ar
	layer39_attn_out_3 -> layer39_attn_ar
	layer39_ffn1_2 [label="Layer 39 FFN1 GPU2\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer39_ffn1_3 [label="Layer 39 FFN1 GPU3\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer39_attn_ar -> layer39_ffn1_2
	layer39_attn_ar -> layer39_ffn1_3
	layer39_act_2 [label="Layer 39 SiLU GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer39_act_3 [label="Layer 39 SiLU GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer39_ffn1_2 -> layer39_act_2
	layer39_ffn1_3 -> layer39_act_3
	layer39_ffn2_2 [label="Layer 39 FFN2 GPU2\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer39_ffn2_3 [label="Layer 39 FFN2 GPU3\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer39_act_2 -> layer39_ffn2_2
	layer39_act_3 -> layer39_ffn2_3
	layer39_ffn_ar [label="Layer 39 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer39_ffn2_2 -> layer39_ffn_ar
	layer39_ffn2_3 -> layer39_ffn_ar
	stage1_to_stage2 [label="Pipeline Forward\nStage 1  Stage 2\n[PP Forward]" fillcolor="#FFE6E6" shape=parallelogram]
	layer39_ffn_ar -> stage1_to_stage2
	layer40_qkv_4 [label="Layer 40 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer40_qkv_5 [label="Layer 40 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	stage1_to_stage2 -> layer40_qkv_4
	stage1_to_stage2 -> layer40_qkv_5
	layer40_attn_4 [label="Layer 40 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer40_attn_5 [label="Layer 40 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer40_qkv_4 -> layer40_attn_4
	layer40_qkv_5 -> layer40_attn_5
	layer40_attn_out_4 [label="Layer 40 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer40_attn_out_5 [label="Layer 40 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer40_attn_4 -> layer40_attn_out_4
	layer40_attn_5 -> layer40_attn_out_5
	layer40_attn_ar [label="Layer 40 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer40_attn_out_4 -> layer40_attn_ar
	layer40_attn_out_5 -> layer40_attn_ar
	layer40_ffn1_4 [label="Layer 40 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer40_ffn1_5 [label="Layer 40 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer40_attn_ar -> layer40_ffn1_4
	layer40_attn_ar -> layer40_ffn1_5
	layer40_act_4 [label="Layer 40 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer40_act_5 [label="Layer 40 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer40_ffn1_4 -> layer40_act_4
	layer40_ffn1_5 -> layer40_act_5
	layer40_ffn2_4 [label="Layer 40 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer40_ffn2_5 [label="Layer 40 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer40_act_4 -> layer40_ffn2_4
	layer40_act_5 -> layer40_ffn2_5
	layer40_ffn_ar [label="Layer 40 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer40_ffn2_4 -> layer40_ffn_ar
	layer40_ffn2_5 -> layer40_ffn_ar
	layer41_qkv_4 [label="Layer 41 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer41_qkv_5 [label="Layer 41 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer40_ffn_ar -> layer41_qkv_4
	layer40_ffn_ar -> layer41_qkv_5
	layer41_attn_4 [label="Layer 41 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer41_attn_5 [label="Layer 41 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer41_qkv_4 -> layer41_attn_4
	layer41_qkv_5 -> layer41_attn_5
	layer41_attn_out_4 [label="Layer 41 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer41_attn_out_5 [label="Layer 41 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer41_attn_4 -> layer41_attn_out_4
	layer41_attn_5 -> layer41_attn_out_5
	layer41_attn_ar [label="Layer 41 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer41_attn_out_4 -> layer41_attn_ar
	layer41_attn_out_5 -> layer41_attn_ar
	layer41_ffn1_4 [label="Layer 41 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer41_ffn1_5 [label="Layer 41 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer41_attn_ar -> layer41_ffn1_4
	layer41_attn_ar -> layer41_ffn1_5
	layer41_act_4 [label="Layer 41 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer41_act_5 [label="Layer 41 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer41_ffn1_4 -> layer41_act_4
	layer41_ffn1_5 -> layer41_act_5
	layer41_ffn2_4 [label="Layer 41 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer41_ffn2_5 [label="Layer 41 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer41_act_4 -> layer41_ffn2_4
	layer41_act_5 -> layer41_ffn2_5
	layer41_ffn_ar [label="Layer 41 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer41_ffn2_4 -> layer41_ffn_ar
	layer41_ffn2_5 -> layer41_ffn_ar
	layer42_qkv_4 [label="Layer 42 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer42_qkv_5 [label="Layer 42 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer41_ffn_ar -> layer42_qkv_4
	layer41_ffn_ar -> layer42_qkv_5
	layer42_attn_4 [label="Layer 42 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer42_attn_5 [label="Layer 42 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer42_qkv_4 -> layer42_attn_4
	layer42_qkv_5 -> layer42_attn_5
	layer42_attn_out_4 [label="Layer 42 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer42_attn_out_5 [label="Layer 42 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer42_attn_4 -> layer42_attn_out_4
	layer42_attn_5 -> layer42_attn_out_5
	layer42_attn_ar [label="Layer 42 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer42_attn_out_4 -> layer42_attn_ar
	layer42_attn_out_5 -> layer42_attn_ar
	layer42_ffn1_4 [label="Layer 42 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer42_ffn1_5 [label="Layer 42 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer42_attn_ar -> layer42_ffn1_4
	layer42_attn_ar -> layer42_ffn1_5
	layer42_act_4 [label="Layer 42 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer42_act_5 [label="Layer 42 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer42_ffn1_4 -> layer42_act_4
	layer42_ffn1_5 -> layer42_act_5
	layer42_ffn2_4 [label="Layer 42 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer42_ffn2_5 [label="Layer 42 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer42_act_4 -> layer42_ffn2_4
	layer42_act_5 -> layer42_ffn2_5
	layer42_ffn_ar [label="Layer 42 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer42_ffn2_4 -> layer42_ffn_ar
	layer42_ffn2_5 -> layer42_ffn_ar
	layer43_qkv_4 [label="Layer 43 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer43_qkv_5 [label="Layer 43 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer42_ffn_ar -> layer43_qkv_4
	layer42_ffn_ar -> layer43_qkv_5
	layer43_attn_4 [label="Layer 43 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer43_attn_5 [label="Layer 43 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer43_qkv_4 -> layer43_attn_4
	layer43_qkv_5 -> layer43_attn_5
	layer43_attn_out_4 [label="Layer 43 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer43_attn_out_5 [label="Layer 43 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer43_attn_4 -> layer43_attn_out_4
	layer43_attn_5 -> layer43_attn_out_5
	layer43_attn_ar [label="Layer 43 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer43_attn_out_4 -> layer43_attn_ar
	layer43_attn_out_5 -> layer43_attn_ar
	layer43_ffn1_4 [label="Layer 43 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer43_ffn1_5 [label="Layer 43 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer43_attn_ar -> layer43_ffn1_4
	layer43_attn_ar -> layer43_ffn1_5
	layer43_act_4 [label="Layer 43 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer43_act_5 [label="Layer 43 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer43_ffn1_4 -> layer43_act_4
	layer43_ffn1_5 -> layer43_act_5
	layer43_ffn2_4 [label="Layer 43 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer43_ffn2_5 [label="Layer 43 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer43_act_4 -> layer43_ffn2_4
	layer43_act_5 -> layer43_ffn2_5
	layer43_ffn_ar [label="Layer 43 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer43_ffn2_4 -> layer43_ffn_ar
	layer43_ffn2_5 -> layer43_ffn_ar
	layer44_qkv_4 [label="Layer 44 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer44_qkv_5 [label="Layer 44 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer43_ffn_ar -> layer44_qkv_4
	layer43_ffn_ar -> layer44_qkv_5
	layer44_attn_4 [label="Layer 44 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer44_attn_5 [label="Layer 44 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer44_qkv_4 -> layer44_attn_4
	layer44_qkv_5 -> layer44_attn_5
	layer44_attn_out_4 [label="Layer 44 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer44_attn_out_5 [label="Layer 44 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer44_attn_4 -> layer44_attn_out_4
	layer44_attn_5 -> layer44_attn_out_5
	layer44_attn_ar [label="Layer 44 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer44_attn_out_4 -> layer44_attn_ar
	layer44_attn_out_5 -> layer44_attn_ar
	layer44_ffn1_4 [label="Layer 44 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer44_ffn1_5 [label="Layer 44 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer44_attn_ar -> layer44_ffn1_4
	layer44_attn_ar -> layer44_ffn1_5
	layer44_act_4 [label="Layer 44 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer44_act_5 [label="Layer 44 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer44_ffn1_4 -> layer44_act_4
	layer44_ffn1_5 -> layer44_act_5
	layer44_ffn2_4 [label="Layer 44 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer44_ffn2_5 [label="Layer 44 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer44_act_4 -> layer44_ffn2_4
	layer44_act_5 -> layer44_ffn2_5
	layer44_ffn_ar [label="Layer 44 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer44_ffn2_4 -> layer44_ffn_ar
	layer44_ffn2_5 -> layer44_ffn_ar
	layer45_qkv_4 [label="Layer 45 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer45_qkv_5 [label="Layer 45 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer44_ffn_ar -> layer45_qkv_4
	layer44_ffn_ar -> layer45_qkv_5
	layer45_attn_4 [label="Layer 45 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer45_attn_5 [label="Layer 45 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer45_qkv_4 -> layer45_attn_4
	layer45_qkv_5 -> layer45_attn_5
	layer45_attn_out_4 [label="Layer 45 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer45_attn_out_5 [label="Layer 45 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer45_attn_4 -> layer45_attn_out_4
	layer45_attn_5 -> layer45_attn_out_5
	layer45_attn_ar [label="Layer 45 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer45_attn_out_4 -> layer45_attn_ar
	layer45_attn_out_5 -> layer45_attn_ar
	layer45_ffn1_4 [label="Layer 45 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer45_ffn1_5 [label="Layer 45 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer45_attn_ar -> layer45_ffn1_4
	layer45_attn_ar -> layer45_ffn1_5
	layer45_act_4 [label="Layer 45 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer45_act_5 [label="Layer 45 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer45_ffn1_4 -> layer45_act_4
	layer45_ffn1_5 -> layer45_act_5
	layer45_ffn2_4 [label="Layer 45 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer45_ffn2_5 [label="Layer 45 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer45_act_4 -> layer45_ffn2_4
	layer45_act_5 -> layer45_ffn2_5
	layer45_ffn_ar [label="Layer 45 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer45_ffn2_4 -> layer45_ffn_ar
	layer45_ffn2_5 -> layer45_ffn_ar
	layer46_qkv_4 [label="Layer 46 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer46_qkv_5 [label="Layer 46 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer45_ffn_ar -> layer46_qkv_4
	layer45_ffn_ar -> layer46_qkv_5
	layer46_attn_4 [label="Layer 46 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer46_attn_5 [label="Layer 46 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer46_qkv_4 -> layer46_attn_4
	layer46_qkv_5 -> layer46_attn_5
	layer46_attn_out_4 [label="Layer 46 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer46_attn_out_5 [label="Layer 46 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer46_attn_4 -> layer46_attn_out_4
	layer46_attn_5 -> layer46_attn_out_5
	layer46_attn_ar [label="Layer 46 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer46_attn_out_4 -> layer46_attn_ar
	layer46_attn_out_5 -> layer46_attn_ar
	layer46_ffn1_4 [label="Layer 46 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer46_ffn1_5 [label="Layer 46 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer46_attn_ar -> layer46_ffn1_4
	layer46_attn_ar -> layer46_ffn1_5
	layer46_act_4 [label="Layer 46 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer46_act_5 [label="Layer 46 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer46_ffn1_4 -> layer46_act_4
	layer46_ffn1_5 -> layer46_act_5
	layer46_ffn2_4 [label="Layer 46 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer46_ffn2_5 [label="Layer 46 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer46_act_4 -> layer46_ffn2_4
	layer46_act_5 -> layer46_ffn2_5
	layer46_ffn_ar [label="Layer 46 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer46_ffn2_4 -> layer46_ffn_ar
	layer46_ffn2_5 -> layer46_ffn_ar
	layer47_qkv_4 [label="Layer 47 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer47_qkv_5 [label="Layer 47 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer46_ffn_ar -> layer47_qkv_4
	layer46_ffn_ar -> layer47_qkv_5
	layer47_attn_4 [label="Layer 47 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer47_attn_5 [label="Layer 47 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer47_qkv_4 -> layer47_attn_4
	layer47_qkv_5 -> layer47_attn_5
	layer47_attn_out_4 [label="Layer 47 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer47_attn_out_5 [label="Layer 47 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer47_attn_4 -> layer47_attn_out_4
	layer47_attn_5 -> layer47_attn_out_5
	layer47_attn_ar [label="Layer 47 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer47_attn_out_4 -> layer47_attn_ar
	layer47_attn_out_5 -> layer47_attn_ar
	layer47_ffn1_4 [label="Layer 47 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer47_ffn1_5 [label="Layer 47 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer47_attn_ar -> layer47_ffn1_4
	layer47_attn_ar -> layer47_ffn1_5
	layer47_act_4 [label="Layer 47 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer47_act_5 [label="Layer 47 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer47_ffn1_4 -> layer47_act_4
	layer47_ffn1_5 -> layer47_act_5
	layer47_ffn2_4 [label="Layer 47 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer47_ffn2_5 [label="Layer 47 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer47_act_4 -> layer47_ffn2_4
	layer47_act_5 -> layer47_ffn2_5
	layer47_ffn_ar [label="Layer 47 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer47_ffn2_4 -> layer47_ffn_ar
	layer47_ffn2_5 -> layer47_ffn_ar
	layer48_qkv_4 [label="Layer 48 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer48_qkv_5 [label="Layer 48 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer47_ffn_ar -> layer48_qkv_4
	layer47_ffn_ar -> layer48_qkv_5
	layer48_attn_4 [label="Layer 48 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer48_attn_5 [label="Layer 48 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer48_qkv_4 -> layer48_attn_4
	layer48_qkv_5 -> layer48_attn_5
	layer48_attn_out_4 [label="Layer 48 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer48_attn_out_5 [label="Layer 48 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer48_attn_4 -> layer48_attn_out_4
	layer48_attn_5 -> layer48_attn_out_5
	layer48_attn_ar [label="Layer 48 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer48_attn_out_4 -> layer48_attn_ar
	layer48_attn_out_5 -> layer48_attn_ar
	layer48_ffn1_4 [label="Layer 48 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer48_ffn1_5 [label="Layer 48 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer48_attn_ar -> layer48_ffn1_4
	layer48_attn_ar -> layer48_ffn1_5
	layer48_act_4 [label="Layer 48 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer48_act_5 [label="Layer 48 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer48_ffn1_4 -> layer48_act_4
	layer48_ffn1_5 -> layer48_act_5
	layer48_ffn2_4 [label="Layer 48 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer48_ffn2_5 [label="Layer 48 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer48_act_4 -> layer48_ffn2_4
	layer48_act_5 -> layer48_ffn2_5
	layer48_ffn_ar [label="Layer 48 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer48_ffn2_4 -> layer48_ffn_ar
	layer48_ffn2_5 -> layer48_ffn_ar
	layer49_qkv_4 [label="Layer 49 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer49_qkv_5 [label="Layer 49 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer48_ffn_ar -> layer49_qkv_4
	layer48_ffn_ar -> layer49_qkv_5
	layer49_attn_4 [label="Layer 49 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer49_attn_5 [label="Layer 49 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer49_qkv_4 -> layer49_attn_4
	layer49_qkv_5 -> layer49_attn_5
	layer49_attn_out_4 [label="Layer 49 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer49_attn_out_5 [label="Layer 49 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer49_attn_4 -> layer49_attn_out_4
	layer49_attn_5 -> layer49_attn_out_5
	layer49_attn_ar [label="Layer 49 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer49_attn_out_4 -> layer49_attn_ar
	layer49_attn_out_5 -> layer49_attn_ar
	layer49_ffn1_4 [label="Layer 49 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer49_ffn1_5 [label="Layer 49 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer49_attn_ar -> layer49_ffn1_4
	layer49_attn_ar -> layer49_ffn1_5
	layer49_act_4 [label="Layer 49 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer49_act_5 [label="Layer 49 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer49_ffn1_4 -> layer49_act_4
	layer49_ffn1_5 -> layer49_act_5
	layer49_ffn2_4 [label="Layer 49 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer49_ffn2_5 [label="Layer 49 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer49_act_4 -> layer49_ffn2_4
	layer49_act_5 -> layer49_ffn2_5
	layer49_ffn_ar [label="Layer 49 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer49_ffn2_4 -> layer49_ffn_ar
	layer49_ffn2_5 -> layer49_ffn_ar
	layer50_qkv_4 [label="Layer 50 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer50_qkv_5 [label="Layer 50 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer49_ffn_ar -> layer50_qkv_4
	layer49_ffn_ar -> layer50_qkv_5
	layer50_attn_4 [label="Layer 50 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer50_attn_5 [label="Layer 50 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer50_qkv_4 -> layer50_attn_4
	layer50_qkv_5 -> layer50_attn_5
	layer50_attn_out_4 [label="Layer 50 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer50_attn_out_5 [label="Layer 50 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer50_attn_4 -> layer50_attn_out_4
	layer50_attn_5 -> layer50_attn_out_5
	layer50_attn_ar [label="Layer 50 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer50_attn_out_4 -> layer50_attn_ar
	layer50_attn_out_5 -> layer50_attn_ar
	layer50_ffn1_4 [label="Layer 50 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer50_ffn1_5 [label="Layer 50 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer50_attn_ar -> layer50_ffn1_4
	layer50_attn_ar -> layer50_ffn1_5
	layer50_act_4 [label="Layer 50 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer50_act_5 [label="Layer 50 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer50_ffn1_4 -> layer50_act_4
	layer50_ffn1_5 -> layer50_act_5
	layer50_ffn2_4 [label="Layer 50 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer50_ffn2_5 [label="Layer 50 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer50_act_4 -> layer50_ffn2_4
	layer50_act_5 -> layer50_ffn2_5
	layer50_ffn_ar [label="Layer 50 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer50_ffn2_4 -> layer50_ffn_ar
	layer50_ffn2_5 -> layer50_ffn_ar
	layer51_qkv_4 [label="Layer 51 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer51_qkv_5 [label="Layer 51 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer50_ffn_ar -> layer51_qkv_4
	layer50_ffn_ar -> layer51_qkv_5
	layer51_attn_4 [label="Layer 51 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer51_attn_5 [label="Layer 51 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer51_qkv_4 -> layer51_attn_4
	layer51_qkv_5 -> layer51_attn_5
	layer51_attn_out_4 [label="Layer 51 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer51_attn_out_5 [label="Layer 51 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer51_attn_4 -> layer51_attn_out_4
	layer51_attn_5 -> layer51_attn_out_5
	layer51_attn_ar [label="Layer 51 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer51_attn_out_4 -> layer51_attn_ar
	layer51_attn_out_5 -> layer51_attn_ar
	layer51_ffn1_4 [label="Layer 51 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer51_ffn1_5 [label="Layer 51 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer51_attn_ar -> layer51_ffn1_4
	layer51_attn_ar -> layer51_ffn1_5
	layer51_act_4 [label="Layer 51 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer51_act_5 [label="Layer 51 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer51_ffn1_4 -> layer51_act_4
	layer51_ffn1_5 -> layer51_act_5
	layer51_ffn2_4 [label="Layer 51 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer51_ffn2_5 [label="Layer 51 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer51_act_4 -> layer51_ffn2_4
	layer51_act_5 -> layer51_ffn2_5
	layer51_ffn_ar [label="Layer 51 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer51_ffn2_4 -> layer51_ffn_ar
	layer51_ffn2_5 -> layer51_ffn_ar
	layer52_qkv_4 [label="Layer 52 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer52_qkv_5 [label="Layer 52 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer51_ffn_ar -> layer52_qkv_4
	layer51_ffn_ar -> layer52_qkv_5
	layer52_attn_4 [label="Layer 52 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer52_attn_5 [label="Layer 52 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer52_qkv_4 -> layer52_attn_4
	layer52_qkv_5 -> layer52_attn_5
	layer52_attn_out_4 [label="Layer 52 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer52_attn_out_5 [label="Layer 52 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer52_attn_4 -> layer52_attn_out_4
	layer52_attn_5 -> layer52_attn_out_5
	layer52_attn_ar [label="Layer 52 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer52_attn_out_4 -> layer52_attn_ar
	layer52_attn_out_5 -> layer52_attn_ar
	layer52_ffn1_4 [label="Layer 52 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer52_ffn1_5 [label="Layer 52 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer52_attn_ar -> layer52_ffn1_4
	layer52_attn_ar -> layer52_ffn1_5
	layer52_act_4 [label="Layer 52 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer52_act_5 [label="Layer 52 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer52_ffn1_4 -> layer52_act_4
	layer52_ffn1_5 -> layer52_act_5
	layer52_ffn2_4 [label="Layer 52 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer52_ffn2_5 [label="Layer 52 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer52_act_4 -> layer52_ffn2_4
	layer52_act_5 -> layer52_ffn2_5
	layer52_ffn_ar [label="Layer 52 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer52_ffn2_4 -> layer52_ffn_ar
	layer52_ffn2_5 -> layer52_ffn_ar
	layer53_qkv_4 [label="Layer 53 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer53_qkv_5 [label="Layer 53 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer52_ffn_ar -> layer53_qkv_4
	layer52_ffn_ar -> layer53_qkv_5
	layer53_attn_4 [label="Layer 53 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer53_attn_5 [label="Layer 53 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer53_qkv_4 -> layer53_attn_4
	layer53_qkv_5 -> layer53_attn_5
	layer53_attn_out_4 [label="Layer 53 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer53_attn_out_5 [label="Layer 53 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer53_attn_4 -> layer53_attn_out_4
	layer53_attn_5 -> layer53_attn_out_5
	layer53_attn_ar [label="Layer 53 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer53_attn_out_4 -> layer53_attn_ar
	layer53_attn_out_5 -> layer53_attn_ar
	layer53_ffn1_4 [label="Layer 53 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer53_ffn1_5 [label="Layer 53 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer53_attn_ar -> layer53_ffn1_4
	layer53_attn_ar -> layer53_ffn1_5
	layer53_act_4 [label="Layer 53 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer53_act_5 [label="Layer 53 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer53_ffn1_4 -> layer53_act_4
	layer53_ffn1_5 -> layer53_act_5
	layer53_ffn2_4 [label="Layer 53 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer53_ffn2_5 [label="Layer 53 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer53_act_4 -> layer53_ffn2_4
	layer53_act_5 -> layer53_ffn2_5
	layer53_ffn_ar [label="Layer 53 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer53_ffn2_4 -> layer53_ffn_ar
	layer53_ffn2_5 -> layer53_ffn_ar
	layer54_qkv_4 [label="Layer 54 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer54_qkv_5 [label="Layer 54 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer53_ffn_ar -> layer54_qkv_4
	layer53_ffn_ar -> layer54_qkv_5
	layer54_attn_4 [label="Layer 54 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer54_attn_5 [label="Layer 54 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer54_qkv_4 -> layer54_attn_4
	layer54_qkv_5 -> layer54_attn_5
	layer54_attn_out_4 [label="Layer 54 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer54_attn_out_5 [label="Layer 54 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer54_attn_4 -> layer54_attn_out_4
	layer54_attn_5 -> layer54_attn_out_5
	layer54_attn_ar [label="Layer 54 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer54_attn_out_4 -> layer54_attn_ar
	layer54_attn_out_5 -> layer54_attn_ar
	layer54_ffn1_4 [label="Layer 54 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer54_ffn1_5 [label="Layer 54 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer54_attn_ar -> layer54_ffn1_4
	layer54_attn_ar -> layer54_ffn1_5
	layer54_act_4 [label="Layer 54 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer54_act_5 [label="Layer 54 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer54_ffn1_4 -> layer54_act_4
	layer54_ffn1_5 -> layer54_act_5
	layer54_ffn2_4 [label="Layer 54 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer54_ffn2_5 [label="Layer 54 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer54_act_4 -> layer54_ffn2_4
	layer54_act_5 -> layer54_ffn2_5
	layer54_ffn_ar [label="Layer 54 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer54_ffn2_4 -> layer54_ffn_ar
	layer54_ffn2_5 -> layer54_ffn_ar
	layer55_qkv_4 [label="Layer 55 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer55_qkv_5 [label="Layer 55 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer54_ffn_ar -> layer55_qkv_4
	layer54_ffn_ar -> layer55_qkv_5
	layer55_attn_4 [label="Layer 55 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer55_attn_5 [label="Layer 55 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer55_qkv_4 -> layer55_attn_4
	layer55_qkv_5 -> layer55_attn_5
	layer55_attn_out_4 [label="Layer 55 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer55_attn_out_5 [label="Layer 55 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer55_attn_4 -> layer55_attn_out_4
	layer55_attn_5 -> layer55_attn_out_5
	layer55_attn_ar [label="Layer 55 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer55_attn_out_4 -> layer55_attn_ar
	layer55_attn_out_5 -> layer55_attn_ar
	layer55_ffn1_4 [label="Layer 55 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer55_ffn1_5 [label="Layer 55 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer55_attn_ar -> layer55_ffn1_4
	layer55_attn_ar -> layer55_ffn1_5
	layer55_act_4 [label="Layer 55 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer55_act_5 [label="Layer 55 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer55_ffn1_4 -> layer55_act_4
	layer55_ffn1_5 -> layer55_act_5
	layer55_ffn2_4 [label="Layer 55 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer55_ffn2_5 [label="Layer 55 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer55_act_4 -> layer55_ffn2_4
	layer55_act_5 -> layer55_ffn2_5
	layer55_ffn_ar [label="Layer 55 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer55_ffn2_4 -> layer55_ffn_ar
	layer55_ffn2_5 -> layer55_ffn_ar
	layer56_qkv_4 [label="Layer 56 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer56_qkv_5 [label="Layer 56 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer55_ffn_ar -> layer56_qkv_4
	layer55_ffn_ar -> layer56_qkv_5
	layer56_attn_4 [label="Layer 56 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer56_attn_5 [label="Layer 56 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer56_qkv_4 -> layer56_attn_4
	layer56_qkv_5 -> layer56_attn_5
	layer56_attn_out_4 [label="Layer 56 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer56_attn_out_5 [label="Layer 56 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer56_attn_4 -> layer56_attn_out_4
	layer56_attn_5 -> layer56_attn_out_5
	layer56_attn_ar [label="Layer 56 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer56_attn_out_4 -> layer56_attn_ar
	layer56_attn_out_5 -> layer56_attn_ar
	layer56_ffn1_4 [label="Layer 56 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer56_ffn1_5 [label="Layer 56 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer56_attn_ar -> layer56_ffn1_4
	layer56_attn_ar -> layer56_ffn1_5
	layer56_act_4 [label="Layer 56 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer56_act_5 [label="Layer 56 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer56_ffn1_4 -> layer56_act_4
	layer56_ffn1_5 -> layer56_act_5
	layer56_ffn2_4 [label="Layer 56 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer56_ffn2_5 [label="Layer 56 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer56_act_4 -> layer56_ffn2_4
	layer56_act_5 -> layer56_ffn2_5
	layer56_ffn_ar [label="Layer 56 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer56_ffn2_4 -> layer56_ffn_ar
	layer56_ffn2_5 -> layer56_ffn_ar
	layer57_qkv_4 [label="Layer 57 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer57_qkv_5 [label="Layer 57 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer56_ffn_ar -> layer57_qkv_4
	layer56_ffn_ar -> layer57_qkv_5
	layer57_attn_4 [label="Layer 57 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer57_attn_5 [label="Layer 57 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer57_qkv_4 -> layer57_attn_4
	layer57_qkv_5 -> layer57_attn_5
	layer57_attn_out_4 [label="Layer 57 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer57_attn_out_5 [label="Layer 57 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer57_attn_4 -> layer57_attn_out_4
	layer57_attn_5 -> layer57_attn_out_5
	layer57_attn_ar [label="Layer 57 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer57_attn_out_4 -> layer57_attn_ar
	layer57_attn_out_5 -> layer57_attn_ar
	layer57_ffn1_4 [label="Layer 57 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer57_ffn1_5 [label="Layer 57 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer57_attn_ar -> layer57_ffn1_4
	layer57_attn_ar -> layer57_ffn1_5
	layer57_act_4 [label="Layer 57 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer57_act_5 [label="Layer 57 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer57_ffn1_4 -> layer57_act_4
	layer57_ffn1_5 -> layer57_act_5
	layer57_ffn2_4 [label="Layer 57 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer57_ffn2_5 [label="Layer 57 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer57_act_4 -> layer57_ffn2_4
	layer57_act_5 -> layer57_ffn2_5
	layer57_ffn_ar [label="Layer 57 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer57_ffn2_4 -> layer57_ffn_ar
	layer57_ffn2_5 -> layer57_ffn_ar
	layer58_qkv_4 [label="Layer 58 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer58_qkv_5 [label="Layer 58 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer57_ffn_ar -> layer58_qkv_4
	layer57_ffn_ar -> layer58_qkv_5
	layer58_attn_4 [label="Layer 58 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer58_attn_5 [label="Layer 58 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer58_qkv_4 -> layer58_attn_4
	layer58_qkv_5 -> layer58_attn_5
	layer58_attn_out_4 [label="Layer 58 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer58_attn_out_5 [label="Layer 58 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer58_attn_4 -> layer58_attn_out_4
	layer58_attn_5 -> layer58_attn_out_5
	layer58_attn_ar [label="Layer 58 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer58_attn_out_4 -> layer58_attn_ar
	layer58_attn_out_5 -> layer58_attn_ar
	layer58_ffn1_4 [label="Layer 58 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer58_ffn1_5 [label="Layer 58 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer58_attn_ar -> layer58_ffn1_4
	layer58_attn_ar -> layer58_ffn1_5
	layer58_act_4 [label="Layer 58 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer58_act_5 [label="Layer 58 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer58_ffn1_4 -> layer58_act_4
	layer58_ffn1_5 -> layer58_act_5
	layer58_ffn2_4 [label="Layer 58 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer58_ffn2_5 [label="Layer 58 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer58_act_4 -> layer58_ffn2_4
	layer58_act_5 -> layer58_ffn2_5
	layer58_ffn_ar [label="Layer 58 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer58_ffn2_4 -> layer58_ffn_ar
	layer58_ffn2_5 -> layer58_ffn_ar
	layer59_qkv_4 [label="Layer 59 QKV Proj GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer59_qkv_5 [label="Layer 59 QKV Proj GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer58_ffn_ar -> layer59_qkv_4
	layer58_ffn_ar -> layer59_qkv_5
	layer59_attn_4 [label="Layer 59 Attention GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer59_attn_5 [label="Layer 59 Attention GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer59_qkv_4 -> layer59_attn_4
	layer59_qkv_5 -> layer59_attn_5
	layer59_attn_out_4 [label="Layer 59 Attn Out GPU4\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer59_attn_out_5 [label="Layer 59 Attn Out GPU5\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer59_attn_4 -> layer59_attn_out_4
	layer59_attn_5 -> layer59_attn_out_5
	layer59_attn_ar [label="Layer 59 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer59_attn_out_4 -> layer59_attn_ar
	layer59_attn_out_5 -> layer59_attn_ar
	layer59_ffn1_4 [label="Layer 59 FFN1 GPU4\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer59_ffn1_5 [label="Layer 59 FFN1 GPU5\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer59_attn_ar -> layer59_ffn1_4
	layer59_attn_ar -> layer59_ffn1_5
	layer59_act_4 [label="Layer 59 SiLU GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer59_act_5 [label="Layer 59 SiLU GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer59_ffn1_4 -> layer59_act_4
	layer59_ffn1_5 -> layer59_act_5
	layer59_ffn2_4 [label="Layer 59 FFN2 GPU4\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer59_ffn2_5 [label="Layer 59 FFN2 GPU5\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer59_act_4 -> layer59_ffn2_4
	layer59_act_5 -> layer59_ffn2_5
	layer59_ffn_ar [label="Layer 59 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer59_ffn2_4 -> layer59_ffn_ar
	layer59_ffn2_5 -> layer59_ffn_ar
	stage2_to_stage3 [label="Pipeline Forward\nStage 2  Stage 3\n[PP Forward]" fillcolor="#FFE6E6" shape=parallelogram]
	layer59_ffn_ar -> stage2_to_stage3
	layer60_qkv_6 [label="Layer 60 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer60_qkv_7 [label="Layer 60 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	stage2_to_stage3 -> layer60_qkv_6
	stage2_to_stage3 -> layer60_qkv_7
	layer60_attn_6 [label="Layer 60 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer60_attn_7 [label="Layer 60 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer60_qkv_6 -> layer60_attn_6
	layer60_qkv_7 -> layer60_attn_7
	layer60_attn_out_6 [label="Layer 60 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer60_attn_out_7 [label="Layer 60 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer60_attn_6 -> layer60_attn_out_6
	layer60_attn_7 -> layer60_attn_out_7
	layer60_attn_ar [label="Layer 60 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer60_attn_out_6 -> layer60_attn_ar
	layer60_attn_out_7 -> layer60_attn_ar
	layer60_ffn1_6 [label="Layer 60 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer60_ffn1_7 [label="Layer 60 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer60_attn_ar -> layer60_ffn1_6
	layer60_attn_ar -> layer60_ffn1_7
	layer60_act_6 [label="Layer 60 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer60_act_7 [label="Layer 60 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer60_ffn1_6 -> layer60_act_6
	layer60_ffn1_7 -> layer60_act_7
	layer60_ffn2_6 [label="Layer 60 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer60_ffn2_7 [label="Layer 60 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer60_act_6 -> layer60_ffn2_6
	layer60_act_7 -> layer60_ffn2_7
	layer60_ffn_ar [label="Layer 60 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer60_ffn2_6 -> layer60_ffn_ar
	layer60_ffn2_7 -> layer60_ffn_ar
	layer61_qkv_6 [label="Layer 61 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer61_qkv_7 [label="Layer 61 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer60_ffn_ar -> layer61_qkv_6
	layer60_ffn_ar -> layer61_qkv_7
	layer61_attn_6 [label="Layer 61 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer61_attn_7 [label="Layer 61 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer61_qkv_6 -> layer61_attn_6
	layer61_qkv_7 -> layer61_attn_7
	layer61_attn_out_6 [label="Layer 61 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer61_attn_out_7 [label="Layer 61 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer61_attn_6 -> layer61_attn_out_6
	layer61_attn_7 -> layer61_attn_out_7
	layer61_attn_ar [label="Layer 61 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer61_attn_out_6 -> layer61_attn_ar
	layer61_attn_out_7 -> layer61_attn_ar
	layer61_ffn1_6 [label="Layer 61 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer61_ffn1_7 [label="Layer 61 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer61_attn_ar -> layer61_ffn1_6
	layer61_attn_ar -> layer61_ffn1_7
	layer61_act_6 [label="Layer 61 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer61_act_7 [label="Layer 61 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer61_ffn1_6 -> layer61_act_6
	layer61_ffn1_7 -> layer61_act_7
	layer61_ffn2_6 [label="Layer 61 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer61_ffn2_7 [label="Layer 61 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer61_act_6 -> layer61_ffn2_6
	layer61_act_7 -> layer61_ffn2_7
	layer61_ffn_ar [label="Layer 61 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer61_ffn2_6 -> layer61_ffn_ar
	layer61_ffn2_7 -> layer61_ffn_ar
	layer62_qkv_6 [label="Layer 62 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer62_qkv_7 [label="Layer 62 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer61_ffn_ar -> layer62_qkv_6
	layer61_ffn_ar -> layer62_qkv_7
	layer62_attn_6 [label="Layer 62 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer62_attn_7 [label="Layer 62 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer62_qkv_6 -> layer62_attn_6
	layer62_qkv_7 -> layer62_attn_7
	layer62_attn_out_6 [label="Layer 62 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer62_attn_out_7 [label="Layer 62 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer62_attn_6 -> layer62_attn_out_6
	layer62_attn_7 -> layer62_attn_out_7
	layer62_attn_ar [label="Layer 62 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer62_attn_out_6 -> layer62_attn_ar
	layer62_attn_out_7 -> layer62_attn_ar
	layer62_ffn1_6 [label="Layer 62 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer62_ffn1_7 [label="Layer 62 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer62_attn_ar -> layer62_ffn1_6
	layer62_attn_ar -> layer62_ffn1_7
	layer62_act_6 [label="Layer 62 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer62_act_7 [label="Layer 62 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer62_ffn1_6 -> layer62_act_6
	layer62_ffn1_7 -> layer62_act_7
	layer62_ffn2_6 [label="Layer 62 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer62_ffn2_7 [label="Layer 62 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer62_act_6 -> layer62_ffn2_6
	layer62_act_7 -> layer62_ffn2_7
	layer62_ffn_ar [label="Layer 62 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer62_ffn2_6 -> layer62_ffn_ar
	layer62_ffn2_7 -> layer62_ffn_ar
	layer63_qkv_6 [label="Layer 63 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer63_qkv_7 [label="Layer 63 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer62_ffn_ar -> layer63_qkv_6
	layer62_ffn_ar -> layer63_qkv_7
	layer63_attn_6 [label="Layer 63 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer63_attn_7 [label="Layer 63 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer63_qkv_6 -> layer63_attn_6
	layer63_qkv_7 -> layer63_attn_7
	layer63_attn_out_6 [label="Layer 63 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer63_attn_out_7 [label="Layer 63 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer63_attn_6 -> layer63_attn_out_6
	layer63_attn_7 -> layer63_attn_out_7
	layer63_attn_ar [label="Layer 63 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer63_attn_out_6 -> layer63_attn_ar
	layer63_attn_out_7 -> layer63_attn_ar
	layer63_ffn1_6 [label="Layer 63 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer63_ffn1_7 [label="Layer 63 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer63_attn_ar -> layer63_ffn1_6
	layer63_attn_ar -> layer63_ffn1_7
	layer63_act_6 [label="Layer 63 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer63_act_7 [label="Layer 63 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer63_ffn1_6 -> layer63_act_6
	layer63_ffn1_7 -> layer63_act_7
	layer63_ffn2_6 [label="Layer 63 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer63_ffn2_7 [label="Layer 63 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer63_act_6 -> layer63_ffn2_6
	layer63_act_7 -> layer63_ffn2_7
	layer63_ffn_ar [label="Layer 63 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer63_ffn2_6 -> layer63_ffn_ar
	layer63_ffn2_7 -> layer63_ffn_ar
	layer64_qkv_6 [label="Layer 64 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer64_qkv_7 [label="Layer 64 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer63_ffn_ar -> layer64_qkv_6
	layer63_ffn_ar -> layer64_qkv_7
	layer64_attn_6 [label="Layer 64 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer64_attn_7 [label="Layer 64 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer64_qkv_6 -> layer64_attn_6
	layer64_qkv_7 -> layer64_attn_7
	layer64_attn_out_6 [label="Layer 64 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer64_attn_out_7 [label="Layer 64 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer64_attn_6 -> layer64_attn_out_6
	layer64_attn_7 -> layer64_attn_out_7
	layer64_attn_ar [label="Layer 64 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer64_attn_out_6 -> layer64_attn_ar
	layer64_attn_out_7 -> layer64_attn_ar
	layer64_ffn1_6 [label="Layer 64 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer64_ffn1_7 [label="Layer 64 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer64_attn_ar -> layer64_ffn1_6
	layer64_attn_ar -> layer64_ffn1_7
	layer64_act_6 [label="Layer 64 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer64_act_7 [label="Layer 64 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer64_ffn1_6 -> layer64_act_6
	layer64_ffn1_7 -> layer64_act_7
	layer64_ffn2_6 [label="Layer 64 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer64_ffn2_7 [label="Layer 64 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer64_act_6 -> layer64_ffn2_6
	layer64_act_7 -> layer64_ffn2_7
	layer64_ffn_ar [label="Layer 64 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer64_ffn2_6 -> layer64_ffn_ar
	layer64_ffn2_7 -> layer64_ffn_ar
	layer65_qkv_6 [label="Layer 65 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer65_qkv_7 [label="Layer 65 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer64_ffn_ar -> layer65_qkv_6
	layer64_ffn_ar -> layer65_qkv_7
	layer65_attn_6 [label="Layer 65 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer65_attn_7 [label="Layer 65 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer65_qkv_6 -> layer65_attn_6
	layer65_qkv_7 -> layer65_attn_7
	layer65_attn_out_6 [label="Layer 65 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer65_attn_out_7 [label="Layer 65 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer65_attn_6 -> layer65_attn_out_6
	layer65_attn_7 -> layer65_attn_out_7
	layer65_attn_ar [label="Layer 65 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer65_attn_out_6 -> layer65_attn_ar
	layer65_attn_out_7 -> layer65_attn_ar
	layer65_ffn1_6 [label="Layer 65 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer65_ffn1_7 [label="Layer 65 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer65_attn_ar -> layer65_ffn1_6
	layer65_attn_ar -> layer65_ffn1_7
	layer65_act_6 [label="Layer 65 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer65_act_7 [label="Layer 65 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer65_ffn1_6 -> layer65_act_6
	layer65_ffn1_7 -> layer65_act_7
	layer65_ffn2_6 [label="Layer 65 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer65_ffn2_7 [label="Layer 65 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer65_act_6 -> layer65_ffn2_6
	layer65_act_7 -> layer65_ffn2_7
	layer65_ffn_ar [label="Layer 65 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer65_ffn2_6 -> layer65_ffn_ar
	layer65_ffn2_7 -> layer65_ffn_ar
	layer66_qkv_6 [label="Layer 66 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer66_qkv_7 [label="Layer 66 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer65_ffn_ar -> layer66_qkv_6
	layer65_ffn_ar -> layer66_qkv_7
	layer66_attn_6 [label="Layer 66 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer66_attn_7 [label="Layer 66 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer66_qkv_6 -> layer66_attn_6
	layer66_qkv_7 -> layer66_attn_7
	layer66_attn_out_6 [label="Layer 66 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer66_attn_out_7 [label="Layer 66 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer66_attn_6 -> layer66_attn_out_6
	layer66_attn_7 -> layer66_attn_out_7
	layer66_attn_ar [label="Layer 66 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer66_attn_out_6 -> layer66_attn_ar
	layer66_attn_out_7 -> layer66_attn_ar
	layer66_ffn1_6 [label="Layer 66 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer66_ffn1_7 [label="Layer 66 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer66_attn_ar -> layer66_ffn1_6
	layer66_attn_ar -> layer66_ffn1_7
	layer66_act_6 [label="Layer 66 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer66_act_7 [label="Layer 66 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer66_ffn1_6 -> layer66_act_6
	layer66_ffn1_7 -> layer66_act_7
	layer66_ffn2_6 [label="Layer 66 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer66_ffn2_7 [label="Layer 66 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer66_act_6 -> layer66_ffn2_6
	layer66_act_7 -> layer66_ffn2_7
	layer66_ffn_ar [label="Layer 66 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer66_ffn2_6 -> layer66_ffn_ar
	layer66_ffn2_7 -> layer66_ffn_ar
	layer67_qkv_6 [label="Layer 67 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer67_qkv_7 [label="Layer 67 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer66_ffn_ar -> layer67_qkv_6
	layer66_ffn_ar -> layer67_qkv_7
	layer67_attn_6 [label="Layer 67 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer67_attn_7 [label="Layer 67 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer67_qkv_6 -> layer67_attn_6
	layer67_qkv_7 -> layer67_attn_7
	layer67_attn_out_6 [label="Layer 67 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer67_attn_out_7 [label="Layer 67 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer67_attn_6 -> layer67_attn_out_6
	layer67_attn_7 -> layer67_attn_out_7
	layer67_attn_ar [label="Layer 67 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer67_attn_out_6 -> layer67_attn_ar
	layer67_attn_out_7 -> layer67_attn_ar
	layer67_ffn1_6 [label="Layer 67 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer67_ffn1_7 [label="Layer 67 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer67_attn_ar -> layer67_ffn1_6
	layer67_attn_ar -> layer67_ffn1_7
	layer67_act_6 [label="Layer 67 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer67_act_7 [label="Layer 67 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer67_ffn1_6 -> layer67_act_6
	layer67_ffn1_7 -> layer67_act_7
	layer67_ffn2_6 [label="Layer 67 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer67_ffn2_7 [label="Layer 67 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer67_act_6 -> layer67_ffn2_6
	layer67_act_7 -> layer67_ffn2_7
	layer67_ffn_ar [label="Layer 67 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer67_ffn2_6 -> layer67_ffn_ar
	layer67_ffn2_7 -> layer67_ffn_ar
	layer68_qkv_6 [label="Layer 68 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer68_qkv_7 [label="Layer 68 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer67_ffn_ar -> layer68_qkv_6
	layer67_ffn_ar -> layer68_qkv_7
	layer68_attn_6 [label="Layer 68 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer68_attn_7 [label="Layer 68 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer68_qkv_6 -> layer68_attn_6
	layer68_qkv_7 -> layer68_attn_7
	layer68_attn_out_6 [label="Layer 68 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer68_attn_out_7 [label="Layer 68 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer68_attn_6 -> layer68_attn_out_6
	layer68_attn_7 -> layer68_attn_out_7
	layer68_attn_ar [label="Layer 68 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer68_attn_out_6 -> layer68_attn_ar
	layer68_attn_out_7 -> layer68_attn_ar
	layer68_ffn1_6 [label="Layer 68 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer68_ffn1_7 [label="Layer 68 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer68_attn_ar -> layer68_ffn1_6
	layer68_attn_ar -> layer68_ffn1_7
	layer68_act_6 [label="Layer 68 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer68_act_7 [label="Layer 68 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer68_ffn1_6 -> layer68_act_6
	layer68_ffn1_7 -> layer68_act_7
	layer68_ffn2_6 [label="Layer 68 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer68_ffn2_7 [label="Layer 68 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer68_act_6 -> layer68_ffn2_6
	layer68_act_7 -> layer68_ffn2_7
	layer68_ffn_ar [label="Layer 68 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer68_ffn2_6 -> layer68_ffn_ar
	layer68_ffn2_7 -> layer68_ffn_ar
	layer69_qkv_6 [label="Layer 69 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer69_qkv_7 [label="Layer 69 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer68_ffn_ar -> layer69_qkv_6
	layer68_ffn_ar -> layer69_qkv_7
	layer69_attn_6 [label="Layer 69 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer69_attn_7 [label="Layer 69 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer69_qkv_6 -> layer69_attn_6
	layer69_qkv_7 -> layer69_attn_7
	layer69_attn_out_6 [label="Layer 69 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer69_attn_out_7 [label="Layer 69 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer69_attn_6 -> layer69_attn_out_6
	layer69_attn_7 -> layer69_attn_out_7
	layer69_attn_ar [label="Layer 69 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer69_attn_out_6 -> layer69_attn_ar
	layer69_attn_out_7 -> layer69_attn_ar
	layer69_ffn1_6 [label="Layer 69 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer69_ffn1_7 [label="Layer 69 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer69_attn_ar -> layer69_ffn1_6
	layer69_attn_ar -> layer69_ffn1_7
	layer69_act_6 [label="Layer 69 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer69_act_7 [label="Layer 69 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer69_ffn1_6 -> layer69_act_6
	layer69_ffn1_7 -> layer69_act_7
	layer69_ffn2_6 [label="Layer 69 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer69_ffn2_7 [label="Layer 69 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer69_act_6 -> layer69_ffn2_6
	layer69_act_7 -> layer69_ffn2_7
	layer69_ffn_ar [label="Layer 69 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer69_ffn2_6 -> layer69_ffn_ar
	layer69_ffn2_7 -> layer69_ffn_ar
	layer70_qkv_6 [label="Layer 70 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer70_qkv_7 [label="Layer 70 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer69_ffn_ar -> layer70_qkv_6
	layer69_ffn_ar -> layer70_qkv_7
	layer70_attn_6 [label="Layer 70 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer70_attn_7 [label="Layer 70 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer70_qkv_6 -> layer70_attn_6
	layer70_qkv_7 -> layer70_attn_7
	layer70_attn_out_6 [label="Layer 70 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer70_attn_out_7 [label="Layer 70 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer70_attn_6 -> layer70_attn_out_6
	layer70_attn_7 -> layer70_attn_out_7
	layer70_attn_ar [label="Layer 70 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer70_attn_out_6 -> layer70_attn_ar
	layer70_attn_out_7 -> layer70_attn_ar
	layer70_ffn1_6 [label="Layer 70 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer70_ffn1_7 [label="Layer 70 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer70_attn_ar -> layer70_ffn1_6
	layer70_attn_ar -> layer70_ffn1_7
	layer70_act_6 [label="Layer 70 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer70_act_7 [label="Layer 70 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer70_ffn1_6 -> layer70_act_6
	layer70_ffn1_7 -> layer70_act_7
	layer70_ffn2_6 [label="Layer 70 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer70_ffn2_7 [label="Layer 70 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer70_act_6 -> layer70_ffn2_6
	layer70_act_7 -> layer70_ffn2_7
	layer70_ffn_ar [label="Layer 70 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer70_ffn2_6 -> layer70_ffn_ar
	layer70_ffn2_7 -> layer70_ffn_ar
	layer71_qkv_6 [label="Layer 71 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer71_qkv_7 [label="Layer 71 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer70_ffn_ar -> layer71_qkv_6
	layer70_ffn_ar -> layer71_qkv_7
	layer71_attn_6 [label="Layer 71 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer71_attn_7 [label="Layer 71 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer71_qkv_6 -> layer71_attn_6
	layer71_qkv_7 -> layer71_attn_7
	layer71_attn_out_6 [label="Layer 71 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer71_attn_out_7 [label="Layer 71 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer71_attn_6 -> layer71_attn_out_6
	layer71_attn_7 -> layer71_attn_out_7
	layer71_attn_ar [label="Layer 71 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer71_attn_out_6 -> layer71_attn_ar
	layer71_attn_out_7 -> layer71_attn_ar
	layer71_ffn1_6 [label="Layer 71 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer71_ffn1_7 [label="Layer 71 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer71_attn_ar -> layer71_ffn1_6
	layer71_attn_ar -> layer71_ffn1_7
	layer71_act_6 [label="Layer 71 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer71_act_7 [label="Layer 71 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer71_ffn1_6 -> layer71_act_6
	layer71_ffn1_7 -> layer71_act_7
	layer71_ffn2_6 [label="Layer 71 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer71_ffn2_7 [label="Layer 71 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer71_act_6 -> layer71_ffn2_6
	layer71_act_7 -> layer71_ffn2_7
	layer71_ffn_ar [label="Layer 71 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer71_ffn2_6 -> layer71_ffn_ar
	layer71_ffn2_7 -> layer71_ffn_ar
	layer72_qkv_6 [label="Layer 72 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer72_qkv_7 [label="Layer 72 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer71_ffn_ar -> layer72_qkv_6
	layer71_ffn_ar -> layer72_qkv_7
	layer72_attn_6 [label="Layer 72 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer72_attn_7 [label="Layer 72 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer72_qkv_6 -> layer72_attn_6
	layer72_qkv_7 -> layer72_attn_7
	layer72_attn_out_6 [label="Layer 72 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer72_attn_out_7 [label="Layer 72 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer72_attn_6 -> layer72_attn_out_6
	layer72_attn_7 -> layer72_attn_out_7
	layer72_attn_ar [label="Layer 72 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer72_attn_out_6 -> layer72_attn_ar
	layer72_attn_out_7 -> layer72_attn_ar
	layer72_ffn1_6 [label="Layer 72 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer72_ffn1_7 [label="Layer 72 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer72_attn_ar -> layer72_ffn1_6
	layer72_attn_ar -> layer72_ffn1_7
	layer72_act_6 [label="Layer 72 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer72_act_7 [label="Layer 72 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer72_ffn1_6 -> layer72_act_6
	layer72_ffn1_7 -> layer72_act_7
	layer72_ffn2_6 [label="Layer 72 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer72_ffn2_7 [label="Layer 72 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer72_act_6 -> layer72_ffn2_6
	layer72_act_7 -> layer72_ffn2_7
	layer72_ffn_ar [label="Layer 72 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer72_ffn2_6 -> layer72_ffn_ar
	layer72_ffn2_7 -> layer72_ffn_ar
	layer73_qkv_6 [label="Layer 73 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer73_qkv_7 [label="Layer 73 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer72_ffn_ar -> layer73_qkv_6
	layer72_ffn_ar -> layer73_qkv_7
	layer73_attn_6 [label="Layer 73 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer73_attn_7 [label="Layer 73 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer73_qkv_6 -> layer73_attn_6
	layer73_qkv_7 -> layer73_attn_7
	layer73_attn_out_6 [label="Layer 73 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer73_attn_out_7 [label="Layer 73 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer73_attn_6 -> layer73_attn_out_6
	layer73_attn_7 -> layer73_attn_out_7
	layer73_attn_ar [label="Layer 73 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer73_attn_out_6 -> layer73_attn_ar
	layer73_attn_out_7 -> layer73_attn_ar
	layer73_ffn1_6 [label="Layer 73 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer73_ffn1_7 [label="Layer 73 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer73_attn_ar -> layer73_ffn1_6
	layer73_attn_ar -> layer73_ffn1_7
	layer73_act_6 [label="Layer 73 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer73_act_7 [label="Layer 73 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer73_ffn1_6 -> layer73_act_6
	layer73_ffn1_7 -> layer73_act_7
	layer73_ffn2_6 [label="Layer 73 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer73_ffn2_7 [label="Layer 73 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer73_act_6 -> layer73_ffn2_6
	layer73_act_7 -> layer73_ffn2_7
	layer73_ffn_ar [label="Layer 73 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer73_ffn2_6 -> layer73_ffn_ar
	layer73_ffn2_7 -> layer73_ffn_ar
	layer74_qkv_6 [label="Layer 74 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer74_qkv_7 [label="Layer 74 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer73_ffn_ar -> layer74_qkv_6
	layer73_ffn_ar -> layer74_qkv_7
	layer74_attn_6 [label="Layer 74 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer74_attn_7 [label="Layer 74 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer74_qkv_6 -> layer74_attn_6
	layer74_qkv_7 -> layer74_attn_7
	layer74_attn_out_6 [label="Layer 74 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer74_attn_out_7 [label="Layer 74 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer74_attn_6 -> layer74_attn_out_6
	layer74_attn_7 -> layer74_attn_out_7
	layer74_attn_ar [label="Layer 74 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer74_attn_out_6 -> layer74_attn_ar
	layer74_attn_out_7 -> layer74_attn_ar
	layer74_ffn1_6 [label="Layer 74 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer74_ffn1_7 [label="Layer 74 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer74_attn_ar -> layer74_ffn1_6
	layer74_attn_ar -> layer74_ffn1_7
	layer74_act_6 [label="Layer 74 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer74_act_7 [label="Layer 74 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer74_ffn1_6 -> layer74_act_6
	layer74_ffn1_7 -> layer74_act_7
	layer74_ffn2_6 [label="Layer 74 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer74_ffn2_7 [label="Layer 74 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer74_act_6 -> layer74_ffn2_6
	layer74_act_7 -> layer74_ffn2_7
	layer74_ffn_ar [label="Layer 74 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer74_ffn2_6 -> layer74_ffn_ar
	layer74_ffn2_7 -> layer74_ffn_ar
	layer75_qkv_6 [label="Layer 75 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer75_qkv_7 [label="Layer 75 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer74_ffn_ar -> layer75_qkv_6
	layer74_ffn_ar -> layer75_qkv_7
	layer75_attn_6 [label="Layer 75 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer75_attn_7 [label="Layer 75 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer75_qkv_6 -> layer75_attn_6
	layer75_qkv_7 -> layer75_attn_7
	layer75_attn_out_6 [label="Layer 75 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer75_attn_out_7 [label="Layer 75 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer75_attn_6 -> layer75_attn_out_6
	layer75_attn_7 -> layer75_attn_out_7
	layer75_attn_ar [label="Layer 75 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer75_attn_out_6 -> layer75_attn_ar
	layer75_attn_out_7 -> layer75_attn_ar
	layer75_ffn1_6 [label="Layer 75 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer75_ffn1_7 [label="Layer 75 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer75_attn_ar -> layer75_ffn1_6
	layer75_attn_ar -> layer75_ffn1_7
	layer75_act_6 [label="Layer 75 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer75_act_7 [label="Layer 75 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer75_ffn1_6 -> layer75_act_6
	layer75_ffn1_7 -> layer75_act_7
	layer75_ffn2_6 [label="Layer 75 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer75_ffn2_7 [label="Layer 75 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer75_act_6 -> layer75_ffn2_6
	layer75_act_7 -> layer75_ffn2_7
	layer75_ffn_ar [label="Layer 75 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer75_ffn2_6 -> layer75_ffn_ar
	layer75_ffn2_7 -> layer75_ffn_ar
	layer76_qkv_6 [label="Layer 76 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer76_qkv_7 [label="Layer 76 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer75_ffn_ar -> layer76_qkv_6
	layer75_ffn_ar -> layer76_qkv_7
	layer76_attn_6 [label="Layer 76 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer76_attn_7 [label="Layer 76 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer76_qkv_6 -> layer76_attn_6
	layer76_qkv_7 -> layer76_attn_7
	layer76_attn_out_6 [label="Layer 76 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer76_attn_out_7 [label="Layer 76 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer76_attn_6 -> layer76_attn_out_6
	layer76_attn_7 -> layer76_attn_out_7
	layer76_attn_ar [label="Layer 76 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer76_attn_out_6 -> layer76_attn_ar
	layer76_attn_out_7 -> layer76_attn_ar
	layer76_ffn1_6 [label="Layer 76 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer76_ffn1_7 [label="Layer 76 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer76_attn_ar -> layer76_ffn1_6
	layer76_attn_ar -> layer76_ffn1_7
	layer76_act_6 [label="Layer 76 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer76_act_7 [label="Layer 76 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer76_ffn1_6 -> layer76_act_6
	layer76_ffn1_7 -> layer76_act_7
	layer76_ffn2_6 [label="Layer 76 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer76_ffn2_7 [label="Layer 76 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer76_act_6 -> layer76_ffn2_6
	layer76_act_7 -> layer76_ffn2_7
	layer76_ffn_ar [label="Layer 76 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer76_ffn2_6 -> layer76_ffn_ar
	layer76_ffn2_7 -> layer76_ffn_ar
	layer77_qkv_6 [label="Layer 77 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer77_qkv_7 [label="Layer 77 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer76_ffn_ar -> layer77_qkv_6
	layer76_ffn_ar -> layer77_qkv_7
	layer77_attn_6 [label="Layer 77 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer77_attn_7 [label="Layer 77 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer77_qkv_6 -> layer77_attn_6
	layer77_qkv_7 -> layer77_attn_7
	layer77_attn_out_6 [label="Layer 77 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer77_attn_out_7 [label="Layer 77 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer77_attn_6 -> layer77_attn_out_6
	layer77_attn_7 -> layer77_attn_out_7
	layer77_attn_ar [label="Layer 77 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer77_attn_out_6 -> layer77_attn_ar
	layer77_attn_out_7 -> layer77_attn_ar
	layer77_ffn1_6 [label="Layer 77 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer77_ffn1_7 [label="Layer 77 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer77_attn_ar -> layer77_ffn1_6
	layer77_attn_ar -> layer77_ffn1_7
	layer77_act_6 [label="Layer 77 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer77_act_7 [label="Layer 77 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer77_ffn1_6 -> layer77_act_6
	layer77_ffn1_7 -> layer77_act_7
	layer77_ffn2_6 [label="Layer 77 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer77_ffn2_7 [label="Layer 77 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer77_act_6 -> layer77_ffn2_6
	layer77_act_7 -> layer77_ffn2_7
	layer77_ffn_ar [label="Layer 77 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer77_ffn2_6 -> layer77_ffn_ar
	layer77_ffn2_7 -> layer77_ffn_ar
	layer78_qkv_6 [label="Layer 78 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer78_qkv_7 [label="Layer 78 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer77_ffn_ar -> layer78_qkv_6
	layer77_ffn_ar -> layer78_qkv_7
	layer78_attn_6 [label="Layer 78 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer78_attn_7 [label="Layer 78 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer78_qkv_6 -> layer78_attn_6
	layer78_qkv_7 -> layer78_attn_7
	layer78_attn_out_6 [label="Layer 78 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer78_attn_out_7 [label="Layer 78 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer78_attn_6 -> layer78_attn_out_6
	layer78_attn_7 -> layer78_attn_out_7
	layer78_attn_ar [label="Layer 78 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer78_attn_out_6 -> layer78_attn_ar
	layer78_attn_out_7 -> layer78_attn_ar
	layer78_ffn1_6 [label="Layer 78 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer78_ffn1_7 [label="Layer 78 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer78_attn_ar -> layer78_ffn1_6
	layer78_attn_ar -> layer78_ffn1_7
	layer78_act_6 [label="Layer 78 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer78_act_7 [label="Layer 78 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer78_ffn1_6 -> layer78_act_6
	layer78_ffn1_7 -> layer78_act_7
	layer78_ffn2_6 [label="Layer 78 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer78_ffn2_7 [label="Layer 78 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer78_act_6 -> layer78_ffn2_6
	layer78_act_7 -> layer78_ffn2_7
	layer78_ffn_ar [label="Layer 78 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer78_ffn2_6 -> layer78_ffn_ar
	layer78_ffn2_7 -> layer78_ffn_ar
	layer79_qkv_6 [label="Layer 79 QKV Proj GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer79_qkv_7 [label="Layer 79 QKV Proj GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer78_ffn_ar -> layer79_qkv_6
	layer78_ffn_ar -> layer79_qkv_7
	layer79_attn_6 [label="Layer 79 Attention GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer79_attn_7 [label="Layer 79 Attention GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]" fillcolor="#E6F3FF"]
	layer79_qkv_6 -> layer79_attn_6
	layer79_qkv_7 -> layer79_attn_7
	layer79_attn_out_6 [label="Layer 79 Attn Out GPU6\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer79_attn_out_7 [label="Layer 79 Attn Out GPU7\nInput: [batch_size=?, seq_len=?, num_heads=32, head_dim=128]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer79_attn_6 -> layer79_attn_out_6
	layer79_attn_7 -> layer79_attn_out_7
	layer79_attn_ar [label="Layer 79 Attn All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer79_attn_out_6 -> layer79_attn_ar
	layer79_attn_out_7 -> layer79_attn_ar
	layer79_ffn1_6 [label="Layer 79 FFN1 GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer79_ffn1_7 [label="Layer 79 FFN1 GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer79_attn_ar -> layer79_ffn1_6
	layer79_attn_ar -> layer79_ffn1_7
	layer79_act_6 [label="Layer 79 SiLU GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer79_act_7 [label="Layer 79 SiLU GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, intermediate_size=14336]" fillcolor="#E6F3FF"]
	layer79_ffn1_6 -> layer79_act_6
	layer79_ffn1_7 -> layer79_act_7
	layer79_ffn2_6 [label="Layer 79 FFN2 GPU6\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer79_ffn2_7 [label="Layer 79 FFN2 GPU7\nInput: [batch_size=?, seq_len=?, intermediate_size=14336]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer79_act_6 -> layer79_ffn2_6
	layer79_act_7 -> layer79_ffn2_7
	layer79_ffn_ar [label="Layer 79 FFN All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	layer79_ffn2_6 -> layer79_ffn_ar
	layer79_ffn2_7 -> layer79_ffn_ar
	output_proj_6 [label="Output Projection GPU6\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	output_proj_7 [label="Output Projection GPU7\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, hidden_size=4096]" fillcolor="#E6F3FF"]
	layer79_ffn_ar -> output_proj_6
	layer79_ffn_ar -> output_proj_7
	final_output_ar [label="Final Output All-Reduce\n[TP All-Reduce]" fillcolor="#FFE6E6" shape=parallelogram]
	output_proj_6 -> final_output_ar
	output_proj_7 -> final_output_ar
	output [label="Output\nInput: [batch_size=?, seq_len=?, hidden_size=8192]\nOutput: [batch_size=?, seq_len=?, vocab_size=128256]" fillcolor="#F0E6FF" shape=ellipse style=filled]
	final_output_ar -> output
}
