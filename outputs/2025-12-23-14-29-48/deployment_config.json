{
  "parallel_strategy": {
    "strategy_type": "hybrid_pp_tp",
    "pp_degree": 4,
    "tp_degree": 2,
    "dp_degree": 1,
    "sp_degree": 1,
    "ep_degree": 1,
    "total_gpus": 8,
    "gpu_assignment": {
      "stage_0": [0, 1],
      "stage_1": [2, 3], 
      "stage_2": [4, 5],
      "stage_3": [6, 7]
    }
  },
  "model_configuration": {
    "model_name": "Llama3_70B_Instruct",
    "num_layers": 80,
    "hidden_size": 8192,
    "num_attention_heads": 64,
    "num_key_value_heads": 8,
    "vocab_size": 128256,
    "intermediate_size": 28672,
    "max_position_embeddings": 8192,
    "quantization": {
      "weights": "fp16",
      "activations": "fp16", 
      "kv_cache": "fp16"
    }
  },
  "memory_configuration": {
    "max_gpu_memory_usage_percent": 85,
    "per_gpu_memory_gb": 80,
    "usable_memory_gb": 68,
    "model_weights_allocation": {
      "per_gpu_gb": 17.5,
      "total_model_gb": 140
    },
    "kv_cache_allocation": {
      "per_gpu_gb": 20,
      "max_tokens_per_gpu": 20480,
      "memory_per_token_kb": 1.0
    },
    "activation_buffer_gb": 10,
    "communication_buffer_gb": 5,
    "system_overhead_gb": 15.5
  },
  "performance_targets": {
    "prefill_latency": {
      "p50_ms": 250,
      "p99_ms": 500,
      "target_p50_ms": 500,
      "target_p99_ms": 1000
    },
    "decode_latency": {
      "p50_ms": 25,
      "p99_ms": 45,
      "target_p50_ms": 50,
      "target_p99_ms": 100
    },
    "first_token_latency": {
      "p99_ms": 750,
      "target_p99_ms": 1500
    },
    "throughput": {
      "target_rps": 8,
      "expected_rps": 10,
      "max_batch_size": 64,
      "max_num_seqs": 128,
      "max_batched_tokens": 8192
    }
  },
  "pipeline_configuration": {
    "num_stages": 4,
    "layers_per_stage": 20,
    "micro_batch_size": {
      "prefill": 4,
      "decode": 1
    },
    "bubble_ratio": 0.25,
    "schedule": "gpipe",
    "flush_interval": 4
  },
  "tensor_parallel_configuration": {
    "tp_size": 2,
    "partition_dims": {
      "attention": "head_dim",
      "ffn": "hidden_dim"
    },
    "collective_operations": {
      "linear_layers": "all_reduce",
      "attention": "all_reduce",
      "embedding": "all_reduce"
    },
    "communication_backend": "nccl",
    "overlap_communication": true
  },
  "sequence_parallel_configuration": {
    "enabled": true,
    "sp_size": 1,
    "threshold_tokens": 2048,
    "partition_dim": "sequence",
    "collective_ops": ["all_gather", "reduce_scatter"]
  },
  "communication_optimization": {
    "intra_node": {
      "backend": "nccl",
      "bandwidth_gbps": 900,
      "latency_us": 5,
      "protocol": "nvlink"
    },
    "inter_node": {
      "backend": "nccl",
      "bandwidth_gbps": 100,
      "latency_us": 50,
      "protocol": "infiniband"
    },
    "collective_optimization": {
      "algorithm": "ring",
      "chunk_size_kb": 256,
      "pipeline_depth": 4
    }
  },
  "load_balancing": {
    "target_gpu_utilization": 70,
    "memory_balance_epsilon": 0.05,
    "compute_balance_epsilon": 0.1,
    "dynamic_adjustment": {
      "enabled": true,
      "interval_ms": 1000,
      "metrics": ["latency", "throughput", "utilization"]
    }
  },
  "runtime_optimization": {
    "kernel_fusion": {
      "attention": "flash_attention_2",
      "activation": "gelu fusion",
      "layer_norm": "rms_norm_fusion"
    },
    "memory_optimization": {
      "activation_recomputation": false,
      "gradient_checkpointing": false,
      "memory_pool": true
    },
    "scheduling": {
      "priority": "fcfs",
      "preemption": false,
      "dynamic_batching": true
    }
  },
  "fault_tolerance": {
    "error_handling": {
      "communication_retry": 3,
      "backoff_factor": 2.0,
      "timeout_ms": 30000
    },
    "health_monitoring": {
      "gpu_temperature_threshold": 85,
      "memory_error_threshold": 0.01,
      "check_interval_ms": 5000
    }
  },
  "monitoring_and_observability": {
    "metrics_collection": {
      "latency_percentiles": [50, 95, 99, 99.9],
      "throughput_samples": 100,
      "utilization_interval_ms": 100,
      "memory_tracking": true
    },
    "logging": {
      "level": "INFO",
      "components": ["parallel_strategy", "communication", "memory"],
      "rotation": {
        "max_size_mb": 100,
        "backup_count": 5
      }
    }
  }
}