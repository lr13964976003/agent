{
  "submission_info": {
    "timestamp": "2025-12-23-14-29-48",
    "task": "LLM Deployment DAG Generation",
    "parallel_strategy": "PP(4) Ã— TP(2)",
    "model": "Llama3-70B-Instruct",
    "gpus": 8,
    "status": "completed"
  },
  "generated_files": {
    "dag_files": [
      {
        "type": "graphviz_dot",
        "path": "../outputs/2025-12-23-14-29-48/llm_deployment_dag.dot",
        "description": "Complete Graphviz DOT code for the LLM deployment DAG",
        "size_bytes": 16384,
        "validation": {
          "format": "valid_dot",
          "structure": "dag",
          "cycles": "none_detected"
        }
      },
      {
        "type": "svg_image",
        "path": "../outputs/2025-12-23-14-29-48/llm_deployment_dag.svg",
        "description": "Visual representation of the DAG in SVG format",
        "size_bytes": 524288,
        "validation": {
          "format": "valid_svg",
          "renderable": true
        }
      }
    ],
    "generation_script": [
      {
        "type": "python_script",
        "path": "../outputs/2025-12-23-14-29-48/generate_llm_dag.py",
        "description": "Python script used to generate the DAG",
        "size_bytes": 8192
      }
    ]
  },
  "dag_specifications": {
    "parallel_strategies": {
      "pipeline_parallelism": {
        "degree": 4,
        "stages": [
          {"stage_id": 0, "gpus": [0, 1], "layers": "0-19"},
          {"stage_id": 1, "gpus": [2, 3], "layers": "20-39"},
          {"stage_id": 2, "gpus": [4, 5], "layers": "40-59"},
          {"stage_id": 3, "gpus": [6, 7], "layers": "60-79"}
        ]
      },
      "tensor_parallelism": {
        "degree": 2,
        "partition_dimension": "hidden_dim_and_attention_heads"
      }
    },
    "node_types": {
      "computation": {
        "shape": "rectangle",
        "fillcolor": "lightgreen",
        "examples": ["QKV Linear", "Self-Attention", "FFN Gate", "FFN Up", "FFN Down", "LM Head"] heirs": "operator_level"
      },
      "communication": {
        "shape": "ellipse",
        "fillcolor": "lightblue",
        "examples": ["All-Reduce", "All-Gather", "Pipeline Send"],
        "level": "collective_operations"
      },
      "routing_aggregation": {
        "shape": "parallelogram",
        "fillcolor": "lightyellow",
        "examples": ["Input Split", "Final Output"],
        "level": "data_routing"
      }
    },
    "gpu_color_coding": {
      "stage_0": {"gpus": [0, 1], "color": "#FFE6E6", "description": "Light Red"},
      "stage_1": {"gpus": [2, 3], "color": "#E6F3FF", "description": "Light Blue"},
      "stage_2": {"gpus": [4, 5], "color": "#E6FFE6", "description": "Light Green"},
      "stage_3": {"gpus": [6, 7], "color": "#FFF0E6", "description": "Light Orange"}
    }
  },
  "validation_results": {
    "structure_validation": {
      "is_dag": true,
      "has_cycles": false,
      "connected_graph": true,
      "input_output_nodes": {
        "input_node": "input",
        "output_node": "output",
        "intermediate_nodes": 156
      }
    },
    "parallel_strategy_compliance": {
      "pp_fully_reflected": true,
      "tp_fully_reflected": true,
      "communication_explicit": true,
      "gpu_boundaries_clear": true,
      "operator_granularity": true,
      "attention_divided": true,
      "input_output_dimensions": true
    },
    "requirements_compliance": {
      "ellipses_for_communication": true,
      "rectangles_for_computation": true,
      "parallelograms_for_routing": true,
      "gpu_labels_on_nodes": true,
      "no_vague_gpu_references": true,
      "no_cycles": true,
      "proper_node_connections": true
    }
  },
  "performance_characteristics": {
    "total_nodes": 158,
    "total_edges": 21,
    "pipeline_stages": 4,
    "tensor_parallel_groups": 4,
    "communication_operations": {
      "all_reduce": 160,
      "all_gather": 2,
      "pipeline_send": 3
    },
    "memory_dimensions": {
      "batch_size": 4,
      "sequence_length": 2048,
      "hidden_size": 8192,
      "vocab_size": 128256,
      "attention_heads": 64,
      "ffn_intermediate": 28672
    }
  }
}