{
  "generated_dag_files": {
    "main_deployment_dag": {
      "dot_file": "../outputs/2025-12-22-10-42-54/moe_30b_deployment_ep64_tp8_pp2_dp2.dot",
      "svg_file": "../outputs/2025-12-22-10-42-54/moe_30b_deployment_ep64_tp8_pp2_dp2.svg",
      "description": "Complete DAG showing EP64-TP8-PP2-DP2 parallel strategy for 30B MoE model with all 16 layers, pipeline stages, and communication patterns"
    },
    "detailed_single_layer_dag": {
      "dot_file": "../outputs/2025-12-22-10-42-54/moe_30b_single_layer_detailed.dot", 
      "svg_file": "../outputs/2025-12-22-10-42-54/moe_30b_single_layer_detailed.svg",
      "description": "Highly detailed single layer DAG showing operator-level breakdown of attention and MoE blocks with exact GPU assignments"
    }
  },
  "parallel_strategy_validation": {
    "ep_degree": 64,
    "tp_degree": 8, 
    "pp_degree": 2,
    "dp_degree": 2,
    "total_gpus": 2048,
    "dag_structure": {
      "has_cycles": false,
      "proper_dependencies": true,
      "communication_patterns": ["TP_AllReduce", "EP_AllToAll", "PP_SendRecv"],
      "node_types": ["computation", "communication", "routing", "aggregation"],
      "shapes": {
        "computation": "rectangle",
        "communication": "ellipse", 
        "routing": "parallelogram",
        "aggregation": "parallelogram"
      }
    }
  },
  "key_features_verified": [
    "All 2048 GPUs assigned to specific parallel dimensions",
    "EP64: 64 experts distributed across 64 EP ranks",
    "TP8: 8-way tensor parallelism within each expert and attention block",
    "PP2: 2 pipeline stages with 8 layers each",
    "Attention mechanisms fully decomposed to operator level",
    "MoE routing with top-2 expert selection (dashed lines)",
    "All communication patterns explicitly represented",
    "Input/output dimensions specified for each node",
    "No cycles in the DAG structure",
    "Proper dependencies between all nodes"
  ]
}