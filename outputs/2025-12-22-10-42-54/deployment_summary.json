{
  "total_gpus": 2048,
  "parallel_dimensions": {
    "expert_parallelism": 64,
    "tensor_parallelism": 8,
    "pipeline_parallelism": 2,
    "data_parallelism": 2
  },
  "module_division": {
    "experts_per_gpu": 1,
    "layers_per_gpu": 0.0078125,
    "batch_per_gpu": 64.0
  },
  "communication_patterns": {
    "all_to_all": "expert_dispatch_combine",
    "all_reduce": "tensor_synchronization",
    "send_recv": "pipeline_communication"
  },
  "optimization_targets": [
    "latency",
    "throughput",
    "memory_efficiency"
  ]
}