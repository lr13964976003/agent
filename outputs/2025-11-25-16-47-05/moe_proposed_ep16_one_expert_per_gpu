digraph MoE_Proposed_EP16_OneExpertPerGPU {
	rankdir=TB size="30,40"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input
batch_size=128, seq_len=10000, d_model=4096" fillcolor=lightgreen shape=ellipse]
	l0_attn_gpu0 [label="Attention Layer 0
All heads on GPU 0
Input: [128,10000,32,128], Output: [128,10000,4096]" fillcolor=lightcoral]
	l0_gate_gpu0 [label="Gate Layer 0
GPU 0
Input: [128,10000,4096], Output: [128,10000,2]" fillcolor=lightpink shape=parallelogram]
	l0_route [label="Token Routing
Layer 0
All-to-All
[128,10000,4096] → 16 experts" fillcolor=lightgray shape=parallelogram]
	l0_expert0_gpu1 [label="Expert 0 Layer 0
GPU 1
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert1_gpu2 [label="Expert 1 Layer 0
GPU 2
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert2_gpu3 [label="Expert 2 Layer 0
GPU 3
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert3_gpu4 [label="Expert 3 Layer 0
GPU 4
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert4_gpu5 [label="Expert 4 Layer 0
GPU 5
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert5_gpu6 [label="Expert 5 Layer 0
GPU 6
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert6_gpu7 [label="Expert 6 Layer 0
GPU 7
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert7_gpu8 [label="Expert 7 Layer 0
GPU 8
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert8_gpu9 [label="Expert 8 Layer 0
GPU 9
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert9_gpu10 [label="Expert 9 Layer 0
GPU 10
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert10_gpu11 [label="Expert 10 Layer 0
GPU 11
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert11_gpu12 [label="Expert 11 Layer 0
GPU 12
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert12_gpu13 [label="Expert 12 Layer 0
GPU 13
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert13_gpu14 [label="Expert 13 Layer 0
GPU 14
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert14_gpu15 [label="Expert 14 Layer 0
GPU 15
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_expert15_gpu16 [label="Expert 15 Layer 0
GPU 16
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l0_aggregate [label="Expert Aggregation
Layer 0
16 experts → [128,10000,4096]" fillcolor=lightgray shape=parallelogram]
	l0_residual [label="Residual Add
Layer 0
Input: [128,10000,4096]+[128,10000,4096], Output: [128,10000,4096]" fillcolor=lightgreen shape=diamond]
	l1_attn_gpu16 [label="Attention Layer 1
All heads on GPU 16
Input: [128,10000,32,128], Output: [128,10000,4096]" fillcolor=lightcoral]
	l1_gate_gpu16 [label="Gate Layer 1
GPU 16
Input: [128,10000,4096], Output: [128,10000,2]" fillcolor=lightpink shape=parallelogram]
	l1_route [label="Token Routing
Layer 1
All-to-All
[128,10000,4096] → 16 experts" fillcolor=lightgray shape=parallelogram]
	l1_expert0_gpu17 [label="Expert 0 Layer 1
GPU 17
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert1_gpu18 [label="Expert 1 Layer 1
GPU 18
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert2_gpu19 [label="Expert 2 Layer 1
GPU 19
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert3_gpu20 [label="Expert 3 Layer 1
GPU 20
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert4_gpu21 [label="Expert 4 Layer 1
GPU 21
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert5_gpu22 [label="Expert 5 Layer 1
GPU 22
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert6_gpu23 [label="Expert 6 Layer 1
GPU 23
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert7_gpu24 [label="Expert 7 Layer 1
GPU 24
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert8_gpu25 [label="Expert 8 Layer 1
GPU 25
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert9_gpu26 [label="Expert 9 Layer 1
GPU 26
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert10_gpu27 [label="Expert 10 Layer 1
GPU 27
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert11_gpu28 [label="Expert 11 Layer 1
GPU 28
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert12_gpu29 [label="Expert 12 Layer 1
GPU 29
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert13_gpu30 [label="Expert 13 Layer 1
GPU 30
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert14_gpu31 [label="Expert 14 Layer 1
GPU 31
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_expert15_gpu32 [label="Expert 15 Layer 1
GPU 32
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l1_aggregate [label="Expert Aggregation
Layer 1
16 experts → [128,10000,4096]" fillcolor=lightgray shape=parallelogram]
	l1_residual [label="Residual Add
Layer 1
Input: [128,10000,4096]+[128,10000,4096], Output: [128,10000,4096]" fillcolor=lightgreen shape=diamond]
	l15_attn_gpu240 [label="Attention Layer 15
All heads on GPU 240
Input: [128,10000,32,128], Output: [128,10000,4096]" fillcolor=lightcoral]
	l15_gate_gpu240 [label="Gate Layer 15
GPU 240
Input: [128,10000,4096], Output: [128,10000,2]" fillcolor=lightpink shape=parallelogram]
	l15_route [label="Token Routing
Layer 15
All-to-All
[128,10000,4096] → 16 experts" fillcolor=lightgray shape=parallelogram]
	l15_expert0_gpu241 [label="Expert 0 Layer 15
GPU 241
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert1_gpu242 [label="Expert 1 Layer 15
GPU 242
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert2_gpu243 [label="Expert 2 Layer 15
GPU 243
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert3_gpu244 [label="Expert 3 Layer 15
GPU 244
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert4_gpu245 [label="Expert 4 Layer 15
GPU 245
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert5_gpu246 [label="Expert 5 Layer 15
GPU 246
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert6_gpu247 [label="Expert 6 Layer 15
GPU 247
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert7_gpu248 [label="Expert 7 Layer 15
GPU 248
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert8_gpu249 [label="Expert 8 Layer 15
GPU 249
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert9_gpu250 [label="Expert 9 Layer 15
GPU 250
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert10_gpu251 [label="Expert 10 Layer 15
GPU 251
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert11_gpu252 [label="Expert 11 Layer 15
GPU 252
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert12_gpu253 [label="Expert 12 Layer 15
GPU 253
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert13_gpu254 [label="Expert 13 Layer 15
GPU 254
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert14_gpu255 [label="Expert 14 Layer 15
GPU 255
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_expert15_gpu256 [label="Expert 15 Layer 15
GPU 256
Input: [tokens,4096], Output: [tokens,4096]" fillcolor=lightyellow]
	l15_aggregate [label="Expert Aggregation
Layer 15
16 experts → [128,10000,4096]" fillcolor=lightgray shape=parallelogram]
	l15_residual [label="Residual Add
Layer 15
Input: [128,10000,4096]+[128,10000,4096], Output: [128,10000,4096]" fillcolor=lightgreen shape=diamond]
	input -> l0_attn_gpu0
	l0_attn_gpu0 -> l0_gate_gpu0
	l0_gate_gpu0 -> l0_route
	l0_route -> l0_expert0_gpu1
	l0_route -> l0_expert1_gpu2
	l0_route -> l0_expert2_gpu3
	l0_route -> l0_expert3_gpu4
	l0_route -> l0_expert4_gpu5
	l0_route -> l0_expert5_gpu6
	l0_route -> l0_expert6_gpu7
	l0_route -> l0_expert7_gpu8
	l0_route -> l0_expert8_gpu9
	l0_route -> l0_expert9_gpu10
	l0_route -> l0_expert10_gpu11
	l0_route -> l0_expert11_gpu12
	l0_route -> l0_expert12_gpu13
	l0_route -> l0_expert13_gpu14
	l0_route -> l0_expert14_gpu15
	l0_route -> l0_expert15_gpu16
	l0_expert0_gpu1 -> l0_aggregate
	l0_expert1_gpu2 -> l0_aggregate
	l0_expert2_gpu3 -> l0_aggregate
	l0_expert3_gpu4 -> l0_aggregate
	l0_expert4_gpu5 -> l0_aggregate
	l0_expert5_gpu6 -> l0_aggregate
	l0_expert6_gpu7 -> l0_aggregate
	l0_expert7_gpu8 -> l0_aggregate
	l0_expert8_gpu9 -> l0_aggregate
	l0_expert9_gpu10 -> l0_aggregate
	l0_expert10_gpu11 -> l0_aggregate
	l0_expert11_gpu12 -> l0_aggregate
	l0_expert12_gpu13 -> l0_aggregate
	l0_expert13_gpu14 -> l0_aggregate
	l0_expert14_gpu15 -> l0_aggregate
	l0_expert15_gpu16 -> l0_aggregate
	l0_aggregate -> l0_residual
	l0_attn_gpu0 -> l0_residual
	l0_residual -> l1_attn_gpu16
	l1_attn_gpu16 -> l1_gate_gpu16
	l1_gate_gpu16 -> l1_route
	l1_route -> l1_expert0_gpu17
	l1_route -> l1_expert1_gpu18
	l1_route -> l1_expert2_gpu19
	l1_route -> l1_expert3_gpu20
	l1_route -> l1_expert4_gpu21
	l1_route -> l1_expert5_gpu22
	l1_route -> l1_expert6_gpu23
	l1_route -> l1_expert7_gpu24
	l1_route -> l1_expert8_gpu25
	l1_route -> l1_expert9_gpu26
	l1_route -> l1_expert10_gpu27
	l1_route -> l1_expert11_gpu28
	l1_route -> l1_expert12_gpu29
	l1_route -> l1_expert13_gpu30
	l1_route -> l1_expert14_gpu31
	l1_route -> l1_expert15_gpu32
	l1_expert0_gpu17 -> l1_aggregate
	l1_expert1_gpu18 -> l1_aggregate
	l1_expert2_gpu19 -> l1_aggregate
	l1_expert3_gpu20 -> l1_aggregate
	l1_expert4_gpu21 -> l1_aggregate
	l1_expert5_gpu22 -> l1_aggregate
	l1_expert6_gpu23 -> l1_aggregate
	l1_expert7_gpu24 -> l1_aggregate
	l1_expert8_gpu25 -> l1_aggregate
	l1_expert9_gpu26 -> l1_aggregate
	l1_expert10_gpu27 -> l1_aggregate
	l1_expert11_gpu28 -> l1_aggregate
	l1_expert12_gpu29 -> l1_aggregate
	l1_expert13_gpu30 -> l1_aggregate
	l1_expert14_gpu31 -> l1_aggregate
	l1_expert15_gpu32 -> l1_aggregate
	l1_aggregate -> l1_residual
	l1_attn_gpu16 -> l1_residual
	l1_residual -> l15_attn_gpu240
	l15_attn_gpu240 -> l15_gate_gpu240
	l15_gate_gpu240 -> l15_route
	l15_route -> l15_expert0_gpu241
	l15_route -> l15_expert1_gpu242
	l15_route -> l15_expert2_gpu243
	l15_route -> l15_expert3_gpu244
	l15_route -> l15_expert4_gpu245
	l15_route -> l15_expert5_gpu246
	l15_route -> l15_expert6_gpu247
	l15_route -> l15_expert7_gpu248
	l15_route -> l15_expert8_gpu249
	l15_route -> l15_expert9_gpu250
	l15_route -> l15_expert10_gpu251
	l15_route -> l15_expert11_gpu252
	l15_route -> l15_expert12_gpu253
	l15_route -> l15_expert13_gpu254
	l15_route -> l15_expert14_gpu255
	l15_route -> l15_expert15_gpu256
	l15_expert0_gpu241 -> l15_aggregate
	l15_expert1_gpu242 -> l15_aggregate
	l15_expert2_gpu243 -> l15_aggregate
	l15_expert3_gpu244 -> l15_aggregate
	l15_expert4_gpu245 -> l15_aggregate
	l15_expert5_gpu246 -> l15_aggregate
	l15_expert6_gpu247 -> l15_aggregate
	l15_expert7_gpu248 -> l15_aggregate
	l15_expert8_gpu249 -> l15_aggregate
	l15_expert9_gpu250 -> l15_aggregate
	l15_expert10_gpu251 -> l15_aggregate
	l15_expert11_gpu252 -> l15_aggregate
	l15_expert12_gpu253 -> l15_aggregate
	l15_expert13_gpu254 -> l15_aggregate
	l15_expert14_gpu255 -> l15_aggregate
	l15_expert15_gpu256 -> l15_aggregate
	l15_aggregate -> l15_residual
	l15_attn_gpu240 -> l15_residual
	output [label="Output
batch_size=128, seq_len=10000, d_model=4096" fillcolor=lightgreen shape=ellipse]
	l15_residual -> output
	note [label="Note: Layers 2-14 follow same pattern
Each layer uses next 16 GPUs" fillcolor=white shape=note]
}
