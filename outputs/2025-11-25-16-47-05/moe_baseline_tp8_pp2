digraph MoE_Baseline_TP8_PP2 {
	rankdir=TB size="20,30"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input
batch_size=128, seq_len=10000, d_model=4096" fillcolor=lightgreen shape=ellipse]
	subgraph cluster_stage_0 {
		label="Pipeline Stage 0
GPUs 0-7"
		color=black style=dashed
		l0_attn_tp0_gpu0 [label="Attention Layer 0
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_attn_tp1_gpu1 [label="Attention Layer 0
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_attn_tp2_gpu2 [label="Attention Layer 0
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_attn_tp3_gpu3 [label="Attention Layer 0
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_attn_tp4_gpu4 [label="Attention Layer 0
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_attn_tp5_gpu5 [label="Attention Layer 0
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_attn_tp6_gpu6 [label="Attention Layer 0
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_attn_tp7_gpu7 [label="Attention Layer 0
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l0_moe_tp0_gpu0 [label="MoE Layer 0
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l0_moe_tp1_gpu1 [label="MoE Layer 0
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l0_moe_tp2_gpu2 [label="MoE Layer 0
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l0_moe_tp3_gpu3 [label="MoE Layer 0
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l0_moe_tp4_gpu4 [label="MoE Layer 0
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l0_moe_tp5_gpu5 [label="MoE Layer 0
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l0_moe_tp6_gpu6 [label="MoE Layer 0
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l0_moe_tp7_gpu7 [label="MoE Layer 0
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_attn_tp0_gpu0 [label="Attention Layer 1
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_attn_tp1_gpu1 [label="Attention Layer 1
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_attn_tp2_gpu2 [label="Attention Layer 1
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_attn_tp3_gpu3 [label="Attention Layer 1
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_attn_tp4_gpu4 [label="Attention Layer 1
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_attn_tp5_gpu5 [label="Attention Layer 1
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_attn_tp6_gpu6 [label="Attention Layer 1
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_attn_tp7_gpu7 [label="Attention Layer 1
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l1_moe_tp0_gpu0 [label="MoE Layer 1
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_moe_tp1_gpu1 [label="MoE Layer 1
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_moe_tp2_gpu2 [label="MoE Layer 1
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_moe_tp3_gpu3 [label="MoE Layer 1
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_moe_tp4_gpu4 [label="MoE Layer 1
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_moe_tp5_gpu5 [label="MoE Layer 1
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_moe_tp6_gpu6 [label="MoE Layer 1
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l1_moe_tp7_gpu7 [label="MoE Layer 1
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_attn_tp0_gpu0 [label="Attention Layer 2
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_attn_tp1_gpu1 [label="Attention Layer 2
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_attn_tp2_gpu2 [label="Attention Layer 2
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_attn_tp3_gpu3 [label="Attention Layer 2
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_attn_tp4_gpu4 [label="Attention Layer 2
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_attn_tp5_gpu5 [label="Attention Layer 2
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_attn_tp6_gpu6 [label="Attention Layer 2
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_attn_tp7_gpu7 [label="Attention Layer 2
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l2_moe_tp0_gpu0 [label="MoE Layer 2
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_moe_tp1_gpu1 [label="MoE Layer 2
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_moe_tp2_gpu2 [label="MoE Layer 2
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_moe_tp3_gpu3 [label="MoE Layer 2
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_moe_tp4_gpu4 [label="MoE Layer 2
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_moe_tp5_gpu5 [label="MoE Layer 2
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_moe_tp6_gpu6 [label="MoE Layer 2
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l2_moe_tp7_gpu7 [label="MoE Layer 2
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_attn_tp0_gpu0 [label="Attention Layer 3
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_attn_tp1_gpu1 [label="Attention Layer 3
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_attn_tp2_gpu2 [label="Attention Layer 3
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_attn_tp3_gpu3 [label="Attention Layer 3
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_attn_tp4_gpu4 [label="Attention Layer 3
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_attn_tp5_gpu5 [label="Attention Layer 3
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_attn_tp6_gpu6 [label="Attention Layer 3
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_attn_tp7_gpu7 [label="Attention Layer 3
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l3_moe_tp0_gpu0 [label="MoE Layer 3
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_moe_tp1_gpu1 [label="MoE Layer 3
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_moe_tp2_gpu2 [label="MoE Layer 3
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_moe_tp3_gpu3 [label="MoE Layer 3
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_moe_tp4_gpu4 [label="MoE Layer 3
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_moe_tp5_gpu5 [label="MoE Layer 3
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_moe_tp6_gpu6 [label="MoE Layer 3
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l3_moe_tp7_gpu7 [label="MoE Layer 3
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_attn_tp0_gpu0 [label="Attention Layer 4
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_attn_tp1_gpu1 [label="Attention Layer 4
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_attn_tp2_gpu2 [label="Attention Layer 4
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_attn_tp3_gpu3 [label="Attention Layer 4
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_attn_tp4_gpu4 [label="Attention Layer 4
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_attn_tp5_gpu5 [label="Attention Layer 4
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_attn_tp6_gpu6 [label="Attention Layer 4
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_attn_tp7_gpu7 [label="Attention Layer 4
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l4_moe_tp0_gpu0 [label="MoE Layer 4
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_moe_tp1_gpu1 [label="MoE Layer 4
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_moe_tp2_gpu2 [label="MoE Layer 4
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_moe_tp3_gpu3 [label="MoE Layer 4
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_moe_tp4_gpu4 [label="MoE Layer 4
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_moe_tp5_gpu5 [label="MoE Layer 4
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_moe_tp6_gpu6 [label="MoE Layer 4
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l4_moe_tp7_gpu7 [label="MoE Layer 4
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_attn_tp0_gpu0 [label="Attention Layer 5
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_attn_tp1_gpu1 [label="Attention Layer 5
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_attn_tp2_gpu2 [label="Attention Layer 5
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_attn_tp3_gpu3 [label="Attention Layer 5
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_attn_tp4_gpu4 [label="Attention Layer 5
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_attn_tp5_gpu5 [label="Attention Layer 5
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_attn_tp6_gpu6 [label="Attention Layer 5
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_attn_tp7_gpu7 [label="Attention Layer 5
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l5_moe_tp0_gpu0 [label="MoE Layer 5
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_moe_tp1_gpu1 [label="MoE Layer 5
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_moe_tp2_gpu2 [label="MoE Layer 5
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_moe_tp3_gpu3 [label="MoE Layer 5
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_moe_tp4_gpu4 [label="MoE Layer 5
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_moe_tp5_gpu5 [label="MoE Layer 5
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_moe_tp6_gpu6 [label="MoE Layer 5
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l5_moe_tp7_gpu7 [label="MoE Layer 5
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_attn_tp0_gpu0 [label="Attention Layer 6
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_attn_tp1_gpu1 [label="Attention Layer 6
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_attn_tp2_gpu2 [label="Attention Layer 6
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_attn_tp3_gpu3 [label="Attention Layer 6
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_attn_tp4_gpu4 [label="Attention Layer 6
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_attn_tp5_gpu5 [label="Attention Layer 6
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_attn_tp6_gpu6 [label="Attention Layer 6
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_attn_tp7_gpu7 [label="Attention Layer 6
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l6_moe_tp0_gpu0 [label="MoE Layer 6
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_moe_tp1_gpu1 [label="MoE Layer 6
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_moe_tp2_gpu2 [label="MoE Layer 6
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_moe_tp3_gpu3 [label="MoE Layer 6
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_moe_tp4_gpu4 [label="MoE Layer 6
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_moe_tp5_gpu5 [label="MoE Layer 6
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_moe_tp6_gpu6 [label="MoE Layer 6
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l6_moe_tp7_gpu7 [label="MoE Layer 6
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_attn_tp0_gpu0 [label="Attention Layer 7
TP Shard 0
GPU 0
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_attn_tp1_gpu1 [label="Attention Layer 7
TP Shard 1
GPU 1
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_attn_tp2_gpu2 [label="Attention Layer 7
TP Shard 2
GPU 2
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_attn_tp3_gpu3 [label="Attention Layer 7
TP Shard 3
GPU 3
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_attn_tp4_gpu4 [label="Attention Layer 7
TP Shard 4
GPU 4
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_attn_tp5_gpu5 [label="Attention Layer 7
TP Shard 5
GPU 5
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_attn_tp6_gpu6 [label="Attention Layer 7
TP Shard 6
GPU 6
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_attn_tp7_gpu7 [label="Attention Layer 7
TP Shard 7
GPU 7
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l7_moe_tp0_gpu0 [label="MoE Layer 7
Experts 0-1
GPU 0
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_moe_tp1_gpu1 [label="MoE Layer 7
Experts 2-3
GPU 1
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_moe_tp2_gpu2 [label="MoE Layer 7
Experts 4-5
GPU 2
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_moe_tp3_gpu3 [label="MoE Layer 7
Experts 6-7
GPU 3
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_moe_tp4_gpu4 [label="MoE Layer 7
Experts 8-9
GPU 4
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_moe_tp5_gpu5 [label="MoE Layer 7
Experts 10-11
GPU 5
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_moe_tp6_gpu6 [label="MoE Layer 7
Experts 12-13
GPU 6
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l7_moe_tp7_gpu7 [label="MoE Layer 7
Experts 14-15
GPU 7
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
	}
	subgraph cluster_stage_1 {
		label="Pipeline Stage 1
GPUs 8-15"
		color=black style=dashed
		l8_attn_tp0_gpu8 [label="Attention Layer 8
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_attn_tp1_gpu9 [label="Attention Layer 8
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_attn_tp2_gpu10 [label="Attention Layer 8
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_attn_tp3_gpu11 [label="Attention Layer 8
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_attn_tp4_gpu12 [label="Attention Layer 8
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_attn_tp5_gpu13 [label="Attention Layer 8
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_attn_tp6_gpu14 [label="Attention Layer 8
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_attn_tp7_gpu15 [label="Attention Layer 8
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l8_moe_tp0_gpu8 [label="MoE Layer 8
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l8_moe_tp1_gpu9 [label="MoE Layer 8
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l8_moe_tp2_gpu10 [label="MoE Layer 8
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l8_moe_tp3_gpu11 [label="MoE Layer 8
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l8_moe_tp4_gpu12 [label="MoE Layer 8
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l8_moe_tp5_gpu13 [label="MoE Layer 8
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l8_moe_tp6_gpu14 [label="MoE Layer 8
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l8_moe_tp7_gpu15 [label="MoE Layer 8
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_attn_tp0_gpu8 [label="Attention Layer 9
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_attn_tp1_gpu9 [label="Attention Layer 9
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_attn_tp2_gpu10 [label="Attention Layer 9
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_attn_tp3_gpu11 [label="Attention Layer 9
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_attn_tp4_gpu12 [label="Attention Layer 9
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_attn_tp5_gpu13 [label="Attention Layer 9
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_attn_tp6_gpu14 [label="Attention Layer 9
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_attn_tp7_gpu15 [label="Attention Layer 9
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l9_moe_tp0_gpu8 [label="MoE Layer 9
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_moe_tp1_gpu9 [label="MoE Layer 9
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_moe_tp2_gpu10 [label="MoE Layer 9
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_moe_tp3_gpu11 [label="MoE Layer 9
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_moe_tp4_gpu12 [label="MoE Layer 9
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_moe_tp5_gpu13 [label="MoE Layer 9
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_moe_tp6_gpu14 [label="MoE Layer 9
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l9_moe_tp7_gpu15 [label="MoE Layer 9
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_attn_tp0_gpu8 [label="Attention Layer 10
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_attn_tp1_gpu9 [label="Attention Layer 10
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_attn_tp2_gpu10 [label="Attention Layer 10
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_attn_tp3_gpu11 [label="Attention Layer 10
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_attn_tp4_gpu12 [label="Attention Layer 10
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_attn_tp5_gpu13 [label="Attention Layer 10
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_attn_tp6_gpu14 [label="Attention Layer 10
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_attn_tp7_gpu15 [label="Attention Layer 10
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l10_moe_tp0_gpu8 [label="MoE Layer 10
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_moe_tp1_gpu9 [label="MoE Layer 10
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_moe_tp2_gpu10 [label="MoE Layer 10
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_moe_tp3_gpu11 [label="MoE Layer 10
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_moe_tp4_gpu12 [label="MoE Layer 10
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_moe_tp5_gpu13 [label="MoE Layer 10
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_moe_tp6_gpu14 [label="MoE Layer 10
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l10_moe_tp7_gpu15 [label="MoE Layer 10
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_attn_tp0_gpu8 [label="Attention Layer 11
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_attn_tp1_gpu9 [label="Attention Layer 11
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_attn_tp2_gpu10 [label="Attention Layer 11
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_attn_tp3_gpu11 [label="Attention Layer 11
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_attn_tp4_gpu12 [label="Attention Layer 11
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_attn_tp5_gpu13 [label="Attention Layer 11
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_attn_tp6_gpu14 [label="Attention Layer 11
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_attn_tp7_gpu15 [label="Attention Layer 11
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l11_moe_tp0_gpu8 [label="MoE Layer 11
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_moe_tp1_gpu9 [label="MoE Layer 11
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_moe_tp2_gpu10 [label="MoE Layer 11
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_moe_tp3_gpu11 [label="MoE Layer 11
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_moe_tp4_gpu12 [label="MoE Layer 11
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_moe_tp5_gpu13 [label="MoE Layer 11
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_moe_tp6_gpu14 [label="MoE Layer 11
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l11_moe_tp7_gpu15 [label="MoE Layer 11
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_attn_tp0_gpu8 [label="Attention Layer 12
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_attn_tp1_gpu9 [label="Attention Layer 12
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_attn_tp2_gpu10 [label="Attention Layer 12
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_attn_tp3_gpu11 [label="Attention Layer 12
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_attn_tp4_gpu12 [label="Attention Layer 12
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_attn_tp5_gpu13 [label="Attention Layer 12
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_attn_tp6_gpu14 [label="Attention Layer 12
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_attn_tp7_gpu15 [label="Attention Layer 12
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l12_moe_tp0_gpu8 [label="MoE Layer 12
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_moe_tp1_gpu9 [label="MoE Layer 12
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_moe_tp2_gpu10 [label="MoE Layer 12
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_moe_tp3_gpu11 [label="MoE Layer 12
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_moe_tp4_gpu12 [label="MoE Layer 12
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_moe_tp5_gpu13 [label="MoE Layer 12
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_moe_tp6_gpu14 [label="MoE Layer 12
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l12_moe_tp7_gpu15 [label="MoE Layer 12
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_attn_tp0_gpu8 [label="Attention Layer 13
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_attn_tp1_gpu9 [label="Attention Layer 13
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_attn_tp2_gpu10 [label="Attention Layer 13
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_attn_tp3_gpu11 [label="Attention Layer 13
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_attn_tp4_gpu12 [label="Attention Layer 13
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_attn_tp5_gpu13 [label="Attention Layer 13
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_attn_tp6_gpu14 [label="Attention Layer 13
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_attn_tp7_gpu15 [label="Attention Layer 13
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l13_moe_tp0_gpu8 [label="MoE Layer 13
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_moe_tp1_gpu9 [label="MoE Layer 13
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_moe_tp2_gpu10 [label="MoE Layer 13
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_moe_tp3_gpu11 [label="MoE Layer 13
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_moe_tp4_gpu12 [label="MoE Layer 13
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_moe_tp5_gpu13 [label="MoE Layer 13
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_moe_tp6_gpu14 [label="MoE Layer 13
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l13_moe_tp7_gpu15 [label="MoE Layer 13
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_attn_tp0_gpu8 [label="Attention Layer 14
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_attn_tp1_gpu9 [label="Attention Layer 14
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_attn_tp2_gpu10 [label="Attention Layer 14
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_attn_tp3_gpu11 [label="Attention Layer 14
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_attn_tp4_gpu12 [label="Attention Layer 14
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_attn_tp5_gpu13 [label="Attention Layer 14
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_attn_tp6_gpu14 [label="Attention Layer 14
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_attn_tp7_gpu15 [label="Attention Layer 14
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l14_moe_tp0_gpu8 [label="MoE Layer 14
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_moe_tp1_gpu9 [label="MoE Layer 14
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_moe_tp2_gpu10 [label="MoE Layer 14
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_moe_tp3_gpu11 [label="MoE Layer 14
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_moe_tp4_gpu12 [label="MoE Layer 14
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_moe_tp5_gpu13 [label="MoE Layer 14
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_moe_tp6_gpu14 [label="MoE Layer 14
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l14_moe_tp7_gpu15 [label="MoE Layer 14
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_attn_tp0_gpu8 [label="Attention Layer 15
TP Shard 0
GPU 8
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_attn_tp1_gpu9 [label="Attention Layer 15
TP Shard 1
GPU 9
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_attn_tp2_gpu10 [label="Attention Layer 15
TP Shard 2
GPU 10
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_attn_tp3_gpu11 [label="Attention Layer 15
TP Shard 3
GPU 11
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_attn_tp4_gpu12 [label="Attention Layer 15
TP Shard 4
GPU 12
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_attn_tp5_gpu13 [label="Attention Layer 15
TP Shard 5
GPU 13
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_attn_tp6_gpu14 [label="Attention Layer 15
TP Shard 6
GPU 14
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_attn_tp7_gpu15 [label="Attention Layer 15
TP Shard 7
GPU 15
Input: [128,10000,4,128], Output: [128,10000,4,128]" fillcolor=lightcoral]
		l15_moe_tp0_gpu8 [label="MoE Layer 15
Experts 0-1
GPU 8
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_moe_tp1_gpu9 [label="MoE Layer 15
Experts 2-3
GPU 9
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_moe_tp2_gpu10 [label="MoE Layer 15
Experts 4-5
GPU 10
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_moe_tp3_gpu11 [label="MoE Layer 15
Experts 6-7
GPU 11
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_moe_tp4_gpu12 [label="MoE Layer 15
Experts 8-9
GPU 12
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_moe_tp5_gpu13 [label="MoE Layer 15
Experts 10-11
GPU 13
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_moe_tp6_gpu14 [label="MoE Layer 15
Experts 12-13
GPU 14
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
		l15_moe_tp7_gpu15 [label="MoE Layer 15
Experts 14-15
GPU 15
Input: [128,10000,4096], Output: [128,10000,4096]" fillcolor=lightyellow]
	}
	input -> l0_attn_tp0_gpu0
	input -> l0_attn_tp1_gpu1
	input -> l0_attn_tp2_gpu2
	input -> l0_attn_tp3_gpu3
	input -> l0_attn_tp4_gpu4
	input -> l0_attn_tp5_gpu5
	input -> l0_attn_tp6_gpu6
	input -> l0_attn_tp7_gpu7
	l0_attn_tp0_gpu0 -> l0_moe_tp0_gpu0
	l0_attn_tp1_gpu1 -> l0_moe_tp1_gpu1
	l0_attn_tp2_gpu2 -> l0_moe_tp2_gpu2
	l0_attn_tp3_gpu3 -> l0_moe_tp3_gpu3
	l0_attn_tp4_gpu4 -> l0_moe_tp4_gpu4
	l0_attn_tp5_gpu5 -> l0_moe_tp5_gpu5
	l0_attn_tp6_gpu6 -> l0_moe_tp6_gpu6
	l0_attn_tp7_gpu7 -> l0_moe_tp7_gpu7
	l0_allreduce [label="TP All-Reduce
Layer 0" fillcolor=lightgray shape=parallelogram]
	l0_moe_tp0_gpu0 -> l0_allreduce
	l0_allreduce -> l1_attn_tp0_gpu0
	l0_moe_tp1_gpu1 -> l0_allreduce
	l0_allreduce -> l1_attn_tp1_gpu1
	l0_moe_tp2_gpu2 -> l0_allreduce
	l0_allreduce -> l1_attn_tp2_gpu2
	l0_moe_tp3_gpu3 -> l0_allreduce
	l0_allreduce -> l1_attn_tp3_gpu3
	l0_moe_tp4_gpu4 -> l0_allreduce
	l0_allreduce -> l1_attn_tp4_gpu4
	l0_moe_tp5_gpu5 -> l0_allreduce
	l0_allreduce -> l1_attn_tp5_gpu5
	l0_moe_tp6_gpu6 -> l0_allreduce
	l0_allreduce -> l1_attn_tp6_gpu6
	l0_moe_tp7_gpu7 -> l0_allreduce
	l0_allreduce -> l1_attn_tp7_gpu7
	l1_attn_tp0_gpu0 -> l1_moe_tp0_gpu0
	l1_attn_tp1_gpu1 -> l1_moe_tp1_gpu1
	l1_attn_tp2_gpu2 -> l1_moe_tp2_gpu2
	l1_attn_tp3_gpu3 -> l1_moe_tp3_gpu3
	l1_attn_tp4_gpu4 -> l1_moe_tp4_gpu4
	l1_attn_tp5_gpu5 -> l1_moe_tp5_gpu5
	l1_attn_tp6_gpu6 -> l1_moe_tp6_gpu6
	l1_attn_tp7_gpu7 -> l1_moe_tp7_gpu7
	l1_allreduce [label="TP All-Reduce
Layer 1" fillcolor=lightgray shape=parallelogram]
	l1_moe_tp0_gpu0 -> l1_allreduce
	l1_allreduce -> l2_attn_tp0_gpu0
	l1_moe_tp1_gpu1 -> l1_allreduce
	l1_allreduce -> l2_attn_tp1_gpu1
	l1_moe_tp2_gpu2 -> l1_allreduce
	l1_allreduce -> l2_attn_tp2_gpu2
	l1_moe_tp3_gpu3 -> l1_allreduce
	l1_allreduce -> l2_attn_tp3_gpu3
	l1_moe_tp4_gpu4 -> l1_allreduce
	l1_allreduce -> l2_attn_tp4_gpu4
	l1_moe_tp5_gpu5 -> l1_allreduce
	l1_allreduce -> l2_attn_tp5_gpu5
	l1_moe_tp6_gpu6 -> l1_allreduce
	l1_allreduce -> l2_attn_tp6_gpu6
	l1_moe_tp7_gpu7 -> l1_allreduce
	l1_allreduce -> l2_attn_tp7_gpu7
	l2_attn_tp0_gpu0 -> l2_moe_tp0_gpu0
	l2_attn_tp1_gpu1 -> l2_moe_tp1_gpu1
	l2_attn_tp2_gpu2 -> l2_moe_tp2_gpu2
	l2_attn_tp3_gpu3 -> l2_moe_tp3_gpu3
	l2_attn_tp4_gpu4 -> l2_moe_tp4_gpu4
	l2_attn_tp5_gpu5 -> l2_moe_tp5_gpu5
	l2_attn_tp6_gpu6 -> l2_moe_tp6_gpu6
	l2_attn_tp7_gpu7 -> l2_moe_tp7_gpu7
	l2_allreduce [label="TP All-Reduce
Layer 2" fillcolor=lightgray shape=parallelogram]
	l2_moe_tp0_gpu0 -> l2_allreduce
	l2_allreduce -> l3_attn_tp0_gpu0
	l2_moe_tp1_gpu1 -> l2_allreduce
	l2_allreduce -> l3_attn_tp1_gpu1
	l2_moe_tp2_gpu2 -> l2_allreduce
	l2_allreduce -> l3_attn_tp2_gpu2
	l2_moe_tp3_gpu3 -> l2_allreduce
	l2_allreduce -> l3_attn_tp3_gpu3
	l2_moe_tp4_gpu4 -> l2_allreduce
	l2_allreduce -> l3_attn_tp4_gpu4
	l2_moe_tp5_gpu5 -> l2_allreduce
	l2_allreduce -> l3_attn_tp5_gpu5
	l2_moe_tp6_gpu6 -> l2_allreduce
	l2_allreduce -> l3_attn_tp6_gpu6
	l2_moe_tp7_gpu7 -> l2_allreduce
	l2_allreduce -> l3_attn_tp7_gpu7
	l3_attn_tp0_gpu0 -> l3_moe_tp0_gpu0
	l3_attn_tp1_gpu1 -> l3_moe_tp1_gpu1
	l3_attn_tp2_gpu2 -> l3_moe_tp2_gpu2
	l3_attn_tp3_gpu3 -> l3_moe_tp3_gpu3
	l3_attn_tp4_gpu4 -> l3_moe_tp4_gpu4
	l3_attn_tp5_gpu5 -> l3_moe_tp5_gpu5
	l3_attn_tp6_gpu6 -> l3_moe_tp6_gpu6
	l3_attn_tp7_gpu7 -> l3_moe_tp7_gpu7
	l3_allreduce [label="TP All-Reduce
Layer 3" fillcolor=lightgray shape=parallelogram]
	l3_moe_tp0_gpu0 -> l3_allreduce
	l3_allreduce -> l4_attn_tp0_gpu0
	l3_moe_tp1_gpu1 -> l3_allreduce
	l3_allreduce -> l4_attn_tp1_gpu1
	l3_moe_tp2_gpu2 -> l3_allreduce
	l3_allreduce -> l4_attn_tp2_gpu2
	l3_moe_tp3_gpu3 -> l3_allreduce
	l3_allreduce -> l4_attn_tp3_gpu3
	l3_moe_tp4_gpu4 -> l3_allreduce
	l3_allreduce -> l4_attn_tp4_gpu4
	l3_moe_tp5_gpu5 -> l3_allreduce
	l3_allreduce -> l4_attn_tp5_gpu5
	l3_moe_tp6_gpu6 -> l3_allreduce
	l3_allreduce -> l4_attn_tp6_gpu6
	l3_moe_tp7_gpu7 -> l3_allreduce
	l3_allreduce -> l4_attn_tp7_gpu7
	l4_attn_tp0_gpu0 -> l4_moe_tp0_gpu0
	l4_attn_tp1_gpu1 -> l4_moe_tp1_gpu1
	l4_attn_tp2_gpu2 -> l4_moe_tp2_gpu2
	l4_attn_tp3_gpu3 -> l4_moe_tp3_gpu3
	l4_attn_tp4_gpu4 -> l4_moe_tp4_gpu4
	l4_attn_tp5_gpu5 -> l4_moe_tp5_gpu5
	l4_attn_tp6_gpu6 -> l4_moe_tp6_gpu6
	l4_attn_tp7_gpu7 -> l4_moe_tp7_gpu7
	l4_allreduce [label="TP All-Reduce
Layer 4" fillcolor=lightgray shape=parallelogram]
	l4_moe_tp0_gpu0 -> l4_allreduce
	l4_allreduce -> l5_attn_tp0_gpu0
	l4_moe_tp1_gpu1 -> l4_allreduce
	l4_allreduce -> l5_attn_tp1_gpu1
	l4_moe_tp2_gpu2 -> l4_allreduce
	l4_allreduce -> l5_attn_tp2_gpu2
	l4_moe_tp3_gpu3 -> l4_allreduce
	l4_allreduce -> l5_attn_tp3_gpu3
	l4_moe_tp4_gpu4 -> l4_allreduce
	l4_allreduce -> l5_attn_tp4_gpu4
	l4_moe_tp5_gpu5 -> l4_allreduce
	l4_allreduce -> l5_attn_tp5_gpu5
	l4_moe_tp6_gpu6 -> l4_allreduce
	l4_allreduce -> l5_attn_tp6_gpu6
	l4_moe_tp7_gpu7 -> l4_allreduce
	l4_allreduce -> l5_attn_tp7_gpu7
	l5_attn_tp0_gpu0 -> l5_moe_tp0_gpu0
	l5_attn_tp1_gpu1 -> l5_moe_tp1_gpu1
	l5_attn_tp2_gpu2 -> l5_moe_tp2_gpu2
	l5_attn_tp3_gpu3 -> l5_moe_tp3_gpu3
	l5_attn_tp4_gpu4 -> l5_moe_tp4_gpu4
	l5_attn_tp5_gpu5 -> l5_moe_tp5_gpu5
	l5_attn_tp6_gpu6 -> l5_moe_tp6_gpu6
	l5_attn_tp7_gpu7 -> l5_moe_tp7_gpu7
	l5_allreduce [label="TP All-Reduce
Layer 5" fillcolor=lightgray shape=parallelogram]
	l5_moe_tp0_gpu0 -> l5_allreduce
	l5_allreduce -> l6_attn_tp0_gpu0
	l5_moe_tp1_gpu1 -> l5_allreduce
	l5_allreduce -> l6_attn_tp1_gpu1
	l5_moe_tp2_gpu2 -> l5_allreduce
	l5_allreduce -> l6_attn_tp2_gpu2
	l5_moe_tp3_gpu3 -> l5_allreduce
	l5_allreduce -> l6_attn_tp3_gpu3
	l5_moe_tp4_gpu4 -> l5_allreduce
	l5_allreduce -> l6_attn_tp4_gpu4
	l5_moe_tp5_gpu5 -> l5_allreduce
	l5_allreduce -> l6_attn_tp5_gpu5
	l5_moe_tp6_gpu6 -> l5_allreduce
	l5_allreduce -> l6_attn_tp6_gpu6
	l5_moe_tp7_gpu7 -> l5_allreduce
	l5_allreduce -> l6_attn_tp7_gpu7
	l6_attn_tp0_gpu0 -> l6_moe_tp0_gpu0
	l6_attn_tp1_gpu1 -> l6_moe_tp1_gpu1
	l6_attn_tp2_gpu2 -> l6_moe_tp2_gpu2
	l6_attn_tp3_gpu3 -> l6_moe_tp3_gpu3
	l6_attn_tp4_gpu4 -> l6_moe_tp4_gpu4
	l6_attn_tp5_gpu5 -> l6_moe_tp5_gpu5
	l6_attn_tp6_gpu6 -> l6_moe_tp6_gpu6
	l6_attn_tp7_gpu7 -> l6_moe_tp7_gpu7
	l6_allreduce [label="TP All-Reduce
Layer 6" fillcolor=lightgray shape=parallelogram]
	l6_moe_tp0_gpu0 -> l6_allreduce
	l6_allreduce -> l7_attn_tp0_gpu0
	l6_moe_tp1_gpu1 -> l6_allreduce
	l6_allreduce -> l7_attn_tp1_gpu1
	l6_moe_tp2_gpu2 -> l6_allreduce
	l6_allreduce -> l7_attn_tp2_gpu2
	l6_moe_tp3_gpu3 -> l6_allreduce
	l6_allreduce -> l7_attn_tp3_gpu3
	l6_moe_tp4_gpu4 -> l6_allreduce
	l6_allreduce -> l7_attn_tp4_gpu4
	l6_moe_tp5_gpu5 -> l6_allreduce
	l6_allreduce -> l7_attn_tp5_gpu5
	l6_moe_tp6_gpu6 -> l6_allreduce
	l6_allreduce -> l7_attn_tp6_gpu6
	l6_moe_tp7_gpu7 -> l6_allreduce
	l6_allreduce -> l7_attn_tp7_gpu7
	l7_attn_tp0_gpu0 -> l7_moe_tp0_gpu0
	l7_attn_tp1_gpu1 -> l7_moe_tp1_gpu1
	l7_attn_tp2_gpu2 -> l7_moe_tp2_gpu2
	l7_attn_tp3_gpu3 -> l7_moe_tp3_gpu3
	l7_attn_tp4_gpu4 -> l7_moe_tp4_gpu4
	l7_attn_tp5_gpu5 -> l7_moe_tp5_gpu5
	l7_attn_tp6_gpu6 -> l7_moe_tp6_gpu6
	l7_attn_tp7_gpu7 -> l7_moe_tp7_gpu7
	l7_allreduce [label="TP All-Reduce
Layer 7" fillcolor=lightgray shape=parallelogram]
	l7_moe_tp0_gpu0 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l7_moe_tp1_gpu1 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l7_moe_tp2_gpu2 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l7_moe_tp3_gpu3 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l7_moe_tp4_gpu4 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l7_moe_tp5_gpu5 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l7_moe_tp6_gpu6 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l7_moe_tp7_gpu7 -> l7_allreduce
	pipeline_stage_0_1 [label="Pipeline Communication
Stage 0 → Stage 1" fillcolor=orange shape=parallelogram style=dashed]
	l7_allreduce -> pipeline_stage_0_1
	pipeline_stage_0_1 -> l8_attn_tp0_gpu8
	pipeline_stage_0_1 -> l8_attn_tp1_gpu9
	pipeline_stage_0_1 -> l8_attn_tp2_gpu10
	pipeline_stage_0_1 -> l8_attn_tp3_gpu11
	pipeline_stage_0_1 -> l8_attn_tp4_gpu12
	pipeline_stage_0_1 -> l8_attn_tp5_gpu13
	pipeline_stage_0_1 -> l8_attn_tp6_gpu14
	pipeline_stage_0_1 -> l8_attn_tp7_gpu15
	l8_attn_tp0_gpu8 -> l8_moe_tp0_gpu8
	l8_attn_tp1_gpu9 -> l8_moe_tp1_gpu9
	l8_attn_tp2_gpu10 -> l8_moe_tp2_gpu10
	l8_attn_tp3_gpu11 -> l8_moe_tp3_gpu11
	l8_attn_tp4_gpu12 -> l8_moe_tp4_gpu12
	l8_attn_tp5_gpu13 -> l8_moe_tp5_gpu13
	l8_attn_tp6_gpu14 -> l8_moe_tp6_gpu14
	l8_attn_tp7_gpu15 -> l8_moe_tp7_gpu15
	l8_allreduce [label="TP All-Reduce
Layer 8" fillcolor=lightgray shape=parallelogram]
	l8_moe_tp0_gpu8 -> l8_allreduce
	l8_allreduce -> l9_attn_tp0_gpu8
	l8_moe_tp1_gpu9 -> l8_allreduce
	l8_allreduce -> l9_attn_tp1_gpu9
	l8_moe_tp2_gpu10 -> l8_allreduce
	l8_allreduce -> l9_attn_tp2_gpu10
	l8_moe_tp3_gpu11 -> l8_allreduce
	l8_allreduce -> l9_attn_tp3_gpu11
	l8_moe_tp4_gpu12 -> l8_allreduce
	l8_allreduce -> l9_attn_tp4_gpu12
	l8_moe_tp5_gpu13 -> l8_allreduce
	l8_allreduce -> l9_attn_tp5_gpu13
	l8_moe_tp6_gpu14 -> l8_allreduce
	l8_allreduce -> l9_attn_tp6_gpu14
	l8_moe_tp7_gpu15 -> l8_allreduce
	l8_allreduce -> l9_attn_tp7_gpu15
	l9_attn_tp0_gpu8 -> l9_moe_tp0_gpu8
	l9_attn_tp1_gpu9 -> l9_moe_tp1_gpu9
	l9_attn_tp2_gpu10 -> l9_moe_tp2_gpu10
	l9_attn_tp3_gpu11 -> l9_moe_tp3_gpu11
	l9_attn_tp4_gpu12 -> l9_moe_tp4_gpu12
	l9_attn_tp5_gpu13 -> l9_moe_tp5_gpu13
	l9_attn_tp6_gpu14 -> l9_moe_tp6_gpu14
	l9_attn_tp7_gpu15 -> l9_moe_tp7_gpu15
	l9_allreduce [label="TP All-Reduce
Layer 9" fillcolor=lightgray shape=parallelogram]
	l9_moe_tp0_gpu8 -> l9_allreduce
	l9_allreduce -> l10_attn_tp0_gpu8
	l9_moe_tp1_gpu9 -> l9_allreduce
	l9_allreduce -> l10_attn_tp1_gpu9
	l9_moe_tp2_gpu10 -> l9_allreduce
	l9_allreduce -> l10_attn_tp2_gpu10
	l9_moe_tp3_gpu11 -> l9_allreduce
	l9_allreduce -> l10_attn_tp3_gpu11
	l9_moe_tp4_gpu12 -> l9_allreduce
	l9_allreduce -> l10_attn_tp4_gpu12
	l9_moe_tp5_gpu13 -> l9_allreduce
	l9_allreduce -> l10_attn_tp5_gpu13
	l9_moe_tp6_gpu14 -> l9_allreduce
	l9_allreduce -> l10_attn_tp6_gpu14
	l9_moe_tp7_gpu15 -> l9_allreduce
	l9_allreduce -> l10_attn_tp7_gpu15
	l10_attn_tp0_gpu8 -> l10_moe_tp0_gpu8
	l10_attn_tp1_gpu9 -> l10_moe_tp1_gpu9
	l10_attn_tp2_gpu10 -> l10_moe_tp2_gpu10
	l10_attn_tp3_gpu11 -> l10_moe_tp3_gpu11
	l10_attn_tp4_gpu12 -> l10_moe_tp4_gpu12
	l10_attn_tp5_gpu13 -> l10_moe_tp5_gpu13
	l10_attn_tp6_gpu14 -> l10_moe_tp6_gpu14
	l10_attn_tp7_gpu15 -> l10_moe_tp7_gpu15
	l10_allreduce [label="TP All-Reduce
Layer 10" fillcolor=lightgray shape=parallelogram]
	l10_moe_tp0_gpu8 -> l10_allreduce
	l10_allreduce -> l11_attn_tp0_gpu8
	l10_moe_tp1_gpu9 -> l10_allreduce
	l10_allreduce -> l11_attn_tp1_gpu9
	l10_moe_tp2_gpu10 -> l10_allreduce
	l10_allreduce -> l11_attn_tp2_gpu10
	l10_moe_tp3_gpu11 -> l10_allreduce
	l10_allreduce -> l11_attn_tp3_gpu11
	l10_moe_tp4_gpu12 -> l10_allreduce
	l10_allreduce -> l11_attn_tp4_gpu12
	l10_moe_tp5_gpu13 -> l10_allreduce
	l10_allreduce -> l11_attn_tp5_gpu13
	l10_moe_tp6_gpu14 -> l10_allreduce
	l10_allreduce -> l11_attn_tp6_gpu14
	l10_moe_tp7_gpu15 -> l10_allreduce
	l10_allreduce -> l11_attn_tp7_gpu15
	l11_attn_tp0_gpu8 -> l11_moe_tp0_gpu8
	l11_attn_tp1_gpu9 -> l11_moe_tp1_gpu9
	l11_attn_tp2_gpu10 -> l11_moe_tp2_gpu10
	l11_attn_tp3_gpu11 -> l11_moe_tp3_gpu11
	l11_attn_tp4_gpu12 -> l11_moe_tp4_gpu12
	l11_attn_tp5_gpu13 -> l11_moe_tp5_gpu13
	l11_attn_tp6_gpu14 -> l11_moe_tp6_gpu14
	l11_attn_tp7_gpu15 -> l11_moe_tp7_gpu15
	l11_allreduce [label="TP All-Reduce
Layer 11" fillcolor=lightgray shape=parallelogram]
	l11_moe_tp0_gpu8 -> l11_allreduce
	l11_allreduce -> l12_attn_tp0_gpu8
	l11_moe_tp1_gpu9 -> l11_allreduce
	l11_allreduce -> l12_attn_tp1_gpu9
	l11_moe_tp2_gpu10 -> l11_allreduce
	l11_allreduce -> l12_attn_tp2_gpu10
	l11_moe_tp3_gpu11 -> l11_allreduce
	l11_allreduce -> l12_attn_tp3_gpu11
	l11_moe_tp4_gpu12 -> l11_allreduce
	l11_allreduce -> l12_attn_tp4_gpu12
	l11_moe_tp5_gpu13 -> l11_allreduce
	l11_allreduce -> l12_attn_tp5_gpu13
	l11_moe_tp6_gpu14 -> l11_allreduce
	l11_allreduce -> l12_attn_tp6_gpu14
	l11_moe_tp7_gpu15 -> l11_allreduce
	l11_allreduce -> l12_attn_tp7_gpu15
	l12_attn_tp0_gpu8 -> l12_moe_tp0_gpu8
	l12_attn_tp1_gpu9 -> l12_moe_tp1_gpu9
	l12_attn_tp2_gpu10 -> l12_moe_tp2_gpu10
	l12_attn_tp3_gpu11 -> l12_moe_tp3_gpu11
	l12_attn_tp4_gpu12 -> l12_moe_tp4_gpu12
	l12_attn_tp5_gpu13 -> l12_moe_tp5_gpu13
	l12_attn_tp6_gpu14 -> l12_moe_tp6_gpu14
	l12_attn_tp7_gpu15 -> l12_moe_tp7_gpu15
	l12_allreduce [label="TP All-Reduce
Layer 12" fillcolor=lightgray shape=parallelogram]
	l12_moe_tp0_gpu8 -> l12_allreduce
	l12_allreduce -> l13_attn_tp0_gpu8
	l12_moe_tp1_gpu9 -> l12_allreduce
	l12_allreduce -> l13_attn_tp1_gpu9
	l12_moe_tp2_gpu10 -> l12_allreduce
	l12_allreduce -> l13_attn_tp2_gpu10
	l12_moe_tp3_gpu11 -> l12_allreduce
	l12_allreduce -> l13_attn_tp3_gpu11
	l12_moe_tp4_gpu12 -> l12_allreduce
	l12_allreduce -> l13_attn_tp4_gpu12
	l12_moe_tp5_gpu13 -> l12_allreduce
	l12_allreduce -> l13_attn_tp5_gpu13
	l12_moe_tp6_gpu14 -> l12_allreduce
	l12_allreduce -> l13_attn_tp6_gpu14
	l12_moe_tp7_gpu15 -> l12_allreduce
	l12_allreduce -> l13_attn_tp7_gpu15
	l13_attn_tp0_gpu8 -> l13_moe_tp0_gpu8
	l13_attn_tp1_gpu9 -> l13_moe_tp1_gpu9
	l13_attn_tp2_gpu10 -> l13_moe_tp2_gpu10
	l13_attn_tp3_gpu11 -> l13_moe_tp3_gpu11
	l13_attn_tp4_gpu12 -> l13_moe_tp4_gpu12
	l13_attn_tp5_gpu13 -> l13_moe_tp5_gpu13
	l13_attn_tp6_gpu14 -> l13_moe_tp6_gpu14
	l13_attn_tp7_gpu15 -> l13_moe_tp7_gpu15
	l13_allreduce [label="TP All-Reduce
Layer 13" fillcolor=lightgray shape=parallelogram]
	l13_moe_tp0_gpu8 -> l13_allreduce
	l13_allreduce -> l14_attn_tp0_gpu8
	l13_moe_tp1_gpu9 -> l13_allreduce
	l13_allreduce -> l14_attn_tp1_gpu9
	l13_moe_tp2_gpu10 -> l13_allreduce
	l13_allreduce -> l14_attn_tp2_gpu10
	l13_moe_tp3_gpu11 -> l13_allreduce
	l13_allreduce -> l14_attn_tp3_gpu11
	l13_moe_tp4_gpu12 -> l13_allreduce
	l13_allreduce -> l14_attn_tp4_gpu12
	l13_moe_tp5_gpu13 -> l13_allreduce
	l13_allreduce -> l14_attn_tp5_gpu13
	l13_moe_tp6_gpu14 -> l13_allreduce
	l13_allreduce -> l14_attn_tp6_gpu14
	l13_moe_tp7_gpu15 -> l13_allreduce
	l13_allreduce -> l14_attn_tp7_gpu15
	l14_attn_tp0_gpu8 -> l14_moe_tp0_gpu8
	l14_attn_tp1_gpu9 -> l14_moe_tp1_gpu9
	l14_attn_tp2_gpu10 -> l14_moe_tp2_gpu10
	l14_attn_tp3_gpu11 -> l14_moe_tp3_gpu11
	l14_attn_tp4_gpu12 -> l14_moe_tp4_gpu12
	l14_attn_tp5_gpu13 -> l14_moe_tp5_gpu13
	l14_attn_tp6_gpu14 -> l14_moe_tp6_gpu14
	l14_attn_tp7_gpu15 -> l14_moe_tp7_gpu15
	l14_allreduce [label="TP All-Reduce
Layer 14" fillcolor=lightgray shape=parallelogram]
	l14_moe_tp0_gpu8 -> l14_allreduce
	l14_allreduce -> l15_attn_tp0_gpu8
	l14_moe_tp1_gpu9 -> l14_allreduce
	l14_allreduce -> l15_attn_tp1_gpu9
	l14_moe_tp2_gpu10 -> l14_allreduce
	l14_allreduce -> l15_attn_tp2_gpu10
	l14_moe_tp3_gpu11 -> l14_allreduce
	l14_allreduce -> l15_attn_tp3_gpu11
	l14_moe_tp4_gpu12 -> l14_allreduce
	l14_allreduce -> l15_attn_tp4_gpu12
	l14_moe_tp5_gpu13 -> l14_allreduce
	l14_allreduce -> l15_attn_tp5_gpu13
	l14_moe_tp6_gpu14 -> l14_allreduce
	l14_allreduce -> l15_attn_tp6_gpu14
	l14_moe_tp7_gpu15 -> l14_allreduce
	l14_allreduce -> l15_attn_tp7_gpu15
	l15_attn_tp0_gpu8 -> l15_moe_tp0_gpu8
	l15_attn_tp1_gpu9 -> l15_moe_tp1_gpu9
	l15_attn_tp2_gpu10 -> l15_moe_tp2_gpu10
	l15_attn_tp3_gpu11 -> l15_moe_tp3_gpu11
	l15_attn_tp4_gpu12 -> l15_moe_tp4_gpu12
	l15_attn_tp5_gpu13 -> l15_moe_tp5_gpu13
	l15_attn_tp6_gpu14 -> l15_moe_tp6_gpu14
	l15_attn_tp7_gpu15 -> l15_moe_tp7_gpu15
	output [label="Output
batch_size=128, seq_len=10000, d_model=4096" fillcolor=lightgreen shape=ellipse]
	l15_moe_tp0_gpu8 -> output
	l15_moe_tp1_gpu9 -> output
	l15_moe_tp2_gpu10 -> output
	l15_moe_tp3_gpu11 -> output
	l15_moe_tp4_gpu12 -> output
	l15_moe_tp5_gpu13 -> output
	l15_moe_tp6_gpu14 -> output
	l15_moe_tp7_gpu15 -> output
}
