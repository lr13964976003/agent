digraph MoE_Proposed_EP16_OneExpertPerGPU {
	rankdir=TB size="30,40"
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input
batch_size=128, seq_len=10000, d_model=4096" fillcolor=lightgreen shape=ellipse]
	l0_attn_gpu0 [label="Attention
L0
GPU0
In:[128,10000,32,128]
Out:[128,10000,4096]" fillcolor=lightcoral]
	l0_gate_gpu0 [label="Gate
L0
GPU0
In:[128,10000,4096]
Out:[128,10000,2]" fillcolor=lightpink shape=parallelogram]
	l0_route [label="Token Routing
L0
All-to-All
[128,10000,4096]→16 experts" fillcolor=lightgray shape=parallelogram]
	l0_expert0_gpu1 [label="Expert0
L0
GPU1
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert1_gpu2 [label="Expert1
L0
GPU2
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert2_gpu3 [label="Expert2
L0
GPU3
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert3_gpu4 [label="Expert3
L0
GPU4
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert4_gpu5 [label="Expert4
L0
GPU5
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert5_gpu6 [label="Expert5
L0
GPU6
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert6_gpu7 [label="Expert6
L0
GPU7
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert7_gpu8 [label="Expert7
L0
GPU8
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert8_gpu9 [label="Expert8
L0
GPU9
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert9_gpu10 [label="Expert9
L0
GPU10
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert10_gpu11 [label="Expert10
L0
GPU11
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert11_gpu12 [label="Expert11
L0
GPU12
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert12_gpu13 [label="Expert12
L0
GPU13
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert13_gpu14 [label="Expert13
L0
GPU14
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert14_gpu15 [label="Expert14
L0
GPU15
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_expert15_gpu16 [label="Expert15
L0
GPU16
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l0_aggregate [label="Expert Aggregation
L0
16 experts→[128,10000,4096]" fillcolor=lightgray shape=parallelogram]
	l0_residual [label="Residual Add
L0
In:[128,10000,4096]+[128,10000,4096]
Out:[128,10000,4096]" fillcolor=lightgreen shape=diamond]
	l1_attn_gpu16 [label="Attention
L1
GPU16
In:[128,10000,32,128]
Out:[128,10000,4096]" fillcolor=lightcoral]
	l1_gate_gpu16 [label="Gate
L1
GPU16
In:[128,10000,4096]
Out:[128,10000,2]" fillcolor=lightpink shape=parallelogram]
	l1_route [label="Token Routing
L1
All-to-All
[128,10000,4096]→16 experts" fillcolor=lightgray shape=parallelogram]
	l1_expert0_gpu17 [label="Expert0
L1
GPU17
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert1_gpu18 [label="Expert1
L1
GPU18
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert2_gpu19 [label="Expert2
L1
GPU19
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert3_gpu20 [label="Expert3
L1
GPU20
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert4_gpu21 [label="Expert4
L1
GPU21
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert5_gpu22 [label="Expert5
L1
GPU22
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert6_gpu23 [label="Expert6
L1
GPU23
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert7_gpu24 [label="Expert7
L1
GPU24
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert8_gpu25 [label="Expert8
L1
GPU25
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert9_gpu26 [label="Expert9
L1
GPU26
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert10_gpu27 [label="Expert10
L1
GPU27
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert11_gpu28 [label="Expert11
L1
GPU28
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert12_gpu29 [label="Expert12
L1
GPU29
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert13_gpu30 [label="Expert13
L1
GPU30
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert14_gpu31 [label="Expert14
L1
GPU31
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_expert15_gpu32 [label="Expert15
L1
GPU32
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l1_aggregate [label="Expert Aggregation
L1
16 experts→[128,10000,4096]" fillcolor=lightgray shape=parallelogram]
	l1_residual [label="Residual Add
L1
In:[128,10000,4096]+[128,10000,4096]
Out:[128,10000,4096]" fillcolor=lightgreen shape=diamond]
	l15_attn_gpu240 [label="Attention
L15
GPU240
In:[128,10000,32,128]
Out:[128,10000,4096]" fillcolor=lightcoral]
	l15_gate_gpu240 [label="Gate
L15
GPU240
In:[128,10000,4096]
Out:[128,10000,2]" fillcolor=lightpink shape=parallelogram]
	l15_route [label="Token Routing
L15
All-to-All
[128,10000,4096]→16 experts" fillcolor=lightgray shape=parallelogram]
	l15_expert0_gpu241 [label="Expert0
L15
GPU241
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert1_gpu242 [label="Expert1
L15
GPU242
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert2_gpu243 [label="Expert2
L15
GPU243
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert3_gpu244 [label="Expert3
L15
GPU244
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert4_gpu245 [label="Expert4
L15
GPU245
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert5_gpu246 [label="Expert5
L15
GPU246
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert6_gpu247 [label="Expert6
L15
GPU247
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert7_gpu248 [label="Expert7
L15
GPU248
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert8_gpu249 [label="Expert8
L15
GPU249
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert9_gpu250 [label="Expert9
L15
GPU250
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert10_gpu251 [label="Expert10
L15
GPU251
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert11_gpu252 [label="Expert11
L15
GPU252
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert12_gpu253 [label="Expert12
L15
GPU253
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert13_gpu254 [label="Expert13
L15
GPU254
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert14_gpu255 [label="Expert14
L15
GPU255
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_expert15_gpu256 [label="Expert15
L15
GPU256
In:[tokens,4096]
Out:[tokens,4096]" fillcolor=lightyellow]
	l15_aggregate [label="Expert Aggregation
L15
16 experts→[128,10000,4096]" fillcolor=lightgray shape=parallelogram]
	l15_residual [label="Residual Add
L15
In:[128,10000,4096]+[128,10000,4096]
Out:[128,10000,4096]" fillcolor=lightgreen shape=diamond]
	input -> l0_attn_gpu0
	l0_attn_gpu0 -> l0_gate_gpu0
	l0_gate_gpu0 -> l0_route
	l0_route -> l0_expert0_gpu1
	l0_route -> l0_expert1_gpu2
	l0_route -> l0_expert2_gpu3
	l0_route -> l0_expert3_gpu4
	l0_route -> l0_expert4_gpu5
	l0_route -> l0_expert5_gpu6
	l0_route -> l0_expert6_gpu7
	l0_route -> l0_expert7_gpu8
	l0_route -> l0_expert8_gpu9
	l0_route -> l0_expert9_gpu10
	l0_route -> l0_expert10_gpu11
	l0_route -> l0_expert11_gpu12
	l0_route -> l0_expert12_gpu13
	l0_route -> l0_expert13_gpu14
	l0_route -> l0_expert14_gpu15
	l0_route -> l0_expert15_gpu16
	l0_expert0_gpu1 -> l0_aggregate
	l0_expert1_gpu2 -> l0_aggregate
	l0_expert2_gpu3 -> l0_aggregate
	l0_expert3_gpu4 -> l0_aggregate
	l0_expert4_gpu5 -> l0_aggregate
	l0_expert5_gpu6 -> l0_aggregate
	l0_expert6_gpu7 -> l0_aggregate
	l0_expert7_gpu8 -> l0_aggregate
	l0_expert8_gpu9 -> l0_aggregate
	l0_expert9_gpu10 -> l0_aggregate
	l0_expert10_gpu11 -> l0_aggregate
	l0_expert11_gpu12 -> l0_aggregate
	l0_expert12_gpu13 -> l0_aggregate
	l0_expert13_gpu14 -> l0_aggregate
	l0_expert14_gpu15 -> l0_aggregate
	l0_expert15_gpu16 -> l0_aggregate
	l0_aggregate -> l0_residual
	l0_attn_gpu0 -> l0_residual
	l0_residual -> l1_attn_gpu16
	l1_attn_gpu16 -> l1_gate_gpu16
	l1_gate_gpu16 -> l1_route
	l1_route -> l1_expert0_gpu17
	l1_route -> l1_expert1_gpu18
	l1_route -> l1_expert2_gpu19
	l1_route -> l1_expert3_gpu20
	l1_route -> l1_expert4_gpu21
	l1_route -> l1_expert5_gpu22
	l1_route -> l1_expert6_gpu23
	l1_route -> l1_expert7_gpu24
	l1_route -> l1_expert8_gpu25
	l1_route -> l1_expert9_gpu26
	l1_route -> l1_expert10_gpu27
	l1_route -> l1_expert11_gpu28
	l1_route -> l1_expert12_gpu29
	l1_route -> l1_expert13_gpu30
	l1_route -> l1_expert14_gpu31
	l1_route -> l1_expert15_gpu32
	l1_expert0_gpu17 -> l1_aggregate
	l1_expert1_gpu18 -> l1_aggregate
	l1_expert2_gpu19 -> l1_aggregate
	l1_expert3_gpu20 -> l1_aggregate
	l1_expert4_gpu21 -> l1_aggregate
	l1_expert5_gpu22 -> l1_aggregate
	l1_expert6_gpu23 -> l1_aggregate
	l1_expert7_gpu24 -> l1_aggregate
	l1_expert8_gpu25 -> l1_aggregate
	l1_expert9_gpu26 -> l1_aggregate
	l1_expert10_gpu27 -> l1_aggregate
	l1_expert11_gpu28 -> l1_aggregate
	l1_expert12_gpu29 -> l1_aggregate
	l1_expert13_gpu30 -> l1_aggregate
	l1_expert14_gpu31 -> l1_aggregate
	l1_expert15_gpu32 -> l1_aggregate
	l1_aggregate -> l1_residual
	l1_attn_gpu16 -> l1_residual
	l1_residual -> l15_attn_gpu240
	l15_attn_gpu240 -> l15_gate_gpu240
	l15_gate_gpu240 -> l15_route
	l15_route -> l15_expert0_gpu241
	l15_route -> l15_expert1_gpu242
	l15_route -> l15_expert2_gpu243
	l15_route -> l15_expert3_gpu244
	l15_route -> l15_expert4_gpu245
	l15_route -> l15_expert5_gpu246
	l15_route -> l15_expert6_gpu247
	l15_route -> l15_expert7_gpu248
	l15_route -> l15_expert8_gpu249
	l15_route -> l15_expert9_gpu250
	l15_route -> l15_expert10_gpu251
	l15_route -> l15_expert11_gpu252
	l15_route -> l15_expert12_gpu253
	l15_route -> l15_expert13_gpu254
	l15_route -> l15_expert14_gpu255
	l15_route -> l15_expert15_gpu256
	l15_expert0_gpu241 -> l15_aggregate
	l15_expert1_gpu242 -> l15_aggregate
	l15_expert2_gpu243 -> l15_aggregate
	l15_expert3_gpu244 -> l15_aggregate
	l15_expert4_gpu245 -> l15_aggregate
	l15_expert5_gpu246 -> l15_aggregate
	l15_expert6_gpu247 -> l15_aggregate
	l15_expert7_gpu248 -> l15_aggregate
	l15_expert8_gpu249 -> l15_aggregate
	l15_expert9_gpu250 -> l15_aggregate
	l15_expert10_gpu251 -> l15_aggregate
	l15_expert11_gpu252 -> l15_aggregate
	l15_expert12_gpu253 -> l15_aggregate
	l15_expert13_gpu254 -> l15_aggregate
	l15_expert14_gpu255 -> l15_aggregate
	l15_expert15_gpu256 -> l15_aggregate
	l15_aggregate -> l15_residual
	l15_attn_gpu240 -> l15_residual
	output [label="Output
batch_size=128, seq_len=10000, d_model=4096" fillcolor=lightgreen shape=ellipse]
	l15_residual -> output
	note [label="Note: Layers 2-14 follow same pattern
Each layer uses next 16 GPUs" fillcolor=white shape=note]
}
