{
  "baseline_dag_dot": "../outputs/2025-11-25-16-47-05/moe_baseline_tp8_pp2.dot",
  "baseline_dag_svg": "../outputs/2025-11-25-16-47-05/moe_baseline_tp8_pp2.svg",
  "proposed_dag_dot": "../outputs/2025-11-25-16-47-05/moe_proposed_ep16_one_expert_per_gpu.dot",
  "proposed_dag_svg": "../outputs/2025-11-25-16-47-05/moe_proposed_ep16_one_expert_per_gpu.svg",
  "all_files": [
    "../outputs/2025-11-25-16-47-05/moe_baseline_tp8_pp2.dot",
    "../outputs/2025-11-25-16-47-05/moe_proposed_ep16_one_expert_per_gpu.dot",
    "../outputs/2025-11-25-16-47-05/moe_baseline_tp8_pp2.svg",
    "../outputs/2025-11-25-16-47-05/moe_proposed_ep16_one_expert_per_gpu.svg",
    "../outputs/2025-11-25-16-47-05/generate_fixed_dags.py"
  ],
  "description": "Complete MoE model deployment DAGs with proper GPU indices, input/output dimensions, and tensor shapes as requested. Both baseline (TP=8, PP=2) and proposed (EP=16, one expert per GPU) strategies are fully represented with all required information including routing, aggregation, and residual operations where applicable."
}