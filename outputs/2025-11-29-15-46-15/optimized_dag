// Optimized DAG: Layer-wise Partitioning
digraph {
	nodesep=0.5 rankdir=TB ranksep=1.0 size="30,20"
	node [fontname=Arial fontsize=10]
	edge [fontname=Arial fontsize=8]
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=rectangle style=filled]
	node [fillcolor=lightyellow shape=parallelogram style=filled]
	input [label="Input
Batch:128, Seq:10000
Dim:4096" fillcolor=lightcoral shape=ellipse]
	subgraph cluster_gpu0 {
		fillcolor=lightblue label="GPU 0: Layers 0-3 (Cache Optimized)" style=rounded
		attn_l0_g0 [label="Attention L0
GPU 0
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l0_g0 [label="MLP L0
GPU 0
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l0_g0 [label="LayerNorm L0
GPU 0" fillcolor=lightgreen shape=rectangle]
		resid_l0_g0 [label="ResidAdd L0
GPU 0" fillcolor=orange shape=parallelogram]
		attn_l1_g0 [label="Attention L1
GPU 0
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l1_g0 [label="MLP L1
GPU 0
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l1_g0 [label="LayerNorm L1
GPU 0" fillcolor=lightgreen shape=rectangle]
		resid_l1_g0 [label="ResidAdd L1
GPU 0" fillcolor=orange shape=parallelogram]
		attn_l2_g0 [label="Attention L2
GPU 0
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l2_g0 [label="MLP L2
GPU 0
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l2_g0 [label="LayerNorm L2
GPU 0" fillcolor=lightgreen shape=rectangle]
		resid_l2_g0 [label="ResidAdd L2
GPU 0" fillcolor=orange shape=parallelogram]
		attn_l3_g0 [label="Attention L3
GPU 0
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l3_g0 [label="MLP L3
GPU 0
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l3_g0 [label="LayerNorm L3
GPU 0" fillcolor=lightgreen shape=rectangle]
		resid_l3_g0 [label="ResidAdd L3
GPU 0" fillcolor=orange shape=parallelogram]
	}
	subgraph cluster_gpu1 {
		fillcolor=lightblue label="GPU 1: Layers 4-7 (Cache Optimized)" style=rounded
		attn_l4_g1 [label="Attention L4
GPU 1
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l4_g1 [label="MLP L4
GPU 1
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l4_g1 [label="LayerNorm L4
GPU 1" fillcolor=lightgreen shape=rectangle]
		resid_l4_g1 [label="ResidAdd L4
GPU 1" fillcolor=orange shape=parallelogram]
		attn_l5_g1 [label="Attention L5
GPU 1
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l5_g1 [label="MLP L5
GPU 1
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l5_g1 [label="LayerNorm L5
GPU 1" fillcolor=lightgreen shape=rectangle]
		resid_l5_g1 [label="ResidAdd L5
GPU 1" fillcolor=orange shape=parallelogram]
		attn_l6_g1 [label="Attention L6
GPU 1
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l6_g1 [label="MLP L6
GPU 1
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l6_g1 [label="LayerNorm L6
GPU 1" fillcolor=lightgreen shape=rectangle]
		resid_l6_g1 [label="ResidAdd L6
GPU 1" fillcolor=orange shape=parallelogram]
		attn_l7_g1 [label="Attention L7
GPU 1
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l7_g1 [label="MLP L7
GPU 1
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l7_g1 [label="LayerNorm L7
GPU 1" fillcolor=lightgreen shape=rectangle]
		resid_l7_g1 [label="ResidAdd L7
GPU 1" fillcolor=orange shape=parallelogram]
	}
	subgraph cluster_gpu2 {
		fillcolor=lightblue label="GPU 2: Layers 8-11 (Cache Optimized)" style=rounded
		attn_l8_g2 [label="Attention L8
GPU 2
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l8_g2 [label="MLP L8
GPU 2
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l8_g2 [label="LayerNorm L8
GPU 2" fillcolor=lightgreen shape=rectangle]
		resid_l8_g2 [label="ResidAdd L8
GPU 2" fillcolor=orange shape=parallelogram]
		attn_l9_g2 [label="Attention L9
GPU 2
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l9_g2 [label="MLP L9
GPU 2
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l9_g2 [label="LayerNorm L9
GPU 2" fillcolor=lightgreen shape=rectangle]
		resid_l9_g2 [label="ResidAdd L9
GPU 2" fillcolor=orange shape=parallelogram]
		attn_l10_g2 [label="Attention L10
GPU 2
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l10_g2 [label="MLP L10
GPU 2
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l10_g2 [label="LayerNorm L10
GPU 2" fillcolor=lightgreen shape=rectangle]
		resid_l10_g2 [label="ResidAdd L10
GPU 2" fillcolor=orange shape=parallelogram]
		attn_l11_g2 [label="Attention L11
GPU 2
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l11_g2 [label="MLP L11
GPU 2
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l11_g2 [label="LayerNorm L11
GPU 2" fillcolor=lightgreen shape=rectangle]
		resid_l11_g2 [label="ResidAdd L11
GPU 2" fillcolor=orange shape=parallelogram]
	}
	subgraph cluster_gpu3 {
		fillcolor=lightblue label="GPU 3: Layers 12-15 (Cache Optimized)" style=rounded
		attn_l12_g3 [label="Attention L12
GPU 3
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l12_g3 [label="MLP L12
GPU 3
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l12_g3 [label="LayerNorm L12
GPU 3" fillcolor=lightgreen shape=rectangle]
		resid_l12_g3 [label="ResidAdd L12
GPU 3" fillcolor=orange shape=parallelogram]
		attn_l13_g3 [label="Attention L13
GPU 3
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l13_g3 [label="MLP L13
GPU 3
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l13_g3 [label="LayerNorm L13
GPU 3" fillcolor=lightgreen shape=rectangle]
		resid_l13_g3 [label="ResidAdd L13
GPU 3" fillcolor=orange shape=parallelogram]
		attn_l14_g3 [label="Attention L14
GPU 3
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l14_g3 [label="MLP L14
GPU 3
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l14_g3 [label="LayerNorm L14
GPU 3" fillcolor=lightgreen shape=rectangle]
		resid_l14_g3 [label="ResidAdd L14
GPU 3" fillcolor=orange shape=parallelogram]
		attn_l15_g3 [label="Attention L15
GPU 3
QKV Proj+Attn+Output
Complete Layer" fillcolor=lightgreen shape=rectangle]
		mlp_l15_g3 [label="MLP L15
GPU 3
16384->16384
Complete Layer" fillcolor=lightgreen shape=rectangle]
		norm_l15_g3 [label="LayerNorm L15
GPU 3" fillcolor=lightgreen shape=rectangle]
		resid_l15_g3 [label="ResidAdd L15
GPU 3" fillcolor=orange shape=parallelogram]
	}
	transfer_g0_g1 [label="Transfer
GPU0->GPU1
819.2MB" fillcolor=red shape=ellipse]
	transfer_g1_g2 [label="Transfer
GPU1->GPU2
819.2MB" fillcolor=red shape=ellipse]
	transfer_g2_g3 [label="Transfer
GPU2->GPU3
819.2MB" fillcolor=red shape=ellipse]
	output [label="Output
Batch:128, Seq:10000
Dim:4096" fillcolor=lightcoral shape=ellipse]
	input -> attn_l0_g0
	attn_l0_g0 -> mlp_l0_g0
	mlp_l0_g0 -> norm_l0_g0
	norm_l0_g0 -> resid_l0_g0
	input -> resid_l0_g0
	resid_l0_g0 -> attn_l1_g0
	attn_l1_g0 -> mlp_l1_g0
	mlp_l1_g0 -> norm_l1_g0
	norm_l1_g0 -> resid_l1_g0
	resid_l0_g0 -> resid_l1_g0
	resid_l1_g0 -> attn_l2_g0
	attn_l2_g0 -> mlp_l2_g0
	mlp_l2_g0 -> norm_l2_g0
	norm_l2_g0 -> resid_l2_g0
	resid_l1_g0 -> resid_l2_g0
	resid_l2_g0 -> attn_l3_g0
	attn_l3_g0 -> mlp_l3_g0
	mlp_l3_g0 -> norm_l3_g0
	norm_l3_g0 -> resid_l3_g0
	resid_l2_g0 -> resid_l3_g0
	resid_l3_g0 -> transfer_g0_g1
	transfer_g0_g1 -> attn_l4_g1
	attn_l4_g1 -> mlp_l4_g1
	mlp_l4_g1 -> norm_l4_g1
	norm_l4_g1 -> resid_l4_g1
	transfer_g0_g1 -> resid_l4_g1
	resid_l4_g1 -> attn_l5_g1
	attn_l5_g1 -> mlp_l5_g1
	mlp_l5_g1 -> norm_l5_g1
	norm_l5_g1 -> resid_l5_g1
	resid_l4_g1 -> resid_l5_g1
	resid_l5_g1 -> attn_l6_g1
	attn_l6_g1 -> mlp_l6_g1
	mlp_l6_g1 -> norm_l6_g1
	norm_l6_g1 -> resid_l6_g1
	resid_l5_g1 -> resid_l6_g1
	resid_l6_g1 -> attn_l7_g1
	attn_l7_g1 -> mlp_l7_g1
	mlp_l7_g1 -> norm_l7_g1
	norm_l7_g1 -> resid_l7_g1
	resid_l6_g1 -> resid_l7_g1
	resid_l7_g1 -> transfer_g1_g2
	transfer_g1_g2 -> attn_l8_g2
	attn_l8_g2 -> mlp_l8_g2
	mlp_l8_g2 -> norm_l8_g2
	norm_l8_g2 -> resid_l8_g2
	transfer_g1_g2 -> resid_l8_g2
	resid_l8_g2 -> attn_l9_g2
	attn_l9_g2 -> mlp_l9_g2
	mlp_l9_g2 -> norm_l9_g2
	norm_l9_g2 -> resid_l9_g2
	resid_l8_g2 -> resid_l9_g2
	resid_l9_g2 -> attn_l10_g2
	attn_l10_g2 -> mlp_l10_g2
	mlp_l10_g2 -> norm_l10_g2
	norm_l10_g2 -> resid_l10_g2
	resid_l9_g2 -> resid_l10_g2
	resid_l10_g2 -> attn_l11_g2
	attn_l11_g2 -> mlp_l11_g2
	mlp_l11_g2 -> norm_l11_g2
	norm_l11_g2 -> resid_l11_g2
	resid_l10_g2 -> resid_l11_g2
	resid_l11_g2 -> transfer_g2_g3
	transfer_g2_g3 -> attn_l12_g3
	attn_l12_g3 -> mlp_l12_g3
	mlp_l12_g3 -> norm_l12_g3
	norm_l12_g3 -> resid_l12_g3
	transfer_g2_g3 -> resid_l12_g3
	resid_l12_g3 -> attn_l13_g3
	attn_l13_g3 -> mlp_l13_g3
	mlp_l13_g3 -> norm_l13_g3
	norm_l13_g3 -> resid_l13_g3
	resid_l12_g3 -> resid_l13_g3
	resid_l13_g3 -> attn_l14_g3
	attn_l14_g3 -> mlp_l14_g3
	mlp_l14_g3 -> norm_l14_g3
	norm_l14_g3 -> resid_l14_g3
	resid_l13_g3 -> resid_l14_g3
	resid_l14_g3 -> attn_l15_g3
	attn_l15_g3 -> mlp_l15_g3
	mlp_l15_g3 -> norm_l15_g3
	norm_l15_g3 -> resid_l15_g3
	resid_l14_g3 -> resid_l15_g3
	resid_l15_g3 -> output
}
