// Proposed: Layer-wise Partitioning Strategy
digraph {
	rankdir=TB size="30,30"
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=rectangle style=filled]
	node [fillcolor=yellow shape=parallelogram style=filled]
	input [label="Model Input
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: N/A" shape=ellipse]
	input -> layer0_ln1
	layer0_ln1 -> layer0_qkv
	layer0_qkv -> layer0_reshape_qkv
	layer0_reshape_qkv -> layer0_split_q
	layer0_reshape_qkv -> layer0_split_k
	layer0_reshape_qkv -> layer0_split_v
	layer0_split_q -> layer0_attn_score
	layer0_split_k -> layer0_attn_score
	layer0_attn_score -> layer0_attn_weights
	layer0_attn_weights -> layer0_attn_out
	layer0_split_v -> layer0_attn_out
	layer0_attn_out -> layer0_attn_reshape
	layer0_attn_reshape -> layer0_attn_proj
	input -> layer0_res1
	layer0_attn_proj -> layer0_res1
	layer0_res1 -> layer0_ln2
	layer0_ln2 -> layer0_mlp1
	layer0_mlp1 -> layer0_gelu
	layer0_gelu -> layer0_mlp2
	layer0_res1 -> layer0_res2
	layer0_mlp2 -> layer0_res2
	layer0_res2 -> send_to_gpu_1
	subgraph cluster_layer_0_gpu_0 {
		color=blue label="Layer 0 on GPU 0
SRAM/L2 Cache Optimized" style=dashed
		layer0_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" shape=rectangle]
		layer0_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 0" shape=rectangle]
		layer0_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 0" shape=rectangle]
		layer0_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 0" shape=rectangle]
		layer0_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 0" shape=rectangle]
		layer0_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 0" shape=rectangle]
		layer0_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 0" shape=rectangle]
		layer0_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 0" shape=rectangle]
		layer0_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 0" shape=rectangle]
		layer0_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" shape=rectangle]
		layer0_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" shape=rectangle]
		layer0_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" shape=rectangle]
		layer0_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" shape=rectangle]
		layer0_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 0" shape=rectangle]
		layer0_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 0" shape=rectangle]
		layer0_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" shape=rectangle]
		layer0_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0" shape=rectangle]
		send_to_gpu_1 [label="Send to GPU 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 0 → GPU 1" shape=parallelogram]
	}
	send_to_gpu_1 -> comm_layer_1
	comm_layer_1 -> layer1_ln1
	layer1_ln1 -> layer1_qkv
	layer1_qkv -> layer1_reshape_qkv
	layer1_reshape_qkv -> layer1_split_q
	layer1_reshape_qkv -> layer1_split_k
	layer1_reshape_qkv -> layer1_split_v
	layer1_split_q -> layer1_attn_score
	layer1_split_k -> layer1_attn_score
	layer1_attn_score -> layer1_attn_weights
	layer1_attn_weights -> layer1_attn_out
	layer1_split_v -> layer1_attn_out
	layer1_attn_out -> layer1_attn_reshape
	layer1_attn_reshape -> layer1_attn_proj
	comm_layer_1 -> layer1_res1
	layer1_attn_proj -> layer1_res1
	layer1_res1 -> layer1_ln2
	layer1_ln2 -> layer1_mlp1
	layer1_mlp1 -> layer1_gelu
	layer1_gelu -> layer1_mlp2
	layer1_res1 -> layer1_res2
	layer1_mlp2 -> layer1_res2
	layer1_res2 -> send_to_gpu_2
	subgraph cluster_layer_1_gpu_1 {
		color=blue label="Layer 1 on GPU 1
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_1 [label="Receive from GPU 0
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=parallelogram]
		layer1_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=rectangle]
		layer1_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 1" shape=rectangle]
		layer1_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 1" shape=rectangle]
		layer1_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 1" shape=rectangle]
		layer1_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 1" shape=rectangle]
		layer1_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 1" shape=rectangle]
		layer1_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 1" shape=rectangle]
		layer1_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 1" shape=rectangle]
		layer1_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 1" shape=rectangle]
		layer1_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=rectangle]
		layer1_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=rectangle]
		layer1_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=rectangle]
		layer1_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=rectangle]
		layer1_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 1" shape=rectangle]
		layer1_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 1" shape=rectangle]
		layer1_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=rectangle]
		layer1_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1" shape=rectangle]
		send_to_gpu_2 [label="Send to GPU 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 1 → GPU 2" shape=parallelogram]
	}
	send_to_gpu_2 -> comm_layer_2
	comm_layer_2 -> layer2_ln1
	layer2_ln1 -> layer2_qkv
	layer2_qkv -> layer2_reshape_qkv
	layer2_reshape_qkv -> layer2_split_q
	layer2_reshape_qkv -> layer2_split_k
	layer2_reshape_qkv -> layer2_split_v
	layer2_split_q -> layer2_attn_score
	layer2_split_k -> layer2_attn_score
	layer2_attn_score -> layer2_attn_weights
	layer2_attn_weights -> layer2_attn_out
	layer2_split_v -> layer2_attn_out
	layer2_attn_out -> layer2_attn_reshape
	layer2_attn_reshape -> layer2_attn_proj
	comm_layer_2 -> layer2_res1
	layer2_attn_proj -> layer2_res1
	layer2_res1 -> layer2_ln2
	layer2_ln2 -> layer2_mlp1
	layer2_mlp1 -> layer2_gelu
	layer2_gelu -> layer2_mlp2
	layer2_res1 -> layer2_res2
	layer2_mlp2 -> layer2_res2
	layer2_res2 -> send_to_gpu_3
	subgraph cluster_layer_2_gpu_2 {
		color=blue label="Layer 2 on GPU 2
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_2 [label="Receive from GPU 1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=parallelogram]
		layer2_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=rectangle]
		layer2_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 2" shape=rectangle]
		layer2_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 2" shape=rectangle]
		layer2_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 2" shape=rectangle]
		layer2_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 2" shape=rectangle]
		layer2_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 2" shape=rectangle]
		layer2_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 2" shape=rectangle]
		layer2_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 2" shape=rectangle]
		layer2_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 2" shape=rectangle]
		layer2_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=rectangle]
		layer2_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=rectangle]
		layer2_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=rectangle]
		layer2_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=rectangle]
		layer2_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 2" shape=rectangle]
		layer2_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 2" shape=rectangle]
		layer2_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=rectangle]
		layer2_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2" shape=rectangle]
		send_to_gpu_3 [label="Send to GPU 3
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 2 → GPU 3" shape=parallelogram]
	}
	send_to_gpu_3 -> comm_layer_3
	comm_layer_3 -> layer3_ln1
	layer3_ln1 -> layer3_qkv
	layer3_qkv -> layer3_reshape_qkv
	layer3_reshape_qkv -> layer3_split_q
	layer3_reshape_qkv -> layer3_split_k
	layer3_reshape_qkv -> layer3_split_v
	layer3_split_q -> layer3_attn_score
	layer3_split_k -> layer3_attn_score
	layer3_attn_score -> layer3_attn_weights
	layer3_attn_weights -> layer3_attn_out
	layer3_split_v -> layer3_attn_out
	layer3_attn_out -> layer3_attn_reshape
	layer3_attn_reshape -> layer3_attn_proj
	comm_layer_3 -> layer3_res1
	layer3_attn_proj -> layer3_res1
	layer3_res1 -> layer3_ln2
	layer3_ln2 -> layer3_mlp1
	layer3_mlp1 -> layer3_gelu
	layer3_gelu -> layer3_mlp2
	layer3_res1 -> layer3_res2
	layer3_mlp2 -> layer3_res2
	layer3_res2 -> send_to_gpu_4
	subgraph cluster_layer_3_gpu_3 {
		color=blue label="Layer 3 on GPU 3
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_3 [label="Receive from GPU 2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=parallelogram]
		layer3_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=rectangle]
		layer3_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 3" shape=rectangle]
		layer3_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 3" shape=rectangle]
		layer3_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 3" shape=rectangle]
		layer3_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 3" shape=rectangle]
		layer3_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 3" shape=rectangle]
		layer3_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 3" shape=rectangle]
		layer3_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 3" shape=rectangle]
		layer3_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 3" shape=rectangle]
		layer3_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=rectangle]
		layer3_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=rectangle]
		layer3_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=rectangle]
		layer3_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=rectangle]
		layer3_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 3" shape=rectangle]
		layer3_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 3" shape=rectangle]
		layer3_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=rectangle]
		layer3_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3" shape=rectangle]
		send_to_gpu_4 [label="Send to GPU 4
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 3 → GPU 4" shape=parallelogram]
	}
	send_to_gpu_4 -> comm_layer_4
	comm_layer_4 -> layer4_ln1
	layer4_ln1 -> layer4_qkv
	layer4_qkv -> layer4_reshape_qkv
	layer4_reshape_qkv -> layer4_split_q
	layer4_reshape_qkv -> layer4_split_k
	layer4_reshape_qkv -> layer4_split_v
	layer4_split_q -> layer4_attn_score
	layer4_split_k -> layer4_attn_score
	layer4_attn_score -> layer4_attn_weights
	layer4_attn_weights -> layer4_attn_out
	layer4_split_v -> layer4_attn_out
	layer4_attn_out -> layer4_attn_reshape
	layer4_attn_reshape -> layer4_attn_proj
	comm_layer_4 -> layer4_res1
	layer4_attn_proj -> layer4_res1
	layer4_res1 -> layer4_ln2
	layer4_ln2 -> layer4_mlp1
	layer4_mlp1 -> layer4_gelu
	layer4_gelu -> layer4_mlp2
	layer4_res1 -> layer4_res2
	layer4_mlp2 -> layer4_res2
	layer4_res2 -> send_to_gpu_5
	subgraph cluster_layer_4_gpu_4 {
		color=blue label="Layer 4 on GPU 4
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_4 [label="Receive from GPU 3
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=parallelogram]
		layer4_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=rectangle]
		layer4_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 4" shape=rectangle]
		layer4_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 4" shape=rectangle]
		layer4_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 4" shape=rectangle]
		layer4_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 4" shape=rectangle]
		layer4_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 4" shape=rectangle]
		layer4_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 4" shape=rectangle]
		layer4_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 4" shape=rectangle]
		layer4_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 4" shape=rectangle]
		layer4_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=rectangle]
		layer4_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=rectangle]
		layer4_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=rectangle]
		layer4_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=rectangle]
		layer4_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 4" shape=rectangle]
		layer4_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 4" shape=rectangle]
		layer4_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=rectangle]
		layer4_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4" shape=rectangle]
		send_to_gpu_5 [label="Send to GPU 5
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 4 → GPU 5" shape=parallelogram]
	}
	send_to_gpu_5 -> comm_layer_5
	comm_layer_5 -> layer5_ln1
	layer5_ln1 -> layer5_qkv
	layer5_qkv -> layer5_reshape_qkv
	layer5_reshape_qkv -> layer5_split_q
	layer5_reshape_qkv -> layer5_split_k
	layer5_reshape_qkv -> layer5_split_v
	layer5_split_q -> layer5_attn_score
	layer5_split_k -> layer5_attn_score
	layer5_attn_score -> layer5_attn_weights
	layer5_attn_weights -> layer5_attn_out
	layer5_split_v -> layer5_attn_out
	layer5_attn_out -> layer5_attn_reshape
	layer5_attn_reshape -> layer5_attn_proj
	comm_layer_5 -> layer5_res1
	layer5_attn_proj -> layer5_res1
	layer5_res1 -> layer5_ln2
	layer5_ln2 -> layer5_mlp1
	layer5_mlp1 -> layer5_gelu
	layer5_gelu -> layer5_mlp2
	layer5_res1 -> layer5_res2
	layer5_mlp2 -> layer5_res2
	layer5_res2 -> send_to_gpu_6
	subgraph cluster_layer_5_gpu_5 {
		color=blue label="Layer 5 on GPU 5
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_5 [label="Receive from GPU 4
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=parallelogram]
		layer5_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=rectangle]
		layer5_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 5" shape=rectangle]
		layer5_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 5" shape=rectangle]
		layer5_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 5" shape=rectangle]
		layer5_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 5" shape=rectangle]
		layer5_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 5" shape=rectangle]
		layer5_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 5" shape=rectangle]
		layer5_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 5" shape=rectangle]
		layer5_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 5" shape=rectangle]
		layer5_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=rectangle]
		layer5_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=rectangle]
		layer5_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=rectangle]
		layer5_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=rectangle]
		layer5_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 5" shape=rectangle]
		layer5_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 5" shape=rectangle]
		layer5_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=rectangle]
		layer5_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5" shape=rectangle]
		send_to_gpu_6 [label="Send to GPU 6
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 5 → GPU 6" shape=parallelogram]
	}
	send_to_gpu_6 -> comm_layer_6
	comm_layer_6 -> layer6_ln1
	layer6_ln1 -> layer6_qkv
	layer6_qkv -> layer6_reshape_qkv
	layer6_reshape_qkv -> layer6_split_q
	layer6_reshape_qkv -> layer6_split_k
	layer6_reshape_qkv -> layer6_split_v
	layer6_split_q -> layer6_attn_score
	layer6_split_k -> layer6_attn_score
	layer6_attn_score -> layer6_attn_weights
	layer6_attn_weights -> layer6_attn_out
	layer6_split_v -> layer6_attn_out
	layer6_attn_out -> layer6_attn_reshape
	layer6_attn_reshape -> layer6_attn_proj
	comm_layer_6 -> layer6_res1
	layer6_attn_proj -> layer6_res1
	layer6_res1 -> layer6_ln2
	layer6_ln2 -> layer6_mlp1
	layer6_mlp1 -> layer6_gelu
	layer6_gelu -> layer6_mlp2
	layer6_res1 -> layer6_res2
	layer6_mlp2 -> layer6_res2
	layer6_res2 -> send_to_gpu_7
	subgraph cluster_layer_6_gpu_6 {
		color=blue label="Layer 6 on GPU 6
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_6 [label="Receive from GPU 5
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=parallelogram]
		layer6_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=rectangle]
		layer6_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 6" shape=rectangle]
		layer6_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 6" shape=rectangle]
		layer6_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 6" shape=rectangle]
		layer6_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 6" shape=rectangle]
		layer6_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 6" shape=rectangle]
		layer6_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 6" shape=rectangle]
		layer6_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 6" shape=rectangle]
		layer6_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 6" shape=rectangle]
		layer6_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=rectangle]
		layer6_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=rectangle]
		layer6_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=rectangle]
		layer6_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=rectangle]
		layer6_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 6" shape=rectangle]
		layer6_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 6" shape=rectangle]
		layer6_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=rectangle]
		layer6_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6" shape=rectangle]
		send_to_gpu_7 [label="Send to GPU 7
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 6 → GPU 7" shape=parallelogram]
	}
	send_to_gpu_7 -> comm_layer_7
	comm_layer_7 -> layer7_ln1
	layer7_ln1 -> layer7_qkv
	layer7_qkv -> layer7_reshape_qkv
	layer7_reshape_qkv -> layer7_split_q
	layer7_reshape_qkv -> layer7_split_k
	layer7_reshape_qkv -> layer7_split_v
	layer7_split_q -> layer7_attn_score
	layer7_split_k -> layer7_attn_score
	layer7_attn_score -> layer7_attn_weights
	layer7_attn_weights -> layer7_attn_out
	layer7_split_v -> layer7_attn_out
	layer7_attn_out -> layer7_attn_reshape
	layer7_attn_reshape -> layer7_attn_proj
	comm_layer_7 -> layer7_res1
	layer7_attn_proj -> layer7_res1
	layer7_res1 -> layer7_ln2
	layer7_ln2 -> layer7_mlp1
	layer7_mlp1 -> layer7_gelu
	layer7_gelu -> layer7_mlp2
	layer7_res1 -> layer7_res2
	layer7_mlp2 -> layer7_res2
	layer7_res2 -> send_to_gpu_8
	subgraph cluster_layer_7_gpu_7 {
		color=blue label="Layer 7 on GPU 7
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_7 [label="Receive from GPU 6
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=parallelogram]
		layer7_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=rectangle]
		layer7_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 7" shape=rectangle]
		layer7_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 7" shape=rectangle]
		layer7_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 7" shape=rectangle]
		layer7_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 7" shape=rectangle]
		layer7_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 7" shape=rectangle]
		layer7_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 7" shape=rectangle]
		layer7_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 7" shape=rectangle]
		layer7_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 7" shape=rectangle]
		layer7_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=rectangle]
		layer7_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=rectangle]
		layer7_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=rectangle]
		layer7_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=rectangle]
		layer7_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 7" shape=rectangle]
		layer7_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 7" shape=rectangle]
		layer7_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=rectangle]
		layer7_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7" shape=rectangle]
		send_to_gpu_8 [label="Send to GPU 8
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 7 → GPU 8" shape=parallelogram]
	}
	send_to_gpu_8 -> comm_layer_8
	comm_layer_8 -> layer8_ln1
	layer8_ln1 -> layer8_qkv
	layer8_qkv -> layer8_reshape_qkv
	layer8_reshape_qkv -> layer8_split_q
	layer8_reshape_qkv -> layer8_split_k
	layer8_reshape_qkv -> layer8_split_v
	layer8_split_q -> layer8_attn_score
	layer8_split_k -> layer8_attn_score
	layer8_attn_score -> layer8_attn_weights
	layer8_attn_weights -> layer8_attn_out
	layer8_split_v -> layer8_attn_out
	layer8_attn_out -> layer8_attn_reshape
	layer8_attn_reshape -> layer8_attn_proj
	comm_layer_8 -> layer8_res1
	layer8_attn_proj -> layer8_res1
	layer8_res1 -> layer8_ln2
	layer8_ln2 -> layer8_mlp1
	layer8_mlp1 -> layer8_gelu
	layer8_gelu -> layer8_mlp2
	layer8_res1 -> layer8_res2
	layer8_mlp2 -> layer8_res2
	layer8_res2 -> send_to_gpu_9
	subgraph cluster_layer_8_gpu_8 {
		color=blue label="Layer 8 on GPU 8
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_8 [label="Receive from GPU 7
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=parallelogram]
		layer8_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=rectangle]
		layer8_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 8" shape=rectangle]
		layer8_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 8" shape=rectangle]
		layer8_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 8" shape=rectangle]
		layer8_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 8" shape=rectangle]
		layer8_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 8" shape=rectangle]
		layer8_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 8" shape=rectangle]
		layer8_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 8" shape=rectangle]
		layer8_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 8" shape=rectangle]
		layer8_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=rectangle]
		layer8_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=rectangle]
		layer8_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=rectangle]
		layer8_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=rectangle]
		layer8_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 8" shape=rectangle]
		layer8_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 8" shape=rectangle]
		layer8_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=rectangle]
		layer8_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8" shape=rectangle]
		send_to_gpu_9 [label="Send to GPU 9
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 8 → GPU 9" shape=parallelogram]
	}
	send_to_gpu_9 -> comm_layer_9
	comm_layer_9 -> layer9_ln1
	layer9_ln1 -> layer9_qkv
	layer9_qkv -> layer9_reshape_qkv
	layer9_reshape_qkv -> layer9_split_q
	layer9_reshape_qkv -> layer9_split_k
	layer9_reshape_qkv -> layer9_split_v
	layer9_split_q -> layer9_attn_score
	layer9_split_k -> layer9_attn_score
	layer9_attn_score -> layer9_attn_weights
	layer9_attn_weights -> layer9_attn_out
	layer9_split_v -> layer9_attn_out
	layer9_attn_out -> layer9_attn_reshape
	layer9_attn_reshape -> layer9_attn_proj
	comm_layer_9 -> layer9_res1
	layer9_attn_proj -> layer9_res1
	layer9_res1 -> layer9_ln2
	layer9_ln2 -> layer9_mlp1
	layer9_mlp1 -> layer9_gelu
	layer9_gelu -> layer9_mlp2
	layer9_res1 -> layer9_res2
	layer9_mlp2 -> layer9_res2
	layer9_res2 -> send_to_gpu_10
	subgraph cluster_layer_9_gpu_9 {
		color=blue label="Layer 9 on GPU 9
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_9 [label="Receive from GPU 8
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=parallelogram]
		layer9_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=rectangle]
		layer9_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 9" shape=rectangle]
		layer9_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 9" shape=rectangle]
		layer9_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 9" shape=rectangle]
		layer9_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 9" shape=rectangle]
		layer9_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 9" shape=rectangle]
		layer9_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 9" shape=rectangle]
		layer9_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 9" shape=rectangle]
		layer9_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 9" shape=rectangle]
		layer9_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=rectangle]
		layer9_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=rectangle]
		layer9_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=rectangle]
		layer9_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=rectangle]
		layer9_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 9" shape=rectangle]
		layer9_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 9" shape=rectangle]
		layer9_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=rectangle]
		layer9_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9" shape=rectangle]
		send_to_gpu_10 [label="Send to GPU 10
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 9 → GPU 10" shape=parallelogram]
	}
	send_to_gpu_10 -> comm_layer_10
	comm_layer_10 -> layer10_ln1
	layer10_ln1 -> layer10_qkv
	layer10_qkv -> layer10_reshape_qkv
	layer10_reshape_qkv -> layer10_split_q
	layer10_reshape_qkv -> layer10_split_k
	layer10_reshape_qkv -> layer10_split_v
	layer10_split_q -> layer10_attn_score
	layer10_split_k -> layer10_attn_score
	layer10_attn_score -> layer10_attn_weights
	layer10_attn_weights -> layer10_attn_out
	layer10_split_v -> layer10_attn_out
	layer10_attn_out -> layer10_attn_reshape
	layer10_attn_reshape -> layer10_attn_proj
	comm_layer_10 -> layer10_res1
	layer10_attn_proj -> layer10_res1
	layer10_res1 -> layer10_ln2
	layer10_ln2 -> layer10_mlp1
	layer10_mlp1 -> layer10_gelu
	layer10_gelu -> layer10_mlp2
	layer10_res1 -> layer10_res2
	layer10_mlp2 -> layer10_res2
	layer10_res2 -> send_to_gpu_11
	subgraph cluster_layer_10_gpu_10 {
		color=blue label="Layer 10 on GPU 10
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_10 [label="Receive from GPU 9
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=parallelogram]
		layer10_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=rectangle]
		layer10_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 10" shape=rectangle]
		layer10_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 10" shape=rectangle]
		layer10_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 10" shape=rectangle]
		layer10_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 10" shape=rectangle]
		layer10_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 10" shape=rectangle]
		layer10_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 10" shape=rectangle]
		layer10_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 10" shape=rectangle]
		layer10_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 10" shape=rectangle]
		layer10_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=rectangle]
		layer10_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=rectangle]
		layer10_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=rectangle]
		layer10_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=rectangle]
		layer10_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 10" shape=rectangle]
		layer10_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 10" shape=rectangle]
		layer10_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=rectangle]
		layer10_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10" shape=rectangle]
		send_to_gpu_11 [label="Send to GPU 11
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 10 → GPU 11" shape=parallelogram]
	}
	send_to_gpu_11 -> comm_layer_11
	comm_layer_11 -> layer11_ln1
	layer11_ln1 -> layer11_qkv
	layer11_qkv -> layer11_reshape_qkv
	layer11_reshape_qkv -> layer11_split_q
	layer11_reshape_qkv -> layer11_split_k
	layer11_reshape_qkv -> layer11_split_v
	layer11_split_q -> layer11_attn_score
	layer11_split_k -> layer11_attn_score
	layer11_attn_score -> layer11_attn_weights
	layer11_attn_weights -> layer11_attn_out
	layer11_split_v -> layer11_attn_out
	layer11_attn_out -> layer11_attn_reshape
	layer11_attn_reshape -> layer11_attn_proj
	comm_layer_11 -> layer11_res1
	layer11_attn_proj -> layer11_res1
	layer11_res1 -> layer11_ln2
	layer11_ln2 -> layer11_mlp1
	layer11_mlp1 -> layer11_gelu
	layer11_gelu -> layer11_mlp2
	layer11_res1 -> layer11_res2
	layer11_mlp2 -> layer11_res2
	layer11_res2 -> send_to_gpu_12
	subgraph cluster_layer_11_gpu_11 {
		color=blue label="Layer 11 on GPU 11
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_11 [label="Receive from GPU 10
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=parallelogram]
		layer11_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=rectangle]
		layer11_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 11" shape=rectangle]
		layer11_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 11" shape=rectangle]
		layer11_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 11" shape=rectangle]
		layer11_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 11" shape=rectangle]
		layer11_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 11" shape=rectangle]
		layer11_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 11" shape=rectangle]
		layer11_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 11" shape=rectangle]
		layer11_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 11" shape=rectangle]
		layer11_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=rectangle]
		layer11_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=rectangle]
		layer11_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=rectangle]
		layer11_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=rectangle]
		layer11_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 11" shape=rectangle]
		layer11_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 11" shape=rectangle]
		layer11_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=rectangle]
		layer11_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11" shape=rectangle]
		send_to_gpu_12 [label="Send to GPU 12
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 11 → GPU 12" shape=parallelogram]
	}
	send_to_gpu_12 -> comm_layer_12
	comm_layer_12 -> layer12_ln1
	layer12_ln1 -> layer12_qkv
	layer12_qkv -> layer12_reshape_qkv
	layer12_reshape_qkv -> layer12_split_q
	layer12_reshape_qkv -> layer12_split_k
	layer12_reshape_qkv -> layer12_split_v
	layer12_split_q -> layer12_attn_score
	layer12_split_k -> layer12_attn_score
	layer12_attn_score -> layer12_attn_weights
	layer12_attn_weights -> layer12_attn_out
	layer12_split_v -> layer12_attn_out
	layer12_attn_out -> layer12_attn_reshape
	layer12_attn_reshape -> layer12_attn_proj
	comm_layer_12 -> layer12_res1
	layer12_attn_proj -> layer12_res1
	layer12_res1 -> layer12_ln2
	layer12_ln2 -> layer12_mlp1
	layer12_mlp1 -> layer12_gelu
	layer12_gelu -> layer12_mlp2
	layer12_res1 -> layer12_res2
	layer12_mlp2 -> layer12_res2
	layer12_res2 -> send_to_gpu_13
	subgraph cluster_layer_12_gpu_12 {
		color=blue label="Layer 12 on GPU 12
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_12 [label="Receive from GPU 11
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=parallelogram]
		layer12_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=rectangle]
		layer12_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 12" shape=rectangle]
		layer12_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 12" shape=rectangle]
		layer12_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 12" shape=rectangle]
		layer12_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 12" shape=rectangle]
		layer12_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 12" shape=rectangle]
		layer12_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 12" shape=rectangle]
		layer12_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 12" shape=rectangle]
		layer12_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 12" shape=rectangle]
		layer12_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=rectangle]
		layer12_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=rectangle]
		layer12_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=rectangle]
		layer12_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=rectangle]
		layer12_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 12" shape=rectangle]
		layer12_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 12" shape=rectangle]
		layer12_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=rectangle]
		layer12_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12" shape=rectangle]
		send_to_gpu_13 [label="Send to GPU 13
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 12 → GPU 13" shape=parallelogram]
	}
	send_to_gpu_13 -> comm_layer_13
	comm_layer_13 -> layer13_ln1
	layer13_ln1 -> layer13_qkv
	layer13_qkv -> layer13_reshape_qkv
	layer13_reshape_qkv -> layer13_split_q
	layer13_reshape_qkv -> layer13_split_k
	layer13_reshape_qkv -> layer13_split_v
	layer13_split_q -> layer13_attn_score
	layer13_split_k -> layer13_attn_score
	layer13_attn_score -> layer13_attn_weights
	layer13_attn_weights -> layer13_attn_out
	layer13_split_v -> layer13_attn_out
	layer13_attn_out -> layer13_attn_reshape
	layer13_attn_reshape -> layer13_attn_proj
	comm_layer_13 -> layer13_res1
	layer13_attn_proj -> layer13_res1
	layer13_res1 -> layer13_ln2
	layer13_ln2 -> layer13_mlp1
	layer13_mlp1 -> layer13_gelu
	layer13_gelu -> layer13_mlp2
	layer13_res1 -> layer13_res2
	layer13_mlp2 -> layer13_res2
	layer13_res2 -> send_to_gpu_14
	subgraph cluster_layer_13_gpu_13 {
		color=blue label="Layer 13 on GPU 13
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_13 [label="Receive from GPU 12
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=parallelogram]
		layer13_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=rectangle]
		layer13_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 13" shape=rectangle]
		layer13_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 13" shape=rectangle]
		layer13_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 13" shape=rectangle]
		layer13_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 13" shape=rectangle]
		layer13_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 13" shape=rectangle]
		layer13_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 13" shape=rectangle]
		layer13_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 13" shape=rectangle]
		layer13_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 13" shape=rectangle]
		layer13_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=rectangle]
		layer13_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=rectangle]
		layer13_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=rectangle]
		layer13_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=rectangle]
		layer13_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 13" shape=rectangle]
		layer13_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 13" shape=rectangle]
		layer13_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=rectangle]
		layer13_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13" shape=rectangle]
		send_to_gpu_14 [label="Send to GPU 14
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 13 → GPU 14" shape=parallelogram]
	}
	send_to_gpu_14 -> comm_layer_14
	comm_layer_14 -> layer14_ln1
	layer14_ln1 -> layer14_qkv
	layer14_qkv -> layer14_reshape_qkv
	layer14_reshape_qkv -> layer14_split_q
	layer14_reshape_qkv -> layer14_split_k
	layer14_reshape_qkv -> layer14_split_v
	layer14_split_q -> layer14_attn_score
	layer14_split_k -> layer14_attn_score
	layer14_attn_score -> layer14_attn_weights
	layer14_attn_weights -> layer14_attn_out
	layer14_split_v -> layer14_attn_out
	layer14_attn_out -> layer14_attn_reshape
	layer14_attn_reshape -> layer14_attn_proj
	comm_layer_14 -> layer14_res1
	layer14_attn_proj -> layer14_res1
	layer14_res1 -> layer14_ln2
	layer14_ln2 -> layer14_mlp1
	layer14_mlp1 -> layer14_gelu
	layer14_gelu -> layer14_mlp2
	layer14_res1 -> layer14_res2
	layer14_mlp2 -> layer14_res2
	layer14_res2 -> send_to_gpu_15
	subgraph cluster_layer_14_gpu_14 {
		color=blue label="Layer 14 on GPU 14
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_14 [label="Receive from GPU 13
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=parallelogram]
		layer14_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=rectangle]
		layer14_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 14" shape=rectangle]
		layer14_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 14" shape=rectangle]
		layer14_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 14" shape=rectangle]
		layer14_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 14" shape=rectangle]
		layer14_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 14" shape=rectangle]
		layer14_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 14" shape=rectangle]
		layer14_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 14" shape=rectangle]
		layer14_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 14" shape=rectangle]
		layer14_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=rectangle]
		layer14_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=rectangle]
		layer14_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=rectangle]
		layer14_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=rectangle]
		layer14_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 14" shape=rectangle]
		layer14_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 14" shape=rectangle]
		layer14_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=rectangle]
		layer14_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14" shape=rectangle]
		send_to_gpu_15 [label="Send to GPU 15
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 14 → GPU 15" shape=parallelogram]
	}
	send_to_gpu_15 -> comm_layer_15
	comm_layer_15 -> layer15_ln1
	layer15_ln1 -> layer15_qkv
	layer15_qkv -> layer15_reshape_qkv
	layer15_reshape_qkv -> layer15_split_q
	layer15_reshape_qkv -> layer15_split_k
	layer15_reshape_qkv -> layer15_split_v
	layer15_split_q -> layer15_attn_score
	layer15_split_k -> layer15_attn_score
	layer15_attn_score -> layer15_attn_weights
	layer15_attn_weights -> layer15_attn_out
	layer15_split_v -> layer15_attn_out
	layer15_attn_out -> layer15_attn_reshape
	layer15_attn_reshape -> layer15_attn_proj
	comm_layer_15 -> layer15_res1
	layer15_attn_proj -> layer15_res1
	layer15_res1 -> layer15_ln2
	layer15_ln2 -> layer15_mlp1
	layer15_mlp1 -> layer15_gelu
	layer15_gelu -> layer15_mlp2
	layer15_res1 -> layer15_res2
	layer15_mlp2 -> layer15_res2
	subgraph cluster_layer_15_gpu_15 {
		color=blue label="Layer 15 on GPU 15
SRAM/L2 Cache Optimized" style=dashed
		comm_layer_15 [label="Receive from GPU 14
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=parallelogram]
		layer15_ln1 [label="LayerNorm1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=rectangle]
		layer15_qkv [label="QKV Linear
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 15" shape=rectangle]
		layer15_reshape_qkv [label="Reshape QKV
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
GPU: 15" shape=rectangle]
		layer15_split_q [label="Split Query
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 15" shape=rectangle]
		layer15_split_k [label="Split Key
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 15" shape=rectangle]
		layer15_split_v [label="Split Value
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512, qkv=3]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 15" shape=rectangle]
		layer15_attn_score [label="Attention Score
Input1: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 15" shape=rectangle]
		layer15_attn_weights [label="Softmax
Input: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Output: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
GPU: 15" shape=rectangle]
		layer15_attn_out [label="Attention Output
Input1: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]
Input2: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
GPU: 15" shape=rectangle]
		layer15_attn_reshape [label="Reshape Attention
Input: [batch_size=1024, seq_len=10000, num_heads=16, head_dim=512]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=rectangle]
		layer15_attn_proj [label="Attention Output Projection
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=rectangle]
		layer15_res1 [label="Residual Add 1
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=rectangle]
		layer15_ln2 [label="LayerNorm2
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=rectangle]
		layer15_mlp1 [label="MLP Linear1
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 15" shape=rectangle]
		layer15_gelu [label="GELU
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
GPU: 15" shape=rectangle]
		layer15_mlp2 [label="MLP Linear2
Input: [batch_size=1024, seq_len=10000, ffn_hidden_size=32768]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=rectangle]
		layer15_res2 [label="Residual Add 2
Input1: [batch_size=1024, seq_len=10000, hidden_size=8192]
Input2: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=rectangle]
	}
	output [label="Model Output
Input: [batch_size=1024, seq_len=10000, hidden_size=8192]
Output: [batch_size=1024, seq_len=10000, hidden_size=8192]
GPU: 15" shape=ellipse]
	layer15_res2 -> output
}
