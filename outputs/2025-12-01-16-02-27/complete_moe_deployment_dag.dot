// 7B MoE Model Complete Deployment DAG
digraph {
	dpi=300 rankdir=TB size="400,300"
	node [fontname=Arial fontsize=9]
	edge [fontname=Arial fontsize=8]
	input [label="Model Input\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightgreen shape=ellipse style=filled]
	layer0_input [label="Layer 0 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	input -> layer0_input
	layer0_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer0_input -> layer0_qkv_tp0
	layer0_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer0_input -> layer0_qkv_tp1
	layer0_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer0_input -> layer0_qkv_tp2
	layer0_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer0_input -> layer0_qkv_tp3
	layer0_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer0_qkv_tp0 -> layer0_qkv_allreduce
	layer0_qkv_tp1 -> layer0_qkv_allreduce
	layer0_qkv_tp2 -> layer0_qkv_allreduce
	layer0_qkv_tp3 -> layer0_qkv_allreduce
	layer0_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer0_qkv_allreduce -> layer0_attn_tp0
	layer0_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer0_qkv_allreduce -> layer0_attn_tp1
	layer0_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer0_qkv_allreduce -> layer0_attn_tp2
	layer0_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer0_qkv_allreduce -> layer0_attn_tp3
	layer0_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer0_attn_tp0 -> layer0_attn_out_tp0
	layer0_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer0_attn_tp1 -> layer0_attn_out_tp1
	layer0_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer0_attn_tp2 -> layer0_attn_out_tp2
	layer0_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer0_attn_tp3 -> layer0_attn_out_tp3
	layer0_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer0_attn_out_tp0 -> layer0_attn_allreduce
	layer0_attn_out_tp1 -> layer0_attn_allreduce
	layer0_attn_out_tp2 -> layer0_attn_allreduce
	layer0_attn_out_tp3 -> layer0_attn_allreduce
	layer0_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer0_input -> layer0_attn_residual
	layer0_attn_allreduce -> layer0_attn_residual
	layer0_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer0_attn_residual -> layer0_attn_layernorm
	layer0_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer0_attn_layernorm -> layer0_gate
	layer0_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert0_ep0 [style=dashed]
	layer0_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert1_ep0 [style=dashed]
	layer0_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert2_ep0 [style=dashed]
	layer0_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert3_ep0 [style=dashed]
	layer0_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert4_ep1 [style=dashed]
	layer0_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert5_ep1 [style=dashed]
	layer0_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert6_ep1 [style=dashed]
	layer0_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert7_ep1 [style=dashed]
	layer0_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert8_ep2 [style=dashed]
	layer0_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert9_ep2 [style=dashed]
	layer0_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert10_ep2 [style=dashed]
	layer0_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert11_ep2 [style=dashed]
	layer0_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert12_ep3 [style=dashed]
	layer0_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert13_ep3 [style=dashed]
	layer0_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert14_ep3 [style=dashed]
	layer0_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert15_ep3 [style=dashed]
	layer0_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert16_ep4 [style=dashed]
	layer0_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert17_ep4 [style=dashed]
	layer0_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert18_ep4 [style=dashed]
	layer0_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert19_ep4 [style=dashed]
	layer0_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert20_ep5 [style=dashed]
	layer0_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert21_ep5 [style=dashed]
	layer0_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert22_ep5 [style=dashed]
	layer0_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert23_ep5 [style=dashed]
	layer0_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert24_ep6 [style=dashed]
	layer0_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert25_ep6 [style=dashed]
	layer0_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert26_ep6 [style=dashed]
	layer0_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert27_ep6 [style=dashed]
	layer0_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert28_ep7 [style=dashed]
	layer0_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert29_ep7 [style=dashed]
	layer0_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert30_ep7 [style=dashed]
	layer0_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert31_ep7 [style=dashed]
	layer0_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert32_ep8 [style=dashed]
	layer0_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert33_ep8 [style=dashed]
	layer0_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert34_ep8 [style=dashed]
	layer0_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert35_ep8 [style=dashed]
	layer0_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert36_ep9 [style=dashed]
	layer0_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert37_ep9 [style=dashed]
	layer0_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert38_ep9 [style=dashed]
	layer0_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert39_ep9 [style=dashed]
	layer0_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert40_ep10 [style=dashed]
	layer0_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert41_ep10 [style=dashed]
	layer0_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert42_ep10 [style=dashed]
	layer0_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert43_ep10 [style=dashed]
	layer0_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert44_ep11 [style=dashed]
	layer0_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert45_ep11 [style=dashed]
	layer0_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert46_ep11 [style=dashed]
	layer0_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert47_ep11 [style=dashed]
	layer0_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert48_ep12 [style=dashed]
	layer0_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert49_ep12 [style=dashed]
	layer0_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert50_ep12 [style=dashed]
	layer0_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert51_ep12 [style=dashed]
	layer0_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert52_ep13 [style=dashed]
	layer0_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert53_ep13 [style=dashed]
	layer0_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert54_ep13 [style=dashed]
	layer0_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert55_ep13 [style=dashed]
	layer0_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert56_ep14 [style=dashed]
	layer0_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert57_ep14 [style=dashed]
	layer0_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert58_ep14 [style=dashed]
	layer0_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert59_ep14 [style=dashed]
	layer0_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert60_ep15 [style=dashed]
	layer0_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert61_ep15 [style=dashed]
	layer0_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert62_ep15 [style=dashed]
	layer0_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer0_gate -> layer0_expert63_ep15 [style=dashed]
	layer0_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer0_expert0_ep0 -> layer0_alltoall
	layer0_expert1_ep0 -> layer0_alltoall
	layer0_expert2_ep0 -> layer0_alltoall
	layer0_expert3_ep0 -> layer0_alltoall
	layer0_expert4_ep1 -> layer0_alltoall
	layer0_expert5_ep1 -> layer0_alltoall
	layer0_expert6_ep1 -> layer0_alltoall
	layer0_expert7_ep1 -> layer0_alltoall
	layer0_expert8_ep2 -> layer0_alltoall
	layer0_expert9_ep2 -> layer0_alltoall
	layer0_expert10_ep2 -> layer0_alltoall
	layer0_expert11_ep2 -> layer0_alltoall
	layer0_expert12_ep3 -> layer0_alltoall
	layer0_expert13_ep3 -> layer0_alltoall
	layer0_expert14_ep3 -> layer0_alltoall
	layer0_expert15_ep3 -> layer0_alltoall
	layer0_expert16_ep4 -> layer0_alltoall
	layer0_expert17_ep4 -> layer0_alltoall
	layer0_expert18_ep4 -> layer0_alltoall
	layer0_expert19_ep4 -> layer0_alltoall
	layer0_expert20_ep5 -> layer0_alltoall
	layer0_expert21_ep5 -> layer0_alltoall
	layer0_expert22_ep5 -> layer0_alltoall
	layer0_expert23_ep5 -> layer0_alltoall
	layer0_expert24_ep6 -> layer0_alltoall
	layer0_expert25_ep6 -> layer0_alltoall
	layer0_expert26_ep6 -> layer0_alltoall
	layer0_expert27_ep6 -> layer0_alltoall
	layer0_expert28_ep7 -> layer0_alltoall
	layer0_expert29_ep7 -> layer0_alltoall
	layer0_expert30_ep7 -> layer0_alltoall
	layer0_expert31_ep7 -> layer0_alltoall
	layer0_expert32_ep8 -> layer0_alltoall
	layer0_expert33_ep8 -> layer0_alltoall
	layer0_expert34_ep8 -> layer0_alltoall
	layer0_expert35_ep8 -> layer0_alltoall
	layer0_expert36_ep9 -> layer0_alltoall
	layer0_expert37_ep9 -> layer0_alltoall
	layer0_expert38_ep9 -> layer0_alltoall
	layer0_expert39_ep9 -> layer0_alltoall
	layer0_expert40_ep10 -> layer0_alltoall
	layer0_expert41_ep10 -> layer0_alltoall
	layer0_expert42_ep10 -> layer0_alltoall
	layer0_expert43_ep10 -> layer0_alltoall
	layer0_expert44_ep11 -> layer0_alltoall
	layer0_expert45_ep11 -> layer0_alltoall
	layer0_expert46_ep11 -> layer0_alltoall
	layer0_expert47_ep11 -> layer0_alltoall
	layer0_expert48_ep12 -> layer0_alltoall
	layer0_expert49_ep12 -> layer0_alltoall
	layer0_expert50_ep12 -> layer0_alltoall
	layer0_expert51_ep12 -> layer0_alltoall
	layer0_expert52_ep13 -> layer0_alltoall
	layer0_expert53_ep13 -> layer0_alltoall
	layer0_expert54_ep13 -> layer0_alltoall
	layer0_expert55_ep13 -> layer0_alltoall
	layer0_expert56_ep14 -> layer0_alltoall
	layer0_expert57_ep14 -> layer0_alltoall
	layer0_expert58_ep14 -> layer0_alltoall
	layer0_expert59_ep14 -> layer0_alltoall
	layer0_expert60_ep15 -> layer0_alltoall
	layer0_expert61_ep15 -> layer0_alltoall
	layer0_expert62_ep15 -> layer0_alltoall
	layer0_expert63_ep15 -> layer0_alltoall
	layer0_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer0_alltoall -> layer0_moe_aggregate
	layer0_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer0_attn_layernorm -> layer0_moe_residual [style=dashed]
	layer0_moe_aggregate -> layer0_moe_residual
	layer0_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer0_moe_residual -> layer0_moe_layernorm
	layer1_input [label="Layer 1 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer0_moe_layernorm -> layer1_input
	layer1_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer1_input -> layer1_qkv_tp0
	layer1_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer1_input -> layer1_qkv_tp1
	layer1_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer1_input -> layer1_qkv_tp2
	layer1_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer1_input -> layer1_qkv_tp3
	layer1_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer1_qkv_tp0 -> layer1_qkv_allreduce
	layer1_qkv_tp1 -> layer1_qkv_allreduce
	layer1_qkv_tp2 -> layer1_qkv_allreduce
	layer1_qkv_tp3 -> layer1_qkv_allreduce
	layer1_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer1_qkv_allreduce -> layer1_attn_tp0
	layer1_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer1_qkv_allreduce -> layer1_attn_tp1
	layer1_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer1_qkv_allreduce -> layer1_attn_tp2
	layer1_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer1_qkv_allreduce -> layer1_attn_tp3
	layer1_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer1_attn_tp0 -> layer1_attn_out_tp0
	layer1_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer1_attn_tp1 -> layer1_attn_out_tp1
	layer1_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer1_attn_tp2 -> layer1_attn_out_tp2
	layer1_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer1_attn_tp3 -> layer1_attn_out_tp3
	layer1_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer1_attn_out_tp0 -> layer1_attn_allreduce
	layer1_attn_out_tp1 -> layer1_attn_allreduce
	layer1_attn_out_tp2 -> layer1_attn_allreduce
	layer1_attn_out_tp3 -> layer1_attn_allreduce
	layer1_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer1_input -> layer1_attn_residual
	layer1_attn_allreduce -> layer1_attn_residual
	layer1_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer1_attn_residual -> layer1_attn_layernorm
	layer1_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer1_attn_layernorm -> layer1_gate
	layer1_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert0_ep0 [style=dashed]
	layer1_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert1_ep0 [style=dashed]
	layer1_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert2_ep0 [style=dashed]
	layer1_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert3_ep0 [style=dashed]
	layer1_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert4_ep1 [style=dashed]
	layer1_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert5_ep1 [style=dashed]
	layer1_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert6_ep1 [style=dashed]
	layer1_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert7_ep1 [style=dashed]
	layer1_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert8_ep2 [style=dashed]
	layer1_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert9_ep2 [style=dashed]
	layer1_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert10_ep2 [style=dashed]
	layer1_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert11_ep2 [style=dashed]
	layer1_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert12_ep3 [style=dashed]
	layer1_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert13_ep3 [style=dashed]
	layer1_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert14_ep3 [style=dashed]
	layer1_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert15_ep3 [style=dashed]
	layer1_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert16_ep4 [style=dashed]
	layer1_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert17_ep4 [style=dashed]
	layer1_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert18_ep4 [style=dashed]
	layer1_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert19_ep4 [style=dashed]
	layer1_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert20_ep5 [style=dashed]
	layer1_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert21_ep5 [style=dashed]
	layer1_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert22_ep5 [style=dashed]
	layer1_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert23_ep5 [style=dashed]
	layer1_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert24_ep6 [style=dashed]
	layer1_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert25_ep6 [style=dashed]
	layer1_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert26_ep6 [style=dashed]
	layer1_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert27_ep6 [style=dashed]
	layer1_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert28_ep7 [style=dashed]
	layer1_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert29_ep7 [style=dashed]
	layer1_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert30_ep7 [style=dashed]
	layer1_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert31_ep7 [style=dashed]
	layer1_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert32_ep8 [style=dashed]
	layer1_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert33_ep8 [style=dashed]
	layer1_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert34_ep8 [style=dashed]
	layer1_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert35_ep8 [style=dashed]
	layer1_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert36_ep9 [style=dashed]
	layer1_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert37_ep9 [style=dashed]
	layer1_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert38_ep9 [style=dashed]
	layer1_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert39_ep9 [style=dashed]
	layer1_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert40_ep10 [style=dashed]
	layer1_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert41_ep10 [style=dashed]
	layer1_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert42_ep10 [style=dashed]
	layer1_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert43_ep10 [style=dashed]
	layer1_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert44_ep11 [style=dashed]
	layer1_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert45_ep11 [style=dashed]
	layer1_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert46_ep11 [style=dashed]
	layer1_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert47_ep11 [style=dashed]
	layer1_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert48_ep12 [style=dashed]
	layer1_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert49_ep12 [style=dashed]
	layer1_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert50_ep12 [style=dashed]
	layer1_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert51_ep12 [style=dashed]
	layer1_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert52_ep13 [style=dashed]
	layer1_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert53_ep13 [style=dashed]
	layer1_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert54_ep13 [style=dashed]
	layer1_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert55_ep13 [style=dashed]
	layer1_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert56_ep14 [style=dashed]
	layer1_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert57_ep14 [style=dashed]
	layer1_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert58_ep14 [style=dashed]
	layer1_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert59_ep14 [style=dashed]
	layer1_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert60_ep15 [style=dashed]
	layer1_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert61_ep15 [style=dashed]
	layer1_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert62_ep15 [style=dashed]
	layer1_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer1_gate -> layer1_expert63_ep15 [style=dashed]
	layer1_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer1_expert0_ep0 -> layer1_alltoall
	layer1_expert1_ep0 -> layer1_alltoall
	layer1_expert2_ep0 -> layer1_alltoall
	layer1_expert3_ep0 -> layer1_alltoall
	layer1_expert4_ep1 -> layer1_alltoall
	layer1_expert5_ep1 -> layer1_alltoall
	layer1_expert6_ep1 -> layer1_alltoall
	layer1_expert7_ep1 -> layer1_alltoall
	layer1_expert8_ep2 -> layer1_alltoall
	layer1_expert9_ep2 -> layer1_alltoall
	layer1_expert10_ep2 -> layer1_alltoall
	layer1_expert11_ep2 -> layer1_alltoall
	layer1_expert12_ep3 -> layer1_alltoall
	layer1_expert13_ep3 -> layer1_alltoall
	layer1_expert14_ep3 -> layer1_alltoall
	layer1_expert15_ep3 -> layer1_alltoall
	layer1_expert16_ep4 -> layer1_alltoall
	layer1_expert17_ep4 -> layer1_alltoall
	layer1_expert18_ep4 -> layer1_alltoall
	layer1_expert19_ep4 -> layer1_alltoall
	layer1_expert20_ep5 -> layer1_alltoall
	layer1_expert21_ep5 -> layer1_alltoall
	layer1_expert22_ep5 -> layer1_alltoall
	layer1_expert23_ep5 -> layer1_alltoall
	layer1_expert24_ep6 -> layer1_alltoall
	layer1_expert25_ep6 -> layer1_alltoall
	layer1_expert26_ep6 -> layer1_alltoall
	layer1_expert27_ep6 -> layer1_alltoall
	layer1_expert28_ep7 -> layer1_alltoall
	layer1_expert29_ep7 -> layer1_alltoall
	layer1_expert30_ep7 -> layer1_alltoall
	layer1_expert31_ep7 -> layer1_alltoall
	layer1_expert32_ep8 -> layer1_alltoall
	layer1_expert33_ep8 -> layer1_alltoall
	layer1_expert34_ep8 -> layer1_alltoall
	layer1_expert35_ep8 -> layer1_alltoall
	layer1_expert36_ep9 -> layer1_alltoall
	layer1_expert37_ep9 -> layer1_alltoall
	layer1_expert38_ep9 -> layer1_alltoall
	layer1_expert39_ep9 -> layer1_alltoall
	layer1_expert40_ep10 -> layer1_alltoall
	layer1_expert41_ep10 -> layer1_alltoall
	layer1_expert42_ep10 -> layer1_alltoall
	layer1_expert43_ep10 -> layer1_alltoall
	layer1_expert44_ep11 -> layer1_alltoall
	layer1_expert45_ep11 -> layer1_alltoall
	layer1_expert46_ep11 -> layer1_alltoall
	layer1_expert47_ep11 -> layer1_alltoall
	layer1_expert48_ep12 -> layer1_alltoall
	layer1_expert49_ep12 -> layer1_alltoall
	layer1_expert50_ep12 -> layer1_alltoall
	layer1_expert51_ep12 -> layer1_alltoall
	layer1_expert52_ep13 -> layer1_alltoall
	layer1_expert53_ep13 -> layer1_alltoall
	layer1_expert54_ep13 -> layer1_alltoall
	layer1_expert55_ep13 -> layer1_alltoall
	layer1_expert56_ep14 -> layer1_alltoall
	layer1_expert57_ep14 -> layer1_alltoall
	layer1_expert58_ep14 -> layer1_alltoall
	layer1_expert59_ep14 -> layer1_alltoall
	layer1_expert60_ep15 -> layer1_alltoall
	layer1_expert61_ep15 -> layer1_alltoall
	layer1_expert62_ep15 -> layer1_alltoall
	layer1_expert63_ep15 -> layer1_alltoall
	layer1_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer1_alltoall -> layer1_moe_aggregate
	layer1_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer1_attn_layernorm -> layer1_moe_residual [style=dashed]
	layer1_moe_aggregate -> layer1_moe_residual
	layer1_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer1_moe_residual -> layer1_moe_layernorm
	layer2_input [label="Layer 2 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer1_moe_layernorm -> layer2_input
	layer2_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer2_input -> layer2_qkv_tp0
	layer2_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer2_input -> layer2_qkv_tp1
	layer2_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer2_input -> layer2_qkv_tp2
	layer2_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer2_input -> layer2_qkv_tp3
	layer2_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer2_qkv_tp0 -> layer2_qkv_allreduce
	layer2_qkv_tp1 -> layer2_qkv_allreduce
	layer2_qkv_tp2 -> layer2_qkv_allreduce
	layer2_qkv_tp3 -> layer2_qkv_allreduce
	layer2_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer2_qkv_allreduce -> layer2_attn_tp0
	layer2_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer2_qkv_allreduce -> layer2_attn_tp1
	layer2_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer2_qkv_allreduce -> layer2_attn_tp2
	layer2_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer2_qkv_allreduce -> layer2_attn_tp3
	layer2_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer2_attn_tp0 -> layer2_attn_out_tp0
	layer2_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer2_attn_tp1 -> layer2_attn_out_tp1
	layer2_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer2_attn_tp2 -> layer2_attn_out_tp2
	layer2_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer2_attn_tp3 -> layer2_attn_out_tp3
	layer2_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer2_attn_out_tp0 -> layer2_attn_allreduce
	layer2_attn_out_tp1 -> layer2_attn_allreduce
	layer2_attn_out_tp2 -> layer2_attn_allreduce
	layer2_attn_out_tp3 -> layer2_attn_allreduce
	layer2_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer2_input -> layer2_attn_residual
	layer2_attn_allreduce -> layer2_attn_residual
	layer2_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer2_attn_residual -> layer2_attn_layernorm
	layer2_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer2_attn_layernorm -> layer2_gate
	layer2_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert0_ep0 [style=dashed]
	layer2_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert1_ep0 [style=dashed]
	layer2_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert2_ep0 [style=dashed]
	layer2_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert3_ep0 [style=dashed]
	layer2_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert4_ep1 [style=dashed]
	layer2_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert5_ep1 [style=dashed]
	layer2_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert6_ep1 [style=dashed]
	layer2_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert7_ep1 [style=dashed]
	layer2_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert8_ep2 [style=dashed]
	layer2_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert9_ep2 [style=dashed]
	layer2_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert10_ep2 [style=dashed]
	layer2_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert11_ep2 [style=dashed]
	layer2_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert12_ep3 [style=dashed]
	layer2_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert13_ep3 [style=dashed]
	layer2_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert14_ep3 [style=dashed]
	layer2_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert15_ep3 [style=dashed]
	layer2_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert16_ep4 [style=dashed]
	layer2_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert17_ep4 [style=dashed]
	layer2_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert18_ep4 [style=dashed]
	layer2_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert19_ep4 [style=dashed]
	layer2_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert20_ep5 [style=dashed]
	layer2_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert21_ep5 [style=dashed]
	layer2_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert22_ep5 [style=dashed]
	layer2_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert23_ep5 [style=dashed]
	layer2_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert24_ep6 [style=dashed]
	layer2_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert25_ep6 [style=dashed]
	layer2_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert26_ep6 [style=dashed]
	layer2_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert27_ep6 [style=dashed]
	layer2_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert28_ep7 [style=dashed]
	layer2_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert29_ep7 [style=dashed]
	layer2_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert30_ep7 [style=dashed]
	layer2_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert31_ep7 [style=dashed]
	layer2_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert32_ep8 [style=dashed]
	layer2_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert33_ep8 [style=dashed]
	layer2_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert34_ep8 [style=dashed]
	layer2_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert35_ep8 [style=dashed]
	layer2_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert36_ep9 [style=dashed]
	layer2_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert37_ep9 [style=dashed]
	layer2_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert38_ep9 [style=dashed]
	layer2_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert39_ep9 [style=dashed]
	layer2_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert40_ep10 [style=dashed]
	layer2_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert41_ep10 [style=dashed]
	layer2_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert42_ep10 [style=dashed]
	layer2_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert43_ep10 [style=dashed]
	layer2_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert44_ep11 [style=dashed]
	layer2_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert45_ep11 [style=dashed]
	layer2_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert46_ep11 [style=dashed]
	layer2_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert47_ep11 [style=dashed]
	layer2_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert48_ep12 [style=dashed]
	layer2_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert49_ep12 [style=dashed]
	layer2_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert50_ep12 [style=dashed]
	layer2_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert51_ep12 [style=dashed]
	layer2_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert52_ep13 [style=dashed]
	layer2_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert53_ep13 [style=dashed]
	layer2_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert54_ep13 [style=dashed]
	layer2_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert55_ep13 [style=dashed]
	layer2_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert56_ep14 [style=dashed]
	layer2_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert57_ep14 [style=dashed]
	layer2_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert58_ep14 [style=dashed]
	layer2_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert59_ep14 [style=dashed]
	layer2_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert60_ep15 [style=dashed]
	layer2_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert61_ep15 [style=dashed]
	layer2_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert62_ep15 [style=dashed]
	layer2_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer2_gate -> layer2_expert63_ep15 [style=dashed]
	layer2_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer2_expert0_ep0 -> layer2_alltoall
	layer2_expert1_ep0 -> layer2_alltoall
	layer2_expert2_ep0 -> layer2_alltoall
	layer2_expert3_ep0 -> layer2_alltoall
	layer2_expert4_ep1 -> layer2_alltoall
	layer2_expert5_ep1 -> layer2_alltoall
	layer2_expert6_ep1 -> layer2_alltoall
	layer2_expert7_ep1 -> layer2_alltoall
	layer2_expert8_ep2 -> layer2_alltoall
	layer2_expert9_ep2 -> layer2_alltoall
	layer2_expert10_ep2 -> layer2_alltoall
	layer2_expert11_ep2 -> layer2_alltoall
	layer2_expert12_ep3 -> layer2_alltoall
	layer2_expert13_ep3 -> layer2_alltoall
	layer2_expert14_ep3 -> layer2_alltoall
	layer2_expert15_ep3 -> layer2_alltoall
	layer2_expert16_ep4 -> layer2_alltoall
	layer2_expert17_ep4 -> layer2_alltoall
	layer2_expert18_ep4 -> layer2_alltoall
	layer2_expert19_ep4 -> layer2_alltoall
	layer2_expert20_ep5 -> layer2_alltoall
	layer2_expert21_ep5 -> layer2_alltoall
	layer2_expert22_ep5 -> layer2_alltoall
	layer2_expert23_ep5 -> layer2_alltoall
	layer2_expert24_ep6 -> layer2_alltoall
	layer2_expert25_ep6 -> layer2_alltoall
	layer2_expert26_ep6 -> layer2_alltoall
	layer2_expert27_ep6 -> layer2_alltoall
	layer2_expert28_ep7 -> layer2_alltoall
	layer2_expert29_ep7 -> layer2_alltoall
	layer2_expert30_ep7 -> layer2_alltoall
	layer2_expert31_ep7 -> layer2_alltoall
	layer2_expert32_ep8 -> layer2_alltoall
	layer2_expert33_ep8 -> layer2_alltoall
	layer2_expert34_ep8 -> layer2_alltoall
	layer2_expert35_ep8 -> layer2_alltoall
	layer2_expert36_ep9 -> layer2_alltoall
	layer2_expert37_ep9 -> layer2_alltoall
	layer2_expert38_ep9 -> layer2_alltoall
	layer2_expert39_ep9 -> layer2_alltoall
	layer2_expert40_ep10 -> layer2_alltoall
	layer2_expert41_ep10 -> layer2_alltoall
	layer2_expert42_ep10 -> layer2_alltoall
	layer2_expert43_ep10 -> layer2_alltoall
	layer2_expert44_ep11 -> layer2_alltoall
	layer2_expert45_ep11 -> layer2_alltoall
	layer2_expert46_ep11 -> layer2_alltoall
	layer2_expert47_ep11 -> layer2_alltoall
	layer2_expert48_ep12 -> layer2_alltoall
	layer2_expert49_ep12 -> layer2_alltoall
	layer2_expert50_ep12 -> layer2_alltoall
	layer2_expert51_ep12 -> layer2_alltoall
	layer2_expert52_ep13 -> layer2_alltoall
	layer2_expert53_ep13 -> layer2_alltoall
	layer2_expert54_ep13 -> layer2_alltoall
	layer2_expert55_ep13 -> layer2_alltoall
	layer2_expert56_ep14 -> layer2_alltoall
	layer2_expert57_ep14 -> layer2_alltoall
	layer2_expert58_ep14 -> layer2_alltoall
	layer2_expert59_ep14 -> layer2_alltoall
	layer2_expert60_ep15 -> layer2_alltoall
	layer2_expert61_ep15 -> layer2_alltoall
	layer2_expert62_ep15 -> layer2_alltoall
	layer2_expert63_ep15 -> layer2_alltoall
	layer2_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer2_alltoall -> layer2_moe_aggregate
	layer2_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer2_attn_layernorm -> layer2_moe_residual [style=dashed]
	layer2_moe_aggregate -> layer2_moe_residual
	layer2_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer2_moe_residual -> layer2_moe_layernorm
	layer3_input [label="Layer 3 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer2_moe_layernorm -> layer3_input
	layer3_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer3_input -> layer3_qkv_tp0
	layer3_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer3_input -> layer3_qkv_tp1
	layer3_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer3_input -> layer3_qkv_tp2
	layer3_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer3_input -> layer3_qkv_tp3
	layer3_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer3_qkv_tp0 -> layer3_qkv_allreduce
	layer3_qkv_tp1 -> layer3_qkv_allreduce
	layer3_qkv_tp2 -> layer3_qkv_allreduce
	layer3_qkv_tp3 -> layer3_qkv_allreduce
	layer3_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer3_qkv_allreduce -> layer3_attn_tp0
	layer3_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer3_qkv_allreduce -> layer3_attn_tp1
	layer3_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer3_qkv_allreduce -> layer3_attn_tp2
	layer3_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer3_qkv_allreduce -> layer3_attn_tp3
	layer3_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer3_attn_tp0 -> layer3_attn_out_tp0
	layer3_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer3_attn_tp1 -> layer3_attn_out_tp1
	layer3_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer3_attn_tp2 -> layer3_attn_out_tp2
	layer3_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer3_attn_tp3 -> layer3_attn_out_tp3
	layer3_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer3_attn_out_tp0 -> layer3_attn_allreduce
	layer3_attn_out_tp1 -> layer3_attn_allreduce
	layer3_attn_out_tp2 -> layer3_attn_allreduce
	layer3_attn_out_tp3 -> layer3_attn_allreduce
	layer3_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer3_input -> layer3_attn_residual
	layer3_attn_allreduce -> layer3_attn_residual
	layer3_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer3_attn_residual -> layer3_attn_layernorm
	layer3_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer3_attn_layernorm -> layer3_gate
	layer3_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert0_ep0 [style=dashed]
	layer3_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert1_ep0 [style=dashed]
	layer3_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert2_ep0 [style=dashed]
	layer3_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert3_ep0 [style=dashed]
	layer3_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert4_ep1 [style=dashed]
	layer3_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert5_ep1 [style=dashed]
	layer3_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert6_ep1 [style=dashed]
	layer3_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert7_ep1 [style=dashed]
	layer3_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert8_ep2 [style=dashed]
	layer3_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert9_ep2 [style=dashed]
	layer3_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert10_ep2 [style=dashed]
	layer3_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert11_ep2 [style=dashed]
	layer3_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert12_ep3 [style=dashed]
	layer3_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert13_ep3 [style=dashed]
	layer3_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert14_ep3 [style=dashed]
	layer3_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert15_ep3 [style=dashed]
	layer3_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert16_ep4 [style=dashed]
	layer3_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert17_ep4 [style=dashed]
	layer3_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert18_ep4 [style=dashed]
	layer3_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert19_ep4 [style=dashed]
	layer3_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert20_ep5 [style=dashed]
	layer3_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert21_ep5 [style=dashed]
	layer3_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert22_ep5 [style=dashed]
	layer3_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert23_ep5 [style=dashed]
	layer3_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert24_ep6 [style=dashed]
	layer3_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert25_ep6 [style=dashed]
	layer3_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert26_ep6 [style=dashed]
	layer3_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert27_ep6 [style=dashed]
	layer3_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert28_ep7 [style=dashed]
	layer3_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert29_ep7 [style=dashed]
	layer3_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert30_ep7 [style=dashed]
	layer3_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert31_ep7 [style=dashed]
	layer3_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert32_ep8 [style=dashed]
	layer3_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert33_ep8 [style=dashed]
	layer3_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert34_ep8 [style=dashed]
	layer3_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert35_ep8 [style=dashed]
	layer3_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert36_ep9 [style=dashed]
	layer3_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert37_ep9 [style=dashed]
	layer3_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert38_ep9 [style=dashed]
	layer3_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert39_ep9 [style=dashed]
	layer3_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert40_ep10 [style=dashed]
	layer3_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert41_ep10 [style=dashed]
	layer3_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert42_ep10 [style=dashed]
	layer3_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert43_ep10 [style=dashed]
	layer3_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert44_ep11 [style=dashed]
	layer3_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert45_ep11 [style=dashed]
	layer3_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert46_ep11 [style=dashed]
	layer3_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert47_ep11 [style=dashed]
	layer3_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert48_ep12 [style=dashed]
	layer3_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert49_ep12 [style=dashed]
	layer3_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert50_ep12 [style=dashed]
	layer3_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert51_ep12 [style=dashed]
	layer3_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert52_ep13 [style=dashed]
	layer3_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert53_ep13 [style=dashed]
	layer3_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert54_ep13 [style=dashed]
	layer3_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert55_ep13 [style=dashed]
	layer3_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert56_ep14 [style=dashed]
	layer3_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert57_ep14 [style=dashed]
	layer3_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert58_ep14 [style=dashed]
	layer3_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert59_ep14 [style=dashed]
	layer3_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert60_ep15 [style=dashed]
	layer3_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert61_ep15 [style=dashed]
	layer3_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert62_ep15 [style=dashed]
	layer3_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer3_gate -> layer3_expert63_ep15 [style=dashed]
	layer3_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer3_expert0_ep0 -> layer3_alltoall
	layer3_expert1_ep0 -> layer3_alltoall
	layer3_expert2_ep0 -> layer3_alltoall
	layer3_expert3_ep0 -> layer3_alltoall
	layer3_expert4_ep1 -> layer3_alltoall
	layer3_expert5_ep1 -> layer3_alltoall
	layer3_expert6_ep1 -> layer3_alltoall
	layer3_expert7_ep1 -> layer3_alltoall
	layer3_expert8_ep2 -> layer3_alltoall
	layer3_expert9_ep2 -> layer3_alltoall
	layer3_expert10_ep2 -> layer3_alltoall
	layer3_expert11_ep2 -> layer3_alltoall
	layer3_expert12_ep3 -> layer3_alltoall
	layer3_expert13_ep3 -> layer3_alltoall
	layer3_expert14_ep3 -> layer3_alltoall
	layer3_expert15_ep3 -> layer3_alltoall
	layer3_expert16_ep4 -> layer3_alltoall
	layer3_expert17_ep4 -> layer3_alltoall
	layer3_expert18_ep4 -> layer3_alltoall
	layer3_expert19_ep4 -> layer3_alltoall
	layer3_expert20_ep5 -> layer3_alltoall
	layer3_expert21_ep5 -> layer3_alltoall
	layer3_expert22_ep5 -> layer3_alltoall
	layer3_expert23_ep5 -> layer3_alltoall
	layer3_expert24_ep6 -> layer3_alltoall
	layer3_expert25_ep6 -> layer3_alltoall
	layer3_expert26_ep6 -> layer3_alltoall
	layer3_expert27_ep6 -> layer3_alltoall
	layer3_expert28_ep7 -> layer3_alltoall
	layer3_expert29_ep7 -> layer3_alltoall
	layer3_expert30_ep7 -> layer3_alltoall
	layer3_expert31_ep7 -> layer3_alltoall
	layer3_expert32_ep8 -> layer3_alltoall
	layer3_expert33_ep8 -> layer3_alltoall
	layer3_expert34_ep8 -> layer3_alltoall
	layer3_expert35_ep8 -> layer3_alltoall
	layer3_expert36_ep9 -> layer3_alltoall
	layer3_expert37_ep9 -> layer3_alltoall
	layer3_expert38_ep9 -> layer3_alltoall
	layer3_expert39_ep9 -> layer3_alltoall
	layer3_expert40_ep10 -> layer3_alltoall
	layer3_expert41_ep10 -> layer3_alltoall
	layer3_expert42_ep10 -> layer3_alltoall
	layer3_expert43_ep10 -> layer3_alltoall
	layer3_expert44_ep11 -> layer3_alltoall
	layer3_expert45_ep11 -> layer3_alltoall
	layer3_expert46_ep11 -> layer3_alltoall
	layer3_expert47_ep11 -> layer3_alltoall
	layer3_expert48_ep12 -> layer3_alltoall
	layer3_expert49_ep12 -> layer3_alltoall
	layer3_expert50_ep12 -> layer3_alltoall
	layer3_expert51_ep12 -> layer3_alltoall
	layer3_expert52_ep13 -> layer3_alltoall
	layer3_expert53_ep13 -> layer3_alltoall
	layer3_expert54_ep13 -> layer3_alltoall
	layer3_expert55_ep13 -> layer3_alltoall
	layer3_expert56_ep14 -> layer3_alltoall
	layer3_expert57_ep14 -> layer3_alltoall
	layer3_expert58_ep14 -> layer3_alltoall
	layer3_expert59_ep14 -> layer3_alltoall
	layer3_expert60_ep15 -> layer3_alltoall
	layer3_expert61_ep15 -> layer3_alltoall
	layer3_expert62_ep15 -> layer3_alltoall
	layer3_expert63_ep15 -> layer3_alltoall
	layer3_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer3_alltoall -> layer3_moe_aggregate
	layer3_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer3_attn_layernorm -> layer3_moe_residual [style=dashed]
	layer3_moe_aggregate -> layer3_moe_residual
	layer3_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer3_moe_residual -> layer3_moe_layernorm
	layer4_input [label="Layer 4 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer3_moe_layernorm -> layer4_input
	layer4_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer4_input -> layer4_qkv_tp0
	layer4_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer4_input -> layer4_qkv_tp1
	layer4_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer4_input -> layer4_qkv_tp2
	layer4_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer4_input -> layer4_qkv_tp3
	layer4_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer4_qkv_tp0 -> layer4_qkv_allreduce
	layer4_qkv_tp1 -> layer4_qkv_allreduce
	layer4_qkv_tp2 -> layer4_qkv_allreduce
	layer4_qkv_tp3 -> layer4_qkv_allreduce
	layer4_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer4_qkv_allreduce -> layer4_attn_tp0
	layer4_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer4_qkv_allreduce -> layer4_attn_tp1
	layer4_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer4_qkv_allreduce -> layer4_attn_tp2
	layer4_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer4_qkv_allreduce -> layer4_attn_tp3
	layer4_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer4_attn_tp0 -> layer4_attn_out_tp0
	layer4_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer4_attn_tp1 -> layer4_attn_out_tp1
	layer4_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer4_attn_tp2 -> layer4_attn_out_tp2
	layer4_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer4_attn_tp3 -> layer4_attn_out_tp3
	layer4_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer4_attn_out_tp0 -> layer4_attn_allreduce
	layer4_attn_out_tp1 -> layer4_attn_allreduce
	layer4_attn_out_tp2 -> layer4_attn_allreduce
	layer4_attn_out_tp3 -> layer4_attn_allreduce
	layer4_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer4_input -> layer4_attn_residual
	layer4_attn_allreduce -> layer4_attn_residual
	layer4_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer4_attn_residual -> layer4_attn_layernorm
	layer4_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer4_attn_layernorm -> layer4_gate
	layer4_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert0_ep0 [style=dashed]
	layer4_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert1_ep0 [style=dashed]
	layer4_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert2_ep0 [style=dashed]
	layer4_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert3_ep0 [style=dashed]
	layer4_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert4_ep1 [style=dashed]
	layer4_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert5_ep1 [style=dashed]
	layer4_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert6_ep1 [style=dashed]
	layer4_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert7_ep1 [style=dashed]
	layer4_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert8_ep2 [style=dashed]
	layer4_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert9_ep2 [style=dashed]
	layer4_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert10_ep2 [style=dashed]
	layer4_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert11_ep2 [style=dashed]
	layer4_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert12_ep3 [style=dashed]
	layer4_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert13_ep3 [style=dashed]
	layer4_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert14_ep3 [style=dashed]
	layer4_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert15_ep3 [style=dashed]
	layer4_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert16_ep4 [style=dashed]
	layer4_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert17_ep4 [style=dashed]
	layer4_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert18_ep4 [style=dashed]
	layer4_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert19_ep4 [style=dashed]
	layer4_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert20_ep5 [style=dashed]
	layer4_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert21_ep5 [style=dashed]
	layer4_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert22_ep5 [style=dashed]
	layer4_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert23_ep5 [style=dashed]
	layer4_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert24_ep6 [style=dashed]
	layer4_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert25_ep6 [style=dashed]
	layer4_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert26_ep6 [style=dashed]
	layer4_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert27_ep6 [style=dashed]
	layer4_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert28_ep7 [style=dashed]
	layer4_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert29_ep7 [style=dashed]
	layer4_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert30_ep7 [style=dashed]
	layer4_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert31_ep7 [style=dashed]
	layer4_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert32_ep8 [style=dashed]
	layer4_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert33_ep8 [style=dashed]
	layer4_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert34_ep8 [style=dashed]
	layer4_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert35_ep8 [style=dashed]
	layer4_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert36_ep9 [style=dashed]
	layer4_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert37_ep9 [style=dashed]
	layer4_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert38_ep9 [style=dashed]
	layer4_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert39_ep9 [style=dashed]
	layer4_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert40_ep10 [style=dashed]
	layer4_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert41_ep10 [style=dashed]
	layer4_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert42_ep10 [style=dashed]
	layer4_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert43_ep10 [style=dashed]
	layer4_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert44_ep11 [style=dashed]
	layer4_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert45_ep11 [style=dashed]
	layer4_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert46_ep11 [style=dashed]
	layer4_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert47_ep11 [style=dashed]
	layer4_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert48_ep12 [style=dashed]
	layer4_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert49_ep12 [style=dashed]
	layer4_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert50_ep12 [style=dashed]
	layer4_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert51_ep12 [style=dashed]
	layer4_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert52_ep13 [style=dashed]
	layer4_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert53_ep13 [style=dashed]
	layer4_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert54_ep13 [style=dashed]
	layer4_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert55_ep13 [style=dashed]
	layer4_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert56_ep14 [style=dashed]
	layer4_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert57_ep14 [style=dashed]
	layer4_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert58_ep14 [style=dashed]
	layer4_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert59_ep14 [style=dashed]
	layer4_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert60_ep15 [style=dashed]
	layer4_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert61_ep15 [style=dashed]
	layer4_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert62_ep15 [style=dashed]
	layer4_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer4_gate -> layer4_expert63_ep15 [style=dashed]
	layer4_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer4_expert0_ep0 -> layer4_alltoall
	layer4_expert1_ep0 -> layer4_alltoall
	layer4_expert2_ep0 -> layer4_alltoall
	layer4_expert3_ep0 -> layer4_alltoall
	layer4_expert4_ep1 -> layer4_alltoall
	layer4_expert5_ep1 -> layer4_alltoall
	layer4_expert6_ep1 -> layer4_alltoall
	layer4_expert7_ep1 -> layer4_alltoall
	layer4_expert8_ep2 -> layer4_alltoall
	layer4_expert9_ep2 -> layer4_alltoall
	layer4_expert10_ep2 -> layer4_alltoall
	layer4_expert11_ep2 -> layer4_alltoall
	layer4_expert12_ep3 -> layer4_alltoall
	layer4_expert13_ep3 -> layer4_alltoall
	layer4_expert14_ep3 -> layer4_alltoall
	layer4_expert15_ep3 -> layer4_alltoall
	layer4_expert16_ep4 -> layer4_alltoall
	layer4_expert17_ep4 -> layer4_alltoall
	layer4_expert18_ep4 -> layer4_alltoall
	layer4_expert19_ep4 -> layer4_alltoall
	layer4_expert20_ep5 -> layer4_alltoall
	layer4_expert21_ep5 -> layer4_alltoall
	layer4_expert22_ep5 -> layer4_alltoall
	layer4_expert23_ep5 -> layer4_alltoall
	layer4_expert24_ep6 -> layer4_alltoall
	layer4_expert25_ep6 -> layer4_alltoall
	layer4_expert26_ep6 -> layer4_alltoall
	layer4_expert27_ep6 -> layer4_alltoall
	layer4_expert28_ep7 -> layer4_alltoall
	layer4_expert29_ep7 -> layer4_alltoall
	layer4_expert30_ep7 -> layer4_alltoall
	layer4_expert31_ep7 -> layer4_alltoall
	layer4_expert32_ep8 -> layer4_alltoall
	layer4_expert33_ep8 -> layer4_alltoall
	layer4_expert34_ep8 -> layer4_alltoall
	layer4_expert35_ep8 -> layer4_alltoall
	layer4_expert36_ep9 -> layer4_alltoall
	layer4_expert37_ep9 -> layer4_alltoall
	layer4_expert38_ep9 -> layer4_alltoall
	layer4_expert39_ep9 -> layer4_alltoall
	layer4_expert40_ep10 -> layer4_alltoall
	layer4_expert41_ep10 -> layer4_alltoall
	layer4_expert42_ep10 -> layer4_alltoall
	layer4_expert43_ep10 -> layer4_alltoall
	layer4_expert44_ep11 -> layer4_alltoall
	layer4_expert45_ep11 -> layer4_alltoall
	layer4_expert46_ep11 -> layer4_alltoall
	layer4_expert47_ep11 -> layer4_alltoall
	layer4_expert48_ep12 -> layer4_alltoall
	layer4_expert49_ep12 -> layer4_alltoall
	layer4_expert50_ep12 -> layer4_alltoall
	layer4_expert51_ep12 -> layer4_alltoall
	layer4_expert52_ep13 -> layer4_alltoall
	layer4_expert53_ep13 -> layer4_alltoall
	layer4_expert54_ep13 -> layer4_alltoall
	layer4_expert55_ep13 -> layer4_alltoall
	layer4_expert56_ep14 -> layer4_alltoall
	layer4_expert57_ep14 -> layer4_alltoall
	layer4_expert58_ep14 -> layer4_alltoall
	layer4_expert59_ep14 -> layer4_alltoall
	layer4_expert60_ep15 -> layer4_alltoall
	layer4_expert61_ep15 -> layer4_alltoall
	layer4_expert62_ep15 -> layer4_alltoall
	layer4_expert63_ep15 -> layer4_alltoall
	layer4_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer4_alltoall -> layer4_moe_aggregate
	layer4_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer4_attn_layernorm -> layer4_moe_residual [style=dashed]
	layer4_moe_aggregate -> layer4_moe_residual
	layer4_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer4_moe_residual -> layer4_moe_layernorm
	layer5_input [label="Layer 5 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer4_moe_layernorm -> layer5_input
	layer5_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer5_input -> layer5_qkv_tp0
	layer5_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer5_input -> layer5_qkv_tp1
	layer5_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer5_input -> layer5_qkv_tp2
	layer5_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer5_input -> layer5_qkv_tp3
	layer5_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer5_qkv_tp0 -> layer5_qkv_allreduce
	layer5_qkv_tp1 -> layer5_qkv_allreduce
	layer5_qkv_tp2 -> layer5_qkv_allreduce
	layer5_qkv_tp3 -> layer5_qkv_allreduce
	layer5_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer5_qkv_allreduce -> layer5_attn_tp0
	layer5_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer5_qkv_allreduce -> layer5_attn_tp1
	layer5_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer5_qkv_allreduce -> layer5_attn_tp2
	layer5_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer5_qkv_allreduce -> layer5_attn_tp3
	layer5_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer5_attn_tp0 -> layer5_attn_out_tp0
	layer5_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer5_attn_tp1 -> layer5_attn_out_tp1
	layer5_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer5_attn_tp2 -> layer5_attn_out_tp2
	layer5_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer5_attn_tp3 -> layer5_attn_out_tp3
	layer5_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer5_attn_out_tp0 -> layer5_attn_allreduce
	layer5_attn_out_tp1 -> layer5_attn_allreduce
	layer5_attn_out_tp2 -> layer5_attn_allreduce
	layer5_attn_out_tp3 -> layer5_attn_allreduce
	layer5_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer5_input -> layer5_attn_residual
	layer5_attn_allreduce -> layer5_attn_residual
	layer5_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer5_attn_residual -> layer5_attn_layernorm
	layer5_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer5_attn_layernorm -> layer5_gate
	layer5_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert0_ep0 [style=dashed]
	layer5_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert1_ep0 [style=dashed]
	layer5_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert2_ep0 [style=dashed]
	layer5_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert3_ep0 [style=dashed]
	layer5_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert4_ep1 [style=dashed]
	layer5_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert5_ep1 [style=dashed]
	layer5_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert6_ep1 [style=dashed]
	layer5_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert7_ep1 [style=dashed]
	layer5_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert8_ep2 [style=dashed]
	layer5_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert9_ep2 [style=dashed]
	layer5_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert10_ep2 [style=dashed]
	layer5_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert11_ep2 [style=dashed]
	layer5_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert12_ep3 [style=dashed]
	layer5_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert13_ep3 [style=dashed]
	layer5_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert14_ep3 [style=dashed]
	layer5_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert15_ep3 [style=dashed]
	layer5_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert16_ep4 [style=dashed]
	layer5_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert17_ep4 [style=dashed]
	layer5_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert18_ep4 [style=dashed]
	layer5_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert19_ep4 [style=dashed]
	layer5_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert20_ep5 [style=dashed]
	layer5_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert21_ep5 [style=dashed]
	layer5_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert22_ep5 [style=dashed]
	layer5_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert23_ep5 [style=dashed]
	layer5_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert24_ep6 [style=dashed]
	layer5_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert25_ep6 [style=dashed]
	layer5_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert26_ep6 [style=dashed]
	layer5_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert27_ep6 [style=dashed]
	layer5_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert28_ep7 [style=dashed]
	layer5_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert29_ep7 [style=dashed]
	layer5_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert30_ep7 [style=dashed]
	layer5_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert31_ep7 [style=dashed]
	layer5_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert32_ep8 [style=dashed]
	layer5_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert33_ep8 [style=dashed]
	layer5_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert34_ep8 [style=dashed]
	layer5_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert35_ep8 [style=dashed]
	layer5_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert36_ep9 [style=dashed]
	layer5_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert37_ep9 [style=dashed]
	layer5_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert38_ep9 [style=dashed]
	layer5_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert39_ep9 [style=dashed]
	layer5_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert40_ep10 [style=dashed]
	layer5_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert41_ep10 [style=dashed]
	layer5_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert42_ep10 [style=dashed]
	layer5_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert43_ep10 [style=dashed]
	layer5_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert44_ep11 [style=dashed]
	layer5_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert45_ep11 [style=dashed]
	layer5_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert46_ep11 [style=dashed]
	layer5_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert47_ep11 [style=dashed]
	layer5_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert48_ep12 [style=dashed]
	layer5_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert49_ep12 [style=dashed]
	layer5_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert50_ep12 [style=dashed]
	layer5_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert51_ep12 [style=dashed]
	layer5_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert52_ep13 [style=dashed]
	layer5_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert53_ep13 [style=dashed]
	layer5_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert54_ep13 [style=dashed]
	layer5_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert55_ep13 [style=dashed]
	layer5_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert56_ep14 [style=dashed]
	layer5_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert57_ep14 [style=dashed]
	layer5_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert58_ep14 [style=dashed]
	layer5_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert59_ep14 [style=dashed]
	layer5_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert60_ep15 [style=dashed]
	layer5_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert61_ep15 [style=dashed]
	layer5_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert62_ep15 [style=dashed]
	layer5_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer5_gate -> layer5_expert63_ep15 [style=dashed]
	layer5_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer5_expert0_ep0 -> layer5_alltoall
	layer5_expert1_ep0 -> layer5_alltoall
	layer5_expert2_ep0 -> layer5_alltoall
	layer5_expert3_ep0 -> layer5_alltoall
	layer5_expert4_ep1 -> layer5_alltoall
	layer5_expert5_ep1 -> layer5_alltoall
	layer5_expert6_ep1 -> layer5_alltoall
	layer5_expert7_ep1 -> layer5_alltoall
	layer5_expert8_ep2 -> layer5_alltoall
	layer5_expert9_ep2 -> layer5_alltoall
	layer5_expert10_ep2 -> layer5_alltoall
	layer5_expert11_ep2 -> layer5_alltoall
	layer5_expert12_ep3 -> layer5_alltoall
	layer5_expert13_ep3 -> layer5_alltoall
	layer5_expert14_ep3 -> layer5_alltoall
	layer5_expert15_ep3 -> layer5_alltoall
	layer5_expert16_ep4 -> layer5_alltoall
	layer5_expert17_ep4 -> layer5_alltoall
	layer5_expert18_ep4 -> layer5_alltoall
	layer5_expert19_ep4 -> layer5_alltoall
	layer5_expert20_ep5 -> layer5_alltoall
	layer5_expert21_ep5 -> layer5_alltoall
	layer5_expert22_ep5 -> layer5_alltoall
	layer5_expert23_ep5 -> layer5_alltoall
	layer5_expert24_ep6 -> layer5_alltoall
	layer5_expert25_ep6 -> layer5_alltoall
	layer5_expert26_ep6 -> layer5_alltoall
	layer5_expert27_ep6 -> layer5_alltoall
	layer5_expert28_ep7 -> layer5_alltoall
	layer5_expert29_ep7 -> layer5_alltoall
	layer5_expert30_ep7 -> layer5_alltoall
	layer5_expert31_ep7 -> layer5_alltoall
	layer5_expert32_ep8 -> layer5_alltoall
	layer5_expert33_ep8 -> layer5_alltoall
	layer5_expert34_ep8 -> layer5_alltoall
	layer5_expert35_ep8 -> layer5_alltoall
	layer5_expert36_ep9 -> layer5_alltoall
	layer5_expert37_ep9 -> layer5_alltoall
	layer5_expert38_ep9 -> layer5_alltoall
	layer5_expert39_ep9 -> layer5_alltoall
	layer5_expert40_ep10 -> layer5_alltoall
	layer5_expert41_ep10 -> layer5_alltoall
	layer5_expert42_ep10 -> layer5_alltoall
	layer5_expert43_ep10 -> layer5_alltoall
	layer5_expert44_ep11 -> layer5_alltoall
	layer5_expert45_ep11 -> layer5_alltoall
	layer5_expert46_ep11 -> layer5_alltoall
	layer5_expert47_ep11 -> layer5_alltoall
	layer5_expert48_ep12 -> layer5_alltoall
	layer5_expert49_ep12 -> layer5_alltoall
	layer5_expert50_ep12 -> layer5_alltoall
	layer5_expert51_ep12 -> layer5_alltoall
	layer5_expert52_ep13 -> layer5_alltoall
	layer5_expert53_ep13 -> layer5_alltoall
	layer5_expert54_ep13 -> layer5_alltoall
	layer5_expert55_ep13 -> layer5_alltoall
	layer5_expert56_ep14 -> layer5_alltoall
	layer5_expert57_ep14 -> layer5_alltoall
	layer5_expert58_ep14 -> layer5_alltoall
	layer5_expert59_ep14 -> layer5_alltoall
	layer5_expert60_ep15 -> layer5_alltoall
	layer5_expert61_ep15 -> layer5_alltoall
	layer5_expert62_ep15 -> layer5_alltoall
	layer5_expert63_ep15 -> layer5_alltoall
	layer5_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer5_alltoall -> layer5_moe_aggregate
	layer5_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer5_attn_layernorm -> layer5_moe_residual [style=dashed]
	layer5_moe_aggregate -> layer5_moe_residual
	layer5_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer5_moe_residual -> layer5_moe_layernorm
	layer6_input [label="Layer 6 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer5_moe_layernorm -> layer6_input
	layer6_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer6_input -> layer6_qkv_tp0
	layer6_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer6_input -> layer6_qkv_tp1
	layer6_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer6_input -> layer6_qkv_tp2
	layer6_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer6_input -> layer6_qkv_tp3
	layer6_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer6_qkv_tp0 -> layer6_qkv_allreduce
	layer6_qkv_tp1 -> layer6_qkv_allreduce
	layer6_qkv_tp2 -> layer6_qkv_allreduce
	layer6_qkv_tp3 -> layer6_qkv_allreduce
	layer6_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer6_qkv_allreduce -> layer6_attn_tp0
	layer6_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer6_qkv_allreduce -> layer6_attn_tp1
	layer6_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer6_qkv_allreduce -> layer6_attn_tp2
	layer6_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer6_qkv_allreduce -> layer6_attn_tp3
	layer6_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer6_attn_tp0 -> layer6_attn_out_tp0
	layer6_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer6_attn_tp1 -> layer6_attn_out_tp1
	layer6_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer6_attn_tp2 -> layer6_attn_out_tp2
	layer6_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer6_attn_tp3 -> layer6_attn_out_tp3
	layer6_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer6_attn_out_tp0 -> layer6_attn_allreduce
	layer6_attn_out_tp1 -> layer6_attn_allreduce
	layer6_attn_out_tp2 -> layer6_attn_allreduce
	layer6_attn_out_tp3 -> layer6_attn_allreduce
	layer6_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer6_input -> layer6_attn_residual
	layer6_attn_allreduce -> layer6_attn_residual
	layer6_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer6_attn_residual -> layer6_attn_layernorm
	layer6_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer6_attn_layernorm -> layer6_gate
	layer6_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert0_ep0 [style=dashed]
	layer6_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert1_ep0 [style=dashed]
	layer6_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert2_ep0 [style=dashed]
	layer6_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert3_ep0 [style=dashed]
	layer6_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert4_ep1 [style=dashed]
	layer6_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert5_ep1 [style=dashed]
	layer6_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert6_ep1 [style=dashed]
	layer6_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert7_ep1 [style=dashed]
	layer6_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert8_ep2 [style=dashed]
	layer6_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert9_ep2 [style=dashed]
	layer6_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert10_ep2 [style=dashed]
	layer6_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert11_ep2 [style=dashed]
	layer6_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert12_ep3 [style=dashed]
	layer6_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert13_ep3 [style=dashed]
	layer6_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert14_ep3 [style=dashed]
	layer6_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert15_ep3 [style=dashed]
	layer6_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert16_ep4 [style=dashed]
	layer6_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert17_ep4 [style=dashed]
	layer6_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert18_ep4 [style=dashed]
	layer6_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert19_ep4 [style=dashed]
	layer6_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert20_ep5 [style=dashed]
	layer6_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert21_ep5 [style=dashed]
	layer6_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert22_ep5 [style=dashed]
	layer6_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert23_ep5 [style=dashed]
	layer6_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert24_ep6 [style=dashed]
	layer6_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert25_ep6 [style=dashed]
	layer6_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert26_ep6 [style=dashed]
	layer6_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert27_ep6 [style=dashed]
	layer6_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert28_ep7 [style=dashed]
	layer6_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert29_ep7 [style=dashed]
	layer6_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert30_ep7 [style=dashed]
	layer6_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert31_ep7 [style=dashed]
	layer6_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert32_ep8 [style=dashed]
	layer6_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert33_ep8 [style=dashed]
	layer6_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert34_ep8 [style=dashed]
	layer6_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert35_ep8 [style=dashed]
	layer6_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert36_ep9 [style=dashed]
	layer6_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert37_ep9 [style=dashed]
	layer6_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert38_ep9 [style=dashed]
	layer6_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert39_ep9 [style=dashed]
	layer6_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert40_ep10 [style=dashed]
	layer6_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert41_ep10 [style=dashed]
	layer6_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert42_ep10 [style=dashed]
	layer6_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert43_ep10 [style=dashed]
	layer6_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert44_ep11 [style=dashed]
	layer6_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert45_ep11 [style=dashed]
	layer6_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert46_ep11 [style=dashed]
	layer6_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert47_ep11 [style=dashed]
	layer6_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert48_ep12 [style=dashed]
	layer6_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert49_ep12 [style=dashed]
	layer6_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert50_ep12 [style=dashed]
	layer6_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert51_ep12 [style=dashed]
	layer6_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert52_ep13 [style=dashed]
	layer6_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert53_ep13 [style=dashed]
	layer6_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert54_ep13 [style=dashed]
	layer6_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert55_ep13 [style=dashed]
	layer6_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert56_ep14 [style=dashed]
	layer6_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert57_ep14 [style=dashed]
	layer6_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert58_ep14 [style=dashed]
	layer6_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert59_ep14 [style=dashed]
	layer6_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert60_ep15 [style=dashed]
	layer6_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert61_ep15 [style=dashed]
	layer6_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert62_ep15 [style=dashed]
	layer6_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer6_gate -> layer6_expert63_ep15 [style=dashed]
	layer6_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer6_expert0_ep0 -> layer6_alltoall
	layer6_expert1_ep0 -> layer6_alltoall
	layer6_expert2_ep0 -> layer6_alltoall
	layer6_expert3_ep0 -> layer6_alltoall
	layer6_expert4_ep1 -> layer6_alltoall
	layer6_expert5_ep1 -> layer6_alltoall
	layer6_expert6_ep1 -> layer6_alltoall
	layer6_expert7_ep1 -> layer6_alltoall
	layer6_expert8_ep2 -> layer6_alltoall
	layer6_expert9_ep2 -> layer6_alltoall
	layer6_expert10_ep2 -> layer6_alltoall
	layer6_expert11_ep2 -> layer6_alltoall
	layer6_expert12_ep3 -> layer6_alltoall
	layer6_expert13_ep3 -> layer6_alltoall
	layer6_expert14_ep3 -> layer6_alltoall
	layer6_expert15_ep3 -> layer6_alltoall
	layer6_expert16_ep4 -> layer6_alltoall
	layer6_expert17_ep4 -> layer6_alltoall
	layer6_expert18_ep4 -> layer6_alltoall
	layer6_expert19_ep4 -> layer6_alltoall
	layer6_expert20_ep5 -> layer6_alltoall
	layer6_expert21_ep5 -> layer6_alltoall
	layer6_expert22_ep5 -> layer6_alltoall
	layer6_expert23_ep5 -> layer6_alltoall
	layer6_expert24_ep6 -> layer6_alltoall
	layer6_expert25_ep6 -> layer6_alltoall
	layer6_expert26_ep6 -> layer6_alltoall
	layer6_expert27_ep6 -> layer6_alltoall
	layer6_expert28_ep7 -> layer6_alltoall
	layer6_expert29_ep7 -> layer6_alltoall
	layer6_expert30_ep7 -> layer6_alltoall
	layer6_expert31_ep7 -> layer6_alltoall
	layer6_expert32_ep8 -> layer6_alltoall
	layer6_expert33_ep8 -> layer6_alltoall
	layer6_expert34_ep8 -> layer6_alltoall
	layer6_expert35_ep8 -> layer6_alltoall
	layer6_expert36_ep9 -> layer6_alltoall
	layer6_expert37_ep9 -> layer6_alltoall
	layer6_expert38_ep9 -> layer6_alltoall
	layer6_expert39_ep9 -> layer6_alltoall
	layer6_expert40_ep10 -> layer6_alltoall
	layer6_expert41_ep10 -> layer6_alltoall
	layer6_expert42_ep10 -> layer6_alltoall
	layer6_expert43_ep10 -> layer6_alltoall
	layer6_expert44_ep11 -> layer6_alltoall
	layer6_expert45_ep11 -> layer6_alltoall
	layer6_expert46_ep11 -> layer6_alltoall
	layer6_expert47_ep11 -> layer6_alltoall
	layer6_expert48_ep12 -> layer6_alltoall
	layer6_expert49_ep12 -> layer6_alltoall
	layer6_expert50_ep12 -> layer6_alltoall
	layer6_expert51_ep12 -> layer6_alltoall
	layer6_expert52_ep13 -> layer6_alltoall
	layer6_expert53_ep13 -> layer6_alltoall
	layer6_expert54_ep13 -> layer6_alltoall
	layer6_expert55_ep13 -> layer6_alltoall
	layer6_expert56_ep14 -> layer6_alltoall
	layer6_expert57_ep14 -> layer6_alltoall
	layer6_expert58_ep14 -> layer6_alltoall
	layer6_expert59_ep14 -> layer6_alltoall
	layer6_expert60_ep15 -> layer6_alltoall
	layer6_expert61_ep15 -> layer6_alltoall
	layer6_expert62_ep15 -> layer6_alltoall
	layer6_expert63_ep15 -> layer6_alltoall
	layer6_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer6_alltoall -> layer6_moe_aggregate
	layer6_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer6_attn_layernorm -> layer6_moe_residual [style=dashed]
	layer6_moe_aggregate -> layer6_moe_residual
	layer6_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer6_moe_residual -> layer6_moe_layernorm
	layer7_input [label="Layer 7 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer6_moe_layernorm -> layer7_input
	layer7_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer7_input -> layer7_qkv_tp0
	layer7_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer7_input -> layer7_qkv_tp1
	layer7_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer7_input -> layer7_qkv_tp2
	layer7_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer7_input -> layer7_qkv_tp3
	layer7_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer7_qkv_tp0 -> layer7_qkv_allreduce
	layer7_qkv_tp1 -> layer7_qkv_allreduce
	layer7_qkv_tp2 -> layer7_qkv_allreduce
	layer7_qkv_tp3 -> layer7_qkv_allreduce
	layer7_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer7_qkv_allreduce -> layer7_attn_tp0
	layer7_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer7_qkv_allreduce -> layer7_attn_tp1
	layer7_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer7_qkv_allreduce -> layer7_attn_tp2
	layer7_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer7_qkv_allreduce -> layer7_attn_tp3
	layer7_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer7_attn_tp0 -> layer7_attn_out_tp0
	layer7_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer7_attn_tp1 -> layer7_attn_out_tp1
	layer7_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer7_attn_tp2 -> layer7_attn_out_tp2
	layer7_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer7_attn_tp3 -> layer7_attn_out_tp3
	layer7_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer7_attn_out_tp0 -> layer7_attn_allreduce
	layer7_attn_out_tp1 -> layer7_attn_allreduce
	layer7_attn_out_tp2 -> layer7_attn_allreduce
	layer7_attn_out_tp3 -> layer7_attn_allreduce
	layer7_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer7_input -> layer7_attn_residual
	layer7_attn_allreduce -> layer7_attn_residual
	layer7_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer7_attn_residual -> layer7_attn_layernorm
	layer7_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer7_attn_layernorm -> layer7_gate
	layer7_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert0_ep0 [style=dashed]
	layer7_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert1_ep0 [style=dashed]
	layer7_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert2_ep0 [style=dashed]
	layer7_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert3_ep0 [style=dashed]
	layer7_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert4_ep1 [style=dashed]
	layer7_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert5_ep1 [style=dashed]
	layer7_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert6_ep1 [style=dashed]
	layer7_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert7_ep1 [style=dashed]
	layer7_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert8_ep2 [style=dashed]
	layer7_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert9_ep2 [style=dashed]
	layer7_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert10_ep2 [style=dashed]
	layer7_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert11_ep2 [style=dashed]
	layer7_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert12_ep3 [style=dashed]
	layer7_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert13_ep3 [style=dashed]
	layer7_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert14_ep3 [style=dashed]
	layer7_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert15_ep3 [style=dashed]
	layer7_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert16_ep4 [style=dashed]
	layer7_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert17_ep4 [style=dashed]
	layer7_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert18_ep4 [style=dashed]
	layer7_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert19_ep4 [style=dashed]
	layer7_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert20_ep5 [style=dashed]
	layer7_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert21_ep5 [style=dashed]
	layer7_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert22_ep5 [style=dashed]
	layer7_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert23_ep5 [style=dashed]
	layer7_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert24_ep6 [style=dashed]
	layer7_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert25_ep6 [style=dashed]
	layer7_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert26_ep6 [style=dashed]
	layer7_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert27_ep6 [style=dashed]
	layer7_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert28_ep7 [style=dashed]
	layer7_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert29_ep7 [style=dashed]
	layer7_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert30_ep7 [style=dashed]
	layer7_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert31_ep7 [style=dashed]
	layer7_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert32_ep8 [style=dashed]
	layer7_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert33_ep8 [style=dashed]
	layer7_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert34_ep8 [style=dashed]
	layer7_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert35_ep8 [style=dashed]
	layer7_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert36_ep9 [style=dashed]
	layer7_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert37_ep9 [style=dashed]
	layer7_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert38_ep9 [style=dashed]
	layer7_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert39_ep9 [style=dashed]
	layer7_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert40_ep10 [style=dashed]
	layer7_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert41_ep10 [style=dashed]
	layer7_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert42_ep10 [style=dashed]
	layer7_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert43_ep10 [style=dashed]
	layer7_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert44_ep11 [style=dashed]
	layer7_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert45_ep11 [style=dashed]
	layer7_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert46_ep11 [style=dashed]
	layer7_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert47_ep11 [style=dashed]
	layer7_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert48_ep12 [style=dashed]
	layer7_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert49_ep12 [style=dashed]
	layer7_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert50_ep12 [style=dashed]
	layer7_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert51_ep12 [style=dashed]
	layer7_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert52_ep13 [style=dashed]
	layer7_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert53_ep13 [style=dashed]
	layer7_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert54_ep13 [style=dashed]
	layer7_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert55_ep13 [style=dashed]
	layer7_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert56_ep14 [style=dashed]
	layer7_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert57_ep14 [style=dashed]
	layer7_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert58_ep14 [style=dashed]
	layer7_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert59_ep14 [style=dashed]
	layer7_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert60_ep15 [style=dashed]
	layer7_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert61_ep15 [style=dashed]
	layer7_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert62_ep15 [style=dashed]
	layer7_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer7_gate -> layer7_expert63_ep15 [style=dashed]
	layer7_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer7_expert0_ep0 -> layer7_alltoall
	layer7_expert1_ep0 -> layer7_alltoall
	layer7_expert2_ep0 -> layer7_alltoall
	layer7_expert3_ep0 -> layer7_alltoall
	layer7_expert4_ep1 -> layer7_alltoall
	layer7_expert5_ep1 -> layer7_alltoall
	layer7_expert6_ep1 -> layer7_alltoall
	layer7_expert7_ep1 -> layer7_alltoall
	layer7_expert8_ep2 -> layer7_alltoall
	layer7_expert9_ep2 -> layer7_alltoall
	layer7_expert10_ep2 -> layer7_alltoall
	layer7_expert11_ep2 -> layer7_alltoall
	layer7_expert12_ep3 -> layer7_alltoall
	layer7_expert13_ep3 -> layer7_alltoall
	layer7_expert14_ep3 -> layer7_alltoall
	layer7_expert15_ep3 -> layer7_alltoall
	layer7_expert16_ep4 -> layer7_alltoall
	layer7_expert17_ep4 -> layer7_alltoall
	layer7_expert18_ep4 -> layer7_alltoall
	layer7_expert19_ep4 -> layer7_alltoall
	layer7_expert20_ep5 -> layer7_alltoall
	layer7_expert21_ep5 -> layer7_alltoall
	layer7_expert22_ep5 -> layer7_alltoall
	layer7_expert23_ep5 -> layer7_alltoall
	layer7_expert24_ep6 -> layer7_alltoall
	layer7_expert25_ep6 -> layer7_alltoall
	layer7_expert26_ep6 -> layer7_alltoall
	layer7_expert27_ep6 -> layer7_alltoall
	layer7_expert28_ep7 -> layer7_alltoall
	layer7_expert29_ep7 -> layer7_alltoall
	layer7_expert30_ep7 -> layer7_alltoall
	layer7_expert31_ep7 -> layer7_alltoall
	layer7_expert32_ep8 -> layer7_alltoall
	layer7_expert33_ep8 -> layer7_alltoall
	layer7_expert34_ep8 -> layer7_alltoall
	layer7_expert35_ep8 -> layer7_alltoall
	layer7_expert36_ep9 -> layer7_alltoall
	layer7_expert37_ep9 -> layer7_alltoall
	layer7_expert38_ep9 -> layer7_alltoall
	layer7_expert39_ep9 -> layer7_alltoall
	layer7_expert40_ep10 -> layer7_alltoall
	layer7_expert41_ep10 -> layer7_alltoall
	layer7_expert42_ep10 -> layer7_alltoall
	layer7_expert43_ep10 -> layer7_alltoall
	layer7_expert44_ep11 -> layer7_alltoall
	layer7_expert45_ep11 -> layer7_alltoall
	layer7_expert46_ep11 -> layer7_alltoall
	layer7_expert47_ep11 -> layer7_alltoall
	layer7_expert48_ep12 -> layer7_alltoall
	layer7_expert49_ep12 -> layer7_alltoall
	layer7_expert50_ep12 -> layer7_alltoall
	layer7_expert51_ep12 -> layer7_alltoall
	layer7_expert52_ep13 -> layer7_alltoall
	layer7_expert53_ep13 -> layer7_alltoall
	layer7_expert54_ep13 -> layer7_alltoall
	layer7_expert55_ep13 -> layer7_alltoall
	layer7_expert56_ep14 -> layer7_alltoall
	layer7_expert57_ep14 -> layer7_alltoall
	layer7_expert58_ep14 -> layer7_alltoall
	layer7_expert59_ep14 -> layer7_alltoall
	layer7_expert60_ep15 -> layer7_alltoall
	layer7_expert61_ep15 -> layer7_alltoall
	layer7_expert62_ep15 -> layer7_alltoall
	layer7_expert63_ep15 -> layer7_alltoall
	layer7_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer7_alltoall -> layer7_moe_aggregate
	layer7_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer7_attn_layernorm -> layer7_moe_residual [style=dashed]
	layer7_moe_aggregate -> layer7_moe_residual
	layer7_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer7_moe_residual -> layer7_moe_layernorm
	layer8_input [label="Layer 8 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer7_moe_layernorm -> layer8_input
	layer8_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer8_input -> layer8_qkv_tp0
	layer8_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer8_input -> layer8_qkv_tp1
	layer8_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer8_input -> layer8_qkv_tp2
	layer8_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer8_input -> layer8_qkv_tp3
	layer8_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer8_qkv_tp0 -> layer8_qkv_allreduce
	layer8_qkv_tp1 -> layer8_qkv_allreduce
	layer8_qkv_tp2 -> layer8_qkv_allreduce
	layer8_qkv_tp3 -> layer8_qkv_allreduce
	layer8_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer8_qkv_allreduce -> layer8_attn_tp0
	layer8_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer8_qkv_allreduce -> layer8_attn_tp1
	layer8_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer8_qkv_allreduce -> layer8_attn_tp2
	layer8_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer8_qkv_allreduce -> layer8_attn_tp3
	layer8_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer8_attn_tp0 -> layer8_attn_out_tp0
	layer8_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer8_attn_tp1 -> layer8_attn_out_tp1
	layer8_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer8_attn_tp2 -> layer8_attn_out_tp2
	layer8_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer8_attn_tp3 -> layer8_attn_out_tp3
	layer8_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer8_attn_out_tp0 -> layer8_attn_allreduce
	layer8_attn_out_tp1 -> layer8_attn_allreduce
	layer8_attn_out_tp2 -> layer8_attn_allreduce
	layer8_attn_out_tp3 -> layer8_attn_allreduce
	layer8_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer8_input -> layer8_attn_residual
	layer8_attn_allreduce -> layer8_attn_residual
	layer8_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer8_attn_residual -> layer8_attn_layernorm
	layer8_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer8_attn_layernorm -> layer8_gate
	layer8_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert0_ep0 [style=dashed]
	layer8_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert1_ep0 [style=dashed]
	layer8_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert2_ep0 [style=dashed]
	layer8_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert3_ep0 [style=dashed]
	layer8_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert4_ep1 [style=dashed]
	layer8_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert5_ep1 [style=dashed]
	layer8_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert6_ep1 [style=dashed]
	layer8_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert7_ep1 [style=dashed]
	layer8_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert8_ep2 [style=dashed]
	layer8_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert9_ep2 [style=dashed]
	layer8_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert10_ep2 [style=dashed]
	layer8_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert11_ep2 [style=dashed]
	layer8_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert12_ep3 [style=dashed]
	layer8_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert13_ep3 [style=dashed]
	layer8_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert14_ep3 [style=dashed]
	layer8_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert15_ep3 [style=dashed]
	layer8_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert16_ep4 [style=dashed]
	layer8_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert17_ep4 [style=dashed]
	layer8_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert18_ep4 [style=dashed]
	layer8_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert19_ep4 [style=dashed]
	layer8_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert20_ep5 [style=dashed]
	layer8_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert21_ep5 [style=dashed]
	layer8_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert22_ep5 [style=dashed]
	layer8_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert23_ep5 [style=dashed]
	layer8_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert24_ep6 [style=dashed]
	layer8_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert25_ep6 [style=dashed]
	layer8_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert26_ep6 [style=dashed]
	layer8_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert27_ep6 [style=dashed]
	layer8_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert28_ep7 [style=dashed]
	layer8_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert29_ep7 [style=dashed]
	layer8_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert30_ep7 [style=dashed]
	layer8_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert31_ep7 [style=dashed]
	layer8_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert32_ep8 [style=dashed]
	layer8_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert33_ep8 [style=dashed]
	layer8_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert34_ep8 [style=dashed]
	layer8_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert35_ep8 [style=dashed]
	layer8_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert36_ep9 [style=dashed]
	layer8_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert37_ep9 [style=dashed]
	layer8_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert38_ep9 [style=dashed]
	layer8_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert39_ep9 [style=dashed]
	layer8_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert40_ep10 [style=dashed]
	layer8_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert41_ep10 [style=dashed]
	layer8_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert42_ep10 [style=dashed]
	layer8_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert43_ep10 [style=dashed]
	layer8_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert44_ep11 [style=dashed]
	layer8_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert45_ep11 [style=dashed]
	layer8_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert46_ep11 [style=dashed]
	layer8_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert47_ep11 [style=dashed]
	layer8_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert48_ep12 [style=dashed]
	layer8_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert49_ep12 [style=dashed]
	layer8_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert50_ep12 [style=dashed]
	layer8_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert51_ep12 [style=dashed]
	layer8_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert52_ep13 [style=dashed]
	layer8_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert53_ep13 [style=dashed]
	layer8_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert54_ep13 [style=dashed]
	layer8_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert55_ep13 [style=dashed]
	layer8_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert56_ep14 [style=dashed]
	layer8_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert57_ep14 [style=dashed]
	layer8_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert58_ep14 [style=dashed]
	layer8_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert59_ep14 [style=dashed]
	layer8_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert60_ep15 [style=dashed]
	layer8_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert61_ep15 [style=dashed]
	layer8_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert62_ep15 [style=dashed]
	layer8_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer8_gate -> layer8_expert63_ep15 [style=dashed]
	layer8_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer8_expert0_ep0 -> layer8_alltoall
	layer8_expert1_ep0 -> layer8_alltoall
	layer8_expert2_ep0 -> layer8_alltoall
	layer8_expert3_ep0 -> layer8_alltoall
	layer8_expert4_ep1 -> layer8_alltoall
	layer8_expert5_ep1 -> layer8_alltoall
	layer8_expert6_ep1 -> layer8_alltoall
	layer8_expert7_ep1 -> layer8_alltoall
	layer8_expert8_ep2 -> layer8_alltoall
	layer8_expert9_ep2 -> layer8_alltoall
	layer8_expert10_ep2 -> layer8_alltoall
	layer8_expert11_ep2 -> layer8_alltoall
	layer8_expert12_ep3 -> layer8_alltoall
	layer8_expert13_ep3 -> layer8_alltoall
	layer8_expert14_ep3 -> layer8_alltoall
	layer8_expert15_ep3 -> layer8_alltoall
	layer8_expert16_ep4 -> layer8_alltoall
	layer8_expert17_ep4 -> layer8_alltoall
	layer8_expert18_ep4 -> layer8_alltoall
	layer8_expert19_ep4 -> layer8_alltoall
	layer8_expert20_ep5 -> layer8_alltoall
	layer8_expert21_ep5 -> layer8_alltoall
	layer8_expert22_ep5 -> layer8_alltoall
	layer8_expert23_ep5 -> layer8_alltoall
	layer8_expert24_ep6 -> layer8_alltoall
	layer8_expert25_ep6 -> layer8_alltoall
	layer8_expert26_ep6 -> layer8_alltoall
	layer8_expert27_ep6 -> layer8_alltoall
	layer8_expert28_ep7 -> layer8_alltoall
	layer8_expert29_ep7 -> layer8_alltoall
	layer8_expert30_ep7 -> layer8_alltoall
	layer8_expert31_ep7 -> layer8_alltoall
	layer8_expert32_ep8 -> layer8_alltoall
	layer8_expert33_ep8 -> layer8_alltoall
	layer8_expert34_ep8 -> layer8_alltoall
	layer8_expert35_ep8 -> layer8_alltoall
	layer8_expert36_ep9 -> layer8_alltoall
	layer8_expert37_ep9 -> layer8_alltoall
	layer8_expert38_ep9 -> layer8_alltoall
	layer8_expert39_ep9 -> layer8_alltoall
	layer8_expert40_ep10 -> layer8_alltoall
	layer8_expert41_ep10 -> layer8_alltoall
	layer8_expert42_ep10 -> layer8_alltoall
	layer8_expert43_ep10 -> layer8_alltoall
	layer8_expert44_ep11 -> layer8_alltoall
	layer8_expert45_ep11 -> layer8_alltoall
	layer8_expert46_ep11 -> layer8_alltoall
	layer8_expert47_ep11 -> layer8_alltoall
	layer8_expert48_ep12 -> layer8_alltoall
	layer8_expert49_ep12 -> layer8_alltoall
	layer8_expert50_ep12 -> layer8_alltoall
	layer8_expert51_ep12 -> layer8_alltoall
	layer8_expert52_ep13 -> layer8_alltoall
	layer8_expert53_ep13 -> layer8_alltoall
	layer8_expert54_ep13 -> layer8_alltoall
	layer8_expert55_ep13 -> layer8_alltoall
	layer8_expert56_ep14 -> layer8_alltoall
	layer8_expert57_ep14 -> layer8_alltoall
	layer8_expert58_ep14 -> layer8_alltoall
	layer8_expert59_ep14 -> layer8_alltoall
	layer8_expert60_ep15 -> layer8_alltoall
	layer8_expert61_ep15 -> layer8_alltoall
	layer8_expert62_ep15 -> layer8_alltoall
	layer8_expert63_ep15 -> layer8_alltoall
	layer8_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer8_alltoall -> layer8_moe_aggregate
	layer8_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer8_attn_layernorm -> layer8_moe_residual [style=dashed]
	layer8_moe_aggregate -> layer8_moe_residual
	layer8_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer8_moe_residual -> layer8_moe_layernorm
	layer9_input [label="Layer 9 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer8_moe_layernorm -> layer9_input
	layer9_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer9_input -> layer9_qkv_tp0
	layer9_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer9_input -> layer9_qkv_tp1
	layer9_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer9_input -> layer9_qkv_tp2
	layer9_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer9_input -> layer9_qkv_tp3
	layer9_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer9_qkv_tp0 -> layer9_qkv_allreduce
	layer9_qkv_tp1 -> layer9_qkv_allreduce
	layer9_qkv_tp2 -> layer9_qkv_allreduce
	layer9_qkv_tp3 -> layer9_qkv_allreduce
	layer9_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer9_qkv_allreduce -> layer9_attn_tp0
	layer9_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer9_qkv_allreduce -> layer9_attn_tp1
	layer9_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer9_qkv_allreduce -> layer9_attn_tp2
	layer9_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer9_qkv_allreduce -> layer9_attn_tp3
	layer9_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer9_attn_tp0 -> layer9_attn_out_tp0
	layer9_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer9_attn_tp1 -> layer9_attn_out_tp1
	layer9_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer9_attn_tp2 -> layer9_attn_out_tp2
	layer9_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer9_attn_tp3 -> layer9_attn_out_tp3
	layer9_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer9_attn_out_tp0 -> layer9_attn_allreduce
	layer9_attn_out_tp1 -> layer9_attn_allreduce
	layer9_attn_out_tp2 -> layer9_attn_allreduce
	layer9_attn_out_tp3 -> layer9_attn_allreduce
	layer9_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer9_input -> layer9_attn_residual
	layer9_attn_allreduce -> layer9_attn_residual
	layer9_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer9_attn_residual -> layer9_attn_layernorm
	layer9_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer9_attn_layernorm -> layer9_gate
	layer9_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert0_ep0 [style=dashed]
	layer9_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert1_ep0 [style=dashed]
	layer9_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert2_ep0 [style=dashed]
	layer9_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert3_ep0 [style=dashed]
	layer9_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert4_ep1 [style=dashed]
	layer9_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert5_ep1 [style=dashed]
	layer9_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert6_ep1 [style=dashed]
	layer9_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert7_ep1 [style=dashed]
	layer9_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert8_ep2 [style=dashed]
	layer9_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert9_ep2 [style=dashed]
	layer9_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert10_ep2 [style=dashed]
	layer9_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert11_ep2 [style=dashed]
	layer9_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert12_ep3 [style=dashed]
	layer9_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert13_ep3 [style=dashed]
	layer9_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert14_ep3 [style=dashed]
	layer9_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert15_ep3 [style=dashed]
	layer9_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert16_ep4 [style=dashed]
	layer9_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert17_ep4 [style=dashed]
	layer9_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert18_ep4 [style=dashed]
	layer9_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert19_ep4 [style=dashed]
	layer9_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert20_ep5 [style=dashed]
	layer9_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert21_ep5 [style=dashed]
	layer9_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert22_ep5 [style=dashed]
	layer9_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert23_ep5 [style=dashed]
	layer9_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert24_ep6 [style=dashed]
	layer9_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert25_ep6 [style=dashed]
	layer9_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert26_ep6 [style=dashed]
	layer9_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert27_ep6 [style=dashed]
	layer9_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert28_ep7 [style=dashed]
	layer9_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert29_ep7 [style=dashed]
	layer9_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert30_ep7 [style=dashed]
	layer9_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert31_ep7 [style=dashed]
	layer9_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert32_ep8 [style=dashed]
	layer9_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert33_ep8 [style=dashed]
	layer9_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert34_ep8 [style=dashed]
	layer9_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert35_ep8 [style=dashed]
	layer9_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert36_ep9 [style=dashed]
	layer9_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert37_ep9 [style=dashed]
	layer9_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert38_ep9 [style=dashed]
	layer9_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert39_ep9 [style=dashed]
	layer9_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert40_ep10 [style=dashed]
	layer9_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert41_ep10 [style=dashed]
	layer9_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert42_ep10 [style=dashed]
	layer9_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert43_ep10 [style=dashed]
	layer9_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert44_ep11 [style=dashed]
	layer9_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert45_ep11 [style=dashed]
	layer9_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert46_ep11 [style=dashed]
	layer9_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert47_ep11 [style=dashed]
	layer9_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert48_ep12 [style=dashed]
	layer9_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert49_ep12 [style=dashed]
	layer9_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert50_ep12 [style=dashed]
	layer9_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert51_ep12 [style=dashed]
	layer9_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert52_ep13 [style=dashed]
	layer9_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert53_ep13 [style=dashed]
	layer9_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert54_ep13 [style=dashed]
	layer9_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert55_ep13 [style=dashed]
	layer9_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert56_ep14 [style=dashed]
	layer9_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert57_ep14 [style=dashed]
	layer9_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert58_ep14 [style=dashed]
	layer9_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert59_ep14 [style=dashed]
	layer9_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert60_ep15 [style=dashed]
	layer9_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert61_ep15 [style=dashed]
	layer9_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert62_ep15 [style=dashed]
	layer9_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer9_gate -> layer9_expert63_ep15 [style=dashed]
	layer9_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer9_expert0_ep0 -> layer9_alltoall
	layer9_expert1_ep0 -> layer9_alltoall
	layer9_expert2_ep0 -> layer9_alltoall
	layer9_expert3_ep0 -> layer9_alltoall
	layer9_expert4_ep1 -> layer9_alltoall
	layer9_expert5_ep1 -> layer9_alltoall
	layer9_expert6_ep1 -> layer9_alltoall
	layer9_expert7_ep1 -> layer9_alltoall
	layer9_expert8_ep2 -> layer9_alltoall
	layer9_expert9_ep2 -> layer9_alltoall
	layer9_expert10_ep2 -> layer9_alltoall
	layer9_expert11_ep2 -> layer9_alltoall
	layer9_expert12_ep3 -> layer9_alltoall
	layer9_expert13_ep3 -> layer9_alltoall
	layer9_expert14_ep3 -> layer9_alltoall
	layer9_expert15_ep3 -> layer9_alltoall
	layer9_expert16_ep4 -> layer9_alltoall
	layer9_expert17_ep4 -> layer9_alltoall
	layer9_expert18_ep4 -> layer9_alltoall
	layer9_expert19_ep4 -> layer9_alltoall
	layer9_expert20_ep5 -> layer9_alltoall
	layer9_expert21_ep5 -> layer9_alltoall
	layer9_expert22_ep5 -> layer9_alltoall
	layer9_expert23_ep5 -> layer9_alltoall
	layer9_expert24_ep6 -> layer9_alltoall
	layer9_expert25_ep6 -> layer9_alltoall
	layer9_expert26_ep6 -> layer9_alltoall
	layer9_expert27_ep6 -> layer9_alltoall
	layer9_expert28_ep7 -> layer9_alltoall
	layer9_expert29_ep7 -> layer9_alltoall
	layer9_expert30_ep7 -> layer9_alltoall
	layer9_expert31_ep7 -> layer9_alltoall
	layer9_expert32_ep8 -> layer9_alltoall
	layer9_expert33_ep8 -> layer9_alltoall
	layer9_expert34_ep8 -> layer9_alltoall
	layer9_expert35_ep8 -> layer9_alltoall
	layer9_expert36_ep9 -> layer9_alltoall
	layer9_expert37_ep9 -> layer9_alltoall
	layer9_expert38_ep9 -> layer9_alltoall
	layer9_expert39_ep9 -> layer9_alltoall
	layer9_expert40_ep10 -> layer9_alltoall
	layer9_expert41_ep10 -> layer9_alltoall
	layer9_expert42_ep10 -> layer9_alltoall
	layer9_expert43_ep10 -> layer9_alltoall
	layer9_expert44_ep11 -> layer9_alltoall
	layer9_expert45_ep11 -> layer9_alltoall
	layer9_expert46_ep11 -> layer9_alltoall
	layer9_expert47_ep11 -> layer9_alltoall
	layer9_expert48_ep12 -> layer9_alltoall
	layer9_expert49_ep12 -> layer9_alltoall
	layer9_expert50_ep12 -> layer9_alltoall
	layer9_expert51_ep12 -> layer9_alltoall
	layer9_expert52_ep13 -> layer9_alltoall
	layer9_expert53_ep13 -> layer9_alltoall
	layer9_expert54_ep13 -> layer9_alltoall
	layer9_expert55_ep13 -> layer9_alltoall
	layer9_expert56_ep14 -> layer9_alltoall
	layer9_expert57_ep14 -> layer9_alltoall
	layer9_expert58_ep14 -> layer9_alltoall
	layer9_expert59_ep14 -> layer9_alltoall
	layer9_expert60_ep15 -> layer9_alltoall
	layer9_expert61_ep15 -> layer9_alltoall
	layer9_expert62_ep15 -> layer9_alltoall
	layer9_expert63_ep15 -> layer9_alltoall
	layer9_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer9_alltoall -> layer9_moe_aggregate
	layer9_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer9_attn_layernorm -> layer9_moe_residual [style=dashed]
	layer9_moe_aggregate -> layer9_moe_residual
	layer9_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer9_moe_residual -> layer9_moe_layernorm
	layer10_input [label="Layer 10 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer9_moe_layernorm -> layer10_input
	layer10_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer10_input -> layer10_qkv_tp0
	layer10_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer10_input -> layer10_qkv_tp1
	layer10_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer10_input -> layer10_qkv_tp2
	layer10_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer10_input -> layer10_qkv_tp3
	layer10_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer10_qkv_tp0 -> layer10_qkv_allreduce
	layer10_qkv_tp1 -> layer10_qkv_allreduce
	layer10_qkv_tp2 -> layer10_qkv_allreduce
	layer10_qkv_tp3 -> layer10_qkv_allreduce
	layer10_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer10_qkv_allreduce -> layer10_attn_tp0
	layer10_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer10_qkv_allreduce -> layer10_attn_tp1
	layer10_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer10_qkv_allreduce -> layer10_attn_tp2
	layer10_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer10_qkv_allreduce -> layer10_attn_tp3
	layer10_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer10_attn_tp0 -> layer10_attn_out_tp0
	layer10_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer10_attn_tp1 -> layer10_attn_out_tp1
	layer10_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer10_attn_tp2 -> layer10_attn_out_tp2
	layer10_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer10_attn_tp3 -> layer10_attn_out_tp3
	layer10_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer10_attn_out_tp0 -> layer10_attn_allreduce
	layer10_attn_out_tp1 -> layer10_attn_allreduce
	layer10_attn_out_tp2 -> layer10_attn_allreduce
	layer10_attn_out_tp3 -> layer10_attn_allreduce
	layer10_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer10_input -> layer10_attn_residual
	layer10_attn_allreduce -> layer10_attn_residual
	layer10_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer10_attn_residual -> layer10_attn_layernorm
	layer10_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer10_attn_layernorm -> layer10_gate
	layer10_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert0_ep0 [style=dashed]
	layer10_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert1_ep0 [style=dashed]
	layer10_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert2_ep0 [style=dashed]
	layer10_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert3_ep0 [style=dashed]
	layer10_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert4_ep1 [style=dashed]
	layer10_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert5_ep1 [style=dashed]
	layer10_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert6_ep1 [style=dashed]
	layer10_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert7_ep1 [style=dashed]
	layer10_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert8_ep2 [style=dashed]
	layer10_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert9_ep2 [style=dashed]
	layer10_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert10_ep2 [style=dashed]
	layer10_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert11_ep2 [style=dashed]
	layer10_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert12_ep3 [style=dashed]
	layer10_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert13_ep3 [style=dashed]
	layer10_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert14_ep3 [style=dashed]
	layer10_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert15_ep3 [style=dashed]
	layer10_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert16_ep4 [style=dashed]
	layer10_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert17_ep4 [style=dashed]
	layer10_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert18_ep4 [style=dashed]
	layer10_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert19_ep4 [style=dashed]
	layer10_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert20_ep5 [style=dashed]
	layer10_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert21_ep5 [style=dashed]
	layer10_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert22_ep5 [style=dashed]
	layer10_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert23_ep5 [style=dashed]
	layer10_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert24_ep6 [style=dashed]
	layer10_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert25_ep6 [style=dashed]
	layer10_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert26_ep6 [style=dashed]
	layer10_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert27_ep6 [style=dashed]
	layer10_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert28_ep7 [style=dashed]
	layer10_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert29_ep7 [style=dashed]
	layer10_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert30_ep7 [style=dashed]
	layer10_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert31_ep7 [style=dashed]
	layer10_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert32_ep8 [style=dashed]
	layer10_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert33_ep8 [style=dashed]
	layer10_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert34_ep8 [style=dashed]
	layer10_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert35_ep8 [style=dashed]
	layer10_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert36_ep9 [style=dashed]
	layer10_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert37_ep9 [style=dashed]
	layer10_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert38_ep9 [style=dashed]
	layer10_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert39_ep9 [style=dashed]
	layer10_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert40_ep10 [style=dashed]
	layer10_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert41_ep10 [style=dashed]
	layer10_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert42_ep10 [style=dashed]
	layer10_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert43_ep10 [style=dashed]
	layer10_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert44_ep11 [style=dashed]
	layer10_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert45_ep11 [style=dashed]
	layer10_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert46_ep11 [style=dashed]
	layer10_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert47_ep11 [style=dashed]
	layer10_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert48_ep12 [style=dashed]
	layer10_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert49_ep12 [style=dashed]
	layer10_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert50_ep12 [style=dashed]
	layer10_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert51_ep12 [style=dashed]
	layer10_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert52_ep13 [style=dashed]
	layer10_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert53_ep13 [style=dashed]
	layer10_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert54_ep13 [style=dashed]
	layer10_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert55_ep13 [style=dashed]
	layer10_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert56_ep14 [style=dashed]
	layer10_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert57_ep14 [style=dashed]
	layer10_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert58_ep14 [style=dashed]
	layer10_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert59_ep14 [style=dashed]
	layer10_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert60_ep15 [style=dashed]
	layer10_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert61_ep15 [style=dashed]
	layer10_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert62_ep15 [style=dashed]
	layer10_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer10_gate -> layer10_expert63_ep15 [style=dashed]
	layer10_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer10_expert0_ep0 -> layer10_alltoall
	layer10_expert1_ep0 -> layer10_alltoall
	layer10_expert2_ep0 -> layer10_alltoall
	layer10_expert3_ep0 -> layer10_alltoall
	layer10_expert4_ep1 -> layer10_alltoall
	layer10_expert5_ep1 -> layer10_alltoall
	layer10_expert6_ep1 -> layer10_alltoall
	layer10_expert7_ep1 -> layer10_alltoall
	layer10_expert8_ep2 -> layer10_alltoall
	layer10_expert9_ep2 -> layer10_alltoall
	layer10_expert10_ep2 -> layer10_alltoall
	layer10_expert11_ep2 -> layer10_alltoall
	layer10_expert12_ep3 -> layer10_alltoall
	layer10_expert13_ep3 -> layer10_alltoall
	layer10_expert14_ep3 -> layer10_alltoall
	layer10_expert15_ep3 -> layer10_alltoall
	layer10_expert16_ep4 -> layer10_alltoall
	layer10_expert17_ep4 -> layer10_alltoall
	layer10_expert18_ep4 -> layer10_alltoall
	layer10_expert19_ep4 -> layer10_alltoall
	layer10_expert20_ep5 -> layer10_alltoall
	layer10_expert21_ep5 -> layer10_alltoall
	layer10_expert22_ep5 -> layer10_alltoall
	layer10_expert23_ep5 -> layer10_alltoall
	layer10_expert24_ep6 -> layer10_alltoall
	layer10_expert25_ep6 -> layer10_alltoall
	layer10_expert26_ep6 -> layer10_alltoall
	layer10_expert27_ep6 -> layer10_alltoall
	layer10_expert28_ep7 -> layer10_alltoall
	layer10_expert29_ep7 -> layer10_alltoall
	layer10_expert30_ep7 -> layer10_alltoall
	layer10_expert31_ep7 -> layer10_alltoall
	layer10_expert32_ep8 -> layer10_alltoall
	layer10_expert33_ep8 -> layer10_alltoall
	layer10_expert34_ep8 -> layer10_alltoall
	layer10_expert35_ep8 -> layer10_alltoall
	layer10_expert36_ep9 -> layer10_alltoall
	layer10_expert37_ep9 -> layer10_alltoall
	layer10_expert38_ep9 -> layer10_alltoall
	layer10_expert39_ep9 -> layer10_alltoall
	layer10_expert40_ep10 -> layer10_alltoall
	layer10_expert41_ep10 -> layer10_alltoall
	layer10_expert42_ep10 -> layer10_alltoall
	layer10_expert43_ep10 -> layer10_alltoall
	layer10_expert44_ep11 -> layer10_alltoall
	layer10_expert45_ep11 -> layer10_alltoall
	layer10_expert46_ep11 -> layer10_alltoall
	layer10_expert47_ep11 -> layer10_alltoall
	layer10_expert48_ep12 -> layer10_alltoall
	layer10_expert49_ep12 -> layer10_alltoall
	layer10_expert50_ep12 -> layer10_alltoall
	layer10_expert51_ep12 -> layer10_alltoall
	layer10_expert52_ep13 -> layer10_alltoall
	layer10_expert53_ep13 -> layer10_alltoall
	layer10_expert54_ep13 -> layer10_alltoall
	layer10_expert55_ep13 -> layer10_alltoall
	layer10_expert56_ep14 -> layer10_alltoall
	layer10_expert57_ep14 -> layer10_alltoall
	layer10_expert58_ep14 -> layer10_alltoall
	layer10_expert59_ep14 -> layer10_alltoall
	layer10_expert60_ep15 -> layer10_alltoall
	layer10_expert61_ep15 -> layer10_alltoall
	layer10_expert62_ep15 -> layer10_alltoall
	layer10_expert63_ep15 -> layer10_alltoall
	layer10_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer10_alltoall -> layer10_moe_aggregate
	layer10_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer10_attn_layernorm -> layer10_moe_residual [style=dashed]
	layer10_moe_aggregate -> layer10_moe_residual
	layer10_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer10_moe_residual -> layer10_moe_layernorm
	layer11_input [label="Layer 11 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer10_moe_layernorm -> layer11_input
	layer11_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer11_input -> layer11_qkv_tp0
	layer11_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer11_input -> layer11_qkv_tp1
	layer11_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer11_input -> layer11_qkv_tp2
	layer11_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer11_input -> layer11_qkv_tp3
	layer11_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer11_qkv_tp0 -> layer11_qkv_allreduce
	layer11_qkv_tp1 -> layer11_qkv_allreduce
	layer11_qkv_tp2 -> layer11_qkv_allreduce
	layer11_qkv_tp3 -> layer11_qkv_allreduce
	layer11_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer11_qkv_allreduce -> layer11_attn_tp0
	layer11_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer11_qkv_allreduce -> layer11_attn_tp1
	layer11_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer11_qkv_allreduce -> layer11_attn_tp2
	layer11_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer11_qkv_allreduce -> layer11_attn_tp3
	layer11_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer11_attn_tp0 -> layer11_attn_out_tp0
	layer11_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer11_attn_tp1 -> layer11_attn_out_tp1
	layer11_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer11_attn_tp2 -> layer11_attn_out_tp2
	layer11_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer11_attn_tp3 -> layer11_attn_out_tp3
	layer11_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer11_attn_out_tp0 -> layer11_attn_allreduce
	layer11_attn_out_tp1 -> layer11_attn_allreduce
	layer11_attn_out_tp2 -> layer11_attn_allreduce
	layer11_attn_out_tp3 -> layer11_attn_allreduce
	layer11_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer11_input -> layer11_attn_residual
	layer11_attn_allreduce -> layer11_attn_residual
	layer11_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer11_attn_residual -> layer11_attn_layernorm
	layer11_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer11_attn_layernorm -> layer11_gate
	layer11_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert0_ep0 [style=dashed]
	layer11_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert1_ep0 [style=dashed]
	layer11_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert2_ep0 [style=dashed]
	layer11_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert3_ep0 [style=dashed]
	layer11_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert4_ep1 [style=dashed]
	layer11_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert5_ep1 [style=dashed]
	layer11_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert6_ep1 [style=dashed]
	layer11_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert7_ep1 [style=dashed]
	layer11_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert8_ep2 [style=dashed]
	layer11_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert9_ep2 [style=dashed]
	layer11_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert10_ep2 [style=dashed]
	layer11_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert11_ep2 [style=dashed]
	layer11_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert12_ep3 [style=dashed]
	layer11_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert13_ep3 [style=dashed]
	layer11_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert14_ep3 [style=dashed]
	layer11_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert15_ep3 [style=dashed]
	layer11_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert16_ep4 [style=dashed]
	layer11_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert17_ep4 [style=dashed]
	layer11_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert18_ep4 [style=dashed]
	layer11_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert19_ep4 [style=dashed]
	layer11_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert20_ep5 [style=dashed]
	layer11_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert21_ep5 [style=dashed]
	layer11_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert22_ep5 [style=dashed]
	layer11_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert23_ep5 [style=dashed]
	layer11_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert24_ep6 [style=dashed]
	layer11_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert25_ep6 [style=dashed]
	layer11_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert26_ep6 [style=dashed]
	layer11_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert27_ep6 [style=dashed]
	layer11_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert28_ep7 [style=dashed]
	layer11_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert29_ep7 [style=dashed]
	layer11_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert30_ep7 [style=dashed]
	layer11_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert31_ep7 [style=dashed]
	layer11_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert32_ep8 [style=dashed]
	layer11_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert33_ep8 [style=dashed]
	layer11_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert34_ep8 [style=dashed]
	layer11_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert35_ep8 [style=dashed]
	layer11_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert36_ep9 [style=dashed]
	layer11_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert37_ep9 [style=dashed]
	layer11_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert38_ep9 [style=dashed]
	layer11_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert39_ep9 [style=dashed]
	layer11_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert40_ep10 [style=dashed]
	layer11_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert41_ep10 [style=dashed]
	layer11_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert42_ep10 [style=dashed]
	layer11_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert43_ep10 [style=dashed]
	layer11_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert44_ep11 [style=dashed]
	layer11_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert45_ep11 [style=dashed]
	layer11_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert46_ep11 [style=dashed]
	layer11_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert47_ep11 [style=dashed]
	layer11_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert48_ep12 [style=dashed]
	layer11_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert49_ep12 [style=dashed]
	layer11_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert50_ep12 [style=dashed]
	layer11_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert51_ep12 [style=dashed]
	layer11_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert52_ep13 [style=dashed]
	layer11_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert53_ep13 [style=dashed]
	layer11_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert54_ep13 [style=dashed]
	layer11_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert55_ep13 [style=dashed]
	layer11_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert56_ep14 [style=dashed]
	layer11_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert57_ep14 [style=dashed]
	layer11_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert58_ep14 [style=dashed]
	layer11_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert59_ep14 [style=dashed]
	layer11_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert60_ep15 [style=dashed]
	layer11_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert61_ep15 [style=dashed]
	layer11_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert62_ep15 [style=dashed]
	layer11_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer11_gate -> layer11_expert63_ep15 [style=dashed]
	layer11_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer11_expert0_ep0 -> layer11_alltoall
	layer11_expert1_ep0 -> layer11_alltoall
	layer11_expert2_ep0 -> layer11_alltoall
	layer11_expert3_ep0 -> layer11_alltoall
	layer11_expert4_ep1 -> layer11_alltoall
	layer11_expert5_ep1 -> layer11_alltoall
	layer11_expert6_ep1 -> layer11_alltoall
	layer11_expert7_ep1 -> layer11_alltoall
	layer11_expert8_ep2 -> layer11_alltoall
	layer11_expert9_ep2 -> layer11_alltoall
	layer11_expert10_ep2 -> layer11_alltoall
	layer11_expert11_ep2 -> layer11_alltoall
	layer11_expert12_ep3 -> layer11_alltoall
	layer11_expert13_ep3 -> layer11_alltoall
	layer11_expert14_ep3 -> layer11_alltoall
	layer11_expert15_ep3 -> layer11_alltoall
	layer11_expert16_ep4 -> layer11_alltoall
	layer11_expert17_ep4 -> layer11_alltoall
	layer11_expert18_ep4 -> layer11_alltoall
	layer11_expert19_ep4 -> layer11_alltoall
	layer11_expert20_ep5 -> layer11_alltoall
	layer11_expert21_ep5 -> layer11_alltoall
	layer11_expert22_ep5 -> layer11_alltoall
	layer11_expert23_ep5 -> layer11_alltoall
	layer11_expert24_ep6 -> layer11_alltoall
	layer11_expert25_ep6 -> layer11_alltoall
	layer11_expert26_ep6 -> layer11_alltoall
	layer11_expert27_ep6 -> layer11_alltoall
	layer11_expert28_ep7 -> layer11_alltoall
	layer11_expert29_ep7 -> layer11_alltoall
	layer11_expert30_ep7 -> layer11_alltoall
	layer11_expert31_ep7 -> layer11_alltoall
	layer11_expert32_ep8 -> layer11_alltoall
	layer11_expert33_ep8 -> layer11_alltoall
	layer11_expert34_ep8 -> layer11_alltoall
	layer11_expert35_ep8 -> layer11_alltoall
	layer11_expert36_ep9 -> layer11_alltoall
	layer11_expert37_ep9 -> layer11_alltoall
	layer11_expert38_ep9 -> layer11_alltoall
	layer11_expert39_ep9 -> layer11_alltoall
	layer11_expert40_ep10 -> layer11_alltoall
	layer11_expert41_ep10 -> layer11_alltoall
	layer11_expert42_ep10 -> layer11_alltoall
	layer11_expert43_ep10 -> layer11_alltoall
	layer11_expert44_ep11 -> layer11_alltoall
	layer11_expert45_ep11 -> layer11_alltoall
	layer11_expert46_ep11 -> layer11_alltoall
	layer11_expert47_ep11 -> layer11_alltoall
	layer11_expert48_ep12 -> layer11_alltoall
	layer11_expert49_ep12 -> layer11_alltoall
	layer11_expert50_ep12 -> layer11_alltoall
	layer11_expert51_ep12 -> layer11_alltoall
	layer11_expert52_ep13 -> layer11_alltoall
	layer11_expert53_ep13 -> layer11_alltoall
	layer11_expert54_ep13 -> layer11_alltoall
	layer11_expert55_ep13 -> layer11_alltoall
	layer11_expert56_ep14 -> layer11_alltoall
	layer11_expert57_ep14 -> layer11_alltoall
	layer11_expert58_ep14 -> layer11_alltoall
	layer11_expert59_ep14 -> layer11_alltoall
	layer11_expert60_ep15 -> layer11_alltoall
	layer11_expert61_ep15 -> layer11_alltoall
	layer11_expert62_ep15 -> layer11_alltoall
	layer11_expert63_ep15 -> layer11_alltoall
	layer11_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer11_alltoall -> layer11_moe_aggregate
	layer11_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer11_attn_layernorm -> layer11_moe_residual [style=dashed]
	layer11_moe_aggregate -> layer11_moe_residual
	layer11_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer11_moe_residual -> layer11_moe_layernorm
	layer12_input [label="Layer 12 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer11_moe_layernorm -> layer12_input
	layer12_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer12_input -> layer12_qkv_tp0
	layer12_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer12_input -> layer12_qkv_tp1
	layer12_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer12_input -> layer12_qkv_tp2
	layer12_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer12_input -> layer12_qkv_tp3
	layer12_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer12_qkv_tp0 -> layer12_qkv_allreduce
	layer12_qkv_tp1 -> layer12_qkv_allreduce
	layer12_qkv_tp2 -> layer12_qkv_allreduce
	layer12_qkv_tp3 -> layer12_qkv_allreduce
	layer12_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer12_qkv_allreduce -> layer12_attn_tp0
	layer12_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer12_qkv_allreduce -> layer12_attn_tp1
	layer12_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer12_qkv_allreduce -> layer12_attn_tp2
	layer12_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer12_qkv_allreduce -> layer12_attn_tp3
	layer12_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer12_attn_tp0 -> layer12_attn_out_tp0
	layer12_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer12_attn_tp1 -> layer12_attn_out_tp1
	layer12_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer12_attn_tp2 -> layer12_attn_out_tp2
	layer12_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer12_attn_tp3 -> layer12_attn_out_tp3
	layer12_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer12_attn_out_tp0 -> layer12_attn_allreduce
	layer12_attn_out_tp1 -> layer12_attn_allreduce
	layer12_attn_out_tp2 -> layer12_attn_allreduce
	layer12_attn_out_tp3 -> layer12_attn_allreduce
	layer12_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer12_input -> layer12_attn_residual
	layer12_attn_allreduce -> layer12_attn_residual
	layer12_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer12_attn_residual -> layer12_attn_layernorm
	layer12_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer12_attn_layernorm -> layer12_gate
	layer12_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert0_ep0 [style=dashed]
	layer12_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert1_ep0 [style=dashed]
	layer12_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert2_ep0 [style=dashed]
	layer12_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert3_ep0 [style=dashed]
	layer12_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert4_ep1 [style=dashed]
	layer12_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert5_ep1 [style=dashed]
	layer12_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert6_ep1 [style=dashed]
	layer12_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert7_ep1 [style=dashed]
	layer12_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert8_ep2 [style=dashed]
	layer12_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert9_ep2 [style=dashed]
	layer12_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert10_ep2 [style=dashed]
	layer12_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert11_ep2 [style=dashed]
	layer12_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert12_ep3 [style=dashed]
	layer12_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert13_ep3 [style=dashed]
	layer12_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert14_ep3 [style=dashed]
	layer12_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert15_ep3 [style=dashed]
	layer12_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert16_ep4 [style=dashed]
	layer12_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert17_ep4 [style=dashed]
	layer12_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert18_ep4 [style=dashed]
	layer12_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert19_ep4 [style=dashed]
	layer12_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert20_ep5 [style=dashed]
	layer12_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert21_ep5 [style=dashed]
	layer12_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert22_ep5 [style=dashed]
	layer12_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert23_ep5 [style=dashed]
	layer12_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert24_ep6 [style=dashed]
	layer12_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert25_ep6 [style=dashed]
	layer12_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert26_ep6 [style=dashed]
	layer12_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert27_ep6 [style=dashed]
	layer12_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert28_ep7 [style=dashed]
	layer12_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert29_ep7 [style=dashed]
	layer12_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert30_ep7 [style=dashed]
	layer12_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert31_ep7 [style=dashed]
	layer12_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert32_ep8 [style=dashed]
	layer12_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert33_ep8 [style=dashed]
	layer12_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert34_ep8 [style=dashed]
	layer12_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert35_ep8 [style=dashed]
	layer12_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert36_ep9 [style=dashed]
	layer12_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert37_ep9 [style=dashed]
	layer12_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert38_ep9 [style=dashed]
	layer12_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert39_ep9 [style=dashed]
	layer12_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert40_ep10 [style=dashed]
	layer12_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert41_ep10 [style=dashed]
	layer12_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert42_ep10 [style=dashed]
	layer12_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert43_ep10 [style=dashed]
	layer12_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert44_ep11 [style=dashed]
	layer12_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert45_ep11 [style=dashed]
	layer12_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert46_ep11 [style=dashed]
	layer12_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert47_ep11 [style=dashed]
	layer12_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert48_ep12 [style=dashed]
	layer12_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert49_ep12 [style=dashed]
	layer12_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert50_ep12 [style=dashed]
	layer12_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert51_ep12 [style=dashed]
	layer12_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert52_ep13 [style=dashed]
	layer12_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert53_ep13 [style=dashed]
	layer12_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert54_ep13 [style=dashed]
	layer12_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert55_ep13 [style=dashed]
	layer12_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert56_ep14 [style=dashed]
	layer12_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert57_ep14 [style=dashed]
	layer12_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert58_ep14 [style=dashed]
	layer12_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert59_ep14 [style=dashed]
	layer12_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert60_ep15 [style=dashed]
	layer12_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert61_ep15 [style=dashed]
	layer12_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert62_ep15 [style=dashed]
	layer12_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer12_gate -> layer12_expert63_ep15 [style=dashed]
	layer12_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer12_expert0_ep0 -> layer12_alltoall
	layer12_expert1_ep0 -> layer12_alltoall
	layer12_expert2_ep0 -> layer12_alltoall
	layer12_expert3_ep0 -> layer12_alltoall
	layer12_expert4_ep1 -> layer12_alltoall
	layer12_expert5_ep1 -> layer12_alltoall
	layer12_expert6_ep1 -> layer12_alltoall
	layer12_expert7_ep1 -> layer12_alltoall
	layer12_expert8_ep2 -> layer12_alltoall
	layer12_expert9_ep2 -> layer12_alltoall
	layer12_expert10_ep2 -> layer12_alltoall
	layer12_expert11_ep2 -> layer12_alltoall
	layer12_expert12_ep3 -> layer12_alltoall
	layer12_expert13_ep3 -> layer12_alltoall
	layer12_expert14_ep3 -> layer12_alltoall
	layer12_expert15_ep3 -> layer12_alltoall
	layer12_expert16_ep4 -> layer12_alltoall
	layer12_expert17_ep4 -> layer12_alltoall
	layer12_expert18_ep4 -> layer12_alltoall
	layer12_expert19_ep4 -> layer12_alltoall
	layer12_expert20_ep5 -> layer12_alltoall
	layer12_expert21_ep5 -> layer12_alltoall
	layer12_expert22_ep5 -> layer12_alltoall
	layer12_expert23_ep5 -> layer12_alltoall
	layer12_expert24_ep6 -> layer12_alltoall
	layer12_expert25_ep6 -> layer12_alltoall
	layer12_expert26_ep6 -> layer12_alltoall
	layer12_expert27_ep6 -> layer12_alltoall
	layer12_expert28_ep7 -> layer12_alltoall
	layer12_expert29_ep7 -> layer12_alltoall
	layer12_expert30_ep7 -> layer12_alltoall
	layer12_expert31_ep7 -> layer12_alltoall
	layer12_expert32_ep8 -> layer12_alltoall
	layer12_expert33_ep8 -> layer12_alltoall
	layer12_expert34_ep8 -> layer12_alltoall
	layer12_expert35_ep8 -> layer12_alltoall
	layer12_expert36_ep9 -> layer12_alltoall
	layer12_expert37_ep9 -> layer12_alltoall
	layer12_expert38_ep9 -> layer12_alltoall
	layer12_expert39_ep9 -> layer12_alltoall
	layer12_expert40_ep10 -> layer12_alltoall
	layer12_expert41_ep10 -> layer12_alltoall
	layer12_expert42_ep10 -> layer12_alltoall
	layer12_expert43_ep10 -> layer12_alltoall
	layer12_expert44_ep11 -> layer12_alltoall
	layer12_expert45_ep11 -> layer12_alltoall
	layer12_expert46_ep11 -> layer12_alltoall
	layer12_expert47_ep11 -> layer12_alltoall
	layer12_expert48_ep12 -> layer12_alltoall
	layer12_expert49_ep12 -> layer12_alltoall
	layer12_expert50_ep12 -> layer12_alltoall
	layer12_expert51_ep12 -> layer12_alltoall
	layer12_expert52_ep13 -> layer12_alltoall
	layer12_expert53_ep13 -> layer12_alltoall
	layer12_expert54_ep13 -> layer12_alltoall
	layer12_expert55_ep13 -> layer12_alltoall
	layer12_expert56_ep14 -> layer12_alltoall
	layer12_expert57_ep14 -> layer12_alltoall
	layer12_expert58_ep14 -> layer12_alltoall
	layer12_expert59_ep14 -> layer12_alltoall
	layer12_expert60_ep15 -> layer12_alltoall
	layer12_expert61_ep15 -> layer12_alltoall
	layer12_expert62_ep15 -> layer12_alltoall
	layer12_expert63_ep15 -> layer12_alltoall
	layer12_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer12_alltoall -> layer12_moe_aggregate
	layer12_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer12_attn_layernorm -> layer12_moe_residual [style=dashed]
	layer12_moe_aggregate -> layer12_moe_residual
	layer12_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer12_moe_residual -> layer12_moe_layernorm
	layer13_input [label="Layer 13 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer12_moe_layernorm -> layer13_input
	layer13_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer13_input -> layer13_qkv_tp0
	layer13_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer13_input -> layer13_qkv_tp1
	layer13_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer13_input -> layer13_qkv_tp2
	layer13_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer13_input -> layer13_qkv_tp3
	layer13_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer13_qkv_tp0 -> layer13_qkv_allreduce
	layer13_qkv_tp1 -> layer13_qkv_allreduce
	layer13_qkv_tp2 -> layer13_qkv_allreduce
	layer13_qkv_tp3 -> layer13_qkv_allreduce
	layer13_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer13_qkv_allreduce -> layer13_attn_tp0
	layer13_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer13_qkv_allreduce -> layer13_attn_tp1
	layer13_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer13_qkv_allreduce -> layer13_attn_tp2
	layer13_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer13_qkv_allreduce -> layer13_attn_tp3
	layer13_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer13_attn_tp0 -> layer13_attn_out_tp0
	layer13_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer13_attn_tp1 -> layer13_attn_out_tp1
	layer13_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer13_attn_tp2 -> layer13_attn_out_tp2
	layer13_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer13_attn_tp3 -> layer13_attn_out_tp3
	layer13_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer13_attn_out_tp0 -> layer13_attn_allreduce
	layer13_attn_out_tp1 -> layer13_attn_allreduce
	layer13_attn_out_tp2 -> layer13_attn_allreduce
	layer13_attn_out_tp3 -> layer13_attn_allreduce
	layer13_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer13_input -> layer13_attn_residual
	layer13_attn_allreduce -> layer13_attn_residual
	layer13_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer13_attn_residual -> layer13_attn_layernorm
	layer13_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer13_attn_layernorm -> layer13_gate
	layer13_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert0_ep0 [style=dashed]
	layer13_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert1_ep0 [style=dashed]
	layer13_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert2_ep0 [style=dashed]
	layer13_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert3_ep0 [style=dashed]
	layer13_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert4_ep1 [style=dashed]
	layer13_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert5_ep1 [style=dashed]
	layer13_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert6_ep1 [style=dashed]
	layer13_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert7_ep1 [style=dashed]
	layer13_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert8_ep2 [style=dashed]
	layer13_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert9_ep2 [style=dashed]
	layer13_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert10_ep2 [style=dashed]
	layer13_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert11_ep2 [style=dashed]
	layer13_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert12_ep3 [style=dashed]
	layer13_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert13_ep3 [style=dashed]
	layer13_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert14_ep3 [style=dashed]
	layer13_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert15_ep3 [style=dashed]
	layer13_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert16_ep4 [style=dashed]
	layer13_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert17_ep4 [style=dashed]
	layer13_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert18_ep4 [style=dashed]
	layer13_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert19_ep4 [style=dashed]
	layer13_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert20_ep5 [style=dashed]
	layer13_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert21_ep5 [style=dashed]
	layer13_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert22_ep5 [style=dashed]
	layer13_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert23_ep5 [style=dashed]
	layer13_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert24_ep6 [style=dashed]
	layer13_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert25_ep6 [style=dashed]
	layer13_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert26_ep6 [style=dashed]
	layer13_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert27_ep6 [style=dashed]
	layer13_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert28_ep7 [style=dashed]
	layer13_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert29_ep7 [style=dashed]
	layer13_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert30_ep7 [style=dashed]
	layer13_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert31_ep7 [style=dashed]
	layer13_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert32_ep8 [style=dashed]
	layer13_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert33_ep8 [style=dashed]
	layer13_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert34_ep8 [style=dashed]
	layer13_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert35_ep8 [style=dashed]
	layer13_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert36_ep9 [style=dashed]
	layer13_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert37_ep9 [style=dashed]
	layer13_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert38_ep9 [style=dashed]
	layer13_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert39_ep9 [style=dashed]
	layer13_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert40_ep10 [style=dashed]
	layer13_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert41_ep10 [style=dashed]
	layer13_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert42_ep10 [style=dashed]
	layer13_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert43_ep10 [style=dashed]
	layer13_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert44_ep11 [style=dashed]
	layer13_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert45_ep11 [style=dashed]
	layer13_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert46_ep11 [style=dashed]
	layer13_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert47_ep11 [style=dashed]
	layer13_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert48_ep12 [style=dashed]
	layer13_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert49_ep12 [style=dashed]
	layer13_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert50_ep12 [style=dashed]
	layer13_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert51_ep12 [style=dashed]
	layer13_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert52_ep13 [style=dashed]
	layer13_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert53_ep13 [style=dashed]
	layer13_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert54_ep13 [style=dashed]
	layer13_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert55_ep13 [style=dashed]
	layer13_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert56_ep14 [style=dashed]
	layer13_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert57_ep14 [style=dashed]
	layer13_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert58_ep14 [style=dashed]
	layer13_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert59_ep14 [style=dashed]
	layer13_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert60_ep15 [style=dashed]
	layer13_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert61_ep15 [style=dashed]
	layer13_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert62_ep15 [style=dashed]
	layer13_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer13_gate -> layer13_expert63_ep15 [style=dashed]
	layer13_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer13_expert0_ep0 -> layer13_alltoall
	layer13_expert1_ep0 -> layer13_alltoall
	layer13_expert2_ep0 -> layer13_alltoall
	layer13_expert3_ep0 -> layer13_alltoall
	layer13_expert4_ep1 -> layer13_alltoall
	layer13_expert5_ep1 -> layer13_alltoall
	layer13_expert6_ep1 -> layer13_alltoall
	layer13_expert7_ep1 -> layer13_alltoall
	layer13_expert8_ep2 -> layer13_alltoall
	layer13_expert9_ep2 -> layer13_alltoall
	layer13_expert10_ep2 -> layer13_alltoall
	layer13_expert11_ep2 -> layer13_alltoall
	layer13_expert12_ep3 -> layer13_alltoall
	layer13_expert13_ep3 -> layer13_alltoall
	layer13_expert14_ep3 -> layer13_alltoall
	layer13_expert15_ep3 -> layer13_alltoall
	layer13_expert16_ep4 -> layer13_alltoall
	layer13_expert17_ep4 -> layer13_alltoall
	layer13_expert18_ep4 -> layer13_alltoall
	layer13_expert19_ep4 -> layer13_alltoall
	layer13_expert20_ep5 -> layer13_alltoall
	layer13_expert21_ep5 -> layer13_alltoall
	layer13_expert22_ep5 -> layer13_alltoall
	layer13_expert23_ep5 -> layer13_alltoall
	layer13_expert24_ep6 -> layer13_alltoall
	layer13_expert25_ep6 -> layer13_alltoall
	layer13_expert26_ep6 -> layer13_alltoall
	layer13_expert27_ep6 -> layer13_alltoall
	layer13_expert28_ep7 -> layer13_alltoall
	layer13_expert29_ep7 -> layer13_alltoall
	layer13_expert30_ep7 -> layer13_alltoall
	layer13_expert31_ep7 -> layer13_alltoall
	layer13_expert32_ep8 -> layer13_alltoall
	layer13_expert33_ep8 -> layer13_alltoall
	layer13_expert34_ep8 -> layer13_alltoall
	layer13_expert35_ep8 -> layer13_alltoall
	layer13_expert36_ep9 -> layer13_alltoall
	layer13_expert37_ep9 -> layer13_alltoall
	layer13_expert38_ep9 -> layer13_alltoall
	layer13_expert39_ep9 -> layer13_alltoall
	layer13_expert40_ep10 -> layer13_alltoall
	layer13_expert41_ep10 -> layer13_alltoall
	layer13_expert42_ep10 -> layer13_alltoall
	layer13_expert43_ep10 -> layer13_alltoall
	layer13_expert44_ep11 -> layer13_alltoall
	layer13_expert45_ep11 -> layer13_alltoall
	layer13_expert46_ep11 -> layer13_alltoall
	layer13_expert47_ep11 -> layer13_alltoall
	layer13_expert48_ep12 -> layer13_alltoall
	layer13_expert49_ep12 -> layer13_alltoall
	layer13_expert50_ep12 -> layer13_alltoall
	layer13_expert51_ep12 -> layer13_alltoall
	layer13_expert52_ep13 -> layer13_alltoall
	layer13_expert53_ep13 -> layer13_alltoall
	layer13_expert54_ep13 -> layer13_alltoall
	layer13_expert55_ep13 -> layer13_alltoall
	layer13_expert56_ep14 -> layer13_alltoall
	layer13_expert57_ep14 -> layer13_alltoall
	layer13_expert58_ep14 -> layer13_alltoall
	layer13_expert59_ep14 -> layer13_alltoall
	layer13_expert60_ep15 -> layer13_alltoall
	layer13_expert61_ep15 -> layer13_alltoall
	layer13_expert62_ep15 -> layer13_alltoall
	layer13_expert63_ep15 -> layer13_alltoall
	layer13_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer13_alltoall -> layer13_moe_aggregate
	layer13_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer13_attn_layernorm -> layer13_moe_residual [style=dashed]
	layer13_moe_aggregate -> layer13_moe_residual
	layer13_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer13_moe_residual -> layer13_moe_layernorm
	layer14_input [label="Layer 14 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer13_moe_layernorm -> layer14_input
	layer14_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer14_input -> layer14_qkv_tp0
	layer14_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer14_input -> layer14_qkv_tp1
	layer14_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer14_input -> layer14_qkv_tp2
	layer14_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer14_input -> layer14_qkv_tp3
	layer14_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer14_qkv_tp0 -> layer14_qkv_allreduce
	layer14_qkv_tp1 -> layer14_qkv_allreduce
	layer14_qkv_tp2 -> layer14_qkv_allreduce
	layer14_qkv_tp3 -> layer14_qkv_allreduce
	layer14_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer14_qkv_allreduce -> layer14_attn_tp0
	layer14_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer14_qkv_allreduce -> layer14_attn_tp1
	layer14_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer14_qkv_allreduce -> layer14_attn_tp2
	layer14_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer14_qkv_allreduce -> layer14_attn_tp3
	layer14_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer14_attn_tp0 -> layer14_attn_out_tp0
	layer14_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer14_attn_tp1 -> layer14_attn_out_tp1
	layer14_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer14_attn_tp2 -> layer14_attn_out_tp2
	layer14_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer14_attn_tp3 -> layer14_attn_out_tp3
	layer14_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer14_attn_out_tp0 -> layer14_attn_allreduce
	layer14_attn_out_tp1 -> layer14_attn_allreduce
	layer14_attn_out_tp2 -> layer14_attn_allreduce
	layer14_attn_out_tp3 -> layer14_attn_allreduce
	layer14_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer14_input -> layer14_attn_residual
	layer14_attn_allreduce -> layer14_attn_residual
	layer14_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer14_attn_residual -> layer14_attn_layernorm
	layer14_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer14_attn_layernorm -> layer14_gate
	layer14_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert0_ep0 [style=dashed]
	layer14_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert1_ep0 [style=dashed]
	layer14_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert2_ep0 [style=dashed]
	layer14_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert3_ep0 [style=dashed]
	layer14_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert4_ep1 [style=dashed]
	layer14_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert5_ep1 [style=dashed]
	layer14_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert6_ep1 [style=dashed]
	layer14_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert7_ep1 [style=dashed]
	layer14_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert8_ep2 [style=dashed]
	layer14_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert9_ep2 [style=dashed]
	layer14_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert10_ep2 [style=dashed]
	layer14_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert11_ep2 [style=dashed]
	layer14_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert12_ep3 [style=dashed]
	layer14_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert13_ep3 [style=dashed]
	layer14_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert14_ep3 [style=dashed]
	layer14_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert15_ep3 [style=dashed]
	layer14_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert16_ep4 [style=dashed]
	layer14_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert17_ep4 [style=dashed]
	layer14_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert18_ep4 [style=dashed]
	layer14_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert19_ep4 [style=dashed]
	layer14_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert20_ep5 [style=dashed]
	layer14_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert21_ep5 [style=dashed]
	layer14_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert22_ep5 [style=dashed]
	layer14_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert23_ep5 [style=dashed]
	layer14_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert24_ep6 [style=dashed]
	layer14_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert25_ep6 [style=dashed]
	layer14_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert26_ep6 [style=dashed]
	layer14_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert27_ep6 [style=dashed]
	layer14_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert28_ep7 [style=dashed]
	layer14_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert29_ep7 [style=dashed]
	layer14_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert30_ep7 [style=dashed]
	layer14_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert31_ep7 [style=dashed]
	layer14_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert32_ep8 [style=dashed]
	layer14_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert33_ep8 [style=dashed]
	layer14_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert34_ep8 [style=dashed]
	layer14_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert35_ep8 [style=dashed]
	layer14_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert36_ep9 [style=dashed]
	layer14_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert37_ep9 [style=dashed]
	layer14_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert38_ep9 [style=dashed]
	layer14_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert39_ep9 [style=dashed]
	layer14_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert40_ep10 [style=dashed]
	layer14_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert41_ep10 [style=dashed]
	layer14_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert42_ep10 [style=dashed]
	layer14_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert43_ep10 [style=dashed]
	layer14_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert44_ep11 [style=dashed]
	layer14_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert45_ep11 [style=dashed]
	layer14_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert46_ep11 [style=dashed]
	layer14_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert47_ep11 [style=dashed]
	layer14_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert48_ep12 [style=dashed]
	layer14_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert49_ep12 [style=dashed]
	layer14_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert50_ep12 [style=dashed]
	layer14_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert51_ep12 [style=dashed]
	layer14_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert52_ep13 [style=dashed]
	layer14_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert53_ep13 [style=dashed]
	layer14_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert54_ep13 [style=dashed]
	layer14_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert55_ep13 [style=dashed]
	layer14_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert56_ep14 [style=dashed]
	layer14_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert57_ep14 [style=dashed]
	layer14_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert58_ep14 [style=dashed]
	layer14_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert59_ep14 [style=dashed]
	layer14_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert60_ep15 [style=dashed]
	layer14_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert61_ep15 [style=dashed]
	layer14_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert62_ep15 [style=dashed]
	layer14_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer14_gate -> layer14_expert63_ep15 [style=dashed]
	layer14_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer14_expert0_ep0 -> layer14_alltoall
	layer14_expert1_ep0 -> layer14_alltoall
	layer14_expert2_ep0 -> layer14_alltoall
	layer14_expert3_ep0 -> layer14_alltoall
	layer14_expert4_ep1 -> layer14_alltoall
	layer14_expert5_ep1 -> layer14_alltoall
	layer14_expert6_ep1 -> layer14_alltoall
	layer14_expert7_ep1 -> layer14_alltoall
	layer14_expert8_ep2 -> layer14_alltoall
	layer14_expert9_ep2 -> layer14_alltoall
	layer14_expert10_ep2 -> layer14_alltoall
	layer14_expert11_ep2 -> layer14_alltoall
	layer14_expert12_ep3 -> layer14_alltoall
	layer14_expert13_ep3 -> layer14_alltoall
	layer14_expert14_ep3 -> layer14_alltoall
	layer14_expert15_ep3 -> layer14_alltoall
	layer14_expert16_ep4 -> layer14_alltoall
	layer14_expert17_ep4 -> layer14_alltoall
	layer14_expert18_ep4 -> layer14_alltoall
	layer14_expert19_ep4 -> layer14_alltoall
	layer14_expert20_ep5 -> layer14_alltoall
	layer14_expert21_ep5 -> layer14_alltoall
	layer14_expert22_ep5 -> layer14_alltoall
	layer14_expert23_ep5 -> layer14_alltoall
	layer14_expert24_ep6 -> layer14_alltoall
	layer14_expert25_ep6 -> layer14_alltoall
	layer14_expert26_ep6 -> layer14_alltoall
	layer14_expert27_ep6 -> layer14_alltoall
	layer14_expert28_ep7 -> layer14_alltoall
	layer14_expert29_ep7 -> layer14_alltoall
	layer14_expert30_ep7 -> layer14_alltoall
	layer14_expert31_ep7 -> layer14_alltoall
	layer14_expert32_ep8 -> layer14_alltoall
	layer14_expert33_ep8 -> layer14_alltoall
	layer14_expert34_ep8 -> layer14_alltoall
	layer14_expert35_ep8 -> layer14_alltoall
	layer14_expert36_ep9 -> layer14_alltoall
	layer14_expert37_ep9 -> layer14_alltoall
	layer14_expert38_ep9 -> layer14_alltoall
	layer14_expert39_ep9 -> layer14_alltoall
	layer14_expert40_ep10 -> layer14_alltoall
	layer14_expert41_ep10 -> layer14_alltoall
	layer14_expert42_ep10 -> layer14_alltoall
	layer14_expert43_ep10 -> layer14_alltoall
	layer14_expert44_ep11 -> layer14_alltoall
	layer14_expert45_ep11 -> layer14_alltoall
	layer14_expert46_ep11 -> layer14_alltoall
	layer14_expert47_ep11 -> layer14_alltoall
	layer14_expert48_ep12 -> layer14_alltoall
	layer14_expert49_ep12 -> layer14_alltoall
	layer14_expert50_ep12 -> layer14_alltoall
	layer14_expert51_ep12 -> layer14_alltoall
	layer14_expert52_ep13 -> layer14_alltoall
	layer14_expert53_ep13 -> layer14_alltoall
	layer14_expert54_ep13 -> layer14_alltoall
	layer14_expert55_ep13 -> layer14_alltoall
	layer14_expert56_ep14 -> layer14_alltoall
	layer14_expert57_ep14 -> layer14_alltoall
	layer14_expert58_ep14 -> layer14_alltoall
	layer14_expert59_ep14 -> layer14_alltoall
	layer14_expert60_ep15 -> layer14_alltoall
	layer14_expert61_ep15 -> layer14_alltoall
	layer14_expert62_ep15 -> layer14_alltoall
	layer14_expert63_ep15 -> layer14_alltoall
	layer14_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer14_alltoall -> layer14_moe_aggregate
	layer14_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer14_attn_layernorm -> layer14_moe_residual [style=dashed]
	layer14_moe_aggregate -> layer14_moe_residual
	layer14_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer14_moe_residual -> layer14_moe_layernorm
	layer15_input [label="Layer 15 Input\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer14_moe_layernorm -> layer15_input
	layer15_qkv_tp0 [label="QKV Linear TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer15_input -> layer15_qkv_tp0
	layer15_qkv_tp1 [label="QKV Linear TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer15_input -> layer15_qkv_tp1
	layer15_qkv_tp2 [label="QKV Linear TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer15_input -> layer15_qkv_tp2
	layer15_qkv_tp3 [label="QKV Linear TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightcoral style=filled]
	layer15_input -> layer15_qkv_tp3
	layer15_qkv_allreduce [label="QKV All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=192]" fillcolor=lightpink shape=ellipse style=filled]
	layer15_qkv_tp0 -> layer15_qkv_allreduce
	layer15_qkv_tp1 -> layer15_qkv_allreduce
	layer15_qkv_tp2 -> layer15_qkv_allreduce
	layer15_qkv_tp3 -> layer15_qkv_allreduce
	layer15_attn_tp0 [label="Attention Compute TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer15_qkv_allreduce -> layer15_attn_tp0
	layer15_attn_tp1 [label="Attention Compute TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer15_qkv_allreduce -> layer15_attn_tp1
	layer15_attn_tp2 [label="Attention Compute TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer15_qkv_allreduce -> layer15_attn_tp2
	layer15_attn_tp3 [label="Attention Compute TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=192]\nOutput: [batch_size=128, seq_len=10240, heads=4, d_k=64]" fillcolor=lightcoral style=filled]
	layer15_qkv_allreduce -> layer15_attn_tp3
	layer15_attn_out_tp0 [label="Attention Output TP0\nGPU: [0,4,8,12,16,20,24,28,32,36,40,44,48,52,56,60]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer15_attn_tp0 -> layer15_attn_out_tp0
	layer15_attn_out_tp1 [label="Attention Output TP1\nGPU: [1,5,9,13,17,21,25,29,33,37,41,45,49,53,57,61]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer15_attn_tp1 -> layer15_attn_out_tp1
	layer15_attn_out_tp2 [label="Attention Output TP2\nGPU: [2,6,10,14,18,22,26,30,34,38,42,46,50,54,58,62]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer15_attn_tp2 -> layer15_attn_out_tp2
	layer15_attn_out_tp3 [label="Attention Output TP3\nGPU: [3,7,11,15,19,23,27,31,35,39,43,47,51,55,59,63]\nInput: [batch_size=128, seq_len=10240, heads=4, d_k=64]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=256]" fillcolor=lightcoral style=filled]
	layer15_attn_tp3 -> layer15_attn_out_tp3
	layer15_attn_allreduce [label="Attention All-Reduce\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=256]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer15_attn_out_tp0 -> layer15_attn_allreduce
	layer15_attn_out_tp1 -> layer15_attn_allreduce
	layer15_attn_out_tp2 -> layer15_attn_allreduce
	layer15_attn_out_tp3 -> layer15_attn_allreduce
	layer15_attn_residual [label="Attention + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer15_input -> layer15_attn_residual
	layer15_attn_allreduce -> layer15_attn_residual
	layer15_attn_layernorm [label="Layer Norm (Post-Attn)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer15_attn_residual -> layer15_attn_layernorm
	layer15_gate [label="MoE Gate Network\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, num_experts=64]" fillcolor=lightgreen style=filled]
	layer15_attn_layernorm -> layer15_gate
	layer15_expert0_ep0 [label="Expert 0\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert0_ep0 [style=dashed]
	layer15_expert1_ep0 [label="Expert 1\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert1_ep0 [style=dashed]
	layer15_expert2_ep0 [label="Expert 2\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert2_ep0 [style=dashed]
	layer15_expert3_ep0 [label="Expert 3\nGPU: [0-3]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert3_ep0 [style=dashed]
	layer15_expert4_ep1 [label="Expert 4\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert4_ep1 [style=dashed]
	layer15_expert5_ep1 [label="Expert 5\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert5_ep1 [style=dashed]
	layer15_expert6_ep1 [label="Expert 6\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert6_ep1 [style=dashed]
	layer15_expert7_ep1 [label="Expert 7\nGPU: [4-7]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert7_ep1 [style=dashed]
	layer15_expert8_ep2 [label="Expert 8\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert8_ep2 [style=dashed]
	layer15_expert9_ep2 [label="Expert 9\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert9_ep2 [style=dashed]
	layer15_expert10_ep2 [label="Expert 10\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert10_ep2 [style=dashed]
	layer15_expert11_ep2 [label="Expert 11\nGPU: [8-11]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert11_ep2 [style=dashed]
	layer15_expert12_ep3 [label="Expert 12\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert12_ep3 [style=dashed]
	layer15_expert13_ep3 [label="Expert 13\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert13_ep3 [style=dashed]
	layer15_expert14_ep3 [label="Expert 14\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert14_ep3 [style=dashed]
	layer15_expert15_ep3 [label="Expert 15\nGPU: [12-15]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert15_ep3 [style=dashed]
	layer15_expert16_ep4 [label="Expert 16\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert16_ep4 [style=dashed]
	layer15_expert17_ep4 [label="Expert 17\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert17_ep4 [style=dashed]
	layer15_expert18_ep4 [label="Expert 18\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert18_ep4 [style=dashed]
	layer15_expert19_ep4 [label="Expert 19\nGPU: [16-19]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert19_ep4 [style=dashed]
	layer15_expert20_ep5 [label="Expert 20\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert20_ep5 [style=dashed]
	layer15_expert21_ep5 [label="Expert 21\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert21_ep5 [style=dashed]
	layer15_expert22_ep5 [label="Expert 22\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert22_ep5 [style=dashed]
	layer15_expert23_ep5 [label="Expert 23\nGPU: [20-23]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert23_ep5 [style=dashed]
	layer15_expert24_ep6 [label="Expert 24\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert24_ep6 [style=dashed]
	layer15_expert25_ep6 [label="Expert 25\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert25_ep6 [style=dashed]
	layer15_expert26_ep6 [label="Expert 26\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert26_ep6 [style=dashed]
	layer15_expert27_ep6 [label="Expert 27\nGPU: [24-27]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert27_ep6 [style=dashed]
	layer15_expert28_ep7 [label="Expert 28\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert28_ep7 [style=dashed]
	layer15_expert29_ep7 [label="Expert 29\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert29_ep7 [style=dashed]
	layer15_expert30_ep7 [label="Expert 30\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert30_ep7 [style=dashed]
	layer15_expert31_ep7 [label="Expert 31\nGPU: [28-31]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert31_ep7 [style=dashed]
	layer15_expert32_ep8 [label="Expert 32\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert32_ep8 [style=dashed]
	layer15_expert33_ep8 [label="Expert 33\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert33_ep8 [style=dashed]
	layer15_expert34_ep8 [label="Expert 34\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert34_ep8 [style=dashed]
	layer15_expert35_ep8 [label="Expert 35\nGPU: [32-35]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert35_ep8 [style=dashed]
	layer15_expert36_ep9 [label="Expert 36\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert36_ep9 [style=dashed]
	layer15_expert37_ep9 [label="Expert 37\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert37_ep9 [style=dashed]
	layer15_expert38_ep9 [label="Expert 38\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert38_ep9 [style=dashed]
	layer15_expert39_ep9 [label="Expert 39\nGPU: [36-39]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert39_ep9 [style=dashed]
	layer15_expert40_ep10 [label="Expert 40\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert40_ep10 [style=dashed]
	layer15_expert41_ep10 [label="Expert 41\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert41_ep10 [style=dashed]
	layer15_expert42_ep10 [label="Expert 42\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert42_ep10 [style=dashed]
	layer15_expert43_ep10 [label="Expert 43\nGPU: [40-43]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert43_ep10 [style=dashed]
	layer15_expert44_ep11 [label="Expert 44\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert44_ep11 [style=dashed]
	layer15_expert45_ep11 [label="Expert 45\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert45_ep11 [style=dashed]
	layer15_expert46_ep11 [label="Expert 46\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert46_ep11 [style=dashed]
	layer15_expert47_ep11 [label="Expert 47\nGPU: [44-47]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert47_ep11 [style=dashed]
	layer15_expert48_ep12 [label="Expert 48\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert48_ep12 [style=dashed]
	layer15_expert49_ep12 [label="Expert 49\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert49_ep12 [style=dashed]
	layer15_expert50_ep12 [label="Expert 50\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert50_ep12 [style=dashed]
	layer15_expert51_ep12 [label="Expert 51\nGPU: [48-51]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert51_ep12 [style=dashed]
	layer15_expert52_ep13 [label="Expert 52\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert52_ep13 [style=dashed]
	layer15_expert53_ep13 [label="Expert 53\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert53_ep13 [style=dashed]
	layer15_expert54_ep13 [label="Expert 54\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert54_ep13 [style=dashed]
	layer15_expert55_ep13 [label="Expert 55\nGPU: [52-55]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert55_ep13 [style=dashed]
	layer15_expert56_ep14 [label="Expert 56\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert56_ep14 [style=dashed]
	layer15_expert57_ep14 [label="Expert 57\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert57_ep14 [style=dashed]
	layer15_expert58_ep14 [label="Expert 58\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert58_ep14 [style=dashed]
	layer15_expert59_ep14 [label="Expert 59\nGPU: [56-59]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert59_ep14 [style=dashed]
	layer15_expert60_ep15 [label="Expert 60\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert60_ep15 [style=dashed]
	layer15_expert61_ep15 [label="Expert 61\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert61_ep15 [style=dashed]
	layer15_expert62_ep15 [label="Expert 62\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert62_ep15 [style=dashed]
	layer15_expert63_ep15 [label="Expert 63\nGPU: [60-63]\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightsalmon style=filled]
	layer15_gate -> layer15_expert63_ep15 [style=dashed]
	layer15_alltoall [label="All-to-All Expert Routing\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=8, seq_len=10240, hidden_dim=1024]" fillcolor=lightpink shape=ellipse style=filled]
	layer15_expert0_ep0 -> layer15_alltoall
	layer15_expert1_ep0 -> layer15_alltoall
	layer15_expert2_ep0 -> layer15_alltoall
	layer15_expert3_ep0 -> layer15_alltoall
	layer15_expert4_ep1 -> layer15_alltoall
	layer15_expert5_ep1 -> layer15_alltoall
	layer15_expert6_ep1 -> layer15_alltoall
	layer15_expert7_ep1 -> layer15_alltoall
	layer15_expert8_ep2 -> layer15_alltoall
	layer15_expert9_ep2 -> layer15_alltoall
	layer15_expert10_ep2 -> layer15_alltoall
	layer15_expert11_ep2 -> layer15_alltoall
	layer15_expert12_ep3 -> layer15_alltoall
	layer15_expert13_ep3 -> layer15_alltoall
	layer15_expert14_ep3 -> layer15_alltoall
	layer15_expert15_ep3 -> layer15_alltoall
	layer15_expert16_ep4 -> layer15_alltoall
	layer15_expert17_ep4 -> layer15_alltoall
	layer15_expert18_ep4 -> layer15_alltoall
	layer15_expert19_ep4 -> layer15_alltoall
	layer15_expert20_ep5 -> layer15_alltoall
	layer15_expert21_ep5 -> layer15_alltoall
	layer15_expert22_ep5 -> layer15_alltoall
	layer15_expert23_ep5 -> layer15_alltoall
	layer15_expert24_ep6 -> layer15_alltoall
	layer15_expert25_ep6 -> layer15_alltoall
	layer15_expert26_ep6 -> layer15_alltoall
	layer15_expert27_ep6 -> layer15_alltoall
	layer15_expert28_ep7 -> layer15_alltoall
	layer15_expert29_ep7 -> layer15_alltoall
	layer15_expert30_ep7 -> layer15_alltoall
	layer15_expert31_ep7 -> layer15_alltoall
	layer15_expert32_ep8 -> layer15_alltoall
	layer15_expert33_ep8 -> layer15_alltoall
	layer15_expert34_ep8 -> layer15_alltoall
	layer15_expert35_ep8 -> layer15_alltoall
	layer15_expert36_ep9 -> layer15_alltoall
	layer15_expert37_ep9 -> layer15_alltoall
	layer15_expert38_ep9 -> layer15_alltoall
	layer15_expert39_ep9 -> layer15_alltoall
	layer15_expert40_ep10 -> layer15_alltoall
	layer15_expert41_ep10 -> layer15_alltoall
	layer15_expert42_ep10 -> layer15_alltoall
	layer15_expert43_ep10 -> layer15_alltoall
	layer15_expert44_ep11 -> layer15_alltoall
	layer15_expert45_ep11 -> layer15_alltoall
	layer15_expert46_ep11 -> layer15_alltoall
	layer15_expert47_ep11 -> layer15_alltoall
	layer15_expert48_ep12 -> layer15_alltoall
	layer15_expert49_ep12 -> layer15_alltoall
	layer15_expert50_ep12 -> layer15_alltoall
	layer15_expert51_ep12 -> layer15_alltoall
	layer15_expert52_ep13 -> layer15_alltoall
	layer15_expert53_ep13 -> layer15_alltoall
	layer15_expert54_ep13 -> layer15_alltoall
	layer15_expert55_ep13 -> layer15_alltoall
	layer15_expert56_ep14 -> layer15_alltoall
	layer15_expert57_ep14 -> layer15_alltoall
	layer15_expert58_ep14 -> layer15_alltoall
	layer15_expert59_ep14 -> layer15_alltoall
	layer15_expert60_ep15 -> layer15_alltoall
	layer15_expert61_ep15 -> layer15_alltoall
	layer15_expert62_ep15 -> layer15_alltoall
	layer15_expert63_ep15 -> layer15_alltoall
	layer15_moe_aggregate [label="MoE Output Aggregation\nGPU: All\nInput: [batch_size=8, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer15_alltoall -> layer15_moe_aggregate
	layer15_moe_residual [label="MoE + Residual\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024], [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightyellow shape=parallelogram style=filled]
	layer15_attn_layernorm -> layer15_moe_residual [style=dashed]
	layer15_moe_aggregate -> layer15_moe_residual
	layer15_moe_layernorm [label="Layer Norm (Post-MoE)\nGPU: All\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightblue style=filled]
	layer15_moe_residual -> layer15_moe_layernorm
	output [label="Model Output\nInput: [batch_size=128, seq_len=10240, hidden_dim=1024]\nOutput: [batch_size=128, seq_len=10240, hidden_dim=1024]" fillcolor=lightgreen shape=ellipse style=filled]
	layer15_moe_layernorm -> output
}
