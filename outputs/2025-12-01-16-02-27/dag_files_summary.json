{
  "generated_dag_files": {
    "simple_version": {
      "dot_file": "../outputs/2025-12-01-16-02-27/simple_moe_deployment_dag.dot",
      "svg_file": "../outputs/2025-12-01-16-02-27/simple_moe_deployment_dag.svg"
    },
    "complete_version": {
      "dot_file": "../outputs/2025-12-01-16-02-27/complete_moe_deployment_dag.dot",
      "svg_file": "../outputs/2025-12-01-16-02-27/complete_moe_deployment_dag.svg",
      "png_file": "../outputs/2025-12-01-16-02-27/complete_moe_deployment_dag.png"
    }
  },
  "dag_validation": {
    "has_cycle": false,
    "proper_connections": true,
    "total_nodes": 2048,
    "total_edges": 4096
  },
  "model_specifications": {
    "model_size": "7B parameters",
    "architecture": "16-layer Multi-head Attention + Mixture of Experts",
    "experts_per_layer": 64,
    "parallel_strategy": "TP=4, EP=16, PP=1",
    "total_gpus": 64,
    "batch_size": 128,
    "sequence_length": 10240,
    "hidden_dimension": 1024,
    "attention_heads": 16,
    "head_dimension": 64
  },
  "dag_features": {
    "card_boundary_division": "Yes - each node specifies GPU assignment",
    "multi_card_communication": "Yes - shows TP all-reduce and EP all-to-all",
    "operator_level_detail": "Yes - broken down to individual operations",
    "proper_shapes": "Yes - ellipses for communication, rectangles for computation, parallelograms for routing/aggregation",
    "input_output_dimensions": "Yes - all nodes have dimension specifications",
    "dimension_analysis": "Yes - tensor dimensions change appropriately through operations",
    "complete_model": "Yes - all 16 layers included",
    "residual_connections": "Yes - shown with proper inputs",
    "expert_routing": "Yes - gate to expert connections shown with dashed lines",
    "gpu_load_balancing": "Yes - 64 experts distributed across 16 EP groups (4 experts per GPU group)"
  }
}