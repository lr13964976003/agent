// Proposed Cross-Node Expert Parallelism EP=16
digraph {
	dpi=300 rankdir=TB size="50,60"
	node [fillcolor=lightblue shape=rectangle style="rounded,filled"]
	input [label="Input\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightgreen shape=ellipse]
	mha_layer0_gpu0 [label="MHA Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu1 [label="MHA Layer 0 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu2 [label="MHA Layer 0 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu3 [label="MHA Layer 0 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu4 [label="MHA Layer 0 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu5 [label="MHA Layer 0 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu6 [label="MHA Layer 0 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu7 [label="MHA Layer 0 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu8 [label="MHA Layer 0 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu9 [label="MHA Layer 0 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu10 [label="MHA Layer 0 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu11 [label="MHA Layer 0 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu12 [label="MHA Layer 0 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu13 [label="MHA Layer 0 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu14 [label="MHA Layer 0 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer0_gpu15 [label="MHA Layer 0 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer0_gpu0 [label="Gate Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu1 [label="Gate Layer 0 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu2 [label="Gate Layer 0 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu3 [label="Gate Layer 0 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu4 [label="Gate Layer 0 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu5 [label="Gate Layer 0 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu6 [label="Gate Layer 0 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu7 [label="Gate Layer 0 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu8 [label="Gate Layer 0 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu9 [label="Gate Layer 0 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu10 [label="Gate Layer 0 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu11 [label="Gate Layer 0 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu12 [label="Gate Layer 0 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu13 [label="Gate Layer 0 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu14 [label="Gate Layer 0 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer0_gpu15 [label="Gate Layer 0 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer0_gpu0_expert0 [label="Expert 0 Layer 0 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu1_expert1 [label="Expert 1 Layer 0 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu2_expert2 [label="Expert 2 Layer 0 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu3_expert3 [label="Expert 3 Layer 0 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu4_expert4 [label="Expert 4 Layer 0 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu5_expert5 [label="Expert 5 Layer 0 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu6_expert6 [label="Expert 6 Layer 0 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu7_expert7 [label="Expert 7 Layer 0 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu8_expert8 [label="Expert 8 Layer 0 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu9_expert9 [label="Expert 9 Layer 0 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu10_expert10 [label="Expert 10 Layer 0 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu11_expert11 [label="Expert 11 Layer 0 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu12_expert12 [label="Expert 12 Layer 0 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu13_expert13 [label="Expert 13 Layer 0 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu14_expert14 [label="Expert 14 Layer 0 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer0_gpu15_expert15 [label="Expert 15 Layer 0 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer0_gpu0 [label="Token Router Layer 0 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu0 [label="Token Aggregator Layer 0 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu1 [label="Token Router Layer 0 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu1 [label="Token Aggregator Layer 0 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu2 [label="Token Router Layer 0 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu2 [label="Token Aggregator Layer 0 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu3 [label="Token Router Layer 0 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu3 [label="Token Aggregator Layer 0 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu4 [label="Token Router Layer 0 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu4 [label="Token Aggregator Layer 0 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu5 [label="Token Router Layer 0 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu5 [label="Token Aggregator Layer 0 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu6 [label="Token Router Layer 0 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu6 [label="Token Aggregator Layer 0 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu7 [label="Token Router Layer 0 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu7 [label="Token Aggregator Layer 0 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu8 [label="Token Router Layer 0 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu8 [label="Token Aggregator Layer 0 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu9 [label="Token Router Layer 0 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu9 [label="Token Aggregator Layer 0 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu10 [label="Token Router Layer 0 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu10 [label="Token Aggregator Layer 0 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu11 [label="Token Router Layer 0 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu11 [label="Token Aggregator Layer 0 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu12 [label="Token Router Layer 0 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu12 [label="Token Aggregator Layer 0 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu13 [label="Token Router Layer 0 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu13 [label="Token Aggregator Layer 0 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu14 [label="Token Router Layer 0 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu14 [label="Token Aggregator Layer 0 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer0_gpu15 [label="Token Router Layer 0 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer0_gpu15 [label="Token Aggregator Layer 0 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer0 [label="Cross-GPU Communication Layer 0\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer1_gpu0 [label="MHA Layer 1 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu1 [label="MHA Layer 1 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu2 [label="MHA Layer 1 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu3 [label="MHA Layer 1 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu4 [label="MHA Layer 1 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu5 [label="MHA Layer 1 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu6 [label="MHA Layer 1 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu7 [label="MHA Layer 1 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu8 [label="MHA Layer 1 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu9 [label="MHA Layer 1 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu10 [label="MHA Layer 1 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu11 [label="MHA Layer 1 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu12 [label="MHA Layer 1 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu13 [label="MHA Layer 1 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu14 [label="MHA Layer 1 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer1_gpu15 [label="MHA Layer 1 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer1_gpu0 [label="Gate Layer 1 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu1 [label="Gate Layer 1 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu2 [label="Gate Layer 1 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu3 [label="Gate Layer 1 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu4 [label="Gate Layer 1 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu5 [label="Gate Layer 1 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu6 [label="Gate Layer 1 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu7 [label="Gate Layer 1 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu8 [label="Gate Layer 1 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu9 [label="Gate Layer 1 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu10 [label="Gate Layer 1 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu11 [label="Gate Layer 1 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu12 [label="Gate Layer 1 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu13 [label="Gate Layer 1 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu14 [label="Gate Layer 1 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer1_gpu15 [label="Gate Layer 1 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer1_gpu0_expert0 [label="Expert 0 Layer 1 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu1_expert1 [label="Expert 1 Layer 1 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu2_expert2 [label="Expert 2 Layer 1 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu3_expert3 [label="Expert 3 Layer 1 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu4_expert4 [label="Expert 4 Layer 1 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu5_expert5 [label="Expert 5 Layer 1 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu6_expert6 [label="Expert 6 Layer 1 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu7_expert7 [label="Expert 7 Layer 1 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu8_expert8 [label="Expert 8 Layer 1 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu9_expert9 [label="Expert 9 Layer 1 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu10_expert10 [label="Expert 10 Layer 1 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu11_expert11 [label="Expert 11 Layer 1 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu12_expert12 [label="Expert 12 Layer 1 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu13_expert13 [label="Expert 13 Layer 1 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu14_expert14 [label="Expert 14 Layer 1 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer1_gpu15_expert15 [label="Expert 15 Layer 1 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer1_gpu0 [label="Token Router Layer 1 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu0 [label="Token Aggregator Layer 1 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu1 [label="Token Router Layer 1 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu1 [label="Token Aggregator Layer 1 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu2 [label="Token Router Layer 1 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu2 [label="Token Aggregator Layer 1 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu3 [label="Token Router Layer 1 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu3 [label="Token Aggregator Layer 1 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu4 [label="Token Router Layer 1 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu4 [label="Token Aggregator Layer 1 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu5 [label="Token Router Layer 1 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu5 [label="Token Aggregator Layer 1 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu6 [label="Token Router Layer 1 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu6 [label="Token Aggregator Layer 1 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu7 [label="Token Router Layer 1 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu7 [label="Token Aggregator Layer 1 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu8 [label="Token Router Layer 1 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu8 [label="Token Aggregator Layer 1 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu9 [label="Token Router Layer 1 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu9 [label="Token Aggregator Layer 1 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu10 [label="Token Router Layer 1 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu10 [label="Token Aggregator Layer 1 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu11 [label="Token Router Layer 1 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu11 [label="Token Aggregator Layer 1 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu12 [label="Token Router Layer 1 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu12 [label="Token Aggregator Layer 1 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu13 [label="Token Router Layer 1 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu13 [label="Token Aggregator Layer 1 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu14 [label="Token Router Layer 1 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu14 [label="Token Aggregator Layer 1 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer1_gpu15 [label="Token Router Layer 1 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer1_gpu15 [label="Token Aggregator Layer 1 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer1 [label="Cross-GPU Communication Layer 1\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer2_gpu0 [label="MHA Layer 2 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu1 [label="MHA Layer 2 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu2 [label="MHA Layer 2 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu3 [label="MHA Layer 2 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu4 [label="MHA Layer 2 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu5 [label="MHA Layer 2 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu6 [label="MHA Layer 2 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu7 [label="MHA Layer 2 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu8 [label="MHA Layer 2 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu9 [label="MHA Layer 2 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu10 [label="MHA Layer 2 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu11 [label="MHA Layer 2 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu12 [label="MHA Layer 2 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu13 [label="MHA Layer 2 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu14 [label="MHA Layer 2 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer2_gpu15 [label="MHA Layer 2 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer2_gpu0 [label="Gate Layer 2 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu1 [label="Gate Layer 2 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu2 [label="Gate Layer 2 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu3 [label="Gate Layer 2 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu4 [label="Gate Layer 2 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu5 [label="Gate Layer 2 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu6 [label="Gate Layer 2 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu7 [label="Gate Layer 2 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu8 [label="Gate Layer 2 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu9 [label="Gate Layer 2 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu10 [label="Gate Layer 2 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu11 [label="Gate Layer 2 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu12 [label="Gate Layer 2 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu13 [label="Gate Layer 2 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu14 [label="Gate Layer 2 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer2_gpu15 [label="Gate Layer 2 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer2_gpu0_expert0 [label="Expert 0 Layer 2 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu1_expert1 [label="Expert 1 Layer 2 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu2_expert2 [label="Expert 2 Layer 2 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu3_expert3 [label="Expert 3 Layer 2 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu4_expert4 [label="Expert 4 Layer 2 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu5_expert5 [label="Expert 5 Layer 2 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu6_expert6 [label="Expert 6 Layer 2 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu7_expert7 [label="Expert 7 Layer 2 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu8_expert8 [label="Expert 8 Layer 2 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu9_expert9 [label="Expert 9 Layer 2 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu10_expert10 [label="Expert 10 Layer 2 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu11_expert11 [label="Expert 11 Layer 2 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu12_expert12 [label="Expert 12 Layer 2 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu13_expert13 [label="Expert 13 Layer 2 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu14_expert14 [label="Expert 14 Layer 2 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer2_gpu15_expert15 [label="Expert 15 Layer 2 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer2_gpu0 [label="Token Router Layer 2 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu0 [label="Token Aggregator Layer 2 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu1 [label="Token Router Layer 2 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu1 [label="Token Aggregator Layer 2 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu2 [label="Token Router Layer 2 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu2 [label="Token Aggregator Layer 2 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu3 [label="Token Router Layer 2 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu3 [label="Token Aggregator Layer 2 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu4 [label="Token Router Layer 2 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu4 [label="Token Aggregator Layer 2 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu5 [label="Token Router Layer 2 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu5 [label="Token Aggregator Layer 2 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu6 [label="Token Router Layer 2 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu6 [label="Token Aggregator Layer 2 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu7 [label="Token Router Layer 2 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu7 [label="Token Aggregator Layer 2 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu8 [label="Token Router Layer 2 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu8 [label="Token Aggregator Layer 2 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu9 [label="Token Router Layer 2 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu9 [label="Token Aggregator Layer 2 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu10 [label="Token Router Layer 2 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu10 [label="Token Aggregator Layer 2 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu11 [label="Token Router Layer 2 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu11 [label="Token Aggregator Layer 2 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu12 [label="Token Router Layer 2 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu12 [label="Token Aggregator Layer 2 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu13 [label="Token Router Layer 2 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu13 [label="Token Aggregator Layer 2 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu14 [label="Token Router Layer 2 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu14 [label="Token Aggregator Layer 2 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer2_gpu15 [label="Token Router Layer 2 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer2_gpu15 [label="Token Aggregator Layer 2 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer2 [label="Cross-GPU Communication Layer 2\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer3_gpu0 [label="MHA Layer 3 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu1 [label="MHA Layer 3 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu2 [label="MHA Layer 3 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu3 [label="MHA Layer 3 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu4 [label="MHA Layer 3 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu5 [label="MHA Layer 3 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu6 [label="MHA Layer 3 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu7 [label="MHA Layer 3 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu8 [label="MHA Layer 3 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu9 [label="MHA Layer 3 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu10 [label="MHA Layer 3 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu11 [label="MHA Layer 3 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu12 [label="MHA Layer 3 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu13 [label="MHA Layer 3 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu14 [label="MHA Layer 3 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer3_gpu15 [label="MHA Layer 3 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer3_gpu0 [label="Gate Layer 3 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu1 [label="Gate Layer 3 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu2 [label="Gate Layer 3 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu3 [label="Gate Layer 3 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu4 [label="Gate Layer 3 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu5 [label="Gate Layer 3 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu6 [label="Gate Layer 3 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu7 [label="Gate Layer 3 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu8 [label="Gate Layer 3 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu9 [label="Gate Layer 3 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu10 [label="Gate Layer 3 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu11 [label="Gate Layer 3 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu12 [label="Gate Layer 3 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu13 [label="Gate Layer 3 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu14 [label="Gate Layer 3 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer3_gpu15 [label="Gate Layer 3 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer3_gpu0_expert0 [label="Expert 0 Layer 3 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu1_expert1 [label="Expert 1 Layer 3 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu2_expert2 [label="Expert 2 Layer 3 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu3_expert3 [label="Expert 3 Layer 3 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu4_expert4 [label="Expert 4 Layer 3 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu5_expert5 [label="Expert 5 Layer 3 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu6_expert6 [label="Expert 6 Layer 3 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu7_expert7 [label="Expert 7 Layer 3 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu8_expert8 [label="Expert 8 Layer 3 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu9_expert9 [label="Expert 9 Layer 3 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu10_expert10 [label="Expert 10 Layer 3 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu11_expert11 [label="Expert 11 Layer 3 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu12_expert12 [label="Expert 12 Layer 3 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu13_expert13 [label="Expert 13 Layer 3 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu14_expert14 [label="Expert 14 Layer 3 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer3_gpu15_expert15 [label="Expert 15 Layer 3 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer3_gpu0 [label="Token Router Layer 3 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu0 [label="Token Aggregator Layer 3 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu1 [label="Token Router Layer 3 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu1 [label="Token Aggregator Layer 3 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu2 [label="Token Router Layer 3 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu2 [label="Token Aggregator Layer 3 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu3 [label="Token Router Layer 3 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu3 [label="Token Aggregator Layer 3 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu4 [label="Token Router Layer 3 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu4 [label="Token Aggregator Layer 3 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu5 [label="Token Router Layer 3 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu5 [label="Token Aggregator Layer 3 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu6 [label="Token Router Layer 3 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu6 [label="Token Aggregator Layer 3 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu7 [label="Token Router Layer 3 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu7 [label="Token Aggregator Layer 3 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu8 [label="Token Router Layer 3 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu8 [label="Token Aggregator Layer 3 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu9 [label="Token Router Layer 3 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu9 [label="Token Aggregator Layer 3 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu10 [label="Token Router Layer 3 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu10 [label="Token Aggregator Layer 3 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu11 [label="Token Router Layer 3 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu11 [label="Token Aggregator Layer 3 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu12 [label="Token Router Layer 3 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu12 [label="Token Aggregator Layer 3 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu13 [label="Token Router Layer 3 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu13 [label="Token Aggregator Layer 3 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu14 [label="Token Router Layer 3 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu14 [label="Token Aggregator Layer 3 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer3_gpu15 [label="Token Router Layer 3 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer3_gpu15 [label="Token Aggregator Layer 3 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer3 [label="Cross-GPU Communication Layer 3\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer4_gpu0 [label="MHA Layer 4 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu1 [label="MHA Layer 4 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu2 [label="MHA Layer 4 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu3 [label="MHA Layer 4 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu4 [label="MHA Layer 4 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu5 [label="MHA Layer 4 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu6 [label="MHA Layer 4 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu7 [label="MHA Layer 4 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu8 [label="MHA Layer 4 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu9 [label="MHA Layer 4 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu10 [label="MHA Layer 4 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu11 [label="MHA Layer 4 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu12 [label="MHA Layer 4 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu13 [label="MHA Layer 4 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu14 [label="MHA Layer 4 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer4_gpu15 [label="MHA Layer 4 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer4_gpu0 [label="Gate Layer 4 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu1 [label="Gate Layer 4 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu2 [label="Gate Layer 4 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu3 [label="Gate Layer 4 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu4 [label="Gate Layer 4 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu5 [label="Gate Layer 4 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu6 [label="Gate Layer 4 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu7 [label="Gate Layer 4 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu8 [label="Gate Layer 4 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu9 [label="Gate Layer 4 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu10 [label="Gate Layer 4 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu11 [label="Gate Layer 4 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu12 [label="Gate Layer 4 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu13 [label="Gate Layer 4 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu14 [label="Gate Layer 4 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer4_gpu15 [label="Gate Layer 4 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer4_gpu0_expert0 [label="Expert 0 Layer 4 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu1_expert1 [label="Expert 1 Layer 4 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu2_expert2 [label="Expert 2 Layer 4 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu3_expert3 [label="Expert 3 Layer 4 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu4_expert4 [label="Expert 4 Layer 4 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu5_expert5 [label="Expert 5 Layer 4 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu6_expert6 [label="Expert 6 Layer 4 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu7_expert7 [label="Expert 7 Layer 4 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu8_expert8 [label="Expert 8 Layer 4 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu9_expert9 [label="Expert 9 Layer 4 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu10_expert10 [label="Expert 10 Layer 4 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu11_expert11 [label="Expert 11 Layer 4 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu12_expert12 [label="Expert 12 Layer 4 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu13_expert13 [label="Expert 13 Layer 4 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu14_expert14 [label="Expert 14 Layer 4 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer4_gpu15_expert15 [label="Expert 15 Layer 4 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer4_gpu0 [label="Token Router Layer 4 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu0 [label="Token Aggregator Layer 4 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu1 [label="Token Router Layer 4 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu1 [label="Token Aggregator Layer 4 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu2 [label="Token Router Layer 4 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu2 [label="Token Aggregator Layer 4 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu3 [label="Token Router Layer 4 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu3 [label="Token Aggregator Layer 4 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu4 [label="Token Router Layer 4 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu4 [label="Token Aggregator Layer 4 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu5 [label="Token Router Layer 4 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu5 [label="Token Aggregator Layer 4 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu6 [label="Token Router Layer 4 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu6 [label="Token Aggregator Layer 4 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu7 [label="Token Router Layer 4 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu7 [label="Token Aggregator Layer 4 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu8 [label="Token Router Layer 4 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu8 [label="Token Aggregator Layer 4 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu9 [label="Token Router Layer 4 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu9 [label="Token Aggregator Layer 4 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu10 [label="Token Router Layer 4 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu10 [label="Token Aggregator Layer 4 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu11 [label="Token Router Layer 4 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu11 [label="Token Aggregator Layer 4 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu12 [label="Token Router Layer 4 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu12 [label="Token Aggregator Layer 4 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu13 [label="Token Router Layer 4 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu13 [label="Token Aggregator Layer 4 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu14 [label="Token Router Layer 4 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu14 [label="Token Aggregator Layer 4 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer4_gpu15 [label="Token Router Layer 4 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer4_gpu15 [label="Token Aggregator Layer 4 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer4 [label="Cross-GPU Communication Layer 4\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer5_gpu0 [label="MHA Layer 5 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu1 [label="MHA Layer 5 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu2 [label="MHA Layer 5 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu3 [label="MHA Layer 5 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu4 [label="MHA Layer 5 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu5 [label="MHA Layer 5 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu6 [label="MHA Layer 5 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu7 [label="MHA Layer 5 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu8 [label="MHA Layer 5 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu9 [label="MHA Layer 5 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu10 [label="MHA Layer 5 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu11 [label="MHA Layer 5 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu12 [label="MHA Layer 5 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu13 [label="MHA Layer 5 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu14 [label="MHA Layer 5 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer5_gpu15 [label="MHA Layer 5 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer5_gpu0 [label="Gate Layer 5 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu1 [label="Gate Layer 5 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu2 [label="Gate Layer 5 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu3 [label="Gate Layer 5 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu4 [label="Gate Layer 5 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu5 [label="Gate Layer 5 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu6 [label="Gate Layer 5 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu7 [label="Gate Layer 5 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu8 [label="Gate Layer 5 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu9 [label="Gate Layer 5 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu10 [label="Gate Layer 5 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu11 [label="Gate Layer 5 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu12 [label="Gate Layer 5 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu13 [label="Gate Layer 5 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu14 [label="Gate Layer 5 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer5_gpu15 [label="Gate Layer 5 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer5_gpu0_expert0 [label="Expert 0 Layer 5 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu1_expert1 [label="Expert 1 Layer 5 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu2_expert2 [label="Expert 2 Layer 5 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu3_expert3 [label="Expert 3 Layer 5 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu4_expert4 [label="Expert 4 Layer 5 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu5_expert5 [label="Expert 5 Layer 5 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu6_expert6 [label="Expert 6 Layer 5 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu7_expert7 [label="Expert 7 Layer 5 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu8_expert8 [label="Expert 8 Layer 5 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu9_expert9 [label="Expert 9 Layer 5 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu10_expert10 [label="Expert 10 Layer 5 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu11_expert11 [label="Expert 11 Layer 5 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu12_expert12 [label="Expert 12 Layer 5 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu13_expert13 [label="Expert 13 Layer 5 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu14_expert14 [label="Expert 14 Layer 5 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer5_gpu15_expert15 [label="Expert 15 Layer 5 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer5_gpu0 [label="Token Router Layer 5 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu0 [label="Token Aggregator Layer 5 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu1 [label="Token Router Layer 5 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu1 [label="Token Aggregator Layer 5 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu2 [label="Token Router Layer 5 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu2 [label="Token Aggregator Layer 5 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu3 [label="Token Router Layer 5 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu3 [label="Token Aggregator Layer 5 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu4 [label="Token Router Layer 5 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu4 [label="Token Aggregator Layer 5 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu5 [label="Token Router Layer 5 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu5 [label="Token Aggregator Layer 5 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu6 [label="Token Router Layer 5 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu6 [label="Token Aggregator Layer 5 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu7 [label="Token Router Layer 5 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu7 [label="Token Aggregator Layer 5 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu8 [label="Token Router Layer 5 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu8 [label="Token Aggregator Layer 5 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu9 [label="Token Router Layer 5 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu9 [label="Token Aggregator Layer 5 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu10 [label="Token Router Layer 5 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu10 [label="Token Aggregator Layer 5 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu11 [label="Token Router Layer 5 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu11 [label="Token Aggregator Layer 5 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu12 [label="Token Router Layer 5 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu12 [label="Token Aggregator Layer 5 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu13 [label="Token Router Layer 5 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu13 [label="Token Aggregator Layer 5 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu14 [label="Token Router Layer 5 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu14 [label="Token Aggregator Layer 5 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer5_gpu15 [label="Token Router Layer 5 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer5_gpu15 [label="Token Aggregator Layer 5 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer5 [label="Cross-GPU Communication Layer 5\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer6_gpu0 [label="MHA Layer 6 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu1 [label="MHA Layer 6 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu2 [label="MHA Layer 6 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu3 [label="MHA Layer 6 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu4 [label="MHA Layer 6 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu5 [label="MHA Layer 6 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu6 [label="MHA Layer 6 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu7 [label="MHA Layer 6 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu8 [label="MHA Layer 6 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu9 [label="MHA Layer 6 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu10 [label="MHA Layer 6 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu11 [label="MHA Layer 6 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu12 [label="MHA Layer 6 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu13 [label="MHA Layer 6 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu14 [label="MHA Layer 6 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer6_gpu15 [label="MHA Layer 6 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer6_gpu0 [label="Gate Layer 6 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu1 [label="Gate Layer 6 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu2 [label="Gate Layer 6 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu3 [label="Gate Layer 6 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu4 [label="Gate Layer 6 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu5 [label="Gate Layer 6 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu6 [label="Gate Layer 6 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu7 [label="Gate Layer 6 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu8 [label="Gate Layer 6 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu9 [label="Gate Layer 6 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu10 [label="Gate Layer 6 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu11 [label="Gate Layer 6 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu12 [label="Gate Layer 6 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu13 [label="Gate Layer 6 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu14 [label="Gate Layer 6 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer6_gpu15 [label="Gate Layer 6 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer6_gpu0_expert0 [label="Expert 0 Layer 6 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu1_expert1 [label="Expert 1 Layer 6 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu2_expert2 [label="Expert 2 Layer 6 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu3_expert3 [label="Expert 3 Layer 6 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu4_expert4 [label="Expert 4 Layer 6 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu5_expert5 [label="Expert 5 Layer 6 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu6_expert6 [label="Expert 6 Layer 6 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu7_expert7 [label="Expert 7 Layer 6 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu8_expert8 [label="Expert 8 Layer 6 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu9_expert9 [label="Expert 9 Layer 6 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu10_expert10 [label="Expert 10 Layer 6 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu11_expert11 [label="Expert 11 Layer 6 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu12_expert12 [label="Expert 12 Layer 6 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu13_expert13 [label="Expert 13 Layer 6 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu14_expert14 [label="Expert 14 Layer 6 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer6_gpu15_expert15 [label="Expert 15 Layer 6 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer6_gpu0 [label="Token Router Layer 6 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu0 [label="Token Aggregator Layer 6 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu1 [label="Token Router Layer 6 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu1 [label="Token Aggregator Layer 6 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu2 [label="Token Router Layer 6 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu2 [label="Token Aggregator Layer 6 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu3 [label="Token Router Layer 6 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu3 [label="Token Aggregator Layer 6 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu4 [label="Token Router Layer 6 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu4 [label="Token Aggregator Layer 6 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu5 [label="Token Router Layer 6 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu5 [label="Token Aggregator Layer 6 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu6 [label="Token Router Layer 6 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu6 [label="Token Aggregator Layer 6 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu7 [label="Token Router Layer 6 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu7 [label="Token Aggregator Layer 6 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu8 [label="Token Router Layer 6 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu8 [label="Token Aggregator Layer 6 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu9 [label="Token Router Layer 6 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu9 [label="Token Aggregator Layer 6 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu10 [label="Token Router Layer 6 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu10 [label="Token Aggregator Layer 6 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu11 [label="Token Router Layer 6 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu11 [label="Token Aggregator Layer 6 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu12 [label="Token Router Layer 6 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu12 [label="Token Aggregator Layer 6 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu13 [label="Token Router Layer 6 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu13 [label="Token Aggregator Layer 6 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu14 [label="Token Router Layer 6 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu14 [label="Token Aggregator Layer 6 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer6_gpu15 [label="Token Router Layer 6 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer6_gpu15 [label="Token Aggregator Layer 6 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer6 [label="Cross-GPU Communication Layer 6\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer7_gpu0 [label="MHA Layer 7 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu1 [label="MHA Layer 7 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu2 [label="MHA Layer 7 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu3 [label="MHA Layer 7 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu4 [label="MHA Layer 7 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu5 [label="MHA Layer 7 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu6 [label="MHA Layer 7 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu7 [label="MHA Layer 7 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu8 [label="MHA Layer 7 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu9 [label="MHA Layer 7 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu10 [label="MHA Layer 7 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu11 [label="MHA Layer 7 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu12 [label="MHA Layer 7 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu13 [label="MHA Layer 7 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu14 [label="MHA Layer 7 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer7_gpu15 [label="MHA Layer 7 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer7_gpu0 [label="Gate Layer 7 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu1 [label="Gate Layer 7 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu2 [label="Gate Layer 7 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu3 [label="Gate Layer 7 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu4 [label="Gate Layer 7 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu5 [label="Gate Layer 7 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu6 [label="Gate Layer 7 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu7 [label="Gate Layer 7 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu8 [label="Gate Layer 7 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu9 [label="Gate Layer 7 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu10 [label="Gate Layer 7 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu11 [label="Gate Layer 7 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu12 [label="Gate Layer 7 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu13 [label="Gate Layer 7 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu14 [label="Gate Layer 7 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer7_gpu15 [label="Gate Layer 7 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer7_gpu0_expert0 [label="Expert 0 Layer 7 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu1_expert1 [label="Expert 1 Layer 7 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu2_expert2 [label="Expert 2 Layer 7 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu3_expert3 [label="Expert 3 Layer 7 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu4_expert4 [label="Expert 4 Layer 7 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu5_expert5 [label="Expert 5 Layer 7 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu6_expert6 [label="Expert 6 Layer 7 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu7_expert7 [label="Expert 7 Layer 7 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu8_expert8 [label="Expert 8 Layer 7 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu9_expert9 [label="Expert 9 Layer 7 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu10_expert10 [label="Expert 10 Layer 7 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu11_expert11 [label="Expert 11 Layer 7 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu12_expert12 [label="Expert 12 Layer 7 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu13_expert13 [label="Expert 13 Layer 7 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu14_expert14 [label="Expert 14 Layer 7 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer7_gpu15_expert15 [label="Expert 15 Layer 7 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer7_gpu0 [label="Token Router Layer 7 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu0 [label="Token Aggregator Layer 7 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu1 [label="Token Router Layer 7 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu1 [label="Token Aggregator Layer 7 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu2 [label="Token Router Layer 7 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu2 [label="Token Aggregator Layer 7 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu3 [label="Token Router Layer 7 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu3 [label="Token Aggregator Layer 7 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu4 [label="Token Router Layer 7 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu4 [label="Token Aggregator Layer 7 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu5 [label="Token Router Layer 7 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu5 [label="Token Aggregator Layer 7 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu6 [label="Token Router Layer 7 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu6 [label="Token Aggregator Layer 7 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu7 [label="Token Router Layer 7 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu7 [label="Token Aggregator Layer 7 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu8 [label="Token Router Layer 7 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu8 [label="Token Aggregator Layer 7 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu9 [label="Token Router Layer 7 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu9 [label="Token Aggregator Layer 7 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu10 [label="Token Router Layer 7 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu10 [label="Token Aggregator Layer 7 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu11 [label="Token Router Layer 7 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu11 [label="Token Aggregator Layer 7 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu12 [label="Token Router Layer 7 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu12 [label="Token Aggregator Layer 7 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu13 [label="Token Router Layer 7 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu13 [label="Token Aggregator Layer 7 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu14 [label="Token Router Layer 7 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu14 [label="Token Aggregator Layer 7 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer7_gpu15 [label="Token Router Layer 7 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer7_gpu15 [label="Token Aggregator Layer 7 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer7 [label="Cross-GPU Communication Layer 7\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer8_gpu0 [label="MHA Layer 8 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu1 [label="MHA Layer 8 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu2 [label="MHA Layer 8 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu3 [label="MHA Layer 8 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu4 [label="MHA Layer 8 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu5 [label="MHA Layer 8 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu6 [label="MHA Layer 8 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu7 [label="MHA Layer 8 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu8 [label="MHA Layer 8 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu9 [label="MHA Layer 8 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu10 [label="MHA Layer 8 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu11 [label="MHA Layer 8 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu12 [label="MHA Layer 8 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu13 [label="MHA Layer 8 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu14 [label="MHA Layer 8 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer8_gpu15 [label="MHA Layer 8 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer8_gpu0 [label="Gate Layer 8 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu1 [label="Gate Layer 8 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu2 [label="Gate Layer 8 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu3 [label="Gate Layer 8 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu4 [label="Gate Layer 8 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu5 [label="Gate Layer 8 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu6 [label="Gate Layer 8 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu7 [label="Gate Layer 8 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu8 [label="Gate Layer 8 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu9 [label="Gate Layer 8 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu10 [label="Gate Layer 8 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu11 [label="Gate Layer 8 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu12 [label="Gate Layer 8 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu13 [label="Gate Layer 8 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu14 [label="Gate Layer 8 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer8_gpu15 [label="Gate Layer 8 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer8_gpu0_expert0 [label="Expert 0 Layer 8 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu1_expert1 [label="Expert 1 Layer 8 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu2_expert2 [label="Expert 2 Layer 8 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu3_expert3 [label="Expert 3 Layer 8 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu4_expert4 [label="Expert 4 Layer 8 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu5_expert5 [label="Expert 5 Layer 8 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu6_expert6 [label="Expert 6 Layer 8 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu7_expert7 [label="Expert 7 Layer 8 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu8_expert8 [label="Expert 8 Layer 8 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu9_expert9 [label="Expert 9 Layer 8 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu10_expert10 [label="Expert 10 Layer 8 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu11_expert11 [label="Expert 11 Layer 8 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu12_expert12 [label="Expert 12 Layer 8 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu13_expert13 [label="Expert 13 Layer 8 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu14_expert14 [label="Expert 14 Layer 8 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer8_gpu15_expert15 [label="Expert 15 Layer 8 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer8_gpu0 [label="Token Router Layer 8 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu0 [label="Token Aggregator Layer 8 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu1 [label="Token Router Layer 8 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu1 [label="Token Aggregator Layer 8 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu2 [label="Token Router Layer 8 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu2 [label="Token Aggregator Layer 8 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu3 [label="Token Router Layer 8 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu3 [label="Token Aggregator Layer 8 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu4 [label="Token Router Layer 8 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu4 [label="Token Aggregator Layer 8 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu5 [label="Token Router Layer 8 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu5 [label="Token Aggregator Layer 8 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu6 [label="Token Router Layer 8 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu6 [label="Token Aggregator Layer 8 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu7 [label="Token Router Layer 8 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu7 [label="Token Aggregator Layer 8 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu8 [label="Token Router Layer 8 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu8 [label="Token Aggregator Layer 8 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu9 [label="Token Router Layer 8 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu9 [label="Token Aggregator Layer 8 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu10 [label="Token Router Layer 8 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu10 [label="Token Aggregator Layer 8 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu11 [label="Token Router Layer 8 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu11 [label="Token Aggregator Layer 8 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu12 [label="Token Router Layer 8 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu12 [label="Token Aggregator Layer 8 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu13 [label="Token Router Layer 8 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu13 [label="Token Aggregator Layer 8 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu14 [label="Token Router Layer 8 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu14 [label="Token Aggregator Layer 8 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer8_gpu15 [label="Token Router Layer 8 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer8_gpu15 [label="Token Aggregator Layer 8 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer8 [label="Cross-GPU Communication Layer 8\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer9_gpu0 [label="MHA Layer 9 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu1 [label="MHA Layer 9 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu2 [label="MHA Layer 9 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu3 [label="MHA Layer 9 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu4 [label="MHA Layer 9 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu5 [label="MHA Layer 9 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu6 [label="MHA Layer 9 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu7 [label="MHA Layer 9 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu8 [label="MHA Layer 9 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu9 [label="MHA Layer 9 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu10 [label="MHA Layer 9 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu11 [label="MHA Layer 9 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu12 [label="MHA Layer 9 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu13 [label="MHA Layer 9 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu14 [label="MHA Layer 9 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer9_gpu15 [label="MHA Layer 9 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer9_gpu0 [label="Gate Layer 9 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu1 [label="Gate Layer 9 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu2 [label="Gate Layer 9 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu3 [label="Gate Layer 9 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu4 [label="Gate Layer 9 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu5 [label="Gate Layer 9 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu6 [label="Gate Layer 9 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu7 [label="Gate Layer 9 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu8 [label="Gate Layer 9 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu9 [label="Gate Layer 9 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu10 [label="Gate Layer 9 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu11 [label="Gate Layer 9 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu12 [label="Gate Layer 9 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu13 [label="Gate Layer 9 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu14 [label="Gate Layer 9 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer9_gpu15 [label="Gate Layer 9 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer9_gpu0_expert0 [label="Expert 0 Layer 9 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu1_expert1 [label="Expert 1 Layer 9 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu2_expert2 [label="Expert 2 Layer 9 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu3_expert3 [label="Expert 3 Layer 9 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu4_expert4 [label="Expert 4 Layer 9 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu5_expert5 [label="Expert 5 Layer 9 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu6_expert6 [label="Expert 6 Layer 9 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu7_expert7 [label="Expert 7 Layer 9 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu8_expert8 [label="Expert 8 Layer 9 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu9_expert9 [label="Expert 9 Layer 9 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu10_expert10 [label="Expert 10 Layer 9 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu11_expert11 [label="Expert 11 Layer 9 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu12_expert12 [label="Expert 12 Layer 9 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu13_expert13 [label="Expert 13 Layer 9 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu14_expert14 [label="Expert 14 Layer 9 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer9_gpu15_expert15 [label="Expert 15 Layer 9 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer9_gpu0 [label="Token Router Layer 9 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu0 [label="Token Aggregator Layer 9 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu1 [label="Token Router Layer 9 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu1 [label="Token Aggregator Layer 9 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu2 [label="Token Router Layer 9 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu2 [label="Token Aggregator Layer 9 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu3 [label="Token Router Layer 9 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu3 [label="Token Aggregator Layer 9 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu4 [label="Token Router Layer 9 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu4 [label="Token Aggregator Layer 9 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu5 [label="Token Router Layer 9 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu5 [label="Token Aggregator Layer 9 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu6 [label="Token Router Layer 9 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu6 [label="Token Aggregator Layer 9 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu7 [label="Token Router Layer 9 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu7 [label="Token Aggregator Layer 9 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu8 [label="Token Router Layer 9 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu8 [label="Token Aggregator Layer 9 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu9 [label="Token Router Layer 9 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu9 [label="Token Aggregator Layer 9 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu10 [label="Token Router Layer 9 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu10 [label="Token Aggregator Layer 9 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu11 [label="Token Router Layer 9 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu11 [label="Token Aggregator Layer 9 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu12 [label="Token Router Layer 9 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu12 [label="Token Aggregator Layer 9 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu13 [label="Token Router Layer 9 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu13 [label="Token Aggregator Layer 9 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu14 [label="Token Router Layer 9 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu14 [label="Token Aggregator Layer 9 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer9_gpu15 [label="Token Router Layer 9 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer9_gpu15 [label="Token Aggregator Layer 9 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer9 [label="Cross-GPU Communication Layer 9\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer10_gpu0 [label="MHA Layer 10 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu1 [label="MHA Layer 10 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu2 [label="MHA Layer 10 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu3 [label="MHA Layer 10 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu4 [label="MHA Layer 10 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu5 [label="MHA Layer 10 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu6 [label="MHA Layer 10 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu7 [label="MHA Layer 10 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu8 [label="MHA Layer 10 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu9 [label="MHA Layer 10 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu10 [label="MHA Layer 10 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu11 [label="MHA Layer 10 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu12 [label="MHA Layer 10 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu13 [label="MHA Layer 10 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu14 [label="MHA Layer 10 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer10_gpu15 [label="MHA Layer 10 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer10_gpu0 [label="Gate Layer 10 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu1 [label="Gate Layer 10 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu2 [label="Gate Layer 10 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu3 [label="Gate Layer 10 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu4 [label="Gate Layer 10 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu5 [label="Gate Layer 10 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu6 [label="Gate Layer 10 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu7 [label="Gate Layer 10 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu8 [label="Gate Layer 10 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu9 [label="Gate Layer 10 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu10 [label="Gate Layer 10 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu11 [label="Gate Layer 10 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu12 [label="Gate Layer 10 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu13 [label="Gate Layer 10 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu14 [label="Gate Layer 10 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer10_gpu15 [label="Gate Layer 10 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer10_gpu0_expert0 [label="Expert 0 Layer 10 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu1_expert1 [label="Expert 1 Layer 10 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu2_expert2 [label="Expert 2 Layer 10 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu3_expert3 [label="Expert 3 Layer 10 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu4_expert4 [label="Expert 4 Layer 10 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu5_expert5 [label="Expert 5 Layer 10 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu6_expert6 [label="Expert 6 Layer 10 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu7_expert7 [label="Expert 7 Layer 10 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu8_expert8 [label="Expert 8 Layer 10 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu9_expert9 [label="Expert 9 Layer 10 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu10_expert10 [label="Expert 10 Layer 10 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu11_expert11 [label="Expert 11 Layer 10 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu12_expert12 [label="Expert 12 Layer 10 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu13_expert13 [label="Expert 13 Layer 10 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu14_expert14 [label="Expert 14 Layer 10 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer10_gpu15_expert15 [label="Expert 15 Layer 10 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer10_gpu0 [label="Token Router Layer 10 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu0 [label="Token Aggregator Layer 10 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu1 [label="Token Router Layer 10 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu1 [label="Token Aggregator Layer 10 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu2 [label="Token Router Layer 10 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu2 [label="Token Aggregator Layer 10 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu3 [label="Token Router Layer 10 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu3 [label="Token Aggregator Layer 10 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu4 [label="Token Router Layer 10 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu4 [label="Token Aggregator Layer 10 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu5 [label="Token Router Layer 10 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu5 [label="Token Aggregator Layer 10 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu6 [label="Token Router Layer 10 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu6 [label="Token Aggregator Layer 10 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu7 [label="Token Router Layer 10 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu7 [label="Token Aggregator Layer 10 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu8 [label="Token Router Layer 10 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu8 [label="Token Aggregator Layer 10 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu9 [label="Token Router Layer 10 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu9 [label="Token Aggregator Layer 10 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu10 [label="Token Router Layer 10 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu10 [label="Token Aggregator Layer 10 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu11 [label="Token Router Layer 10 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu11 [label="Token Aggregator Layer 10 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu12 [label="Token Router Layer 10 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu12 [label="Token Aggregator Layer 10 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu13 [label="Token Router Layer 10 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu13 [label="Token Aggregator Layer 10 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu14 [label="Token Router Layer 10 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu14 [label="Token Aggregator Layer 10 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer10_gpu15 [label="Token Router Layer 10 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer10_gpu15 [label="Token Aggregator Layer 10 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer10 [label="Cross-GPU Communication Layer 10\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer11_gpu0 [label="MHA Layer 11 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu1 [label="MHA Layer 11 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu2 [label="MHA Layer 11 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu3 [label="MHA Layer 11 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu4 [label="MHA Layer 11 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu5 [label="MHA Layer 11 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu6 [label="MHA Layer 11 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu7 [label="MHA Layer 11 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu8 [label="MHA Layer 11 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu9 [label="MHA Layer 11 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu10 [label="MHA Layer 11 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu11 [label="MHA Layer 11 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu12 [label="MHA Layer 11 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu13 [label="MHA Layer 11 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu14 [label="MHA Layer 11 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer11_gpu15 [label="MHA Layer 11 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer11_gpu0 [label="Gate Layer 11 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu1 [label="Gate Layer 11 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu2 [label="Gate Layer 11 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu3 [label="Gate Layer 11 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu4 [label="Gate Layer 11 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu5 [label="Gate Layer 11 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu6 [label="Gate Layer 11 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu7 [label="Gate Layer 11 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu8 [label="Gate Layer 11 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu9 [label="Gate Layer 11 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu10 [label="Gate Layer 11 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu11 [label="Gate Layer 11 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu12 [label="Gate Layer 11 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu13 [label="Gate Layer 11 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu14 [label="Gate Layer 11 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer11_gpu15 [label="Gate Layer 11 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer11_gpu0_expert0 [label="Expert 0 Layer 11 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu1_expert1 [label="Expert 1 Layer 11 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu2_expert2 [label="Expert 2 Layer 11 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu3_expert3 [label="Expert 3 Layer 11 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu4_expert4 [label="Expert 4 Layer 11 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu5_expert5 [label="Expert 5 Layer 11 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu6_expert6 [label="Expert 6 Layer 11 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu7_expert7 [label="Expert 7 Layer 11 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu8_expert8 [label="Expert 8 Layer 11 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu9_expert9 [label="Expert 9 Layer 11 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu10_expert10 [label="Expert 10 Layer 11 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu11_expert11 [label="Expert 11 Layer 11 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu12_expert12 [label="Expert 12 Layer 11 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu13_expert13 [label="Expert 13 Layer 11 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu14_expert14 [label="Expert 14 Layer 11 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer11_gpu15_expert15 [label="Expert 15 Layer 11 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer11_gpu0 [label="Token Router Layer 11 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu0 [label="Token Aggregator Layer 11 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu1 [label="Token Router Layer 11 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu1 [label="Token Aggregator Layer 11 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu2 [label="Token Router Layer 11 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu2 [label="Token Aggregator Layer 11 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu3 [label="Token Router Layer 11 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu3 [label="Token Aggregator Layer 11 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu4 [label="Token Router Layer 11 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu4 [label="Token Aggregator Layer 11 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu5 [label="Token Router Layer 11 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu5 [label="Token Aggregator Layer 11 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu6 [label="Token Router Layer 11 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu6 [label="Token Aggregator Layer 11 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu7 [label="Token Router Layer 11 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu7 [label="Token Aggregator Layer 11 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu8 [label="Token Router Layer 11 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu8 [label="Token Aggregator Layer 11 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu9 [label="Token Router Layer 11 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu9 [label="Token Aggregator Layer 11 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu10 [label="Token Router Layer 11 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu10 [label="Token Aggregator Layer 11 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu11 [label="Token Router Layer 11 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu11 [label="Token Aggregator Layer 11 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu12 [label="Token Router Layer 11 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu12 [label="Token Aggregator Layer 11 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu13 [label="Token Router Layer 11 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu13 [label="Token Aggregator Layer 11 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu14 [label="Token Router Layer 11 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu14 [label="Token Aggregator Layer 11 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer11_gpu15 [label="Token Router Layer 11 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer11_gpu15 [label="Token Aggregator Layer 11 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer11 [label="Cross-GPU Communication Layer 11\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer12_gpu0 [label="MHA Layer 12 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu1 [label="MHA Layer 12 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu2 [label="MHA Layer 12 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu3 [label="MHA Layer 12 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu4 [label="MHA Layer 12 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu5 [label="MHA Layer 12 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu6 [label="MHA Layer 12 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu7 [label="MHA Layer 12 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu8 [label="MHA Layer 12 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu9 [label="MHA Layer 12 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu10 [label="MHA Layer 12 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu11 [label="MHA Layer 12 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu12 [label="MHA Layer 12 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu13 [label="MHA Layer 12 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu14 [label="MHA Layer 12 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer12_gpu15 [label="MHA Layer 12 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer12_gpu0 [label="Gate Layer 12 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu1 [label="Gate Layer 12 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu2 [label="Gate Layer 12 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu3 [label="Gate Layer 12 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu4 [label="Gate Layer 12 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu5 [label="Gate Layer 12 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu6 [label="Gate Layer 12 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu7 [label="Gate Layer 12 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu8 [label="Gate Layer 12 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu9 [label="Gate Layer 12 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu10 [label="Gate Layer 12 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu11 [label="Gate Layer 12 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu12 [label="Gate Layer 12 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu13 [label="Gate Layer 12 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu14 [label="Gate Layer 12 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer12_gpu15 [label="Gate Layer 12 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer12_gpu0_expert0 [label="Expert 0 Layer 12 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu1_expert1 [label="Expert 1 Layer 12 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu2_expert2 [label="Expert 2 Layer 12 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu3_expert3 [label="Expert 3 Layer 12 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu4_expert4 [label="Expert 4 Layer 12 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu5_expert5 [label="Expert 5 Layer 12 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu6_expert6 [label="Expert 6 Layer 12 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu7_expert7 [label="Expert 7 Layer 12 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu8_expert8 [label="Expert 8 Layer 12 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu9_expert9 [label="Expert 9 Layer 12 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu10_expert10 [label="Expert 10 Layer 12 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu11_expert11 [label="Expert 11 Layer 12 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu12_expert12 [label="Expert 12 Layer 12 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu13_expert13 [label="Expert 13 Layer 12 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu14_expert14 [label="Expert 14 Layer 12 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer12_gpu15_expert15 [label="Expert 15 Layer 12 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer12_gpu0 [label="Token Router Layer 12 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu0 [label="Token Aggregator Layer 12 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu1 [label="Token Router Layer 12 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu1 [label="Token Aggregator Layer 12 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu2 [label="Token Router Layer 12 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu2 [label="Token Aggregator Layer 12 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu3 [label="Token Router Layer 12 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu3 [label="Token Aggregator Layer 12 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu4 [label="Token Router Layer 12 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu4 [label="Token Aggregator Layer 12 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu5 [label="Token Router Layer 12 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu5 [label="Token Aggregator Layer 12 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu6 [label="Token Router Layer 12 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu6 [label="Token Aggregator Layer 12 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu7 [label="Token Router Layer 12 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu7 [label="Token Aggregator Layer 12 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu8 [label="Token Router Layer 12 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu8 [label="Token Aggregator Layer 12 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu9 [label="Token Router Layer 12 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu9 [label="Token Aggregator Layer 12 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu10 [label="Token Router Layer 12 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu10 [label="Token Aggregator Layer 12 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu11 [label="Token Router Layer 12 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu11 [label="Token Aggregator Layer 12 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu12 [label="Token Router Layer 12 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu12 [label="Token Aggregator Layer 12 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu13 [label="Token Router Layer 12 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu13 [label="Token Aggregator Layer 12 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu14 [label="Token Router Layer 12 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu14 [label="Token Aggregator Layer 12 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer12_gpu15 [label="Token Router Layer 12 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer12_gpu15 [label="Token Aggregator Layer 12 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer12 [label="Cross-GPU Communication Layer 12\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer13_gpu0 [label="MHA Layer 13 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu1 [label="MHA Layer 13 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu2 [label="MHA Layer 13 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu3 [label="MHA Layer 13 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu4 [label="MHA Layer 13 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu5 [label="MHA Layer 13 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu6 [label="MHA Layer 13 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu7 [label="MHA Layer 13 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu8 [label="MHA Layer 13 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu9 [label="MHA Layer 13 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu10 [label="MHA Layer 13 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu11 [label="MHA Layer 13 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu12 [label="MHA Layer 13 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu13 [label="MHA Layer 13 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu14 [label="MHA Layer 13 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer13_gpu15 [label="MHA Layer 13 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer13_gpu0 [label="Gate Layer 13 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu1 [label="Gate Layer 13 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu2 [label="Gate Layer 13 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu3 [label="Gate Layer 13 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu4 [label="Gate Layer 13 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu5 [label="Gate Layer 13 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu6 [label="Gate Layer 13 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu7 [label="Gate Layer 13 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu8 [label="Gate Layer 13 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu9 [label="Gate Layer 13 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu10 [label="Gate Layer 13 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu11 [label="Gate Layer 13 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu12 [label="Gate Layer 13 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu13 [label="Gate Layer 13 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu14 [label="Gate Layer 13 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer13_gpu15 [label="Gate Layer 13 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer13_gpu0_expert0 [label="Expert 0 Layer 13 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu1_expert1 [label="Expert 1 Layer 13 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu2_expert2 [label="Expert 2 Layer 13 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu3_expert3 [label="Expert 3 Layer 13 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu4_expert4 [label="Expert 4 Layer 13 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu5_expert5 [label="Expert 5 Layer 13 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu6_expert6 [label="Expert 6 Layer 13 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu7_expert7 [label="Expert 7 Layer 13 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu8_expert8 [label="Expert 8 Layer 13 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu9_expert9 [label="Expert 9 Layer 13 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu10_expert10 [label="Expert 10 Layer 13 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu11_expert11 [label="Expert 11 Layer 13 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu12_expert12 [label="Expert 12 Layer 13 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu13_expert13 [label="Expert 13 Layer 13 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu14_expert14 [label="Expert 14 Layer 13 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer13_gpu15_expert15 [label="Expert 15 Layer 13 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer13_gpu0 [label="Token Router Layer 13 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu0 [label="Token Aggregator Layer 13 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu1 [label="Token Router Layer 13 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu1 [label="Token Aggregator Layer 13 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu2 [label="Token Router Layer 13 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu2 [label="Token Aggregator Layer 13 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu3 [label="Token Router Layer 13 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu3 [label="Token Aggregator Layer 13 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu4 [label="Token Router Layer 13 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu4 [label="Token Aggregator Layer 13 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu5 [label="Token Router Layer 13 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu5 [label="Token Aggregator Layer 13 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu6 [label="Token Router Layer 13 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu6 [label="Token Aggregator Layer 13 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu7 [label="Token Router Layer 13 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu7 [label="Token Aggregator Layer 13 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu8 [label="Token Router Layer 13 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu8 [label="Token Aggregator Layer 13 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu9 [label="Token Router Layer 13 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu9 [label="Token Aggregator Layer 13 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu10 [label="Token Router Layer 13 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu10 [label="Token Aggregator Layer 13 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu11 [label="Token Router Layer 13 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu11 [label="Token Aggregator Layer 13 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu12 [label="Token Router Layer 13 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu12 [label="Token Aggregator Layer 13 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu13 [label="Token Router Layer 13 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu13 [label="Token Aggregator Layer 13 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu14 [label="Token Router Layer 13 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu14 [label="Token Aggregator Layer 13 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer13_gpu15 [label="Token Router Layer 13 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer13_gpu15 [label="Token Aggregator Layer 13 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer13 [label="Cross-GPU Communication Layer 13\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer14_gpu0 [label="MHA Layer 14 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu1 [label="MHA Layer 14 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu2 [label="MHA Layer 14 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu3 [label="MHA Layer 14 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu4 [label="MHA Layer 14 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu5 [label="MHA Layer 14 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu6 [label="MHA Layer 14 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu7 [label="MHA Layer 14 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu8 [label="MHA Layer 14 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu9 [label="MHA Layer 14 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu10 [label="MHA Layer 14 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu11 [label="MHA Layer 14 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu12 [label="MHA Layer 14 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu13 [label="MHA Layer 14 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu14 [label="MHA Layer 14 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer14_gpu15 [label="MHA Layer 14 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer14_gpu0 [label="Gate Layer 14 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu1 [label="Gate Layer 14 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu2 [label="Gate Layer 14 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu3 [label="Gate Layer 14 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu4 [label="Gate Layer 14 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu5 [label="Gate Layer 14 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu6 [label="Gate Layer 14 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu7 [label="Gate Layer 14 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu8 [label="Gate Layer 14 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu9 [label="Gate Layer 14 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu10 [label="Gate Layer 14 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu11 [label="Gate Layer 14 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu12 [label="Gate Layer 14 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu13 [label="Gate Layer 14 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu14 [label="Gate Layer 14 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer14_gpu15 [label="Gate Layer 14 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer14_gpu0_expert0 [label="Expert 0 Layer 14 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu1_expert1 [label="Expert 1 Layer 14 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu2_expert2 [label="Expert 2 Layer 14 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu3_expert3 [label="Expert 3 Layer 14 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu4_expert4 [label="Expert 4 Layer 14 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu5_expert5 [label="Expert 5 Layer 14 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu6_expert6 [label="Expert 6 Layer 14 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu7_expert7 [label="Expert 7 Layer 14 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu8_expert8 [label="Expert 8 Layer 14 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu9_expert9 [label="Expert 9 Layer 14 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu10_expert10 [label="Expert 10 Layer 14 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu11_expert11 [label="Expert 11 Layer 14 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu12_expert12 [label="Expert 12 Layer 14 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu13_expert13 [label="Expert 13 Layer 14 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu14_expert14 [label="Expert 14 Layer 14 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer14_gpu15_expert15 [label="Expert 15 Layer 14 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer14_gpu0 [label="Token Router Layer 14 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu0 [label="Token Aggregator Layer 14 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu1 [label="Token Router Layer 14 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu1 [label="Token Aggregator Layer 14 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu2 [label="Token Router Layer 14 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu2 [label="Token Aggregator Layer 14 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu3 [label="Token Router Layer 14 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu3 [label="Token Aggregator Layer 14 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu4 [label="Token Router Layer 14 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu4 [label="Token Aggregator Layer 14 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu5 [label="Token Router Layer 14 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu5 [label="Token Aggregator Layer 14 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu6 [label="Token Router Layer 14 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu6 [label="Token Aggregator Layer 14 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu7 [label="Token Router Layer 14 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu7 [label="Token Aggregator Layer 14 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu8 [label="Token Router Layer 14 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu8 [label="Token Aggregator Layer 14 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu9 [label="Token Router Layer 14 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu9 [label="Token Aggregator Layer 14 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu10 [label="Token Router Layer 14 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu10 [label="Token Aggregator Layer 14 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu11 [label="Token Router Layer 14 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu11 [label="Token Aggregator Layer 14 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu12 [label="Token Router Layer 14 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu12 [label="Token Aggregator Layer 14 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu13 [label="Token Router Layer 14 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu13 [label="Token Aggregator Layer 14 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu14 [label="Token Router Layer 14 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu14 [label="Token Aggregator Layer 14 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer14_gpu15 [label="Token Router Layer 14 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer14_gpu15 [label="Token Aggregator Layer 14 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer14 [label="Cross-GPU Communication Layer 14\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	mha_layer15_gpu0 [label="MHA Layer 15 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu1 [label="MHA Layer 15 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu2 [label="MHA Layer 15 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu3 [label="MHA Layer 15 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu4 [label="MHA Layer 15 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu5 [label="MHA Layer 15 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu6 [label="MHA Layer 15 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu7 [label="MHA Layer 15 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu8 [label="MHA Layer 15 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu9 [label="MHA Layer 15 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu10 [label="MHA Layer 15 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu11 [label="MHA Layer 15 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu12 [label="MHA Layer 15 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu13 [label="MHA Layer 15 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu14 [label="MHA Layer 15 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	mha_layer15_gpu15 [label="MHA Layer 15 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=32, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=32, d_k=128]" fillcolor=lightblue]
	gate_layer15_gpu0 [label="Gate Layer 15 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu1 [label="Gate Layer 15 GPU 1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu2 [label="Gate Layer 15 GPU 2\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu3 [label="Gate Layer 15 GPU 3\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu4 [label="Gate Layer 15 GPU 4\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu5 [label="Gate Layer 15 GPU 5\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu6 [label="Gate Layer 15 GPU 6\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu7 [label="Gate Layer 15 GPU 7\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu8 [label="Gate Layer 15 GPU 8\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu9 [label="Gate Layer 15 GPU 9\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu10 [label="Gate Layer 15 GPU 10\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu11 [label="Gate Layer 15 GPU 11\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu12 [label="Gate Layer 15 GPU 12\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu13 [label="Gate Layer 15 GPU 13\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu14 [label="Gate Layer 15 GPU 14\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	gate_layer15_gpu15 [label="Gate Layer 15 GPU 15\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, expert_selections=2]" fillcolor=lightsteelblue shape=diamond]
	expert_layer15_gpu0_expert0 [label="Expert 0 Layer 15 GPU 0\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu1_expert1 [label="Expert 1 Layer 15 GPU 1\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu2_expert2 [label="Expert 2 Layer 15 GPU 2\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu3_expert3 [label="Expert 3 Layer 15 GPU 3\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu4_expert4 [label="Expert 4 Layer 15 GPU 4\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu5_expert5 [label="Expert 5 Layer 15 GPU 5\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu6_expert6 [label="Expert 6 Layer 15 GPU 6\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu7_expert7 [label="Expert 7 Layer 15 GPU 7\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu8_expert8 [label="Expert 8 Layer 15 GPU 8\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu9_expert9 [label="Expert 9 Layer 15 GPU 9\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu10_expert10 [label="Expert 10 Layer 15 GPU 10\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu11_expert11 [label="Expert 11 Layer 15 GPU 11\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu12_expert12 [label="Expert 12 Layer 15 GPU 12\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu13_expert13 [label="Expert 13 Layer 15 GPU 13\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu14_expert14 [label="Expert 14 Layer 15 GPU 14\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	expert_layer15_gpu15_expert15 [label="Expert 15 Layer 15 GPU 15\nInput: [batch_size=variable, seq_len=variable, token_dim=4096]\nOutput: [batch_size=variable, seq_len=variable, token_dim=4096]\nMLP: 4096->16384akey=4096" fillcolor=lightcoral]
	route_layer15_gpu0 [label="Token Router Layer 15 GPU 0\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu0 [label="Token Aggregator Layer 15 GPU 0\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu1 [label="Token Router Layer 15 GPU 1\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu1 [label="Token Aggregator Layer 15 GPU 1\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu2 [label="Token Router Layer 15 GPU 2\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu2 [label="Token Aggregator Layer 15 GPU 2\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu3 [label="Token Router Layer 15 GPU 3\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu3 [label="Token Aggregator Layer 15 GPU 3\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu4 [label="Token Router Layer 15 GPU 4\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu4 [label="Token Aggregator Layer 15 GPU 4\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu5 [label="Token Router Layer 15 GPU 5\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu5 [label="Token Aggregator Layer 15 GPU 5\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu6 [label="Token Router Layer 15 GPU 6\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu6 [label="Token Aggregator Layer 15 GPU 6\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu7 [label="Token Router Layer 15 GPU 7\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu7 [label="Token Aggregator Layer 15 GPU 7\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu8 [label="Token Router Layer 15 GPU 8\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu8 [label="Token Aggregator Layer 15 GPU 8\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu9 [label="Token Router Layer 15 GPU 9\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu9 [label="Token Aggregator Layer 15 GPU 9\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu10 [label="Token Router Layer 15 GPU 10\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu10 [label="Token Aggregator Layer 15 GPU 10\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu11 [label="Token Router Layer 15 GPU 11\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu11 [label="Token Aggregator Layer 15 GPU 11\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu12 [label="Token Router Layer 15 GPU 12\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu12 [label="Token Aggregator Layer 15 GPU 12\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu13 [label="Token Router Layer 15 GPU 13\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu13 [label="Token Aggregator Layer 15 GPU 13\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu14 [label="Token Router Layer 15 GPU 14\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu14 [label="Token Aggregator Layer 15 GPU 14\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	route_layer15_gpu15 [label="Token Router Layer 15 GPU 15\nInput: Tokens with routing decisions\nOutput: Tokens sent to destination experts" fillcolor=orange shape=parallelogram]
	aggregate_layer15_gpu15 [label="Token Aggregator Layer 15 GPU 15\nInput: Processed tokens from all experts\nOutput: Combined token representations" fillcolor=orange shape=parallelogram]
	comm_layer15 [label="Cross-GPU Communication Layer 15\nInput: Tokens from routing decisions\nOutput: Tokens delivered to destination GPUs" fillcolor=lightpink shape=ellipse]
	output [label="Output\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightgray shape=ellipse]
	input -> mha_layer0_gpu0
	input -> mha_layer0_gpu1
	input -> mha_layer0_gpu2
	input -> mha_layer0_gpu3
	input -> mha_layer0_gpu4
	input -> mha_layer0_gpu5
	input -> mha_layer0_gpu6
	input -> mha_layer0_gpu7
	input -> mha_layer0_gpu8
	input -> mha_layer0_gpu9
	input -> mha_layer0_gpu10
	input -> mha_layer0_gpu11
	input -> mha_layer0_gpu12
	input -> mha_layer0_gpu13
	input -> mha_layer0_gpu14
	input -> mha_layer0_gpu15
	mha_layer0_gpu0 -> gate_layer0_gpu0
	mha_layer0_gpu1 -> gate_layer0_gpu1
	mha_layer0_gpu2 -> gate_layer0_gpu2
	mha_layer0_gpu3 -> gate_layer0_gpu3
	mha_layer0_gpu4 -> gate_layer0_gpu4
	mha_layer0_gpu5 -> gate_layer0_gpu5
	mha_layer0_gpu6 -> gate_layer0_gpu6
	mha_layer0_gpu7 -> gate_layer0_gpu7
	mha_layer0_gpu8 -> gate_layer0_gpu8
	mha_layer0_gpu9 -> gate_layer0_gpu9
	mha_layer0_gpu10 -> gate_layer0_gpu10
	mha_layer0_gpu11 -> gate_layer0_gpu11
	mha_layer0_gpu12 -> gate_layer0_gpu12
	mha_layer0_gpu13 -> gate_layer0_gpu13
	mha_layer0_gpu14 -> gate_layer0_gpu14
	mha_layer0_gpu15 -> gate_layer0_gpu15
	gate_layer0_gpu0 -> route_layer0_gpu0 [style=dashed]
	gate_layer0_gpu1 -> route_layer0_gpu1 [style=dashed]
	gate_layer0_gpu2 -> route_layer0_gpu2 [style=dashed]
	gate_layer0_gpu3 -> route_layer0_gpu3 [style=dashed]
	gate_layer0_gpu4 -> route_layer0_gpu4 [style=dashed]
	gate_layer0_gpu5 -> route_layer0_gpu5 [style=dashed]
	gate_layer0_gpu6 -> route_layer0_gpu6 [style=dashed]
	gate_layer0_gpu7 -> route_layer0_gpu7 [style=dashed]
	gate_layer0_gpu8 -> route_layer0_gpu8 [style=dashed]
	gate_layer0_gpu9 -> route_layer0_gpu9 [style=dashed]
	gate_layer0_gpu10 -> route_layer0_gpu10 [style=dashed]
	gate_layer0_gpu11 -> route_layer0_gpu11 [style=dashed]
	gate_layer0_gpu12 -> route_layer0_gpu12 [style=dashed]
	gate_layer0_gpu13 -> route_layer0_gpu13 [style=dashed]
	gate_layer0_gpu14 -> route_layer0_gpu14 [style=dashed]
	gate_layer0_gpu15 -> route_layer0_gpu15 [style=dashed]
	route_layer0_gpu0 -> comm_layer0
	route_layer0_gpu1 -> comm_layer0
	route_layer0_gpu2 -> comm_layer0
	route_layer0_gpu3 -> comm_layer0
	route_layer0_gpu4 -> comm_layer0
	route_layer0_gpu5 -> comm_layer0
	route_layer0_gpu6 -> comm_layer0
	route_layer0_gpu7 -> comm_layer0
	route_layer0_gpu8 -> comm_layer0
	route_layer0_gpu9 -> comm_layer0
	route_layer0_gpu10 -> comm_layer0
	route_layer0_gpu11 -> comm_layer0
	route_layer0_gpu12 -> comm_layer0
	route_layer0_gpu13 -> comm_layer0
	route_layer0_gpu14 -> comm_layer0
	route_layer0_gpu15 -> comm_layer0
	comm_layer0 -> expert_layer0_gpu0_expert0
	comm_layer0 -> expert_layer0_gpu1_expert1
	comm_layer0 -> expert_layer0_gpu2_expert2
	comm_layer0 -> expert_layer0_gpu3_expert3
	comm_layer0 -> expert_layer0_gpu4_expert4
	comm_layer0 -> expert_layer0_gpu5_expert5
	comm_layer0 -> expert_layer0_gpu6_expert6
	comm_layer0 -> expert_layer0_gpu7_expert7
	comm_layer0 -> expert_layer0_gpu8_expert8
	comm_layer0 -> expert_layer0_gpu9_expert9
	comm_layer0 -> expert_layer0_gpu10_expert10
	comm_layer0 -> expert_layer0_gpu11_expert11
	comm_layer0 -> expert_layer0_gpu12_expert12
	comm_layer0 -> expert_layer0_gpu13_expert13
	comm_layer0 -> expert_layer0_gpu14_expert14
	comm_layer0 -> expert_layer0_gpu15_expert15
	expert_layer0_gpu0_expert0 -> aggregate_layer0_gpu0
	expert_layer0_gpu1_expert1 -> aggregate_layer0_gpu1
	expert_layer0_gpu2_expert2 -> aggregate_layer0_gpu2
	expert_layer0_gpu3_expert3 -> aggregate_layer0_gpu3
	expert_layer0_gpu4_expert4 -> aggregate_layer0_gpu4
	expert_layer0_gpu5_expert5 -> aggregate_layer0_gpu5
	expert_layer0_gpu6_expert6 -> aggregate_layer0_gpu6
	expert_layer0_gpu7_expert7 -> aggregate_layer0_gpu7
	expert_layer0_gpu8_expert8 -> aggregate_layer0_gpu8
	expert_layer0_gpu9_expert9 -> aggregate_layer0_gpu9
	expert_layer0_gpu10_expert10 -> aggregate_layer0_gpu10
	expert_layer0_gpu11_expert11 -> aggregate_layer0_gpu11
	expert_layer0_gpu12_expert12 -> aggregate_layer0_gpu12
	expert_layer0_gpu13_expert13 -> aggregate_layer0_gpu13
	expert_layer0_gpu14_expert14 -> aggregate_layer0_gpu14
	expert_layer0_gpu15_expert15 -> aggregate_layer0_gpu15
	aggregate_layer0_gpu0 -> mha_layer1_gpu0
	aggregate_layer0_gpu1 -> mha_layer1_gpu1
	aggregate_layer0_gpu2 -> mha_layer1_gpu2
	aggregate_layer0_gpu3 -> mha_layer1_gpu3
	aggregate_layer0_gpu4 -> mha_layer1_gpu4
	aggregate_layer0_gpu5 -> mha_layer1_gpu5
	aggregate_layer0_gpu6 -> mha_layer1_gpu6
	aggregate_layer0_gpu7 -> mha_layer1_gpu7
	aggregate_layer0_gpu8 -> mha_layer1_gpu8
	aggregate_layer0_gpu9 -> mha_layer1_gpu9
	aggregate_layer0_gpu10 -> mha_layer1_gpu10
	aggregate_layer0_gpu11 -> mha_layer1_gpu11
	aggregate_layer0_gpu12 -> mha_layer1_gpu12
	aggregate_layer0_gpu13 -> mha_layer1_gpu13
	aggregate_layer0_gpu14 -> mha_layer1_gpu14
	aggregate_layer0_gpu15 -> mha_layer1_gpu15
	mha_layer1_gpu0 -> gate_layer1_gpu0
	mha_layer1_gpu1 -> gate_layer1_gpu1
	mha_layer1_gpu2 -> gate_layer1_gpu2
	mha_layer1_gpu3 -> gate_layer1_gpu3
	mha_layer1_gpu4 -> gate_layer1_gpu4
	mha_layer1_gpu5 -> gate_layer1_gpu5
	mha_layer1_gpu6 -> gate_layer1_gpu6
	mha_layer1_gpu7 -> gate_layer1_gpu7
	mha_layer1_gpu8 -> gate_layer1_gpu8
	mha_layer1_gpu9 -> gate_layer1_gpu9
	mha_layer1_gpu10 -> gate_layer1_gpu10
	mha_layer1_gpu11 -> gate_layer1_gpu11
	mha_layer1_gpu12 -> gate_layer1_gpu12
	mha_layer1_gpu13 -> gate_layer1_gpu13
	mha_layer1_gpu14 -> gate_layer1_gpu14
	mha_layer1_gpu15 -> gate_layer1_gpu15
	gate_layer1_gpu0 -> route_layer1_gpu0 [style=dashed]
	gate_layer1_gpu1 -> route_layer1_gpu1 [style=dashed]
	gate_layer1_gpu2 -> route_layer1_gpu2 [style=dashed]
	gate_layer1_gpu3 -> route_layer1_gpu3 [style=dashed]
	gate_layer1_gpu4 -> route_layer1_gpu4 [style=dashed]
	gate_layer1_gpu5 -> route_layer1_gpu5 [style=dashed]
	gate_layer1_gpu6 -> route_layer1_gpu6 [style=dashed]
	gate_layer1_gpu7 -> route_layer1_gpu7 [style=dashed]
	gate_layer1_gpu8 -> route_layer1_gpu8 [style=dashed]
	gate_layer1_gpu9 -> route_layer1_gpu9 [style=dashed]
	gate_layer1_gpu10 -> route_layer1_gpu10 [style=dashed]
	gate_layer1_gpu11 -> route_layer1_gpu11 [style=dashed]
	gate_layer1_gpu12 -> route_layer1_gpu12 [style=dashed]
	gate_layer1_gpu13 -> route_layer1_gpu13 [style=dashed]
	gate_layer1_gpu14 -> route_layer1_gpu14 [style=dashed]
	gate_layer1_gpu15 -> route_layer1_gpu15 [style=dashed]
	route_layer1_gpu0 -> comm_layer1
	route_layer1_gpu1 -> comm_layer1
	route_layer1_gpu2 -> comm_layer1
	route_layer1_gpu3 -> comm_layer1
	route_layer1_gpu4 -> comm_layer1
	route_layer1_gpu5 -> comm_layer1
	route_layer1_gpu6 -> comm_layer1
	route_layer1_gpu7 -> comm_layer1
	route_layer1_gpu8 -> comm_layer1
	route_layer1_gpu9 -> comm_layer1
	route_layer1_gpu10 -> comm_layer1
	route_layer1_gpu11 -> comm_layer1
	route_layer1_gpu12 -> comm_layer1
	route_layer1_gpu13 -> comm_layer1
	route_layer1_gpu14 -> comm_layer1
	route_layer1_gpu15 -> comm_layer1
	comm_layer1 -> expert_layer1_gpu0_expert0
	comm_layer1 -> expert_layer1_gpu1_expert1
	comm_layer1 -> expert_layer1_gpu2_expert2
	comm_layer1 -> expert_layer1_gpu3_expert3
	comm_layer1 -> expert_layer1_gpu4_expert4
	comm_layer1 -> expert_layer1_gpu5_expert5
	comm_layer1 -> expert_layer1_gpu6_expert6
	comm_layer1 -> expert_layer1_gpu7_expert7
	comm_layer1 -> expert_layer1_gpu8_expert8
	comm_layer1 -> expert_layer1_gpu9_expert9
	comm_layer1 -> expert_layer1_gpu10_expert10
	comm_layer1 -> expert_layer1_gpu11_expert11
	comm_layer1 -> expert_layer1_gpu12_expert12
	comm_layer1 -> expert_layer1_gpu13_expert13
	comm_layer1 -> expert_layer1_gpu14_expert14
	comm_layer1 -> expert_layer1_gpu15_expert15
	expert_layer1_gpu0_expert0 -> aggregate_layer1_gpu0
	expert_layer1_gpu1_expert1 -> aggregate_layer1_gpu1
	expert_layer1_gpu2_expert2 -> aggregate_layer1_gpu2
	expert_layer1_gpu3_expert3 -> aggregate_layer1_gpu3
	expert_layer1_gpu4_expert4 -> aggregate_layer1_gpu4
	expert_layer1_gpu5_expert5 -> aggregate_layer1_gpu5
	expert_layer1_gpu6_expert6 -> aggregate_layer1_gpu6
	expert_layer1_gpu7_expert7 -> aggregate_layer1_gpu7
	expert_layer1_gpu8_expert8 -> aggregate_layer1_gpu8
	expert_layer1_gpu9_expert9 -> aggregate_layer1_gpu9
	expert_layer1_gpu10_expert10 -> aggregate_layer1_gpu10
	expert_layer1_gpu11_expert11 -> aggregate_layer1_gpu11
	expert_layer1_gpu12_expert12 -> aggregate_layer1_gpu12
	expert_layer1_gpu13_expert13 -> aggregate_layer1_gpu13
	expert_layer1_gpu14_expert14 -> aggregate_layer1_gpu14
	expert_layer1_gpu15_expert15 -> aggregate_layer1_gpu15
	aggregate_layer1_gpu0 -> mha_layer2_gpu0
	aggregate_layer1_gpu1 -> mha_layer2_gpu1
	aggregate_layer1_gpu2 -> mha_layer2_gpu2
	aggregate_layer1_gpu3 -> mha_layer2_gpu3
	aggregate_layer1_gpu4 -> mha_layer2_gpu4
	aggregate_layer1_gpu5 -> mha_layer2_gpu5
	aggregate_layer1_gpu6 -> mha_layer2_gpu6
	aggregate_layer1_gpu7 -> mha_layer2_gpu7
	aggregate_layer1_gpu8 -> mha_layer2_gpu8
	aggregate_layer1_gpu9 -> mha_layer2_gpu9
	aggregate_layer1_gpu10 -> mha_layer2_gpu10
	aggregate_layer1_gpu11 -> mha_layer2_gpu11
	aggregate_layer1_gpu12 -> mha_layer2_gpu12
	aggregate_layer1_gpu13 -> mha_layer2_gpu13
	aggregate_layer1_gpu14 -> mha_layer2_gpu14
	aggregate_layer1_gpu15 -> mha_layer2_gpu15
	mha_layer2_gpu0 -> gate_layer2_gpu0
	mha_layer2_gpu1 -> gate_layer2_gpu1
	mha_layer2_gpu2 -> gate_layer2_gpu2
	mha_layer2_gpu3 -> gate_layer2_gpu3
	mha_layer2_gpu4 -> gate_layer2_gpu4
	mha_layer2_gpu5 -> gate_layer2_gpu5
	mha_layer2_gpu6 -> gate_layer2_gpu6
	mha_layer2_gpu7 -> gate_layer2_gpu7
	mha_layer2_gpu8 -> gate_layer2_gpu8
	mha_layer2_gpu9 -> gate_layer2_gpu9
	mha_layer2_gpu10 -> gate_layer2_gpu10
	mha_layer2_gpu11 -> gate_layer2_gpu11
	mha_layer2_gpu12 -> gate_layer2_gpu12
	mha_layer2_gpu13 -> gate_layer2_gpu13
	mha_layer2_gpu14 -> gate_layer2_gpu14
	mha_layer2_gpu15 -> gate_layer2_gpu15
	gate_layer2_gpu0 -> route_layer2_gpu0 [style=dashed]
	gate_layer2_gpu1 -> route_layer2_gpu1 [style=dashed]
	gate_layer2_gpu2 -> route_layer2_gpu2 [style=dashed]
	gate_layer2_gpu3 -> route_layer2_gpu3 [style=dashed]
	gate_layer2_gpu4 -> route_layer2_gpu4 [style=dashed]
	gate_layer2_gpu5 -> route_layer2_gpu5 [style=dashed]
	gate_layer2_gpu6 -> route_layer2_gpu6 [style=dashed]
	gate_layer2_gpu7 -> route_layer2_gpu7 [style=dashed]
	gate_layer2_gpu8 -> route_layer2_gpu8 [style=dashed]
	gate_layer2_gpu9 -> route_layer2_gpu9 [style=dashed]
	gate_layer2_gpu10 -> route_layer2_gpu10 [style=dashed]
	gate_layer2_gpu11 -> route_layer2_gpu11 [style=dashed]
	gate_layer2_gpu12 -> route_layer2_gpu12 [style=dashed]
	gate_layer2_gpu13 -> route_layer2_gpu13 [style=dashed]
	gate_layer2_gpu14 -> route_layer2_gpu14 [style=dashed]
	gate_layer2_gpu15 -> route_layer2_gpu15 [style=dashed]
	route_layer2_gpu0 -> comm_layer2
	route_layer2_gpu1 -> comm_layer2
	route_layer2_gpu2 -> comm_layer2
	route_layer2_gpu3 -> comm_layer2
	route_layer2_gpu4 -> comm_layer2
	route_layer2_gpu5 -> comm_layer2
	route_layer2_gpu6 -> comm_layer2
	route_layer2_gpu7 -> comm_layer2
	route_layer2_gpu8 -> comm_layer2
	route_layer2_gpu9 -> comm_layer2
	route_layer2_gpu10 -> comm_layer2
	route_layer2_gpu11 -> comm_layer2
	route_layer2_gpu12 -> comm_layer2
	route_layer2_gpu13 -> comm_layer2
	route_layer2_gpu14 -> comm_layer2
	route_layer2_gpu15 -> comm_layer2
	comm_layer2 -> expert_layer2_gpu0_expert0
	comm_layer2 -> expert_layer2_gpu1_expert1
	comm_layer2 -> expert_layer2_gpu2_expert2
	comm_layer2 -> expert_layer2_gpu3_expert3
	comm_layer2 -> expert_layer2_gpu4_expert4
	comm_layer2 -> expert_layer2_gpu5_expert5
	comm_layer2 -> expert_layer2_gpu6_expert6
	comm_layer2 -> expert_layer2_gpu7_expert7
	comm_layer2 -> expert_layer2_gpu8_expert8
	comm_layer2 -> expert_layer2_gpu9_expert9
	comm_layer2 -> expert_layer2_gpu10_expert10
	comm_layer2 -> expert_layer2_gpu11_expert11
	comm_layer2 -> expert_layer2_gpu12_expert12
	comm_layer2 -> expert_layer2_gpu13_expert13
	comm_layer2 -> expert_layer2_gpu14_expert14
	comm_layer2 -> expert_layer2_gpu15_expert15
	expert_layer2_gpu0_expert0 -> aggregate_layer2_gpu0
	expert_layer2_gpu1_expert1 -> aggregate_layer2_gpu1
	expert_layer2_gpu2_expert2 -> aggregate_layer2_gpu2
	expert_layer2_gpu3_expert3 -> aggregate_layer2_gpu3
	expert_layer2_gpu4_expert4 -> aggregate_layer2_gpu4
	expert_layer2_gpu5_expert5 -> aggregate_layer2_gpu5
	expert_layer2_gpu6_expert6 -> aggregate_layer2_gpu6
	expert_layer2_gpu7_expert7 -> aggregate_layer2_gpu7
	expert_layer2_gpu8_expert8 -> aggregate_layer2_gpu8
	expert_layer2_gpu9_expert9 -> aggregate_layer2_gpu9
	expert_layer2_gpu10_expert10 -> aggregate_layer2_gpu10
	expert_layer2_gpu11_expert11 -> aggregate_layer2_gpu11
	expert_layer2_gpu12_expert12 -> aggregate_layer2_gpu12
	expert_layer2_gpu13_expert13 -> aggregate_layer2_gpu13
	expert_layer2_gpu14_expert14 -> aggregate_layer2_gpu14
	expert_layer2_gpu15_expert15 -> aggregate_layer2_gpu15
	aggregate_layer2_gpu0 -> mha_layer3_gpu0
	aggregate_layer2_gpu1 -> mha_layer3_gpu1
	aggregate_layer2_gpu2 -> mha_layer3_gpu2
	aggregate_layer2_gpu3 -> mha_layer3_gpu3
	aggregate_layer2_gpu4 -> mha_layer3_gpu4
	aggregate_layer2_gpu5 -> mha_layer3_gpu5
	aggregate_layer2_gpu6 -> mha_layer3_gpu6
	aggregate_layer2_gpu7 -> mha_layer3_gpu7
	aggregate_layer2_gpu8 -> mha_layer3_gpu8
	aggregate_layer2_gpu9 -> mha_layer3_gpu9
	aggregate_layer2_gpu10 -> mha_layer3_gpu10
	aggregate_layer2_gpu11 -> mha_layer3_gpu11
	aggregate_layer2_gpu12 -> mha_layer3_gpu12
	aggregate_layer2_gpu13 -> mha_layer3_gpu13
	aggregate_layer2_gpu14 -> mha_layer3_gpu14
	aggregate_layer2_gpu15 -> mha_layer3_gpu15
	mha_layer3_gpu0 -> gate_layer3_gpu0
	mha_layer3_gpu1 -> gate_layer3_gpu1
	mha_layer3_gpu2 -> gate_layer3_gpu2
	mha_layer3_gpu3 -> gate_layer3_gpu3
	mha_layer3_gpu4 -> gate_layer3_gpu4
	mha_layer3_gpu5 -> gate_layer3_gpu5
	mha_layer3_gpu6 -> gate_layer3_gpu6
	mha_layer3_gpu7 -> gate_layer3_gpu7
	mha_layer3_gpu8 -> gate_layer3_gpu8
	mha_layer3_gpu9 -> gate_layer3_gpu9
	mha_layer3_gpu10 -> gate_layer3_gpu10
	mha_layer3_gpu11 -> gate_layer3_gpu11
	mha_layer3_gpu12 -> gate_layer3_gpu12
	mha_layer3_gpu13 -> gate_layer3_gpu13
	mha_layer3_gpu14 -> gate_layer3_gpu14
	mha_layer3_gpu15 -> gate_layer3_gpu15
	gate_layer3_gpu0 -> route_layer3_gpu0 [style=dashed]
	gate_layer3_gpu1 -> route_layer3_gpu1 [style=dashed]
	gate_layer3_gpu2 -> route_layer3_gpu2 [style=dashed]
	gate_layer3_gpu3 -> route_layer3_gpu3 [style=dashed]
	gate_layer3_gpu4 -> route_layer3_gpu4 [style=dashed]
	gate_layer3_gpu5 -> route_layer3_gpu5 [style=dashed]
	gate_layer3_gpu6 -> route_layer3_gpu6 [style=dashed]
	gate_layer3_gpu7 -> route_layer3_gpu7 [style=dashed]
	gate_layer3_gpu8 -> route_layer3_gpu8 [style=dashed]
	gate_layer3_gpu9 -> route_layer3_gpu9 [style=dashed]
	gate_layer3_gpu10 -> route_layer3_gpu10 [style=dashed]
	gate_layer3_gpu11 -> route_layer3_gpu11 [style=dashed]
	gate_layer3_gpu12 -> route_layer3_gpu12 [style=dashed]
	gate_layer3_gpu13 -> route_layer3_gpu13 [style=dashed]
	gate_layer3_gpu14 -> route_layer3_gpu14 [style=dashed]
	gate_layer3_gpu15 -> route_layer3_gpu15 [style=dashed]
	route_layer3_gpu0 -> comm_layer3
	route_layer3_gpu1 -> comm_layer3
	route_layer3_gpu2 -> comm_layer3
	route_layer3_gpu3 -> comm_layer3
	route_layer3_gpu4 -> comm_layer3
	route_layer3_gpu5 -> comm_layer3
	route_layer3_gpu6 -> comm_layer3
	route_layer3_gpu7 -> comm_layer3
	route_layer3_gpu8 -> comm_layer3
	route_layer3_gpu9 -> comm_layer3
	route_layer3_gpu10 -> comm_layer3
	route_layer3_gpu11 -> comm_layer3
	route_layer3_gpu12 -> comm_layer3
	route_layer3_gpu13 -> comm_layer3
	route_layer3_gpu14 -> comm_layer3
	route_layer3_gpu15 -> comm_layer3
	comm_layer3 -> expert_layer3_gpu0_expert0
	comm_layer3 -> expert_layer3_gpu1_expert1
	comm_layer3 -> expert_layer3_gpu2_expert2
	comm_layer3 -> expert_layer3_gpu3_expert3
	comm_layer3 -> expert_layer3_gpu4_expert4
	comm_layer3 -> expert_layer3_gpu5_expert5
	comm_layer3 -> expert_layer3_gpu6_expert6
	comm_layer3 -> expert_layer3_gpu7_expert7
	comm_layer3 -> expert_layer3_gpu8_expert8
	comm_layer3 -> expert_layer3_gpu9_expert9
	comm_layer3 -> expert_layer3_gpu10_expert10
	comm_layer3 -> expert_layer3_gpu11_expert11
	comm_layer3 -> expert_layer3_gpu12_expert12
	comm_layer3 -> expert_layer3_gpu13_expert13
	comm_layer3 -> expert_layer3_gpu14_expert14
	comm_layer3 -> expert_layer3_gpu15_expert15
	expert_layer3_gpu0_expert0 -> aggregate_layer3_gpu0
	expert_layer3_gpu1_expert1 -> aggregate_layer3_gpu1
	expert_layer3_gpu2_expert2 -> aggregate_layer3_gpu2
	expert_layer3_gpu3_expert3 -> aggregate_layer3_gpu3
	expert_layer3_gpu4_expert4 -> aggregate_layer3_gpu4
	expert_layer3_gpu5_expert5 -> aggregate_layer3_gpu5
	expert_layer3_gpu6_expert6 -> aggregate_layer3_gpu6
	expert_layer3_gpu7_expert7 -> aggregate_layer3_gpu7
	expert_layer3_gpu8_expert8 -> aggregate_layer3_gpu8
	expert_layer3_gpu9_expert9 -> aggregate_layer3_gpu9
	expert_layer3_gpu10_expert10 -> aggregate_layer3_gpu10
	expert_layer3_gpu11_expert11 -> aggregate_layer3_gpu11
	expert_layer3_gpu12_expert12 -> aggregate_layer3_gpu12
	expert_layer3_gpu13_expert13 -> aggregate_layer3_gpu13
	expert_layer3_gpu14_expert14 -> aggregate_layer3_gpu14
	expert_layer3_gpu15_expert15 -> aggregate_layer3_gpu15
	aggregate_layer3_gpu0 -> mha_layer4_gpu0
	aggregate_layer3_gpu1 -> mha_layer4_gpu1
	aggregate_layer3_gpu2 -> mha_layer4_gpu2
	aggregate_layer3_gpu3 -> mha_layer4_gpu3
	aggregate_layer3_gpu4 -> mha_layer4_gpu4
	aggregate_layer3_gpu5 -> mha_layer4_gpu5
	aggregate_layer3_gpu6 -> mha_layer4_gpu6
	aggregate_layer3_gpu7 -> mha_layer4_gpu7
	aggregate_layer3_gpu8 -> mha_layer4_gpu8
	aggregate_layer3_gpu9 -> mha_layer4_gpu9
	aggregate_layer3_gpu10 -> mha_layer4_gpu10
	aggregate_layer3_gpu11 -> mha_layer4_gpu11
	aggregate_layer3_gpu12 -> mha_layer4_gpu12
	aggregate_layer3_gpu13 -> mha_layer4_gpu13
	aggregate_layer3_gpu14 -> mha_layer4_gpu14
	aggregate_layer3_gpu15 -> mha_layer4_gpu15
	mha_layer4_gpu0 -> gate_layer4_gpu0
	mha_layer4_gpu1 -> gate_layer4_gpu1
	mha_layer4_gpu2 -> gate_layer4_gpu2
	mha_layer4_gpu3 -> gate_layer4_gpu3
	mha_layer4_gpu4 -> gate_layer4_gpu4
	mha_layer4_gpu5 -> gate_layer4_gpu5
	mha_layer4_gpu6 -> gate_layer4_gpu6
	mha_layer4_gpu7 -> gate_layer4_gpu7
	mha_layer4_gpu8 -> gate_layer4_gpu8
	mha_layer4_gpu9 -> gate_layer4_gpu9
	mha_layer4_gpu10 -> gate_layer4_gpu10
	mha_layer4_gpu11 -> gate_layer4_gpu11
	mha_layer4_gpu12 -> gate_layer4_gpu12
	mha_layer4_gpu13 -> gate_layer4_gpu13
	mha_layer4_gpu14 -> gate_layer4_gpu14
	mha_layer4_gpu15 -> gate_layer4_gpu15
	gate_layer4_gpu0 -> route_layer4_gpu0 [style=dashed]
	gate_layer4_gpu1 -> route_layer4_gpu1 [style=dashed]
	gate_layer4_gpu2 -> route_layer4_gpu2 [style=dashed]
	gate_layer4_gpu3 -> route_layer4_gpu3 [style=dashed]
	gate_layer4_gpu4 -> route_layer4_gpu4 [style=dashed]
	gate_layer4_gpu5 -> route_layer4_gpu5 [style=dashed]
	gate_layer4_gpu6 -> route_layer4_gpu6 [style=dashed]
	gate_layer4_gpu7 -> route_layer4_gpu7 [style=dashed]
	gate_layer4_gpu8 -> route_layer4_gpu8 [style=dashed]
	gate_layer4_gpu9 -> route_layer4_gpu9 [style=dashed]
	gate_layer4_gpu10 -> route_layer4_gpu10 [style=dashed]
	gate_layer4_gpu11 -> route_layer4_gpu11 [style=dashed]
	gate_layer4_gpu12 -> route_layer4_gpu12 [style=dashed]
	gate_layer4_gpu13 -> route_layer4_gpu13 [style=dashed]
	gate_layer4_gpu14 -> route_layer4_gpu14 [style=dashed]
	gate_layer4_gpu15 -> route_layer4_gpu15 [style=dashed]
	route_layer4_gpu0 -> comm_layer4
	route_layer4_gpu1 -> comm_layer4
	route_layer4_gpu2 -> comm_layer4
	route_layer4_gpu3 -> comm_layer4
	route_layer4_gpu4 -> comm_layer4
	route_layer4_gpu5 -> comm_layer4
	route_layer4_gpu6 -> comm_layer4
	route_layer4_gpu7 -> comm_layer4
	route_layer4_gpu8 -> comm_layer4
	route_layer4_gpu9 -> comm_layer4
	route_layer4_gpu10 -> comm_layer4
	route_layer4_gpu11 -> comm_layer4
	route_layer4_gpu12 -> comm_layer4
	route_layer4_gpu13 -> comm_layer4
	route_layer4_gpu14 -> comm_layer4
	route_layer4_gpu15 -> comm_layer4
	comm_layer4 -> expert_layer4_gpu0_expert0
	comm_layer4 -> expert_layer4_gpu1_expert1
	comm_layer4 -> expert_layer4_gpu2_expert2
	comm_layer4 -> expert_layer4_gpu3_expert3
	comm_layer4 -> expert_layer4_gpu4_expert4
	comm_layer4 -> expert_layer4_gpu5_expert5
	comm_layer4 -> expert_layer4_gpu6_expert6
	comm_layer4 -> expert_layer4_gpu7_expert7
	comm_layer4 -> expert_layer4_gpu8_expert8
	comm_layer4 -> expert_layer4_gpu9_expert9
	comm_layer4 -> expert_layer4_gpu10_expert10
	comm_layer4 -> expert_layer4_gpu11_expert11
	comm_layer4 -> expert_layer4_gpu12_expert12
	comm_layer4 -> expert_layer4_gpu13_expert13
	comm_layer4 -> expert_layer4_gpu14_expert14
	comm_layer4 -> expert_layer4_gpu15_expert15
	expert_layer4_gpu0_expert0 -> aggregate_layer4_gpu0
	expert_layer4_gpu1_expert1 -> aggregate_layer4_gpu1
	expert_layer4_gpu2_expert2 -> aggregate_layer4_gpu2
	expert_layer4_gpu3_expert3 -> aggregate_layer4_gpu3
	expert_layer4_gpu4_expert4 -> aggregate_layer4_gpu4
	expert_layer4_gpu5_expert5 -> aggregate_layer4_gpu5
	expert_layer4_gpu6_expert6 -> aggregate_layer4_gpu6
	expert_layer4_gpu7_expert7 -> aggregate_layer4_gpu7
	expert_layer4_gpu8_expert8 -> aggregate_layer4_gpu8
	expert_layer4_gpu9_expert9 -> aggregate_layer4_gpu9
	expert_layer4_gpu10_expert10 -> aggregate_layer4_gpu10
	expert_layer4_gpu11_expert11 -> aggregate_layer4_gpu11
	expert_layer4_gpu12_expert12 -> aggregate_layer4_gpu12
	expert_layer4_gpu13_expert13 -> aggregate_layer4_gpu13
	expert_layer4_gpu14_expert14 -> aggregate_layer4_gpu14
	expert_layer4_gpu15_expert15 -> aggregate_layer4_gpu15
	aggregate_layer4_gpu0 -> mha_layer5_gpu0
	aggregate_layer4_gpu1 -> mha_layer5_gpu1
	aggregate_layer4_gpu2 -> mha_layer5_gpu2
	aggregate_layer4_gpu3 -> mha_layer5_gpu3
	aggregate_layer4_gpu4 -> mha_layer5_gpu4
	aggregate_layer4_gpu5 -> mha_layer5_gpu5
	aggregate_layer4_gpu6 -> mha_layer5_gpu6
	aggregate_layer4_gpu7 -> mha_layer5_gpu7
	aggregate_layer4_gpu8 -> mha_layer5_gpu8
	aggregate_layer4_gpu9 -> mha_layer5_gpu9
	aggregate_layer4_gpu10 -> mha_layer5_gpu10
	aggregate_layer4_gpu11 -> mha_layer5_gpu11
	aggregate_layer4_gpu12 -> mha_layer5_gpu12
	aggregate_layer4_gpu13 -> mha_layer5_gpu13
	aggregate_layer4_gpu14 -> mha_layer5_gpu14
	aggregate_layer4_gpu15 -> mha_layer5_gpu15
	mha_layer5_gpu0 -> gate_layer5_gpu0
	mha_layer5_gpu1 -> gate_layer5_gpu1
	mha_layer5_gpu2 -> gate_layer5_gpu2
	mha_layer5_gpu3 -> gate_layer5_gpu3
	mha_layer5_gpu4 -> gate_layer5_gpu4
	mha_layer5_gpu5 -> gate_layer5_gpu5
	mha_layer5_gpu6 -> gate_layer5_gpu6
	mha_layer5_gpu7 -> gate_layer5_gpu7
	mha_layer5_gpu8 -> gate_layer5_gpu8
	mha_layer5_gpu9 -> gate_layer5_gpu9
	mha_layer5_gpu10 -> gate_layer5_gpu10
	mha_layer5_gpu11 -> gate_layer5_gpu11
	mha_layer5_gpu12 -> gate_layer5_gpu12
	mha_layer5_gpu13 -> gate_layer5_gpu13
	mha_layer5_gpu14 -> gate_layer5_gpu14
	mha_layer5_gpu15 -> gate_layer5_gpu15
	gate_layer5_gpu0 -> route_layer5_gpu0 [style=dashed]
	gate_layer5_gpu1 -> route_layer5_gpu1 [style=dashed]
	gate_layer5_gpu2 -> route_layer5_gpu2 [style=dashed]
	gate_layer5_gpu3 -> route_layer5_gpu3 [style=dashed]
	gate_layer5_gpu4 -> route_layer5_gpu4 [style=dashed]
	gate_layer5_gpu5 -> route_layer5_gpu5 [style=dashed]
	gate_layer5_gpu6 -> route_layer5_gpu6 [style=dashed]
	gate_layer5_gpu7 -> route_layer5_gpu7 [style=dashed]
	gate_layer5_gpu8 -> route_layer5_gpu8 [style=dashed]
	gate_layer5_gpu9 -> route_layer5_gpu9 [style=dashed]
	gate_layer5_gpu10 -> route_layer5_gpu10 [style=dashed]
	gate_layer5_gpu11 -> route_layer5_gpu11 [style=dashed]
	gate_layer5_gpu12 -> route_layer5_gpu12 [style=dashed]
	gate_layer5_gpu13 -> route_layer5_gpu13 [style=dashed]
	gate_layer5_gpu14 -> route_layer5_gpu14 [style=dashed]
	gate_layer5_gpu15 -> route_layer5_gpu15 [style=dashed]
	route_layer5_gpu0 -> comm_layer5
	route_layer5_gpu1 -> comm_layer5
	route_layer5_gpu2 -> comm_layer5
	route_layer5_gpu3 -> comm_layer5
	route_layer5_gpu4 -> comm_layer5
	route_layer5_gpu5 -> comm_layer5
	route_layer5_gpu6 -> comm_layer5
	route_layer5_gpu7 -> comm_layer5
	route_layer5_gpu8 -> comm_layer5
	route_layer5_gpu9 -> comm_layer5
	route_layer5_gpu10 -> comm_layer5
	route_layer5_gpu11 -> comm_layer5
	route_layer5_gpu12 -> comm_layer5
	route_layer5_gpu13 -> comm_layer5
	route_layer5_gpu14 -> comm_layer5
	route_layer5_gpu15 -> comm_layer5
	comm_layer5 -> expert_layer5_gpu0_expert0
	comm_layer5 -> expert_layer5_gpu1_expert1
	comm_layer5 -> expert_layer5_gpu2_expert2
	comm_layer5 -> expert_layer5_gpu3_expert3
	comm_layer5 -> expert_layer5_gpu4_expert4
	comm_layer5 -> expert_layer5_gpu5_expert5
	comm_layer5 -> expert_layer5_gpu6_expert6
	comm_layer5 -> expert_layer5_gpu7_expert7
	comm_layer5 -> expert_layer5_gpu8_expert8
	comm_layer5 -> expert_layer5_gpu9_expert9
	comm_layer5 -> expert_layer5_gpu10_expert10
	comm_layer5 -> expert_layer5_gpu11_expert11
	comm_layer5 -> expert_layer5_gpu12_expert12
	comm_layer5 -> expert_layer5_gpu13_expert13
	comm_layer5 -> expert_layer5_gpu14_expert14
	comm_layer5 -> expert_layer5_gpu15_expert15
	expert_layer5_gpu0_expert0 -> aggregate_layer5_gpu0
	expert_layer5_gpu1_expert1 -> aggregate_layer5_gpu1
	expert_layer5_gpu2_expert2 -> aggregate_layer5_gpu2
	expert_layer5_gpu3_expert3 -> aggregate_layer5_gpu3
	expert_layer5_gpu4_expert4 -> aggregate_layer5_gpu4
	expert_layer5_gpu5_expert5 -> aggregate_layer5_gpu5
	expert_layer5_gpu6_expert6 -> aggregate_layer5_gpu6
	expert_layer5_gpu7_expert7 -> aggregate_layer5_gpu7
	expert_layer5_gpu8_expert8 -> aggregate_layer5_gpu8
	expert_layer5_gpu9_expert9 -> aggregate_layer5_gpu9
	expert_layer5_gpu10_expert10 -> aggregate_layer5_gpu10
	expert_layer5_gpu11_expert11 -> aggregate_layer5_gpu11
	expert_layer5_gpu12_expert12 -> aggregate_layer5_gpu12
	expert_layer5_gpu13_expert13 -> aggregate_layer5_gpu13
	expert_layer5_gpu14_expert14 -> aggregate_layer5_gpu14
	expert_layer5_gpu15_expert15 -> aggregate_layer5_gpu15
	aggregate_layer5_gpu0 -> mha_layer6_gpu0
	aggregate_layer5_gpu1 -> mha_layer6_gpu1
	aggregate_layer5_gpu2 -> mha_layer6_gpu2
	aggregate_layer5_gpu3 -> mha_layer6_gpu3
	aggregate_layer5_gpu4 -> mha_layer6_gpu4
	aggregate_layer5_gpu5 -> mha_layer6_gpu5
	aggregate_layer5_gpu6 -> mha_layer6_gpu6
	aggregate_layer5_gpu7 -> mha_layer6_gpu7
	aggregate_layer5_gpu8 -> mha_layer6_gpu8
	aggregate_layer5_gpu9 -> mha_layer6_gpu9
	aggregate_layer5_gpu10 -> mha_layer6_gpu10
	aggregate_layer5_gpu11 -> mha_layer6_gpu11
	aggregate_layer5_gpu12 -> mha_layer6_gpu12
	aggregate_layer5_gpu13 -> mha_layer6_gpu13
	aggregate_layer5_gpu14 -> mha_layer6_gpu14
	aggregate_layer5_gpu15 -> mha_layer6_gpu15
	mha_layer6_gpu0 -> gate_layer6_gpu0
	mha_layer6_gpu1 -> gate_layer6_gpu1
	mha_layer6_gpu2 -> gate_layer6_gpu2
	mha_layer6_gpu3 -> gate_layer6_gpu3
	mha_layer6_gpu4 -> gate_layer6_gpu4
	mha_layer6_gpu5 -> gate_layer6_gpu5
	mha_layer6_gpu6 -> gate_layer6_gpu6
	mha_layer6_gpu7 -> gate_layer6_gpu7
	mha_layer6_gpu8 -> gate_layer6_gpu8
	mha_layer6_gpu9 -> gate_layer6_gpu9
	mha_layer6_gpu10 -> gate_layer6_gpu10
	mha_layer6_gpu11 -> gate_layer6_gpu11
	mha_layer6_gpu12 -> gate_layer6_gpu12
	mha_layer6_gpu13 -> gate_layer6_gpu13
	mha_layer6_gpu14 -> gate_layer6_gpu14
	mha_layer6_gpu15 -> gate_layer6_gpu15
	gate_layer6_gpu0 -> route_layer6_gpu0 [style=dashed]
	gate_layer6_gpu1 -> route_layer6_gpu1 [style=dashed]
	gate_layer6_gpu2 -> route_layer6_gpu2 [style=dashed]
	gate_layer6_gpu3 -> route_layer6_gpu3 [style=dashed]
	gate_layer6_gpu4 -> route_layer6_gpu4 [style=dashed]
	gate_layer6_gpu5 -> route_layer6_gpu5 [style=dashed]
	gate_layer6_gpu6 -> route_layer6_gpu6 [style=dashed]
	gate_layer6_gpu7 -> route_layer6_gpu7 [style=dashed]
	gate_layer6_gpu8 -> route_layer6_gpu8 [style=dashed]
	gate_layer6_gpu9 -> route_layer6_gpu9 [style=dashed]
	gate_layer6_gpu10 -> route_layer6_gpu10 [style=dashed]
	gate_layer6_gpu11 -> route_layer6_gpu11 [style=dashed]
	gate_layer6_gpu12 -> route_layer6_gpu12 [style=dashed]
	gate_layer6_gpu13 -> route_layer6_gpu13 [style=dashed]
	gate_layer6_gpu14 -> route_layer6_gpu14 [style=dashed]
	gate_layer6_gpu15 -> route_layer6_gpu15 [style=dashed]
	route_layer6_gpu0 -> comm_layer6
	route_layer6_gpu1 -> comm_layer6
	route_layer6_gpu2 -> comm_layer6
	route_layer6_gpu3 -> comm_layer6
	route_layer6_gpu4 -> comm_layer6
	route_layer6_gpu5 -> comm_layer6
	route_layer6_gpu6 -> comm_layer6
	route_layer6_gpu7 -> comm_layer6
	route_layer6_gpu8 -> comm_layer6
	route_layer6_gpu9 -> comm_layer6
	route_layer6_gpu10 -> comm_layer6
	route_layer6_gpu11 -> comm_layer6
	route_layer6_gpu12 -> comm_layer6
	route_layer6_gpu13 -> comm_layer6
	route_layer6_gpu14 -> comm_layer6
	route_layer6_gpu15 -> comm_layer6
	comm_layer6 -> expert_layer6_gpu0_expert0
	comm_layer6 -> expert_layer6_gpu1_expert1
	comm_layer6 -> expert_layer6_gpu2_expert2
	comm_layer6 -> expert_layer6_gpu3_expert3
	comm_layer6 -> expert_layer6_gpu4_expert4
	comm_layer6 -> expert_layer6_gpu5_expert5
	comm_layer6 -> expert_layer6_gpu6_expert6
	comm_layer6 -> expert_layer6_gpu7_expert7
	comm_layer6 -> expert_layer6_gpu8_expert8
	comm_layer6 -> expert_layer6_gpu9_expert9
	comm_layer6 -> expert_layer6_gpu10_expert10
	comm_layer6 -> expert_layer6_gpu11_expert11
	comm_layer6 -> expert_layer6_gpu12_expert12
	comm_layer6 -> expert_layer6_gpu13_expert13
	comm_layer6 -> expert_layer6_gpu14_expert14
	comm_layer6 -> expert_layer6_gpu15_expert15
	expert_layer6_gpu0_expert0 -> aggregate_layer6_gpu0
	expert_layer6_gpu1_expert1 -> aggregate_layer6_gpu1
	expert_layer6_gpu2_expert2 -> aggregate_layer6_gpu2
	expert_layer6_gpu3_expert3 -> aggregate_layer6_gpu3
	expert_layer6_gpu4_expert4 -> aggregate_layer6_gpu4
	expert_layer6_gpu5_expert5 -> aggregate_layer6_gpu5
	expert_layer6_gpu6_expert6 -> aggregate_layer6_gpu6
	expert_layer6_gpu7_expert7 -> aggregate_layer6_gpu7
	expert_layer6_gpu8_expert8 -> aggregate_layer6_gpu8
	expert_layer6_gpu9_expert9 -> aggregate_layer6_gpu9
	expert_layer6_gpu10_expert10 -> aggregate_layer6_gpu10
	expert_layer6_gpu11_expert11 -> aggregate_layer6_gpu11
	expert_layer6_gpu12_expert12 -> aggregate_layer6_gpu12
	expert_layer6_gpu13_expert13 -> aggregate_layer6_gpu13
	expert_layer6_gpu14_expert14 -> aggregate_layer6_gpu14
	expert_layer6_gpu15_expert15 -> aggregate_layer6_gpu15
	aggregate_layer6_gpu0 -> mha_layer7_gpu0
	aggregate_layer6_gpu1 -> mha_layer7_gpu1
	aggregate_layer6_gpu2 -> mha_layer7_gpu2
	aggregate_layer6_gpu3 -> mha_layer7_gpu3
	aggregate_layer6_gpu4 -> mha_layer7_gpu4
	aggregate_layer6_gpu5 -> mha_layer7_gpu5
	aggregate_layer6_gpu6 -> mha_layer7_gpu6
	aggregate_layer6_gpu7 -> mha_layer7_gpu7
	aggregate_layer6_gpu8 -> mha_layer7_gpu8
	aggregate_layer6_gpu9 -> mha_layer7_gpu9
	aggregate_layer6_gpu10 -> mha_layer7_gpu10
	aggregate_layer6_gpu11 -> mha_layer7_gpu11
	aggregate_layer6_gpu12 -> mha_layer7_gpu12
	aggregate_layer6_gpu13 -> mha_layer7_gpu13
	aggregate_layer6_gpu14 -> mha_layer7_gpu14
	aggregate_layer6_gpu15 -> mha_layer7_gpu15
	mha_layer7_gpu0 -> gate_layer7_gpu0
	mha_layer7_gpu1 -> gate_layer7_gpu1
	mha_layer7_gpu2 -> gate_layer7_gpu2
	mha_layer7_gpu3 -> gate_layer7_gpu3
	mha_layer7_gpu4 -> gate_layer7_gpu4
	mha_layer7_gpu5 -> gate_layer7_gpu5
	mha_layer7_gpu6 -> gate_layer7_gpu6
	mha_layer7_gpu7 -> gate_layer7_gpu7
	mha_layer7_gpu8 -> gate_layer7_gpu8
	mha_layer7_gpu9 -> gate_layer7_gpu9
	mha_layer7_gpu10 -> gate_layer7_gpu10
	mha_layer7_gpu11 -> gate_layer7_gpu11
	mha_layer7_gpu12 -> gate_layer7_gpu12
	mha_layer7_gpu13 -> gate_layer7_gpu13
	mha_layer7_gpu14 -> gate_layer7_gpu14
	mha_layer7_gpu15 -> gate_layer7_gpu15
	gate_layer7_gpu0 -> route_layer7_gpu0 [style=dashed]
	gate_layer7_gpu1 -> route_layer7_gpu1 [style=dashed]
	gate_layer7_gpu2 -> route_layer7_gpu2 [style=dashed]
	gate_layer7_gpu3 -> route_layer7_gpu3 [style=dashed]
	gate_layer7_gpu4 -> route_layer7_gpu4 [style=dashed]
	gate_layer7_gpu5 -> route_layer7_gpu5 [style=dashed]
	gate_layer7_gpu6 -> route_layer7_gpu6 [style=dashed]
	gate_layer7_gpu7 -> route_layer7_gpu7 [style=dashed]
	gate_layer7_gpu8 -> route_layer7_gpu8 [style=dashed]
	gate_layer7_gpu9 -> route_layer7_gpu9 [style=dashed]
	gate_layer7_gpu10 -> route_layer7_gpu10 [style=dashed]
	gate_layer7_gpu11 -> route_layer7_gpu11 [style=dashed]
	gate_layer7_gpu12 -> route_layer7_gpu12 [style=dashed]
	gate_layer7_gpu13 -> route_layer7_gpu13 [style=dashed]
	gate_layer7_gpu14 -> route_layer7_gpu14 [style=dashed]
	gate_layer7_gpu15 -> route_layer7_gpu15 [style=dashed]
	route_layer7_gpu0 -> comm_layer7
	route_layer7_gpu1 -> comm_layer7
	route_layer7_gpu2 -> comm_layer7
	route_layer7_gpu3 -> comm_layer7
	route_layer7_gpu4 -> comm_layer7
	route_layer7_gpu5 -> comm_layer7
	route_layer7_gpu6 -> comm_layer7
	route_layer7_gpu7 -> comm_layer7
	route_layer7_gpu8 -> comm_layer7
	route_layer7_gpu9 -> comm_layer7
	route_layer7_gpu10 -> comm_layer7
	route_layer7_gpu11 -> comm_layer7
	route_layer7_gpu12 -> comm_layer7
	route_layer7_gpu13 -> comm_layer7
	route_layer7_gpu14 -> comm_layer7
	route_layer7_gpu15 -> comm_layer7
	comm_layer7 -> expert_layer7_gpu0_expert0
	comm_layer7 -> expert_layer7_gpu1_expert1
	comm_layer7 -> expert_layer7_gpu2_expert2
	comm_layer7 -> expert_layer7_gpu3_expert3
	comm_layer7 -> expert_layer7_gpu4_expert4
	comm_layer7 -> expert_layer7_gpu5_expert5
	comm_layer7 -> expert_layer7_gpu6_expert6
	comm_layer7 -> expert_layer7_gpu7_expert7
	comm_layer7 -> expert_layer7_gpu8_expert8
	comm_layer7 -> expert_layer7_gpu9_expert9
	comm_layer7 -> expert_layer7_gpu10_expert10
	comm_layer7 -> expert_layer7_gpu11_expert11
	comm_layer7 -> expert_layer7_gpu12_expert12
	comm_layer7 -> expert_layer7_gpu13_expert13
	comm_layer7 -> expert_layer7_gpu14_expert14
	comm_layer7 -> expert_layer7_gpu15_expert15
	expert_layer7_gpu0_expert0 -> aggregate_layer7_gpu0
	expert_layer7_gpu1_expert1 -> aggregate_layer7_gpu1
	expert_layer7_gpu2_expert2 -> aggregate_layer7_gpu2
	expert_layer7_gpu3_expert3 -> aggregate_layer7_gpu3
	expert_layer7_gpu4_expert4 -> aggregate_layer7_gpu4
	expert_layer7_gpu5_expert5 -> aggregate_layer7_gpu5
	expert_layer7_gpu6_expert6 -> aggregate_layer7_gpu6
	expert_layer7_gpu7_expert7 -> aggregate_layer7_gpu7
	expert_layer7_gpu8_expert8 -> aggregate_layer7_gpu8
	expert_layer7_gpu9_expert9 -> aggregate_layer7_gpu9
	expert_layer7_gpu10_expert10 -> aggregate_layer7_gpu10
	expert_layer7_gpu11_expert11 -> aggregate_layer7_gpu11
	expert_layer7_gpu12_expert12 -> aggregate_layer7_gpu12
	expert_layer7_gpu13_expert13 -> aggregate_layer7_gpu13
	expert_layer7_gpu14_expert14 -> aggregate_layer7_gpu14
	expert_layer7_gpu15_expert15 -> aggregate_layer7_gpu15
	aggregate_layer7_gpu0 -> mha_layer8_gpu0
	aggregate_layer7_gpu1 -> mha_layer8_gpu1
	aggregate_layer7_gpu2 -> mha_layer8_gpu2
	aggregate_layer7_gpu3 -> mha_layer8_gpu3
	aggregate_layer7_gpu4 -> mha_layer8_gpu4
	aggregate_layer7_gpu5 -> mha_layer8_gpu5
	aggregate_layer7_gpu6 -> mha_layer8_gpu6
	aggregate_layer7_gpu7 -> mha_layer8_gpu7
	aggregate_layer7_gpu8 -> mha_layer8_gpu8
	aggregate_layer7_gpu9 -> mha_layer8_gpu9
	aggregate_layer7_gpu10 -> mha_layer8_gpu10
	aggregate_layer7_gpu11 -> mha_layer8_gpu11
	aggregate_layer7_gpu12 -> mha_layer8_gpu12
	aggregate_layer7_gpu13 -> mha_layer8_gpu13
	aggregate_layer7_gpu14 -> mha_layer8_gpu14
	aggregate_layer7_gpu15 -> mha_layer8_gpu15
	mha_layer8_gpu0 -> gate_layer8_gpu0
	mha_layer8_gpu1 -> gate_layer8_gpu1
	mha_layer8_gpu2 -> gate_layer8_gpu2
	mha_layer8_gpu3 -> gate_layer8_gpu3
	mha_layer8_gpu4 -> gate_layer8_gpu4
	mha_layer8_gpu5 -> gate_layer8_gpu5
	mha_layer8_gpu6 -> gate_layer8_gpu6
	mha_layer8_gpu7 -> gate_layer8_gpu7
	mha_layer8_gpu8 -> gate_layer8_gpu8
	mha_layer8_gpu9 -> gate_layer8_gpu9
	mha_layer8_gpu10 -> gate_layer8_gpu10
	mha_layer8_gpu11 -> gate_layer8_gpu11
	mha_layer8_gpu12 -> gate_layer8_gpu12
	mha_layer8_gpu13 -> gate_layer8_gpu13
	mha_layer8_gpu14 -> gate_layer8_gpu14
	mha_layer8_gpu15 -> gate_layer8_gpu15
	gate_layer8_gpu0 -> route_layer8_gpu0 [style=dashed]
	gate_layer8_gpu1 -> route_layer8_gpu1 [style=dashed]
	gate_layer8_gpu2 -> route_layer8_gpu2 [style=dashed]
	gate_layer8_gpu3 -> route_layer8_gpu3 [style=dashed]
	gate_layer8_gpu4 -> route_layer8_gpu4 [style=dashed]
	gate_layer8_gpu5 -> route_layer8_gpu5 [style=dashed]
	gate_layer8_gpu6 -> route_layer8_gpu6 [style=dashed]
	gate_layer8_gpu7 -> route_layer8_gpu7 [style=dashed]
	gate_layer8_gpu8 -> route_layer8_gpu8 [style=dashed]
	gate_layer8_gpu9 -> route_layer8_gpu9 [style=dashed]
	gate_layer8_gpu10 -> route_layer8_gpu10 [style=dashed]
	gate_layer8_gpu11 -> route_layer8_gpu11 [style=dashed]
	gate_layer8_gpu12 -> route_layer8_gpu12 [style=dashed]
	gate_layer8_gpu13 -> route_layer8_gpu13 [style=dashed]
	gate_layer8_gpu14 -> route_layer8_gpu14 [style=dashed]
	gate_layer8_gpu15 -> route_layer8_gpu15 [style=dashed]
	route_layer8_gpu0 -> comm_layer8
	route_layer8_gpu1 -> comm_layer8
	route_layer8_gpu2 -> comm_layer8
	route_layer8_gpu3 -> comm_layer8
	route_layer8_gpu4 -> comm_layer8
	route_layer8_gpu5 -> comm_layer8
	route_layer8_gpu6 -> comm_layer8
	route_layer8_gpu7 -> comm_layer8
	route_layer8_gpu8 -> comm_layer8
	route_layer8_gpu9 -> comm_layer8
	route_layer8_gpu10 -> comm_layer8
	route_layer8_gpu11 -> comm_layer8
	route_layer8_gpu12 -> comm_layer8
	route_layer8_gpu13 -> comm_layer8
	route_layer8_gpu14 -> comm_layer8
	route_layer8_gpu15 -> comm_layer8
	comm_layer8 -> expert_layer8_gpu0_expert0
	comm_layer8 -> expert_layer8_gpu1_expert1
	comm_layer8 -> expert_layer8_gpu2_expert2
	comm_layer8 -> expert_layer8_gpu3_expert3
	comm_layer8 -> expert_layer8_gpu4_expert4
	comm_layer8 -> expert_layer8_gpu5_expert5
	comm_layer8 -> expert_layer8_gpu6_expert6
	comm_layer8 -> expert_layer8_gpu7_expert7
	comm_layer8 -> expert_layer8_gpu8_expert8
	comm_layer8 -> expert_layer8_gpu9_expert9
	comm_layer8 -> expert_layer8_gpu10_expert10
	comm_layer8 -> expert_layer8_gpu11_expert11
	comm_layer8 -> expert_layer8_gpu12_expert12
	comm_layer8 -> expert_layer8_gpu13_expert13
	comm_layer8 -> expert_layer8_gpu14_expert14
	comm_layer8 -> expert_layer8_gpu15_expert15
	expert_layer8_gpu0_expert0 -> aggregate_layer8_gpu0
	expert_layer8_gpu1_expert1 -> aggregate_layer8_gpu1
	expert_layer8_gpu2_expert2 -> aggregate_layer8_gpu2
	expert_layer8_gpu3_expert3 -> aggregate_layer8_gpu3
	expert_layer8_gpu4_expert4 -> aggregate_layer8_gpu4
	expert_layer8_gpu5_expert5 -> aggregate_layer8_gpu5
	expert_layer8_gpu6_expert6 -> aggregate_layer8_gpu6
	expert_layer8_gpu7_expert7 -> aggregate_layer8_gpu7
	expert_layer8_gpu8_expert8 -> aggregate_layer8_gpu8
	expert_layer8_gpu9_expert9 -> aggregate_layer8_gpu9
	expert_layer8_gpu10_expert10 -> aggregate_layer8_gpu10
	expert_layer8_gpu11_expert11 -> aggregate_layer8_gpu11
	expert_layer8_gpu12_expert12 -> aggregate_layer8_gpu12
	expert_layer8_gpu13_expert13 -> aggregate_layer8_gpu13
	expert_layer8_gpu14_expert14 -> aggregate_layer8_gpu14
	expert_layer8_gpu15_expert15 -> aggregate_layer8_gpu15
	aggregate_layer8_gpu0 -> mha_layer9_gpu0
	aggregate_layer8_gpu1 -> mha_layer9_gpu1
	aggregate_layer8_gpu2 -> mha_layer9_gpu2
	aggregate_layer8_gpu3 -> mha_layer9_gpu3
	aggregate_layer8_gpu4 -> mha_layer9_gpu4
	aggregate_layer8_gpu5 -> mha_layer9_gpu5
	aggregate_layer8_gpu6 -> mha_layer9_gpu6
	aggregate_layer8_gpu7 -> mha_layer9_gpu7
	aggregate_layer8_gpu8 -> mha_layer9_gpu8
	aggregate_layer8_gpu9 -> mha_layer9_gpu9
	aggregate_layer8_gpu10 -> mha_layer9_gpu10
	aggregate_layer8_gpu11 -> mha_layer9_gpu11
	aggregate_layer8_gpu12 -> mha_layer9_gpu12
	aggregate_layer8_gpu13 -> mha_layer9_gpu13
	aggregate_layer8_gpu14 -> mha_layer9_gpu14
	aggregate_layer8_gpu15 -> mha_layer9_gpu15
	mha_layer9_gpu0 -> gate_layer9_gpu0
	mha_layer9_gpu1 -> gate_layer9_gpu1
	mha_layer9_gpu2 -> gate_layer9_gpu2
	mha_layer9_gpu3 -> gate_layer9_gpu3
	mha_layer9_gpu4 -> gate_layer9_gpu4
	mha_layer9_gpu5 -> gate_layer9_gpu5
	mha_layer9_gpu6 -> gate_layer9_gpu6
	mha_layer9_gpu7 -> gate_layer9_gpu7
	mha_layer9_gpu8 -> gate_layer9_gpu8
	mha_layer9_gpu9 -> gate_layer9_gpu9
	mha_layer9_gpu10 -> gate_layer9_gpu10
	mha_layer9_gpu11 -> gate_layer9_gpu11
	mha_layer9_gpu12 -> gate_layer9_gpu12
	mha_layer9_gpu13 -> gate_layer9_gpu13
	mha_layer9_gpu14 -> gate_layer9_gpu14
	mha_layer9_gpu15 -> gate_layer9_gpu15
	gate_layer9_gpu0 -> route_layer9_gpu0 [style=dashed]
	gate_layer9_gpu1 -> route_layer9_gpu1 [style=dashed]
	gate_layer9_gpu2 -> route_layer9_gpu2 [style=dashed]
	gate_layer9_gpu3 -> route_layer9_gpu3 [style=dashed]
	gate_layer9_gpu4 -> route_layer9_gpu4 [style=dashed]
	gate_layer9_gpu5 -> route_layer9_gpu5 [style=dashed]
	gate_layer9_gpu6 -> route_layer9_gpu6 [style=dashed]
	gate_layer9_gpu7 -> route_layer9_gpu7 [style=dashed]
	gate_layer9_gpu8 -> route_layer9_gpu8 [style=dashed]
	gate_layer9_gpu9 -> route_layer9_gpu9 [style=dashed]
	gate_layer9_gpu10 -> route_layer9_gpu10 [style=dashed]
	gate_layer9_gpu11 -> route_layer9_gpu11 [style=dashed]
	gate_layer9_gpu12 -> route_layer9_gpu12 [style=dashed]
	gate_layer9_gpu13 -> route_layer9_gpu13 [style=dashed]
	gate_layer9_gpu14 -> route_layer9_gpu14 [style=dashed]
	gate_layer9_gpu15 -> route_layer9_gpu15 [style=dashed]
	route_layer9_gpu0 -> comm_layer9
	route_layer9_gpu1 -> comm_layer9
	route_layer9_gpu2 -> comm_layer9
	route_layer9_gpu3 -> comm_layer9
	route_layer9_gpu4 -> comm_layer9
	route_layer9_gpu5 -> comm_layer9
	route_layer9_gpu6 -> comm_layer9
	route_layer9_gpu7 -> comm_layer9
	route_layer9_gpu8 -> comm_layer9
	route_layer9_gpu9 -> comm_layer9
	route_layer9_gpu10 -> comm_layer9
	route_layer9_gpu11 -> comm_layer9
	route_layer9_gpu12 -> comm_layer9
	route_layer9_gpu13 -> comm_layer9
	route_layer9_gpu14 -> comm_layer9
	route_layer9_gpu15 -> comm_layer9
	comm_layer9 -> expert_layer9_gpu0_expert0
	comm_layer9 -> expert_layer9_gpu1_expert1
	comm_layer9 -> expert_layer9_gpu2_expert2
	comm_layer9 -> expert_layer9_gpu3_expert3
	comm_layer9 -> expert_layer9_gpu4_expert4
	comm_layer9 -> expert_layer9_gpu5_expert5
	comm_layer9 -> expert_layer9_gpu6_expert6
	comm_layer9 -> expert_layer9_gpu7_expert7
	comm_layer9 -> expert_layer9_gpu8_expert8
	comm_layer9 -> expert_layer9_gpu9_expert9
	comm_layer9 -> expert_layer9_gpu10_expert10
	comm_layer9 -> expert_layer9_gpu11_expert11
	comm_layer9 -> expert_layer9_gpu12_expert12
	comm_layer9 -> expert_layer9_gpu13_expert13
	comm_layer9 -> expert_layer9_gpu14_expert14
	comm_layer9 -> expert_layer9_gpu15_expert15
	expert_layer9_gpu0_expert0 -> aggregate_layer9_gpu0
	expert_layer9_gpu1_expert1 -> aggregate_layer9_gpu1
	expert_layer9_gpu2_expert2 -> aggregate_layer9_gpu2
	expert_layer9_gpu3_expert3 -> aggregate_layer9_gpu3
	expert_layer9_gpu4_expert4 -> aggregate_layer9_gpu4
	expert_layer9_gpu5_expert5 -> aggregate_layer9_gpu5
	expert_layer9_gpu6_expert6 -> aggregate_layer9_gpu6
	expert_layer9_gpu7_expert7 -> aggregate_layer9_gpu7
	expert_layer9_gpu8_expert8 -> aggregate_layer9_gpu8
	expert_layer9_gpu9_expert9 -> aggregate_layer9_gpu9
	expert_layer9_gpu10_expert10 -> aggregate_layer9_gpu10
	expert_layer9_gpu11_expert11 -> aggregate_layer9_gpu11
	expert_layer9_gpu12_expert12 -> aggregate_layer9_gpu12
	expert_layer9_gpu13_expert13 -> aggregate_layer9_gpu13
	expert_layer9_gpu14_expert14 -> aggregate_layer9_gpu14
	expert_layer9_gpu15_expert15 -> aggregate_layer9_gpu15
	aggregate_layer9_gpu0 -> mha_layer10_gpu0
	aggregate_layer9_gpu1 -> mha_layer10_gpu1
	aggregate_layer9_gpu2 -> mha_layer10_gpu2
	aggregate_layer9_gpu3 -> mha_layer10_gpu3
	aggregate_layer9_gpu4 -> mha_layer10_gpu4
	aggregate_layer9_gpu5 -> mha_layer10_gpu5
	aggregate_layer9_gpu6 -> mha_layer10_gpu6
	aggregate_layer9_gpu7 -> mha_layer10_gpu7
	aggregate_layer9_gpu8 -> mha_layer10_gpu8
	aggregate_layer9_gpu9 -> mha_layer10_gpu9
	aggregate_layer9_gpu10 -> mha_layer10_gpu10
	aggregate_layer9_gpu11 -> mha_layer10_gpu11
	aggregate_layer9_gpu12 -> mha_layer10_gpu12
	aggregate_layer9_gpu13 -> mha_layer10_gpu13
	aggregate_layer9_gpu14 -> mha_layer10_gpu14
	aggregate_layer9_gpu15 -> mha_layer10_gpu15
	mha_layer10_gpu0 -> gate_layer10_gpu0
	mha_layer10_gpu1 -> gate_layer10_gpu1
	mha_layer10_gpu2 -> gate_layer10_gpu2
	mha_layer10_gpu3 -> gate_layer10_gpu3
	mha_layer10_gpu4 -> gate_layer10_gpu4
	mha_layer10_gpu5 -> gate_layer10_gpu5
	mha_layer10_gpu6 -> gate_layer10_gpu6
	mha_layer10_gpu7 -> gate_layer10_gpu7
	mha_layer10_gpu8 -> gate_layer10_gpu8
	mha_layer10_gpu9 -> gate_layer10_gpu9
	mha_layer10_gpu10 -> gate_layer10_gpu10
	mha_layer10_gpu11 -> gate_layer10_gpu11
	mha_layer10_gpu12 -> gate_layer10_gpu12
	mha_layer10_gpu13 -> gate_layer10_gpu13
	mha_layer10_gpu14 -> gate_layer10_gpu14
	mha_layer10_gpu15 -> gate_layer10_gpu15
	gate_layer10_gpu0 -> route_layer10_gpu0 [style=dashed]
	gate_layer10_gpu1 -> route_layer10_gpu1 [style=dashed]
	gate_layer10_gpu2 -> route_layer10_gpu2 [style=dashed]
	gate_layer10_gpu3 -> route_layer10_gpu3 [style=dashed]
	gate_layer10_gpu4 -> route_layer10_gpu4 [style=dashed]
	gate_layer10_gpu5 -> route_layer10_gpu5 [style=dashed]
	gate_layer10_gpu6 -> route_layer10_gpu6 [style=dashed]
	gate_layer10_gpu7 -> route_layer10_gpu7 [style=dashed]
	gate_layer10_gpu8 -> route_layer10_gpu8 [style=dashed]
	gate_layer10_gpu9 -> route_layer10_gpu9 [style=dashed]
	gate_layer10_gpu10 -> route_layer10_gpu10 [style=dashed]
	gate_layer10_gpu11 -> route_layer10_gpu11 [style=dashed]
	gate_layer10_gpu12 -> route_layer10_gpu12 [style=dashed]
	gate_layer10_gpu13 -> route_layer10_gpu13 [style=dashed]
	gate_layer10_gpu14 -> route_layer10_gpu14 [style=dashed]
	gate_layer10_gpu15 -> route_layer10_gpu15 [style=dashed]
	route_layer10_gpu0 -> comm_layer10
	route_layer10_gpu1 -> comm_layer10
	route_layer10_gpu2 -> comm_layer10
	route_layer10_gpu3 -> comm_layer10
	route_layer10_gpu4 -> comm_layer10
	route_layer10_gpu5 -> comm_layer10
	route_layer10_gpu6 -> comm_layer10
	route_layer10_gpu7 -> comm_layer10
	route_layer10_gpu8 -> comm_layer10
	route_layer10_gpu9 -> comm_layer10
	route_layer10_gpu10 -> comm_layer10
	route_layer10_gpu11 -> comm_layer10
	route_layer10_gpu12 -> comm_layer10
	route_layer10_gpu13 -> comm_layer10
	route_layer10_gpu14 -> comm_layer10
	route_layer10_gpu15 -> comm_layer10
	comm_layer10 -> expert_layer10_gpu0_expert0
	comm_layer10 -> expert_layer10_gpu1_expert1
	comm_layer10 -> expert_layer10_gpu2_expert2
	comm_layer10 -> expert_layer10_gpu3_expert3
	comm_layer10 -> expert_layer10_gpu4_expert4
	comm_layer10 -> expert_layer10_gpu5_expert5
	comm_layer10 -> expert_layer10_gpu6_expert6
	comm_layer10 -> expert_layer10_gpu7_expert7
	comm_layer10 -> expert_layer10_gpu8_expert8
	comm_layer10 -> expert_layer10_gpu9_expert9
	comm_layer10 -> expert_layer10_gpu10_expert10
	comm_layer10 -> expert_layer10_gpu11_expert11
	comm_layer10 -> expert_layer10_gpu12_expert12
	comm_layer10 -> expert_layer10_gpu13_expert13
	comm_layer10 -> expert_layer10_gpu14_expert14
	comm_layer10 -> expert_layer10_gpu15_expert15
	expert_layer10_gpu0_expert0 -> aggregate_layer10_gpu0
	expert_layer10_gpu1_expert1 -> aggregate_layer10_gpu1
	expert_layer10_gpu2_expert2 -> aggregate_layer10_gpu2
	expert_layer10_gpu3_expert3 -> aggregate_layer10_gpu3
	expert_layer10_gpu4_expert4 -> aggregate_layer10_gpu4
	expert_layer10_gpu5_expert5 -> aggregate_layer10_gpu5
	expert_layer10_gpu6_expert6 -> aggregate_layer10_gpu6
	expert_layer10_gpu7_expert7 -> aggregate_layer10_gpu7
	expert_layer10_gpu8_expert8 -> aggregate_layer10_gpu8
	expert_layer10_gpu9_expert9 -> aggregate_layer10_gpu9
	expert_layer10_gpu10_expert10 -> aggregate_layer10_gpu10
	expert_layer10_gpu11_expert11 -> aggregate_layer10_gpu11
	expert_layer10_gpu12_expert12 -> aggregate_layer10_gpu12
	expert_layer10_gpu13_expert13 -> aggregate_layer10_gpu13
	expert_layer10_gpu14_expert14 -> aggregate_layer10_gpu14
	expert_layer10_gpu15_expert15 -> aggregate_layer10_gpu15
	aggregate_layer10_gpu0 -> mha_layer11_gpu0
	aggregate_layer10_gpu1 -> mha_layer11_gpu1
	aggregate_layer10_gpu2 -> mha_layer11_gpu2
	aggregate_layer10_gpu3 -> mha_layer11_gpu3
	aggregate_layer10_gpu4 -> mha_layer11_gpu4
	aggregate_layer10_gpu5 -> mha_layer11_gpu5
	aggregate_layer10_gpu6 -> mha_layer11_gpu6
	aggregate_layer10_gpu7 -> mha_layer11_gpu7
	aggregate_layer10_gpu8 -> mha_layer11_gpu8
	aggregate_layer10_gpu9 -> mha_layer11_gpu9
	aggregate_layer10_gpu10 -> mha_layer11_gpu10
	aggregate_layer10_gpu11 -> mha_layer11_gpu11
	aggregate_layer10_gpu12 -> mha_layer11_gpu12
	aggregate_layer10_gpu13 -> mha_layer11_gpu13
	aggregate_layer10_gpu14 -> mha_layer11_gpu14
	aggregate_layer10_gpu15 -> mha_layer11_gpu15
	mha_layer11_gpu0 -> gate_layer11_gpu0
	mha_layer11_gpu1 -> gate_layer11_gpu1
	mha_layer11_gpu2 -> gate_layer11_gpu2
	mha_layer11_gpu3 -> gate_layer11_gpu3
	mha_layer11_gpu4 -> gate_layer11_gpu4
	mha_layer11_gpu5 -> gate_layer11_gpu5
	mha_layer11_gpu6 -> gate_layer11_gpu6
	mha_layer11_gpu7 -> gate_layer11_gpu7
	mha_layer11_gpu8 -> gate_layer11_gpu8
	mha_layer11_gpu9 -> gate_layer11_gpu9
	mha_layer11_gpu10 -> gate_layer11_gpu10
	mha_layer11_gpu11 -> gate_layer11_gpu11
	mha_layer11_gpu12 -> gate_layer11_gpu12
	mha_layer11_gpu13 -> gate_layer11_gpu13
	mha_layer11_gpu14 -> gate_layer11_gpu14
	mha_layer11_gpu15 -> gate_layer11_gpu15
	gate_layer11_gpu0 -> route_layer11_gpu0 [style=dashed]
	gate_layer11_gpu1 -> route_layer11_gpu1 [style=dashed]
	gate_layer11_gpu2 -> route_layer11_gpu2 [style=dashed]
	gate_layer11_gpu3 -> route_layer11_gpu3 [style=dashed]
	gate_layer11_gpu4 -> route_layer11_gpu4 [style=dashed]
	gate_layer11_gpu5 -> route_layer11_gpu5 [style=dashed]
	gate_layer11_gpu6 -> route_layer11_gpu6 [style=dashed]
	gate_layer11_gpu7 -> route_layer11_gpu7 [style=dashed]
	gate_layer11_gpu8 -> route_layer11_gpu8 [style=dashed]
	gate_layer11_gpu9 -> route_layer11_gpu9 [style=dashed]
	gate_layer11_gpu10 -> route_layer11_gpu10 [style=dashed]
	gate_layer11_gpu11 -> route_layer11_gpu11 [style=dashed]
	gate_layer11_gpu12 -> route_layer11_gpu12 [style=dashed]
	gate_layer11_gpu13 -> route_layer11_gpu13 [style=dashed]
	gate_layer11_gpu14 -> route_layer11_gpu14 [style=dashed]
	gate_layer11_gpu15 -> route_layer11_gpu15 [style=dashed]
	route_layer11_gpu0 -> comm_layer11
	route_layer11_gpu1 -> comm_layer11
	route_layer11_gpu2 -> comm_layer11
	route_layer11_gpu3 -> comm_layer11
	route_layer11_gpu4 -> comm_layer11
	route_layer11_gpu5 -> comm_layer11
	route_layer11_gpu6 -> comm_layer11
	route_layer11_gpu7 -> comm_layer11
	route_layer11_gpu8 -> comm_layer11
	route_layer11_gpu9 -> comm_layer11
	route_layer11_gpu10 -> comm_layer11
	route_layer11_gpu11 -> comm_layer11
	route_layer11_gpu12 -> comm_layer11
	route_layer11_gpu13 -> comm_layer11
	route_layer11_gpu14 -> comm_layer11
	route_layer11_gpu15 -> comm_layer11
	comm_layer11 -> expert_layer11_gpu0_expert0
	comm_layer11 -> expert_layer11_gpu1_expert1
	comm_layer11 -> expert_layer11_gpu2_expert2
	comm_layer11 -> expert_layer11_gpu3_expert3
	comm_layer11 -> expert_layer11_gpu4_expert4
	comm_layer11 -> expert_layer11_gpu5_expert5
	comm_layer11 -> expert_layer11_gpu6_expert6
	comm_layer11 -> expert_layer11_gpu7_expert7
	comm_layer11 -> expert_layer11_gpu8_expert8
	comm_layer11 -> expert_layer11_gpu9_expert9
	comm_layer11 -> expert_layer11_gpu10_expert10
	comm_layer11 -> expert_layer11_gpu11_expert11
	comm_layer11 -> expert_layer11_gpu12_expert12
	comm_layer11 -> expert_layer11_gpu13_expert13
	comm_layer11 -> expert_layer11_gpu14_expert14
	comm_layer11 -> expert_layer11_gpu15_expert15
	expert_layer11_gpu0_expert0 -> aggregate_layer11_gpu0
	expert_layer11_gpu1_expert1 -> aggregate_layer11_gpu1
	expert_layer11_gpu2_expert2 -> aggregate_layer11_gpu2
	expert_layer11_gpu3_expert3 -> aggregate_layer11_gpu3
	expert_layer11_gpu4_expert4 -> aggregate_layer11_gpu4
	expert_layer11_gpu5_expert5 -> aggregate_layer11_gpu5
	expert_layer11_gpu6_expert6 -> aggregate_layer11_gpu6
	expert_layer11_gpu7_expert7 -> aggregate_layer11_gpu7
	expert_layer11_gpu8_expert8 -> aggregate_layer11_gpu8
	expert_layer11_gpu9_expert9 -> aggregate_layer11_gpu9
	expert_layer11_gpu10_expert10 -> aggregate_layer11_gpu10
	expert_layer11_gpu11_expert11 -> aggregate_layer11_gpu11
	expert_layer11_gpu12_expert12 -> aggregate_layer11_gpu12
	expert_layer11_gpu13_expert13 -> aggregate_layer11_gpu13
	expert_layer11_gpu14_expert14 -> aggregate_layer11_gpu14
	expert_layer11_gpu15_expert15 -> aggregate_layer11_gpu15
	aggregate_layer11_gpu0 -> mha_layer12_gpu0
	aggregate_layer11_gpu1 -> mha_layer12_gpu1
	aggregate_layer11_gpu2 -> mha_layer12_gpu2
	aggregate_layer11_gpu3 -> mha_layer12_gpu3
	aggregate_layer11_gpu4 -> mha_layer12_gpu4
	aggregate_layer11_gpu5 -> mha_layer12_gpu5
	aggregate_layer11_gpu6 -> mha_layer12_gpu6
	aggregate_layer11_gpu7 -> mha_layer12_gpu7
	aggregate_layer11_gpu8 -> mha_layer12_gpu8
	aggregate_layer11_gpu9 -> mha_layer12_gpu9
	aggregate_layer11_gpu10 -> mha_layer12_gpu10
	aggregate_layer11_gpu11 -> mha_layer12_gpu11
	aggregate_layer11_gpu12 -> mha_layer12_gpu12
	aggregate_layer11_gpu13 -> mha_layer12_gpu13
	aggregate_layer11_gpu14 -> mha_layer12_gpu14
	aggregate_layer11_gpu15 -> mha_layer12_gpu15
	mha_layer12_gpu0 -> gate_layer12_gpu0
	mha_layer12_gpu1 -> gate_layer12_gpu1
	mha_layer12_gpu2 -> gate_layer12_gpu2
	mha_layer12_gpu3 -> gate_layer12_gpu3
	mha_layer12_gpu4 -> gate_layer12_gpu4
	mha_layer12_gpu5 -> gate_layer12_gpu5
	mha_layer12_gpu6 -> gate_layer12_gpu6
	mha_layer12_gpu7 -> gate_layer12_gpu7
	mha_layer12_gpu8 -> gate_layer12_gpu8
	mha_layer12_gpu9 -> gate_layer12_gpu9
	mha_layer12_gpu10 -> gate_layer12_gpu10
	mha_layer12_gpu11 -> gate_layer12_gpu11
	mha_layer12_gpu12 -> gate_layer12_gpu12
	mha_layer12_gpu13 -> gate_layer12_gpu13
	mha_layer12_gpu14 -> gate_layer12_gpu14
	mha_layer12_gpu15 -> gate_layer12_gpu15
	gate_layer12_gpu0 -> route_layer12_gpu0 [style=dashed]
	gate_layer12_gpu1 -> route_layer12_gpu1 [style=dashed]
	gate_layer12_gpu2 -> route_layer12_gpu2 [style=dashed]
	gate_layer12_gpu3 -> route_layer12_gpu3 [style=dashed]
	gate_layer12_gpu4 -> route_layer12_gpu4 [style=dashed]
	gate_layer12_gpu5 -> route_layer12_gpu5 [style=dashed]
	gate_layer12_gpu6 -> route_layer12_gpu6 [style=dashed]
	gate_layer12_gpu7 -> route_layer12_gpu7 [style=dashed]
	gate_layer12_gpu8 -> route_layer12_gpu8 [style=dashed]
	gate_layer12_gpu9 -> route_layer12_gpu9 [style=dashed]
	gate_layer12_gpu10 -> route_layer12_gpu10 [style=dashed]
	gate_layer12_gpu11 -> route_layer12_gpu11 [style=dashed]
	gate_layer12_gpu12 -> route_layer12_gpu12 [style=dashed]
	gate_layer12_gpu13 -> route_layer12_gpu13 [style=dashed]
	gate_layer12_gpu14 -> route_layer12_gpu14 [style=dashed]
	gate_layer12_gpu15 -> route_layer12_gpu15 [style=dashed]
	route_layer12_gpu0 -> comm_layer12
	route_layer12_gpu1 -> comm_layer12
	route_layer12_gpu2 -> comm_layer12
	route_layer12_gpu3 -> comm_layer12
	route_layer12_gpu4 -> comm_layer12
	route_layer12_gpu5 -> comm_layer12
	route_layer12_gpu6 -> comm_layer12
	route_layer12_gpu7 -> comm_layer12
	route_layer12_gpu8 -> comm_layer12
	route_layer12_gpu9 -> comm_layer12
	route_layer12_gpu10 -> comm_layer12
	route_layer12_gpu11 -> comm_layer12
	route_layer12_gpu12 -> comm_layer12
	route_layer12_gpu13 -> comm_layer12
	route_layer12_gpu14 -> comm_layer12
	route_layer12_gpu15 -> comm_layer12
	comm_layer12 -> expert_layer12_gpu0_expert0
	comm_layer12 -> expert_layer12_gpu1_expert1
	comm_layer12 -> expert_layer12_gpu2_expert2
	comm_layer12 -> expert_layer12_gpu3_expert3
	comm_layer12 -> expert_layer12_gpu4_expert4
	comm_layer12 -> expert_layer12_gpu5_expert5
	comm_layer12 -> expert_layer12_gpu6_expert6
	comm_layer12 -> expert_layer12_gpu7_expert7
	comm_layer12 -> expert_layer12_gpu8_expert8
	comm_layer12 -> expert_layer12_gpu9_expert9
	comm_layer12 -> expert_layer12_gpu10_expert10
	comm_layer12 -> expert_layer12_gpu11_expert11
	comm_layer12 -> expert_layer12_gpu12_expert12
	comm_layer12 -> expert_layer12_gpu13_expert13
	comm_layer12 -> expert_layer12_gpu14_expert14
	comm_layer12 -> expert_layer12_gpu15_expert15
	expert_layer12_gpu0_expert0 -> aggregate_layer12_gpu0
	expert_layer12_gpu1_expert1 -> aggregate_layer12_gpu1
	expert_layer12_gpu2_expert2 -> aggregate_layer12_gpu2
	expert_layer12_gpu3_expert3 -> aggregate_layer12_gpu3
	expert_layer12_gpu4_expert4 -> aggregate_layer12_gpu4
	expert_layer12_gpu5_expert5 -> aggregate_layer12_gpu5
	expert_layer12_gpu6_expert6 -> aggregate_layer12_gpu6
	expert_layer12_gpu7_expert7 -> aggregate_layer12_gpu7
	expert_layer12_gpu8_expert8 -> aggregate_layer12_gpu8
	expert_layer12_gpu9_expert9 -> aggregate_layer12_gpu9
	expert_layer12_gpu10_expert10 -> aggregate_layer12_gpu10
	expert_layer12_gpu11_expert11 -> aggregate_layer12_gpu11
	expert_layer12_gpu12_expert12 -> aggregate_layer12_gpu12
	expert_layer12_gpu13_expert13 -> aggregate_layer12_gpu13
	expert_layer12_gpu14_expert14 -> aggregate_layer12_gpu14
	expert_layer12_gpu15_expert15 -> aggregate_layer12_gpu15
	aggregate_layer12_gpu0 -> mha_layer13_gpu0
	aggregate_layer12_gpu1 -> mha_layer13_gpu1
	aggregate_layer12_gpu2 -> mha_layer13_gpu2
	aggregate_layer12_gpu3 -> mha_layer13_gpu3
	aggregate_layer12_gpu4 -> mha_layer13_gpu4
	aggregate_layer12_gpu5 -> mha_layer13_gpu5
	aggregate_layer12_gpu6 -> mha_layer13_gpu6
	aggregate_layer12_gpu7 -> mha_layer13_gpu7
	aggregate_layer12_gpu8 -> mha_layer13_gpu8
	aggregate_layer12_gpu9 -> mha_layer13_gpu9
	aggregate_layer12_gpu10 -> mha_layer13_gpu10
	aggregate_layer12_gpu11 -> mha_layer13_gpu11
	aggregate_layer12_gpu12 -> mha_layer13_gpu12
	aggregate_layer12_gpu13 -> mha_layer13_gpu13
	aggregate_layer12_gpu14 -> mha_layer13_gpu14
	aggregate_layer12_gpu15 -> mha_layer13_gpu15
	mha_layer13_gpu0 -> gate_layer13_gpu0
	mha_layer13_gpu1 -> gate_layer13_gpu1
	mha_layer13_gpu2 -> gate_layer13_gpu2
	mha_layer13_gpu3 -> gate_layer13_gpu3
	mha_layer13_gpu4 -> gate_layer13_gpu4
	mha_layer13_gpu5 -> gate_layer13_gpu5
	mha_layer13_gpu6 -> gate_layer13_gpu6
	mha_layer13_gpu7 -> gate_layer13_gpu7
	mha_layer13_gpu8 -> gate_layer13_gpu8
	mha_layer13_gpu9 -> gate_layer13_gpu9
	mha_layer13_gpu10 -> gate_layer13_gpu10
	mha_layer13_gpu11 -> gate_layer13_gpu11
	mha_layer13_gpu12 -> gate_layer13_gpu12
	mha_layer13_gpu13 -> gate_layer13_gpu13
	mha_layer13_gpu14 -> gate_layer13_gpu14
	mha_layer13_gpu15 -> gate_layer13_gpu15
	gate_layer13_gpu0 -> route_layer13_gpu0 [style=dashed]
	gate_layer13_gpu1 -> route_layer13_gpu1 [style=dashed]
	gate_layer13_gpu2 -> route_layer13_gpu2 [style=dashed]
	gate_layer13_gpu3 -> route_layer13_gpu3 [style=dashed]
	gate_layer13_gpu4 -> route_layer13_gpu4 [style=dashed]
	gate_layer13_gpu5 -> route_layer13_gpu5 [style=dashed]
	gate_layer13_gpu6 -> route_layer13_gpu6 [style=dashed]
	gate_layer13_gpu7 -> route_layer13_gpu7 [style=dashed]
	gate_layer13_gpu8 -> route_layer13_gpu8 [style=dashed]
	gate_layer13_gpu9 -> route_layer13_gpu9 [style=dashed]
	gate_layer13_gpu10 -> route_layer13_gpu10 [style=dashed]
	gate_layer13_gpu11 -> route_layer13_gpu11 [style=dashed]
	gate_layer13_gpu12 -> route_layer13_gpu12 [style=dashed]
	gate_layer13_gpu13 -> route_layer13_gpu13 [style=dashed]
	gate_layer13_gpu14 -> route_layer13_gpu14 [style=dashed]
	gate_layer13_gpu15 -> route_layer13_gpu15 [style=dashed]
	route_layer13_gpu0 -> comm_layer13
	route_layer13_gpu1 -> comm_layer13
	route_layer13_gpu2 -> comm_layer13
	route_layer13_gpu3 -> comm_layer13
	route_layer13_gpu4 -> comm_layer13
	route_layer13_gpu5 -> comm_layer13
	route_layer13_gpu6 -> comm_layer13
	route_layer13_gpu7 -> comm_layer13
	route_layer13_gpu8 -> comm_layer13
	route_layer13_gpu9 -> comm_layer13
	route_layer13_gpu10 -> comm_layer13
	route_layer13_gpu11 -> comm_layer13
	route_layer13_gpu12 -> comm_layer13
	route_layer13_gpu13 -> comm_layer13
	route_layer13_gpu14 -> comm_layer13
	route_layer13_gpu15 -> comm_layer13
	comm_layer13 -> expert_layer13_gpu0_expert0
	comm_layer13 -> expert_layer13_gpu1_expert1
	comm_layer13 -> expert_layer13_gpu2_expert2
	comm_layer13 -> expert_layer13_gpu3_expert3
	comm_layer13 -> expert_layer13_gpu4_expert4
	comm_layer13 -> expert_layer13_gpu5_expert5
	comm_layer13 -> expert_layer13_gpu6_expert6
	comm_layer13 -> expert_layer13_gpu7_expert7
	comm_layer13 -> expert_layer13_gpu8_expert8
	comm_layer13 -> expert_layer13_gpu9_expert9
	comm_layer13 -> expert_layer13_gpu10_expert10
	comm_layer13 -> expert_layer13_gpu11_expert11
	comm_layer13 -> expert_layer13_gpu12_expert12
	comm_layer13 -> expert_layer13_gpu13_expert13
	comm_layer13 -> expert_layer13_gpu14_expert14
	comm_layer13 -> expert_layer13_gpu15_expert15
	expert_layer13_gpu0_expert0 -> aggregate_layer13_gpu0
	expert_layer13_gpu1_expert1 -> aggregate_layer13_gpu1
	expert_layer13_gpu2_expert2 -> aggregate_layer13_gpu2
	expert_layer13_gpu3_expert3 -> aggregate_layer13_gpu3
	expert_layer13_gpu4_expert4 -> aggregate_layer13_gpu4
	expert_layer13_gpu5_expert5 -> aggregate_layer13_gpu5
	expert_layer13_gpu6_expert6 -> aggregate_layer13_gpu6
	expert_layer13_gpu7_expert7 -> aggregate_layer13_gpu7
	expert_layer13_gpu8_expert8 -> aggregate_layer13_gpu8
	expert_layer13_gpu9_expert9 -> aggregate_layer13_gpu9
	expert_layer13_gpu10_expert10 -> aggregate_layer13_gpu10
	expert_layer13_gpu11_expert11 -> aggregate_layer13_gpu11
	expert_layer13_gpu12_expert12 -> aggregate_layer13_gpu12
	expert_layer13_gpu13_expert13 -> aggregate_layer13_gpu13
	expert_layer13_gpu14_expert14 -> aggregate_layer13_gpu14
	expert_layer13_gpu15_expert15 -> aggregate_layer13_gpu15
	aggregate_layer13_gpu0 -> mha_layer14_gpu0
	aggregate_layer13_gpu1 -> mha_layer14_gpu1
	aggregate_layer13_gpu2 -> mha_layer14_gpu2
	aggregate_layer13_gpu3 -> mha_layer14_gpu3
	aggregate_layer13_gpu4 -> mha_layer14_gpu4
	aggregate_layer13_gpu5 -> mha_layer14_gpu5
	aggregate_layer13_gpu6 -> mha_layer14_gpu6
	aggregate_layer13_gpu7 -> mha_layer14_gpu7
	aggregate_layer13_gpu8 -> mha_layer14_gpu8
	aggregate_layer13_gpu9 -> mha_layer14_gpu9
	aggregate_layer13_gpu10 -> mha_layer14_gpu10
	aggregate_layer13_gpu11 -> mha_layer14_gpu11
	aggregate_layer13_gpu12 -> mha_layer14_gpu12
	aggregate_layer13_gpu13 -> mha_layer14_gpu13
	aggregate_layer13_gpu14 -> mha_layer14_gpu14
	aggregate_layer13_gpu15 -> mha_layer14_gpu15
	mha_layer14_gpu0 -> gate_layer14_gpu0
	mha_layer14_gpu1 -> gate_layer14_gpu1
	mha_layer14_gpu2 -> gate_layer14_gpu2
	mha_layer14_gpu3 -> gate_layer14_gpu3
	mha_layer14_gpu4 -> gate_layer14_gpu4
	mha_layer14_gpu5 -> gate_layer14_gpu5
	mha_layer14_gpu6 -> gate_layer14_gpu6
	mha_layer14_gpu7 -> gate_layer14_gpu7
	mha_layer14_gpu8 -> gate_layer14_gpu8
	mha_layer14_gpu9 -> gate_layer14_gpu9
	mha_layer14_gpu10 -> gate_layer14_gpu10
	mha_layer14_gpu11 -> gate_layer14_gpu11
	mha_layer14_gpu12 -> gate_layer14_gpu12
	mha_layer14_gpu13 -> gate_layer14_gpu13
	mha_layer14_gpu14 -> gate_layer14_gpu14
	mha_layer14_gpu15 -> gate_layer14_gpu15
	gate_layer14_gpu0 -> route_layer14_gpu0 [style=dashed]
	gate_layer14_gpu1 -> route_layer14_gpu1 [style=dashed]
	gate_layer14_gpu2 -> route_layer14_gpu2 [style=dashed]
	gate_layer14_gpu3 -> route_layer14_gpu3 [style=dashed]
	gate_layer14_gpu4 -> route_layer14_gpu4 [style=dashed]
	gate_layer14_gpu5 -> route_layer14_gpu5 [style=dashed]
	gate_layer14_gpu6 -> route_layer14_gpu6 [style=dashed]
	gate_layer14_gpu7 -> route_layer14_gpu7 [style=dashed]
	gate_layer14_gpu8 -> route_layer14_gpu8 [style=dashed]
	gate_layer14_gpu9 -> route_layer14_gpu9 [style=dashed]
	gate_layer14_gpu10 -> route_layer14_gpu10 [style=dashed]
	gate_layer14_gpu11 -> route_layer14_gpu11 [style=dashed]
	gate_layer14_gpu12 -> route_layer14_gpu12 [style=dashed]
	gate_layer14_gpu13 -> route_layer14_gpu13 [style=dashed]
	gate_layer14_gpu14 -> route_layer14_gpu14 [style=dashed]
	gate_layer14_gpu15 -> route_layer14_gpu15 [style=dashed]
	route_layer14_gpu0 -> comm_layer14
	route_layer14_gpu1 -> comm_layer14
	route_layer14_gpu2 -> comm_layer14
	route_layer14_gpu3 -> comm_layer14
	route_layer14_gpu4 -> comm_layer14
	route_layer14_gpu5 -> comm_layer14
	route_layer14_gpu6 -> comm_layer14
	route_layer14_gpu7 -> comm_layer14
	route_layer14_gpu8 -> comm_layer14
	route_layer14_gpu9 -> comm_layer14
	route_layer14_gpu10 -> comm_layer14
	route_layer14_gpu11 -> comm_layer14
	route_layer14_gpu12 -> comm_layer14
	route_layer14_gpu13 -> comm_layer14
	route_layer14_gpu14 -> comm_layer14
	route_layer14_gpu15 -> comm_layer14
	comm_layer14 -> expert_layer14_gpu0_expert0
	comm_layer14 -> expert_layer14_gpu1_expert1
	comm_layer14 -> expert_layer14_gpu2_expert2
	comm_layer14 -> expert_layer14_gpu3_expert3
	comm_layer14 -> expert_layer14_gpu4_expert4
	comm_layer14 -> expert_layer14_gpu5_expert5
	comm_layer14 -> expert_layer14_gpu6_expert6
	comm_layer14 -> expert_layer14_gpu7_expert7
	comm_layer14 -> expert_layer14_gpu8_expert8
	comm_layer14 -> expert_layer14_gpu9_expert9
	comm_layer14 -> expert_layer14_gpu10_expert10
	comm_layer14 -> expert_layer14_gpu11_expert11
	comm_layer14 -> expert_layer14_gpu12_expert12
	comm_layer14 -> expert_layer14_gpu13_expert13
	comm_layer14 -> expert_layer14_gpu14_expert14
	comm_layer14 -> expert_layer14_gpu15_expert15
	expert_layer14_gpu0_expert0 -> aggregate_layer14_gpu0
	expert_layer14_gpu1_expert1 -> aggregate_layer14_gpu1
	expert_layer14_gpu2_expert2 -> aggregate_layer14_gpu2
	expert_layer14_gpu3_expert3 -> aggregate_layer14_gpu3
	expert_layer14_gpu4_expert4 -> aggregate_layer14_gpu4
	expert_layer14_gpu5_expert5 -> aggregate_layer14_gpu5
	expert_layer14_gpu6_expert6 -> aggregate_layer14_gpu6
	expert_layer14_gpu7_expert7 -> aggregate_layer14_gpu7
	expert_layer14_gpu8_expert8 -> aggregate_layer14_gpu8
	expert_layer14_gpu9_expert9 -> aggregate_layer14_gpu9
	expert_layer14_gpu10_expert10 -> aggregate_layer14_gpu10
	expert_layer14_gpu11_expert11 -> aggregate_layer14_gpu11
	expert_layer14_gpu12_expert12 -> aggregate_layer14_gpu12
	expert_layer14_gpu13_expert13 -> aggregate_layer14_gpu13
	expert_layer14_gpu14_expert14 -> aggregate_layer14_gpu14
	expert_layer14_gpu15_expert15 -> aggregate_layer14_gpu15
	aggregate_layer14_gpu0 -> mha_layer15_gpu0
	aggregate_layer14_gpu1 -> mha_layer15_gpu1
	aggregate_layer14_gpu2 -> mha_layer15_gpu2
	aggregate_layer14_gpu3 -> mha_layer15_gpu3
	aggregate_layer14_gpu4 -> mha_layer15_gpu4
	aggregate_layer14_gpu5 -> mha_layer15_gpu5
	aggregate_layer14_gpu6 -> mha_layer15_gpu6
	aggregate_layer14_gpu7 -> mha_layer15_gpu7
	aggregate_layer14_gpu8 -> mha_layer15_gpu8
	aggregate_layer14_gpu9 -> mha_layer15_gpu9
	aggregate_layer14_gpu10 -> mha_layer15_gpu10
	aggregate_layer14_gpu11 -> mha_layer15_gpu11
	aggregate_layer14_gpu12 -> mha_layer15_gpu12
	aggregate_layer14_gpu13 -> mha_layer15_gpu13
	aggregate_layer14_gpu14 -> mha_layer15_gpu14
	aggregate_layer14_gpu15 -> mha_layer15_gpu15
	mha_layer15_gpu0 -> gate_layer15_gpu0
	mha_layer15_gpu1 -> gate_layer15_gpu1
	mha_layer15_gpu2 -> gate_layer15_gpu2
	mha_layer15_gpu3 -> gate_layer15_gpu3
	mha_layer15_gpu4 -> gate_layer15_gpu4
	mha_layer15_gpu5 -> gate_layer15_gpu5
	mha_layer15_gpu6 -> gate_layer15_gpu6
	mha_layer15_gpu7 -> gate_layer15_gpu7
	mha_layer15_gpu8 -> gate_layer15_gpu8
	mha_layer15_gpu9 -> gate_layer15_gpu9
	mha_layer15_gpu10 -> gate_layer15_gpu10
	mha_layer15_gpu11 -> gate_layer15_gpu11
	mha_layer15_gpu12 -> gate_layer15_gpu12
	mha_layer15_gpu13 -> gate_layer15_gpu13
	mha_layer15_gpu14 -> gate_layer15_gpu14
	mha_layer15_gpu15 -> gate_layer15_gpu15
	gate_layer15_gpu0 -> route_layer15_gpu0 [style=dashed]
	gate_layer15_gpu1 -> route_layer15_gpu1 [style=dashed]
	gate_layer15_gpu2 -> route_layer15_gpu2 [style=dashed]
	gate_layer15_gpu3 -> route_layer15_gpu3 [style=dashed]
	gate_layer15_gpu4 -> route_layer15_gpu4 [style=dashed]
	gate_layer15_gpu5 -> route_layer15_gpu5 [style=dashed]
	gate_layer15_gpu6 -> route_layer15_gpu6 [style=dashed]
	gate_layer15_gpu7 -> route_layer15_gpu7 [style=dashed]
	gate_layer15_gpu8 -> route_layer15_gpu8 [style=dashed]
	gate_layer15_gpu9 -> route_layer15_gpu9 [style=dashed]
	gate_layer15_gpu10 -> route_layer15_gpu10 [style=dashed]
	gate_layer15_gpu11 -> route_layer15_gpu11 [style=dashed]
	gate_layer15_gpu12 -> route_layer15_gpu12 [style=dashed]
	gate_layer15_gpu13 -> route_layer15_gpu13 [style=dashed]
	gate_layer15_gpu14 -> route_layer15_gpu14 [style=dashed]
	gate_layer15_gpu15 -> route_layer15_gpu15 [style=dashed]
	route_layer15_gpu0 -> comm_layer15
	route_layer15_gpu1 -> comm_layer15
	route_layer15_gpu2 -> comm_layer15
	route_layer15_gpu3 -> comm_layer15
	route_layer15_gpu4 -> comm_layer15
	route_layer15_gpu5 -> comm_layer15
	route_layer15_gpu6 -> comm_layer15
	route_layer15_gpu7 -> comm_layer15
	route_layer15_gpu8 -> comm_layer15
	route_layer15_gpu9 -> comm_layer15
	route_layer15_gpu10 -> comm_layer15
	route_layer15_gpu11 -> comm_layer15
	route_layer15_gpu12 -> comm_layer15
	route_layer15_gpu13 -> comm_layer15
	route_layer15_gpu14 -> comm_layer15
	route_layer15_gpu15 -> comm_layer15
	comm_layer15 -> expert_layer15_gpu0_expert0
	comm_layer15 -> expert_layer15_gpu1_expert1
	comm_layer15 -> expert_layer15_gpu2_expert2
	comm_layer15 -> expert_layer15_gpu3_expert3
	comm_layer15 -> expert_layer15_gpu4_expert4
	comm_layer15 -> expert_layer15_gpu5_expert5
	comm_layer15 -> expert_layer15_gpu6_expert6
	comm_layer15 -> expert_layer15_gpu7_expert7
	comm_layer15 -> expert_layer15_gpu8_expert8
	comm_layer15 -> expert_layer15_gpu9_expert9
	comm_layer15 -> expert_layer15_gpu10_expert10
	comm_layer15 -> expert_layer15_gpu11_expert11
	comm_layer15 -> expert_layer15_gpu12_expert12
	comm_layer15 -> expert_layer15_gpu13_expert13
	comm_layer15 -> expert_layer15_gpu14_expert14
	comm_layer15 -> expert_layer15_gpu15_expert15
	expert_layer15_gpu0_expert0 -> aggregate_layer15_gpu0
	expert_layer15_gpu1_expert1 -> aggregate_layer15_gpu1
	expert_layer15_gpu2_expert2 -> aggregate_layer15_gpu2
	expert_layer15_gpu3_expert3 -> aggregate_layer15_gpu3
	expert_layer15_gpu4_expert4 -> aggregate_layer15_gpu4
	expert_layer15_gpu5_expert5 -> aggregate_layer15_gpu5
	expert_layer15_gpu6_expert6 -> aggregate_layer15_gpu6
	expert_layer15_gpu7_expert7 -> aggregate_layer15_gpu7
	expert_layer15_gpu8_expert8 -> aggregate_layer15_gpu8
	expert_layer15_gpu9_expert9 -> aggregate_layer15_gpu9
	expert_layer15_gpu10_expert10 -> aggregate_layer15_gpu10
	expert_layer15_gpu11_expert11 -> aggregate_layer15_gpu11
	expert_layer15_gpu12_expert12 -> aggregate_layer15_gpu12
	expert_layer15_gpu13_expert13 -> aggregate_layer15_gpu13
	expert_layer15_gpu14_expert14 -> aggregate_layer15_gpu14
	expert_layer15_gpu15_expert15 -> aggregate_layer15_gpu15
	aggregate_layer15_gpu0 -> output
	aggregate_layer15_gpu1 -> output
	aggregate_layer15_gpu2 -> output
	aggregate_layer15_gpu3 -> output
	aggregate_layer15_gpu4 -> output
	aggregate_layer15_gpu5 -> output
	aggregate_layer15_gpu6 -> output
	aggregate_layer15_gpu7 -> output
	aggregate_layer15_gpu8 -> output
	aggregate_layer15_gpu9 -> output
	aggregate_layer15_gpu10 -> output
	aggregate_layer15_gpu11 -> output
	aggregate_layer15_gpu12 -> output
	aggregate_layer15_gpu13 -> output
	aggregate_layer15_gpu14 -> output
	aggregate_layer15_gpu15 -> output
}
