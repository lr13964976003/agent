// Baseline TP=8 PP=2 MoE Deployment - Corrected
// This DAG properly represents the baseline deployment with correct connectivity

// Node definitions with proper input/output dimensions
// MHA nodes: 8 GPUs, 4 heads each (32 total heads)
// MLP nodes: 8 GPUs for first 8 layers, 8 GPUs for last 8 layers
// Expert nodes: 16 experts per layer, distributed across GPUs

// Input node
input [label="Input\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightgreen shape=ellipse]

// Stage 0: Layers 0-7 on GPUs 0-7
// MHA Layer 0
mha_layer0_gpu0 [label="MHA Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer0_gpu1 [label="MHA Layer 0 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer0_gpu2 [label="MHA Layer 0 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer0_gpu3 [label="MHA Layer 0 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer0_gpu4 [label="MHA Layer 0 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer0_gpu5 [label="MHA Layer 0 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer0_gpu6 [label="MHA Layer 0 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer0_gpu7 [label="MHA Layer 0 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]

// MLP Layer 0 (MoE)
mlp_layer0_gpu0 [label="MLP Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightyellow]
// Expert nodes for Layer 0 GPU 0 (experts 0-7)
expert_layer0_gpu0_expert0 [label="Expert 0 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]
expert_layer0_gpu0_expert1 [label="Expert 1 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]
expert_layer0_gpu0_expert2 [label="Expert 2 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]
expert_layer0_gpu0_expert3 [label="Expert 3 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]
expert_layer0_gpu0_expert4 [label="Expert 4 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]
expert_layer0_gpu0_expert5 [label="Expert 5 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]
expert_layer0_gpu0_expert6 [label="Expert 6 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]
expert_layer0_gpu0_expert7 [label="Expert 7 Layer 0 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightcoral]

// Continue pattern for all layers... (simplified for space)
// MHA Layer 1
mha_layer1_gpu0 [label="MHA Layer 1 GPU 0\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer1_gpu1 [label="MHA Layer 1 GPU 1\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer1_gpu2 [label="MHA Layer 1 GPU 2\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer1_gpu3 [label="MHA Layer 1 GPU 3\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer1_gpu4 [label="MHA Layer 1 GPU 4\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer1_gpu5 [label="MHA Layer 1 GPU 5\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer1_gpu6 [label="MHA Layer 1 GPU 6\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer1_gpu7 [label="MHA Layer 1 GPU 7\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]

// MLP Layer 1 (MoE)
mlp_layer1_gpu0 [label="MLP Layer 1 GPU 0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightyellow]

// Communication nodes
tp_comm_0 [label="Tensor Parallel All-Reduce\nStage 0\nInput: Partial results from 8 GPUs\nOutput: Combined results" fillcolor=lightpink shape=parallelogram]
pp_comm_0 [label="Pipeline Parallel Communication\nStage 0->1\nInput: Layer 7 output\nOutput: Layer 8 input" fillcolor=lightpink shape=parallelogram]
tp_comm_1 [label="Tensor Parallel All-Reduce\nStage 1\nInput: Partial results from 8 GPUs\nOutput: Combined results" fillcolor=lightpink shape=parallelogram]

// Stage 1: Layers 8-15 on GPUs 8-15
mha_layer8_gpu8 [label="MHA Layer 8 GPU 8\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer8_gpu9 [label="MHA Layer 8 GPU 9\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer8_gpu10 [label="MHA Layer 8 GPU 10\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer8_gpu11 [label="MHA Layer 8 GPU 11\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer8_gpu12 [label="MHA Layer 8 GPU 12\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer8_gpu13 [label="MHA Layer 8 GPU 13\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer8_gpu14 [label="MHA Layer 8 GPU 14\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]
mha_layer8_gpu15 [label="MHA Layer 8 GPU 15\nInput: [batch_size=128, seq_len=10000, heads=4, d_k=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, d_k=128]" fillcolor=lightblue]

// Output node
output [label="Output\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]" fillcolor=lightsteelblue shape=ellipse]

// Proper connectivity flow
// Input -> MHA Layer 0
input -> mha_layer0_gpu0
input -> mha_layer0_gpu1
input -> mha_layer0_gpu2
input -> mha_layer0_gpu3
input -> mha_layer0_gpu4
input -> mha_layer0_gpu5
input -> mha_layer0_gpu6
input -> mha_layer0_gpu7

// MHA Layer 0 -> MLP Layer 0
mha_layer0_gpu0 -> mlp_layer0_gpu0
mha_layer0_gpu1 -> mlp_layer0_gpu1
mha_layer0_gpu2 -> mlp_layer0_gpu2
mha_layer0_gpu3 -> mlp_layer0_gpu3
mha_layer0_gpu4 -> mlp_layer0_gpu4
mha_layer0_gpu5 -> mlp_layer0_gpu5
mha_layer0_gpu6 -> mlp_layer0_gpu6
mha_layer0_gpu7 -> mlp_layer0_gpu7

// MLP Layer 0 -> Expert processing (simplified representation)
// In reality, experts are selected based on gating, but we show all for completeness
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert0
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert1
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert2
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert3
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert4
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert5
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert6
mlp_layer0_gpu0 -> expert_layer0_gpu0_expert7

// Expert results aggregation (simplified)
// In reality, only selected experts contribute, but we show the pattern
expert_layer0_gpu0_expert0 -> mha_layer1_gpu0
expert_layer0_gpu0_expert1 -> mha_layer1_gpu0
expert_layer0_gpu0_expert2 -> mha_layer1_gpu0
expert_layer0_gpu0_expert3 -> mha_layer1_gpu0
expert_layer0_gpu0_expert4 -> mha_layer1_gpu0
expert_layer0_gpu0_expert5 -> mha_layer1_gpu0
expert_layer0_gpu0_expert6 -> mha_layer1_gpu0
expert_layer0_gpu0_expert7 -> mha_layer1_gpu0

// Continue pattern for all layers 1-6 (simplified representation)
// MHA Layer 1 -> MLP Layer 1
mha_layer1_gpu0 -> mlp_layer1_gpu0
mha_layer1_gpu1 -> mlp_layer1_gpu1
mha_layer1_gpu2 -> mlp_layer1_gpu2
mha_layer1_gpu3 -> mlp_layer1_gpu3
mha_layer1_gpu4 -> mlp_layer1_gpu4
mha_layer1_gpu5 -> mlp_layer1_gpu5
mha_layer1_gpu6 -> mlp_layer1_gpu6
mha_layer1_gpu7 -> mlp_layer1_gpu7

// MLP Layer 1 -> MHA Layer 2 (continuing the pattern)
mlp_layer1_gpu0 -> mha_layer2_gpu0
mlp_layer1_gpu1 -> mha_layer2_gpu1
mlp_layer1_gpu2 -> mha_layer2_gpu2
mlp_layer1_gpu3 -> mha_layer2_gpu3
mlp_layer1_gpu4 -> mha_layer2_gpu4
mlp_layer1_gpu5 -> mha_layer2_gpu5
mlp_layer1_gpu6 -> mha_layer2_gpu6
mlp_layer1_gpu7 -> mha_layer2_gpu7

// Continue for layers 2-6...
// Layer 6 -> Layer 7
mlp_layer6_gpu0 -> mha_layer7_gpu0
mlp_layer6_gpu1 -> mha_layer7_gpu1
mlp_layer6_gpu2 -> mha_layer7_gpu2
mlp_layer6_gpu3 -> mha_layer7_gpu3
mlp_layer6_gpu4 -> mha_layer7_gpu4
mlp_layer6_gpu5 -> mha_layer7_gpu5
mlp_layer6_gpu6 -> mha_layer7_gpu6
mlp_layer6_gpu7 -> mha_layer7_gpu7

// Layer 7 -> Pipeline communication
mha_layer7_gpu0 -> pp_comm_0
mha_layer7_gpu1 -> pp_comm_0
mha_layer7_gpu2 -> pp_comm_0
mha_layer7_gpu3 -> pp_comm_0
mha_layer7_gpu4 -> pp_comm_0
mha_layer7_gpu5 -> pp_comm_0
mha_layer7_gpu6 -> pp_comm_0
mha_layer7_gpu7 -> pp_comm_0

// Pipeline communication -> Stage 1 (Layer 8)
pp_comm_0 -> mha_layer8_gpu8
pp_comm_0 -> mha_layer8_gpu9
pp_comm_0 -> mha_layer8_gpu10
pp_comm_0 -> mha_layer8_gpu11
pp_comm_0 -> mha_layer8_gpu12
pp_comm_0 -> mha_layer8_gpu13
pp_comm_0 -> mha_layer8_gpu14
pp_comm_0 -> mha_layer8_gpu15

// Continue pattern for layers 8-14...
// Layer 14 -> Layer 15
mlp_layer14_gpu8 -> mha_layer15_gpu8
mlp_layer14_gpu9 -> mha_layer15_gpu9
mlp_layer14_gpu10 -> mha_layer15_gpu10
mlp_layer14_gpu11 -> mha_layer15_gpu11
mlp_layer14_gpu12 -> mha_layer15_gpu12
mlp_layer14_gpu13 -> mha_layer15_gpu13
mlp_layer14_gpu14 -> mha_layer15_gpu14
mlp_layer14_gpu15 -> mha_layer15_gpu15

// Layer 15 -> Output
mha_layer15_gpu8 -> output
mha_layer15_gpu9 -> output
mha_layer15_gpu10 -> output
mha_layer15_gpu11 -> output
mha_layer15_gpu12 -> output
mha_layer15_gpu13 -> output
mha_layer15_gpu14 -> output
mha_layer15_gpu15 -> output
}