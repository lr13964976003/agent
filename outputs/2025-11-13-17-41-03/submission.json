{
  "baseline_dag": {
    "dot_file_path": "../outputs/2025-11-13-17-41-03/baseline_dag.dot",
    "svg_file_path": "../outputs/2025-11-13-17-41-03/baseline_dag.svg",
    "description": "Transformer with Tensor Parallelism (TP=8) + Pipeline Parallelism (PP=2)",
    "total_devices": 16,
    "parallel_strategy": "tensor_parallel_plus_pipeline",
    "device_allocation": {
      "pipeline_stage_0": "GPUs 0-7 (Layers 0-1)",
      "pipeline_stage_1": "GPUs 8-15 (Layers 2-3)"
    },
    "tensor_splits": {
      "attention_qkv": "column_parallel_split=8",
      "attention_output": "row_parallel_split=8",
      "mlp_gate_up": "column_parallel_split=8",
      "mlp_down": "row_parallel_split=8"
    }
  },
  "proposed_dag": {
    "dot_file_path": "../outputs/2025-11-13-17-41-03/proposed_dag.dot",
    "svg_file_path": "../outputs/2025-11-13-17-41-03/proposed_dag.svg",
    "description": "Transformer with Ring Attention + Sequence Parallelism",
    "total_devices": 16,
    "parallel_strategy": "ring_attention_plus_sequence_parallelism",
    "sequence_partition": {
      "tokens_per_device": 625,
      "total_sequence_length": 10000,
      "partition_scheme": "sequence_dimension_split"
    },
    "ring_topology": "0→1→2→...→15→0",
    "kv_exchange_stages": 16
  },
  "file_locations": [
    "../outputs/2025-11-13-17-41-03/baseline_dag.dot",
    "../outputs/2025-11-13-17-41-03/baseline_dag.svg",
    "../outputs/2025-11-13-17-41-03/proposed_dag.dot",
    "../outputs/2025-11-13-17-41-03/proposed_dag.svg",
    "../outputs/2025-11-13-17-41-03/generate_dags.py"
  ]
}