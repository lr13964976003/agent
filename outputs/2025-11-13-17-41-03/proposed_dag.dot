digraph G {
    rankdir=TB;
    node [shape=rectangle, style=filled];
    
    // Title
    label="Proposed: Ring Attention + Sequence Parallelism (16 devices)";
    labelloc="t";
    fontsize=24;
    
    // Input node
    input [shape=ellipse, label="Input
Input: [batch=1024, seq=10000, hidden=8192]
GPU: Host"];
    
    // Sequence split across devices
    sequence_split [shape=parallelogram, label="Sequence Split
Input: [batch=1024, seq=10000, hidden=8192]
Output: [batch=1024, seq=625, hidden=8192] x 16
GPU: All 16"];
    
    // ===== Layer 0 =====
    subgraph cluster_layer0 {
        label="Layer 0 (All Devices)";
        style=dashed;
        
        // Loop through all devices
        subgraph cluster_device0 {
            label="GPU 0 (Seq 0-624)";
            style=dotted;
            
            // Embedding for device 0
            emb0 [label="Embedding
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            
            // Layer Norm
            ln0_0 [label="LayerNorm
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            
            // QKV projections (full parameters, local computation)
            q_proj0_0 [label="Q Projection
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            k_proj0_0 [label="K Projection
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            v_proj0_0 [label="V Projection
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            
            // Ring attention components
            ring_attn0_0 [label="Local Q Computation
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, heads=16, dim=512]
GPU: 0"];
            
            // KV ring exchange for 16 stages
            kv_send0_0 [shape=ellipse, label="Send KV
Input: [batch=1024, seq=625, hidden=8192]
Output: → GPU 1
GPU: 0"];
            kv_recv0_0 [shape=ellipse, label="Recv KV
Input: ← GPU 15
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            
            // Attention computation
            attn0_0 [label="Attention
Input: Q[batch=1024, seq=625, heads=16, dim=512]
       K[batch=1024, seq=625, heads=16, dim=512]
       V[batch=1024, seq=625, heads=16, dim=512]
Output: [batch=1024, seq=625, heads=16, dim=512]
GPU: 0"];
            
            // Output projection
            out_proj0_0 [label="Output Projection
Input: [batch=1024, seq=625, heads=16, dim=512]
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            
            // Residual
            residual0_0 [shape=parallelogram, label="Add
Input: [batch=1024, seq=625, hidden=8192] x2
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            
            // MLP
            gate0_0 [label="Gate Projection
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, hidden=32768]
GPU: 0"];
            up0_0 [label="Up Projection
Input: [batch=1024, seq=625, hidden=8192]
Output: [batch=1024, seq=625, hidden=32768]
GPU: 0"];
            silu0_0 [label="SiLU
Input: [batch=1024, seq=625, hidden=32768]
Output: [batch=1024, seq=625, hidden=32768]
GPU: 0"];
            mul0_0 [label="Elementwise Mul
Input: [batch=1024, seq=625, hidden=32768] x2
Output: [batch=1024, seq=625, hidden=32768]
GPU: 0"];
            down0_0 [label="Down Projection
Input: [batch=1024, seq=625, hidden=32768]
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
            residual_mlp0_0 [shape=parallelogram, label="Add
Input: [batch=1024, seq=625, hidden=8192] x2
Output: [batch=1024, seq=625, hidden=8192]
GPU: 0"];
        }
        
        // Repeat for device 1 (simplified representation)
        device1_rep [label="...
GPU 1-15
(Same structure as GPU 0)"];
    }
    
    // Ring topology connections
    ring_topology [shape=ellipse, label="Ring Topology
KV exchange in 16 stages
GPU 0→1→2→...→15→0"];
    
    // ===== Layer 1 (repeated for all layers) =====
    subgraph cluster_layer1_all {
        label="Layer 1-3 (All Devices - Same Structure)";
        style=dashed;
        
        layer1_rep [label="Layers 1-3
Same structure repeated
16 devices, 625 tokens each"];
    }
    
    // Sequence gather
    sequence_gather [shape=parallelogram, label="Sequence Gather
Input: [batch=1024, seq=625, hidden=8192] x 16
Output: [batch=1024, seq=10000, hidden=8192]
GPU: All 16"];
    
    // Final output
    output [shape=ellipse, label="Output
Input: [batch=1024, seq=10000, hidden=8192]
Output: [batch=1024, seq=10000, vocab_size]
GPU: All 16"];
    
    // Connections
    input -> sequence_split;
    
    // Device 0 connections
    sequence_split -> emb0;
    emb0 -> ln0_0;
    ln0_0 -> q_proj0_0;
    ln0_0 -> k_proj0_0;
    ln0_0 -> v_proj0_0;
    
    q_proj0_0 -> ring_attn0_0;
    k_proj0_0 -> kv_send0_0;
    v_proj0_0 -> kv_send0_0;
    
    kv_recv0_0 -> attn0_0;
    ring_attn0_0 -> attn0_0;
    
    attn0_0 -> out_proj0_0;
    out_proj0_0 -> residual0_0;
    emb0 -> residual0_0;
    
    residual0_0 -> gate0_0;
    residual0_0 -> up0_0;
    gate0_0 -> silu0_0;
    up0_0 -> mul0_0;
    silu0_0 -> mul0_0;
    mul0_0 -> down0_0;
    down0_0 -> residual_mlp0_0;
    residual0_0 -> residual_mlp0_0;
    
    // Connect to layer representation
    residual_mlp0_0 -> layer1_rep;
    layer1_rep -> sequence_gather;
    sequence_gather -> output;
    
    // Ring topology visualization
    kv_send0_0 -> ring_topology [style=dashed];
    ring_topology -> kv_recv0_0 [style=dashed];
}
