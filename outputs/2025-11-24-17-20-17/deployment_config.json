{
  "deployment_configurations": {
    "baseline": {
      "name": "Baseline Tensor+Pipeline Parallelism",
      "description": "Traditional TP=8 + PP=2 deployment",
      "parallel_strategy": {
        "type": "tensor_parallelism_plus_pipeline_parallelism",
        "tensor_parallelism_degree": 8,
        "pipeline_parallelism_degree": 2,
        "total_devices": 16
      },
      "model_parameters": {
        "layers": 4,
        "hidden_size": 4096,
        "num_attention_heads": 32,
        "attention_head_size": 128,
        "sequence_length": 10000,
        "batch_size": 128,
        "mlp_hidden_size": 16384
      },
      "device_mapping": {
        "pipeline_stages": {
          "stage_0": {
            "devices": [0, 1, 2, 3, 4, 5, 6, 7],
            "tensor_parallel_group": "tp_group_0",
            "layers": [0, 1]
          },
          "stage_1": {
            "devices": [8, 9, 10, 11, 12, 13, 14, 15],
            "tensor_parallel_group": "tp_group_1",
            "layers": [2, 3]
          }
        },
        "tensor_parallel_distribution": {
          "attention_weights": {
            "qkv_projection": {
              "partitioning": "column_wise",
              "per_device_dimension": 512,
              "devices_per_head": 0.25
            },
            "output_projection": {
              "partitioning": "row_wise",
              "per_device_dimension": 512
            }
          },
          "mlp_weights": {
            "gate_up_projection": {
              "partitioning": "column_wise",
              "per_device_dimension": 2048
            },
            "down_projection": {
              "partitioning": "row_wise",
              "per_device_dimension": 2048
            }
          }
        }
      }
    },
    "helix_proposed": {
      "name": "Helix Two-Level Attention Partitioning",
      "description": "Novel m√ón partitioning combining head and dimension splits",
      "parallel_strategy": {
        "type": "two_level_attention_partitioning",
        "head_partitioning": {
          "n": 4,
          "heads_per_group": 8,
          "total_head_groups": 4
        },
        "dimension_partitioning": {
          "m": 4,
          "dimension_per_slice": 32,
          "total_slices": 4
        },
        "total_partitions": 16,
        "total_devices": 16,
        "partition_to_device_mapping": "one_to_one"
      },
      "model_parameters": {
        "layers": 4,
        "hidden_size": 4096,
        "num_attention_heads": 32,
        "attention_head_size": 128,
        "sequence_length": 10000,
        "batch_size": 128,
        "mlp_hidden_size": 16384
      },
      "attention_partitioning_details": {
        "query_projection": {
          "weight_matrix_shape": [4096, 4096],
          "partition_count": 16,
          "per_partition_shape": [1024, 1024],
          "partition_structure": "head_group_dim_slice",
          "heads_per_partition": 8,
          "dimensions_per_partition": 32
        },
        "key_projection": {
          "weight_matrix_shape": [4096, 4096],
          "partition_count": 16,
          "per_partition_shape": [1024, 1024],
          "partition_structure": "head_group_dim_slice",
          "heads_per_partition": 8,
          "dimensions_per_partition": 32
        },
        "value_projection": {
          "weight_matrix_shape": [4096, 4096],
          "partition_count": 16,
          "per_partition_shape": [1024, 1024],
          "partition_structure": "head_group_dim_slice",
          "heads_per_partition": 8,
          "dimensions_per_partition": 32
        },
        "output_projection": {
          "weight_matrix_shape": [4096, 4096],
          "partition_count": 16,
          "per_partition_shape": [1024, 1024],
          "partition_structure": "head_group_dim_slice",
          "heads_per_partition": 8,
          "dimensions_per_partition": 32
        }
      },
      "device_mapping": {
        "grid_mapping": {
          "rows": 4,
          "cols": 4,
          "mapping_strategy": "head_group_major"
        },
        "device_assignments": {
          "device_0": {"head_group": 0, "dimension_slice": 0, "partition_id": "(0,0)"},
          "device_1": {"head_group": 0, "dimension_slice": 1, "partition_id": "(0,1)"},
          "device_2": {"head_group": 0, "dimension_slice": 2, "partition_id": "(0,2)"},
          "device_3": {"head_group": 0, "dimension_slice": 3, "partition_id": "(0,3)"},
          "device_4": {"head_group": 1, "dimension_slice": 0, "partition_id": "(1,0)"},
          "device_5": {"head_group": 1, "dimension_slice": 1, "partition_id": "(1,1)"},
          "device_6": {"head_group": 1, "dimension_slice": 2, "partition_id": "(1,2)"},
          "device_7": {"head_group": 1, "dimension_slice": 3, "partition_id": "(1,3)"},
          "device_8": {"head_group": 2, "dimension_slice": 0, "partition_id": "(2,0)"},
          "device_9": {"head_group": 2, "dimension_slice": 1, "partition_id": "(2,1)"},
          "device_10": {"head_group": 2, "dimension_slice": 2, "partition_id": "(2,2)"},
          "device_11": {"head_group": 2, "dimension_slice": 3, "partition_id": "(2,3)"},
          "device_12": {"head_group": 3, "dimension_slice": 0, "partition_id": "(3,0)"},
          "device_13": {"head_group": 3, "dimension_slice": 1, "partition_id": "(3,1)"},
          "device_14": {"head_group": 3, "dimension_slice": 2, "partition_id": "(3,2)"},
          "device_15": {"head_group": 3, "dimension_slice": 3, "partition_id": "(3,3)"}
        }
      },
      "communication_pattern": {
        "input_distribution": {
          "type": "broadcast",
          "source": "host",
          "target": "all_devices",
          "tensor_shape": [128, 10000, 4096]
        },
        "intra_group_concatenation": {
          "groups": [
            {"name": "head_group_0", "devices": [0, 1, 2, 3], "concat_dimension": 3},
            {"name": "head_group_1", "devices": [4, 5, 6, 7], "concat_dimension": 3},
            {"name": "head_group_2", "devices": [8, 9, 10, 11], "concat_dimension": 3},
            {"name": "head_group_3", "devices": [12, 13, 14, 15], "concat_dimension": 3}
          ]
        },
        "final_concatenation": {
          "type": "hierarchical",
          "order": "head_groups_then_dimensions",
          "final_output_shape": [128, 10000, 4096]
        }
      },
      "memory_requirements": {
        "parameters_per_device": 1048576,
        "activations_per_device": 40960000,
        "total_memory_reduction": 16
      }
    }
  },
  "performance_comparison": {
    "throughput_improvement": {
      "baseline_tps": 1200000,
      "proposed_tps": 1580000,
      "absolute_gain": 380000,
      "percentage_gain": 31.7
    },
    "communication_overhead_reduction": {
      "baseline_tpot_ms": 0.35,
      "proposed_tpot_ms": 0.22,
      "absolute_reduction_ms": 0.13,
      "percentage_reduction": 37.1
    }
  },
  "deployment_requirements": {
    "minimum_gpus": 16,
    "precision": "FP16",
    "framework_support": ["PyTorch", "TensorFlow", "JAX"],
    "network_requirements": {
      "intra_node_bandwidth": "high",
      "inter_node_bandwidth": "high",
      "communication_primitive": "NCCL"
    }
  }
}