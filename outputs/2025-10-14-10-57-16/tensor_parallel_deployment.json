{
  "deployment_strategy": "tensor_parallel_with_pipeline",
  "total_gpus": 16,
  "pipeline_stages": 2,
  "tensor_parallel_size": 8,
  "layers_per_stage": 8,
  "gpu_mapping": {
    "stage_0": {
      "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
      "layers": [0, 1, 2, 3, 4, 5, 6, 7]
    },
    "stage_1": {
      "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
      "layers": [8, 9, 10, 11, 12, 13, 14, 15]
    }
  },
  "tensor_parallel_config": {
    "attention": {
      "qkv_projection": "column_parallel",
      "output_projection": "row_parallel",
      "head_parallel": true
    },
    "mlp": {
      "linear1": "column_parallel",
      "linear2": "row_parallel"
    },
    "communication": {
      "attention_all_reduce": true,
      "mlp_all_reduce": true,
      "pipeline_communication": "point_to_point"
    }
  },
  "memory_optimization": {
    "activation_checkpointing": true,
    "sequence_parallel": true,
    "gradient_accumulation": 4
  }
}