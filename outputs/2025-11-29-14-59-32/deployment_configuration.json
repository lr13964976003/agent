{
  "deployment_configurations": {
    "baseline_tensor_pipeline_parallelism": {
      "parallel_strategy": {
        "type": "hybrid_tensor_pipeline_parallelism",
        "tensor_parallelism": 8,
        "pipeline_parallelism": 2,
        "total_gpus": 16
      },
      "model_partitions": {
        "pipeline_stage_0": {
          "gpus": [0, 1, 2, 3, 4, 5, 6, 7],
          "tensor_parallel_size": 8,
          "layers": [0, 1, 2, 3, 4, 5, 6, 7],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 117.84
        },
        "pipeline_stage_1": {
          "gpus": [8, 9, 10, 11, 12, 13, 14, 15],
          "tensor_parallel_size": 8,
          "layers": [8, 9, 10, 11, 12, 13, 14, 15],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 117.84
        }
      },
      "device_mapping": {
        "gpu_0": {"pipeline_stage": 0, "tensor_parallel_rank": 0, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_1": {"pipeline_stage": 0, "tensor_parallel_rank": 1, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_2": {"pipeline_stage": 0, "tensor_parallel_rank": 2, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_3": {"pipeline_stage": 0, "tensor_parallel_rank": 3, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_4": {"pipeline_stage": 0, "tensor_parallel_rank": 4, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_5": {"pipeline_stage": 0, "tensor_parallel_rank": 5, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_6": {"pipeline_stage": 0, "tensor_parallel_rank": 6, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_7": {"pipeline_stage": 0, "tensor_parallel_rank": 7, "layers": [0, 1, 2, 3, 4, 5, 6, 7]},
        "gpu_8": {"pipeline_stage": 1, "tensor_parallel_rank": 0, "layers": [8, 9, 10, 11, 12, 13, 14, 15]},
        "gpu_9": {"pipeline_stage": 1, "tensor_parallel_rank": 1, "layers": [8, 9, 10, 11, 12, 13, 14, 15]},
        "gpu_10": {"pipeline_stage": 1, "tensor_parallel_rank": 2, "layers": [8, 9, 10, 11, 12, 13, 14, 15]},
        "gpu_11": {"pipeline_stage": 1, "tensor_parallel_rank": 3, "layers": [8, 9, 10, 11, 12, 13, 14, 15]},
        "gpu_12": {"pipeline_stage": 1, "tensor_parallel_rank": 4, "layers": [8, 9, 10, 11, 12, 13, 14, 15]},
        "gpu_13": {"pipeline_stage": 1, "tensor_parallel_rank": 5, "layers": [8, 9, 10, 11, 12, 13, 14, 15]},
        "gpu_14": {"pipeline_stage": 1, "tensor_parallel_rank": 6, "layers": [8, 9, 10, 11, 12, 13, 14, 15]},
        "gpu_15": {"pipeline_stage": 1, "tensor_parallel_rank": 7, "layers": [8, 9, 10, 11, 12, 13, 14, 15]}
      },
      "memory_breakdown": {
        "weights_per_layer_gb": 3.75,
        "activations_per_layer_gb": 10.48,
        "buffers_per_layer_gb": 0.5,
        "total_per_layer_gb": 14.73
      },
      "communication": {
        "tensor_parallel_communication": "all_reduce",
        "pipeline_parallel_communication": "send_receive"
      }
    },
    "proposed_layer_wise": {
      "parallel_strategy": {
        "type": "layer_wise_partitioning",
        "method": "greedy_layer_aggregation",
        "cache_constraint": "sram_l2_cache_capacity",
        "total_gpus": 16,
        "partitioning_algorithm": "contiguous_layer_assignment"
      },
      "model_partitions": {
        "partition_0": {
          "gpu": 0,
          "layers": [0, 1],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        },
        "partition_1": {
          "gpu": 1,
          "layers": [2, 3],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        },
        "partition_2": {
          "gpu": 2,
          "layers": [4, 5],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        },
        "partition_3": {
          "gpu": 3,
          "layers": [6, 7],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        },
        "partition_4": {
          "gpu": 4,
          "layers": [8, 9],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        },
        "partition_5": {
          "gpu": 5,
          "layers": [10, 11],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        },
        "partition_6": {
          "gpu": 6,
          "layers": [12, 13],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        },
        "partition_7": {
          "gpu": 7,
          "layers": [14, 15],
          "memory_per_layer_gb": 14.73,
          "total_memory_gb": 29.46,
          "cache_requirement_gb": 29.46
        }
      },
      "device_mapping": {
        "gpu_0": {"partition_id": 0, "layers": [0, 1], "cache_allocation_gb": 29.46},
        "gpu_1": {"partition_id": 1, "layers": [2, 3], "cache_allocation_gb": 29.46},
        "gpu_2": {"partition_id": 2, "layers": [4, 5], "cache_allocation_gb": 29.46},
        "gpu_3": {"partition_id": 3, "layers": [6, 7], "cache_allocation_gb": 29.46},
        "gpu_4": {"partition_id": 4, "layers": [8, 9], "cache_allocation_gb": 29.46},
        "gpu_5": {"partition_id": 5, "layers": [10, 11], "cache_allocation_gb": 29.46},
        "gpu_6": {"partition_id": 6, "layers": [12, 13], "cache_allocation_gb": 29.46},
        "gpu_7": {"partition_id": 7, "layers": [14, 15], "cache_allocation_gb": 29.46},
        "gpu_8-15": {"status": "unused_in_this_configuration"}
      },
      "memory_breakdown": {
        "weights_per_layer_gb": 3.75,
        "activations_per_layer_gb": 10.48,
        "buffers_per_layer_gb": 0.5,
        "total_per_layer_gb": 14.73,
        "cache_utilization_per_gpu_gb": 29.46
      },
      "execution_flow": {
        "stage_0": {"gpu": 0, "layers": [0, 1], "input": "external", "output": "to_gpu_1"},
        "stage_1": {"gpu": 1, "layers": [2, 3], "input": "from_gpu_0", "output": "to_gpu_2"},
        "stage_2": {"gpu": 2, "layers": [4, 5], "input": "from_gpu_1", "output": "to_gpu_3"},
        "stage_3": {"gpu": 3, "layers": [6, 7], "input": "from_gpu_2", "output": "to_gpu_4"},
        "stage_4": {"gpu": 4, "layers": [8, 9], "input": "from_gpu_3", "output": "to_gpu_5"},
        "stage_5": {"gpu": 5, "layers": [10, 11], "input": "from_gpu_4", "output": "to_gpu_6"},
        "stage_6": {"gpu": 6, "layers": [12, 13], "input": "from_gpu_5", "output": "to_gpu_7"},
        "stage_7": {"gpu": 7, "layers": [14, 15], "input": "from_gpu_6", "output": "external"}
      },
      "communication": {
        "type": "point_to_point_send_receive",
        "bandwidth_requirement": "high_between_partitions",
        "latency_optimization": "minimized_transfers"
      },
      "performance_targets": {
        "tps_improvement": "20%",
        "tpot_reduction": "17%",
        "baseline_tps": 12800,
        "target_tps": 15360,
        "baseline_tpot_ms": 0.078,
        "target_tpot_ms": 0.065
      }
    }
  },
  "model_specifications": {
    "total_layers": 16,
    "total_parameters": "30B",
    "parameters_per_layer": "1.875B",
    "precision": "BF16",
    "batch_size": 128,
    "sequence_length": 10000,
    "num_heads": 32,
    "head_dimension": 128,
    "mlp_hidden_size": 16384,
    "hidden_dimension": 4096
  },
  "hardware_requirements": {
    "gpu_type": "NVIDIA_H100",
    "total_gpus": 16,
    "minimum_cache_per_gpu_gb": 29.46,
    "memory_bandwidth": "high",
    "inter_gpu_bandwidth": "high"
  }
}