{
  "submission_paths": {
    "baseline_tensor_pipeline_dag": {
      "dot_file": "../outputs/2025-11-29-14-59-32/baseline_tensor_pipeline_dag.dot",
      "svg_file": "../outputs/2025-11-29-14-59-32/baseline_tensor_pipeline_dag.svg"
    },
    "proposed_layer_wise_dag": {
      "dot_file": "../outputs/2025-11-29-14-59-32/proposed_layer_wise_dag.dot",
      "svg_file": "../outputs/2025-11-29-14-59-32/proposed_layer_wise_dag.svg"
    }
  },
  "dag_validation": {
    "baseline_dag": {
      "has_cycle": false,
      "total_edges": 848,
      "nodes_with_only_in_degree": 9,
      "nodes_with_only_out_degree": 288
    },
    "proposed_dag": {
      "has_cycle": false,
      "total_edges": 95,
      "nodes_with_only_in_degree": 1,
      "nodes_with_only_out_degree": 1
    }
  },
  "optimization_summary": {
    "baseline_configuration": {
      "strategy": "Tensor Parallelism (TP=8) + Pipeline Parallelism (PP=2)",
      "gpus_used": 16,
      "communication_pattern": "All-reduce within stages, send/receive between stages",
      "memory_per_layer_gb": 14.73,
      "total_memory_gb": 235.68
    },
    "proposed_configuration": {
      "strategy": "Layer-wise partitioning with cache optimization",
      "gpus_used": 8,
      "communication_pattern": "Point-to-point send/receive between GPUs",
      "memory_per_gpu_gb": 29.46,
      "total_memory_gb": 235.68,
      "cache_utilization": "Fits in SRAM/L2 cache per GPU"
    },
    "performance_improvement": {
      "tps_improvement": "20%",
      "baseline_tps": 12800,
      "proposed_tps": 15360,
      "tpot_reduction": "17%",
      "baseline_tpot_ms": 0.078,
      "proposed_tpot_ms": 0.065
    }
  }
}