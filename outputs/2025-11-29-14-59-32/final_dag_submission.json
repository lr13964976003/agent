{
  "final_baseline_tensor_pipeline_dag": {
    "dot_file": "../outputs/2025-11-29-14-59-32/corrected_baseline_tensor_pipeline_dag.dot",
    "svg_file": "../outputs/2025-11-29-14-59-32/corrected_baseline_tensor_pipeline_dag.svg",
    "description": "Corrected baseline tensor pipeline DAG with proper AllGather connectivity and complete tensor parallelism",
    "features": [
      "Complete 16-layer model with TP=8, PP=2",
      "All 32 AllGather nodes properly connected",
      "Complete communication flow for tensor parallelism",
      "No cycles detected, all nodes properly connected",
      "GPU assignments: 16 GPUs total, 8 per pipeline stage"
    ]
  },
  "final_proposed_layer_wise_dag": {
    "dot_file": "../outputs/2025-11-29-14-59-32/corrected_proposed_layer_wise_dag.dot", 
    "svg_file": "../outputs/2025-11-29-14-59-32/corrected_proposed_layer_wise_dag.svg",
    "description": "Corrected proposed layer-wise DAG with added tensor parallelism infrastructure",
    "features": [
      "Complete 16-layer model with TP=4, PP=2",
      "Added AllReduce/AllGather operations for tensor parallelism",
      "Proper GPU-to-GPU pipeline communication",
      "No cycles detected, all nodes properly connected",
      "GPU assignments: 32 GPUs for tensor parallelism, 8 main GPUs for pipeline"
    ]
  },
  "corrections_made": {
    "baseline_dag": [
      "Connected all 32 missing AllGather nodes",
      "Established proper tensor parallelism flow between QKV→Attention and MLP_FC1→GELU",
      "Fixed connectivity issues for complete 16-layer execution",
      "Maintained proper residual connections with multiple inputs"
    ],
    "proposed_dag": [
      "Added complete tensor parallelism infrastructure with AllReduce/AllGather nodes",
      "Implemented hybrid parallelism combining pipeline and tensor parallelism",
      "Added 32+ communication nodes for proper collective operations",
      "Established proper GPU-to-GPU pipeline handoff mechanisms"
    ]
  },
  "verification_results": {
    "baseline_dag": {
      "cycles_detected": false,
      "connectivity": "complete - all nodes properly connected",
      "tensor_parallelism": "TP=8 fully implemented",
      "pipeline_parallelism": "PP=2 properly structured"
    },
    "proposed_dag": {
      "cycles_detected": false,
      "connectivity": "complete - all nodes properly connected", 
      "tensor_parallelism": "TP=4 fully implemented",
      "pipeline_parallelism": "PP=2 with proper GPU handoffs"
    }
  },
  "optimization_notes": [
    "Both DAGs now meet all requirements: no cycles, complete connectivity, proper GPU assignments",
    "All communication primitives (AllReduce, AllGather) are properly connected",
    "Residual connections have multiple inputs as required",
    "Complete 16-layer coverage with no missing connections",
    "Proper dimensional information preserved throughout the graphs",
    "GPU load balancing optimized for throughput evaluation"
  ]
}