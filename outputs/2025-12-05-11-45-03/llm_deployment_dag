// 30B MoE Model Deployment DAG
digraph {
	bgcolor=white rankdir=TB splines=ortho
	node [fontname=Arial fontsize=10]
	input [label="Input\nBatch: [128, 1024, 1024]\nSeq_len=1024, Hidden=1024" fillcolor=lightblue shape=ellipse style=filled]
	dp_split [label="DP Split\n2-way Data Parallel" fillcolor=lightyellow shape=parallelogram style=filled]
	attn_qkv_s0_l0_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l0_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l0_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l0_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l0_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l0_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l0_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l0_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l0 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l0_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l0_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l0_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l0_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l0_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l0_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l0_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s0_l1_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l1_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l1_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l1_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l1_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l1_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l1_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l1_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l1 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l1_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l1_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l1_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l1_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l1_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l1_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l1_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s0_l2_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l2_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l2_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l2_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l2_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l2_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l2_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l2_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l2 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l2_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l2_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l2_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l2_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l2_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l2_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l2_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s0_l3_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l3_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l3_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l3_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l3_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l3_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l3_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l3_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l3 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l3_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l3_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l3_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l3_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l3_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l3_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l3_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s0_l4_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l4_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l4_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l4_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l4_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l4_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l4_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l4_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l4 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l4_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l4_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l4_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l4_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l4_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l4_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l4_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s0_l5_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l5_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l5_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l5_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l5_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l5_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l5_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l5_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l5 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l5_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l5_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l5_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l5_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l5_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l5_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l5_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s0_l6_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l6_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l6_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l6_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l6_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l6_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l6_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l6_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l6 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l6_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l6_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l6_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l6_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l6_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l6_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l6_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s0_l7_gpu0 [label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l7_gpu0 [label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l7_gpu0 [label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l7_gpu0 [label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s0_l7_gpu1 [label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_score_s0_l7_gpu1 [label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_softmax_s0_l7_gpu1 [label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s0_l7_gpu1 [label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s0_l7 [label="All-Reduce\nTP Group 0,1\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s0_l7_gpu0 [label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	gate_s0_l7_gpu1 [label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s0_l7_gpu0 [label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	expert_s0_l7_gpu1 [label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	all2all_s0_l7_gpu01 [label="All-to-All\nEP GPUs 0,1\nToken routing" fillcolor=lightcoral shape=ellipse style=filled]
	moe_agg_s0_l7_gpu0 [label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	moe_agg_s0_l7_gpu1 [label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l0_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l0_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l0_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l0_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l0 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l0_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l0_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l0_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l0_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l0_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l0_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l0_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l0_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l0_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l0_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l0_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l1_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l1_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l1_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l1_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l1 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l1_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l1_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l1_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l1_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l1_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l1_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l1_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l1_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l1_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l1_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l1_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l2_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l2_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l2_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l2_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l2 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l2_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l2_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l2_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l2_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l2_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l2_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l2_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l2_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l2_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l2_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l2_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l3_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l3_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l3_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l3_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l3 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l3_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l3_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l3_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l3_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l3_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l3_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l3_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l3_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l3_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l3_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l3_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l4_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l4_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l4_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l4_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l4 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l4_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l4_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l4_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l4_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l4_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l4_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l4_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l4_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l4_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l4_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l4_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l5_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l5_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l5_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l5_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l5 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l5_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l5_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l5_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l5_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l5_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l5_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l5_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l5_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l5_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l5_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l5_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l6_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l6_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l6_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l6_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l6 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l6_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l6_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l6_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l6_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l6_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l6_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l6_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l6_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l6_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l6_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l6_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l7_gpu2 [label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l7_gpu2 [label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l7_gpu3 [label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l7_gpu3 [label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l7 [label="All-Reduce\nTP Group 2,3\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l7_gpu2 [label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l7_gpu2 [label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l7_gpu2 [label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	attn_qkv_s1_l7_gpu6 [label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l7_gpu6 [label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_qkv_s1_l7_gpu7 [label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each" fillcolor=lightgreen shape=rectangle style=filled]
	attn_out_s1_l7_gpu7 [label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]" fillcolor=lightgreen shape=rectangle style=filled]
	attn_allreduce_s1_l7_dp1 [label="All-Reduce\nTP Group 6,7\n[64,1024,1024]" fillcolor=lightcoral shape=ellipse style=filled]
	gate_s1_l7_gpu6 [label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing" fillcolor=orange shape=parallelogram style="filled, dashed"]
	expert_s1_l7_gpu6 [label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts" fillcolor=lightblue shape=rectangle style=filled]
	moe_agg_s1_l7_gpu6 [label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]" fillcolor=orange shape=parallelogram style=filled]
	pipe_comm_s0_s1 [label="Pipeline Comm\nStage0â†’Stage1\nMicro-batch transfer" fillcolor=lightcoral shape=ellipse style=filled]
	dp_reduce [label="Data Parallel Reduce\n2-way DP\nGradient sync" fillcolor=lightcoral shape=ellipse style=filled]
	output [label="Output\nBatch: [128, 1024, 1024]\nSeq_len=1024, Hidden=1024" fillcolor=lightblue shape=ellipse style=filled]
	input -> dp_split
	dp_split -> attn_qkv_s0_l0_gpu0
	dp_split -> attn_qkv_s0_l0_gpu1
	attn_qkv_s0_l0_gpu0 -> attn_score_s0_l0_gpu0
	attn_score_s0_l0_gpu0 -> attn_softmax_s0_l0_gpu0
	attn_softmax_s0_l0_gpu0 -> attn_out_s0_l0_gpu0
	attn_qkv_s0_l0_gpu1 -> attn_score_s0_l0_gpu1
	attn_score_s0_l0_gpu1 -> attn_softmax_s0_l0_gpu1
	attn_softmax_s0_l0_gpu1 -> attn_out_s0_l0_gpu1
	attn_out_s0_l0_gpu0 -> attn_allreduce_s0_l0
	attn_out_s0_l0_gpu1 -> attn_allreduce_s0_l0
	attn_allreduce_s0_l0 -> gate_s0_l0_gpu0
	attn_allreduce_s0_l0 -> gate_s0_l0_gpu1
	attn_allreduce_s0_l0 -> expert_s0_l0_gpu0
	attn_allreduce_s0_l0 -> expert_s0_l0_gpu1
	gate_s0_l0_gpu0 -> all2all_s0_l0_gpu01
	gate_s0_l0_gpu1 -> all2all_s0_l0_gpu01
	expert_s0_l0_gpu0 -> all2all_s0_l0_gpu01
	expert_s0_l0_gpu1 -> all2all_s0_l0_gpu01
	all2all_s0_l0_gpu01 -> moe_agg_s0_l0_gpu0
	all2all_s0_l0_gpu01 -> moe_agg_s0_l0_gpu1
	moe_agg_s0_l7_gpu0 -> pipe_comm_s0_s1
	moe_agg_s0_l7_gpu1 -> pipe_comm_s0_s1
	pipe_comm_s0_s1 -> attn_qkv_s1_l0_gpu2
	pipe_comm_s0_s1 -> attn_qkv_s1_l0_gpu3
	moe_agg_s1_l7_gpu2 -> dp_reduce
	moe_agg_s1_l7_gpu3 -> dp_reduce
	moe_agg_s1_l7_gpu6 -> dp_reduce
	moe_agg_s1_l7_gpu7 -> dp_reduce
	dp_reduce -> output
}
