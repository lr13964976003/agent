digraph {
	graph [bb="0,0,39755,1075",
		bgcolor=white,
		rankdir=TB,
		splines=ortho
	];
	node [fontname=Arial,
		fontsize=10,
		label="\N"
	];
	input	[fillcolor=lightblue,
		height=0.80532,
		label="Input\nBatch: [128, 1024, 1024]\nSeq_len=1024, Hidden=1024",
		pos="455.01,1012",
		shape=ellipse,
		style=filled,
		width=2.8088];
	dp_split	[fillcolor=lightyellow,
		height=0.83333,
		label="DP Split\n2-way Data Parallel",
		pos="455.01,882.97",
		shape=parallelogram,
		style=filled,
		width=2.9375];
	input -> dp_split	[pos="e,455.01,913.02 455.01,982.69 455.01,982.69 455.01,923.02 455.01,923.02"];
	attn_qkv_s0_l0_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="387.01,785.47",
		shape=rectangle,
		style=filled,
		width=1.5417];
	dp_split -> attn_qkv_s0_l0_gpu0	[pos="e,395.88,817.27 395.88,852.92 395.88,852.92 395.88,827.27 395.88,827.27"];
	attn_qkv_s0_l0_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="520.01,785.47",
		shape=rectangle,
		style=filled,
		width=1.5417];
	dp_split -> attn_qkv_s0_l0_gpu1	[pos="e,512.63,817.27 512.63,852.92 512.63,852.92 512.63,827.27 512.63,827.27"];
	attn_score_s0_l0_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="380.01,691.97",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_qkv_s0_l0_gpu0 -> attn_score_s0_l0_gpu0	[pos="e,387.01,717.99 387.01,753.81 387.01,753.81 387.01,727.99 387.01,727.99"];
	attn_softmax_s0_l0_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="380.01,603.97",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_score_s0_l0_gpu0 -> attn_softmax_s0_l0_gpu0	[pos="e,380.01,630.05 380.01,665.56 380.01,665.56 380.01,640.05 380.01,640.05"];
	attn_out_s0_l0_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="384.01,515.97",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_softmax_s0_l0_gpu0 -> attn_out_s0_l0_gpu0	[pos="e,384.01,542.05 384.01,577.56 384.01,577.56 384.01,552.05 384.01,552.05"];
	attn_allreduce_s0_l0	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="451.01,424.97",
		shape=ellipse,
		style=filled,
		width=1.591];
	attn_out_s0_l0_gpu0 -> attn_allreduce_s0_l0	[pos="e,417.87,448.75 417.87,489.65 417.87,489.65 417.87,458.75 417.87,458.75"];
	attn_score_s0_l0_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="523.01,691.97",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_qkv_s0_l0_gpu1 -> attn_score_s0_l0_gpu1	[pos="e,520.01,717.99 520.01,753.81 520.01,753.81 520.01,727.99 520.01,727.99"];
	attn_softmax_s0_l0_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="523.01,603.97",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_score_s0_l0_gpu1 -> attn_softmax_s0_l0_gpu1	[pos="e,523.01,630.05 523.01,665.56 523.01,665.56 523.01,640.05 523.01,640.05"];
	attn_out_s0_l0_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="520.01,515.97",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_softmax_s0_l0_gpu1 -> attn_out_s0_l0_gpu1	[pos="e,520.01,542.05 520.01,577.56 520.01,577.56 520.01,552.05 520.01,552.05"];
	attn_out_s0_l0_gpu1 -> attn_allreduce_s0_l0	[pos="e,485.14,448.44 485.14,489.65 485.14,489.65 485.14,458.44 485.14,458.44"];
	gate_s0_l0_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="113.01,296.98",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	attn_allreduce_s0_l0 -> gate_s0_l0_gpu0	[pos="e,113.01,360.12 393.67,424 297.24,424 113.01,424 113.01,424 113.01,424 113.01,370.12 113.01,370.12"];
	gate_s0_l0_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="357.01,296.98",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	attn_allreduce_s0_l0 -> gate_s0_l0_gpu1	[pos="e,431.87,360.16 431.87,397.52 431.87,397.52 431.87,370.16 431.87,370.16"];
	expert_s0_l0_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="546.01,296.98",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l0 -> expert_s0_l0_gpu0	[pos="e,498.14,328.6 498.14,408.5 498.14,408.5 498.14,338.6 498.14,338.6"];
	expert_s0_l0_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="680.01,296.98",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l0 -> expert_s0_l0_gpu1	[pos="e,653.76,328.71 508.29,424 568.25,424 653.76,424 653.76,424 653.76,424 653.76,338.71 653.76,338.71"];
	all2all_s0_l0_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="451.01,168.99",
		shape=ellipse,
		style=filled,
		width=1.5124];
	gate_s0_l0_gpu0 -> all2all_s0_l0_gpu01	[pos="e,396.31,168 213.88,296.89 213.88,292.98 213.88,168 213.88,168 213.88,168 386.31,168 386.31,168"];
	gate_s0_l0_gpu1 -> all2all_s0_l0_gpu01	[pos="e,433.29,196.43 433.29,259.62 433.29,259.62 433.29,206.43 433.29,206.43"];
	expert_s0_l0_gpu0 -> all2all_s0_l0_gpu01	[pos="e,496.73,184.73 496.73,265.34 496.73,265.34 496.73,194.73 496.73,194.73"];
	expert_s0_l0_gpu1 -> all2all_s0_l0_gpu01	[pos="e,505.52,168 661.64,265.43 661.64,227.24 661.64,168 661.64,168 661.64,168 515.52,168 515.52,168"];
	moe_agg_s0_l0_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="322.01,52",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	all2all_s0_l0_gpu01 -> moe_agg_s0_l0_gpu0	[pos="e,419.41,104.15 419.41,145.32 419.41,145.32 419.41,114.15 419.41,114.15"];
	moe_agg_s0_l0_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="581.01,52",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	all2all_s0_l0_gpu01 -> moe_agg_s0_l0_gpu1	[pos="e,483.1,52.043 483.1,145.32 483.1,145.32 483.1,62.043 483.1,62.043"];
	attn_qkv_s0_l1_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="630.01,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l1_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="766.01,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l1_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="909.01,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l1_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="1048,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s0_l1_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="1180,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l1_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="1316,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l1_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="1459,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l1_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="1598,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="1731,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s0_l1_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="1919,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	gate_s0_l1_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="2163,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s0_l1_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="2352,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	expert_s0_l1_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="2486,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	all2all_s0_l1_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="2616,1012",
		shape=ellipse,
		style=filled,
		width=1.5124];
	moe_agg_s0_l1_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="2809,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s0_l1_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="3068,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s0_l2_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="3262,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l2_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="3398,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l2_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="3541,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l2_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="3680,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s0_l2_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="3812,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l2_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="3948,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l2_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="4091,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l2_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="4230,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l2	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="4363,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s0_l2_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="4551,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	gate_s0_l2_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="4795,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s0_l2_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="4984,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	expert_s0_l2_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="5118,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	all2all_s0_l2_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="5248,1012",
		shape=ellipse,
		style=filled,
		width=1.5124];
	moe_agg_s0_l2_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="5441,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s0_l2_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="5700,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s0_l3_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="5894,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l3_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="6030,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l3_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="6173,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l3_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="6312,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s0_l3_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="6444,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l3_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="6580,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l3_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="6723,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l3_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="6862,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l3	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="6995,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s0_l3_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="7183,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	gate_s0_l3_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="7427,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s0_l3_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="7616,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	expert_s0_l3_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="7750,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	all2all_s0_l3_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="7880,1012",
		shape=ellipse,
		style=filled,
		width=1.5124];
	moe_agg_s0_l3_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="8073,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s0_l3_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="8332,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s0_l4_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="8526,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l4_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="8662,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l4_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="8805,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l4_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="8944,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s0_l4_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="9076,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l4_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="9212,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l4_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="9355,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l4_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="9494,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l4	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="9627,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s0_l4_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="9815,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	gate_s0_l4_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="10059,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s0_l4_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="10248,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	expert_s0_l4_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="10382,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	all2all_s0_l4_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="10512,1012",
		shape=ellipse,
		style=filled,
		width=1.5124];
	moe_agg_s0_l4_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="10705,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s0_l4_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="10964,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s0_l5_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="11158,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l5_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="11294,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l5_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="11437,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l5_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="11576,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s0_l5_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="11708,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l5_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="11844,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l5_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="11987,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l5_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="12126,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l5	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="12259,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s0_l5_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="12447,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	gate_s0_l5_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="12691,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s0_l5_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="12880,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	expert_s0_l5_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="13014,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	all2all_s0_l5_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="13144,1012",
		shape=ellipse,
		style=filled,
		width=1.5124];
	moe_agg_s0_l5_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="13337,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s0_l5_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="13596,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s0_l6_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="13790,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l6_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="13926,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l6_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="14069,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l6_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="14208,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s0_l6_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="14340,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l6_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="14476,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l6_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="14619,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l6_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="14758,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l6	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="14891,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s0_l6_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="15079,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	gate_s0_l6_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="15323,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s0_l6_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="15512,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	expert_s0_l6_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="15646,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	all2all_s0_l6_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="15776,1012",
		shape=ellipse,
		style=filled,
		width=1.5124];
	moe_agg_s0_l6_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="15969,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s0_l6_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="16228,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s0_l7_gpu0	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="16422,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l7_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-0\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="16558,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l7_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-0\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="16701,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l7_gpu0	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-0\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="16840,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s0_l7_gpu1	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="16972,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_score_s0_l7_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Score\nGPU-1\nInput: [64,8,1024,64]\nOutput: [64,8,1024,1024]",
		pos="17108,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_softmax_s0_l7_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Softmax\nGPU-1\nInput: [64,8,1024,1024]\nOutput: [64,8,1024,1024]",
		pos="17251,1012",
		shape=rectangle,
		style=filled,
		width=1.7361];
	attn_out_s0_l7_gpu1	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-1\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="17390,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s0_l7	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 0,1\n[64,1024,1024]",
		pos="17523,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s0_l7_gpu0	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="17711,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	gate_s0_l7_gpu1	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="17955,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s0_l7_gpu0	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-0\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="18144,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	expert_s0_l7_gpu1	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 16-31\nGPU-1\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="18278,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	all2all_s0_l7_gpu01	[fillcolor=lightcoral,
		height=0.80532,
		label="All-to-All\nEP GPUs 0,1\nToken routing",
		pos="18408,1012",
		shape=ellipse,
		style=filled,
		width=1.5124];
	moe_agg_s0_l7_gpu0	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-0\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="18601,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	pipe_comm_s0_s1	[fillcolor=lightcoral,
		height=0.80532,
		label="Pipeline Comm\nStage0â†’Stage1\nMicro-batch transfer",
		pos="18730,882.97",
		shape=ellipse,
		style=filled,
		width=2.0428];
	moe_agg_s0_l7_gpu0 -> pipe_comm_s0_s1	[pos="e,18689,907.07 18689,995.63 18689,995.63 18689,917.07 18689,917.07"];
	moe_agg_s0_l7_gpu1	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-1\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="18860,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s0_l7_gpu1 -> pipe_comm_s0_s1	[pos="e,18772,906.96 18772,959.87 18772,959.87 18772,916.96 18772,916.96"];
	attn_qkv_s1_l0_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="18665,785.47",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l0_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="19056,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l0_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="18794,785.47",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l0_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="19190,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l0	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="19323,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l0_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="19511,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l0_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="19700,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l0_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="19896,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l0_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="20090,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l0_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="20222,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l0_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="20354,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l0_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="20486,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l0_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="20619,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l0_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="20807,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l0_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="20996,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l0_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="21192,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l1_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="21386,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l1_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="21518,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l1_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="21650,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l1_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="21782,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="21915,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l1_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="22103,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l1_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="22292,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l1_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="22488,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l1_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="22682,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l1_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="22814,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l1_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="22946,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l1_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="23078,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l1_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="23211,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l1_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="23399,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l1_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="23588,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l1_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="23784,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l2_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="23978,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l2_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="24110,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l2_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="24242,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l2_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="24374,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l2	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="24507,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l2_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="24695,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l2_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="24884,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l2_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="25080,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l2_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="25274,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l2_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="25406,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l2_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="25538,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l2_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="25670,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l2_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="25803,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l2_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="25991,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l2_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="26180,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l2_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="26376,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l3_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="26570,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l3_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="26702,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l3_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="26834,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l3_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="26966,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l3	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="27099,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l3_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="27287,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l3_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="27476,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l3_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="27672,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l3_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="27866,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l3_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="27998,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l3_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="28130,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l3_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="28262,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l3_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="28395,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l3_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="28583,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l3_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="28772,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l3_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="28968,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l4_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="29162,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l4_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="29294,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l4_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="29426,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l4_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="29558,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l4	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="29691,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l4_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="29879,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l4_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="30068,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l4_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="30264,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l4_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="30458,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l4_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="30590,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l4_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="30722,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l4_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="30854,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l4_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="30987,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l4_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="31175,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l4_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="31364,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l4_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="31560,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l5_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="31754,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l5_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="31886,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l5_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="32018,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l5_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="32150,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l5	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="32283,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l5_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="32471,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l5_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="32660,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l5_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="32856,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l5_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="33050,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l5_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="33182,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l5_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="33314,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l5_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="33446,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l5_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="33579,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l5_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="33767,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l5_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="33956,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l5_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="34152,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l6_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="34346,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l6_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="34478,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l6_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="34610,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l6_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="34742,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l6	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="34875,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l6_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="35063,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l6_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="35252,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l6_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="35448,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l6_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="35642,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l6_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="35774,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l6_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="35906,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l6_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="36038,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l6_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="36171,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l6_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="36359,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l6_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="36548,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l6_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="36744,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	attn_qkv_s1_l7_gpu2	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="36938,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l7_gpu2	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-2\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="37070,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l7_gpu3	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-3\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="37202,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l7_gpu3	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-3\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="37334,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l7	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 2,3\n[64,1024,1024]",
		pos="37467,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l7_gpu2	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="37655,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l7_gpu2	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 0-15\nGPU-2\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="37844,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l7_gpu2	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-2\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="38040,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	dp_reduce	[fillcolor=lightcoral,
		height=0.80532,
		label="Data Parallel Reduce\n2-way DP\nGradient sync",
		pos="38401,882.97",
		shape=ellipse,
		style=filled,
		width=2.1606];
	moe_agg_s1_l7_gpu2 -> dp_reduce	[pos="e,38323,882 38040,959.85 38040,923.7 38040,882 38040,882 38040,882 38313,882 38313,882"];
	attn_qkv_s1_l7_gpu6	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="38791,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l7_gpu6	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-6\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="38923,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_qkv_s1_l7_gpu7	[fillcolor=lightgreen,
		height=0.875,
		label="Attention QKV\nGPU-7\nInput: [64,1024,1024]\nOutput: [64,1024,512]\n8 heads, 64d each",
		pos="39055,1012",
		shape=rectangle,
		style=filled,
		width=1.5417];
	attn_out_s1_l7_gpu7	[fillcolor=lightgreen,
		height=0.72222,
		label="Attention Output\nGPU-7\nInput: [64,1024,512]\nOutput: [64,1024,1024]",
		pos="39187,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	attn_allreduce_s1_l7_dp1	[fillcolor=lightcoral,
		height=0.80532,
		label="All-Reduce\nTP Group 6,7\n[64,1024,1024]",
		pos="39320,1012",
		shape=ellipse,
		style=filled,
		width=1.591];
	gate_s1_l7_gpu6	[fillcolor=orange,
		height=1.75,
		label="MoE Gate\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,4]\nTop-1 routing",
		pos="39508,1012",
		shape=parallelogram,
		style="filled, dashed",
		width=3.1391];
	expert_s1_l7_gpu6	[fillcolor=lightblue,
		height=0.875,
		label="MoE Experts 32-47\nGPU-6\nInput: [64,1024,1024]\nOutput: [64,1024,2048]\n16 experts",
		pos="39697,1012",
		shape=rectangle,
		style=filled,
		width=1.6111];
	moe_agg_s1_l7_gpu6	[fillcolor=orange,
		height=1.4444,
		label="MoE Aggregate\nGPU-6\nInput: [64,1024,2048]\nOutput: [64,1024,1024]",
		pos="38299,1012",
		shape=parallelogram,
		style=filled,
		width=3.3406];
	moe_agg_s1_l7_gpu6 -> dp_reduce	[pos="e,38371,909.79 38371,962.46 38371,962.46 38371,919.79 38371,919.79"];
	pipe_comm_s0_s1 -> attn_qkv_s1_l0_gpu2	[pos="e,18688,817.02 18688,858.86 18688,858.86 18688,827.02 18688,827.02"];
	pipe_comm_s0_s1 -> attn_qkv_s1_l0_gpu3	[pos="e,18771,817.02 18771,858.86 18771,858.86 18771,827.02 18771,827.02"];
	output	[fillcolor=lightblue,
		height=0.80532,
		label="Output\nBatch: [128, 1024, 1024]\nSeq_len=1024, Hidden=1024",
		pos="38401,785.47",
		shape=ellipse,
		style=filled,
		width=2.8088];
	dp_reduce -> output	[pos="e,38401,814.7 38401,853.71 38401,853.71 38401,824.7 38401,824.7"];
	moe_agg_s1_l7_gpu3	[height=0.5,
		pos="38503,1012",
		width=1.815];
	moe_agg_s1_l7_gpu3 -> dp_reduce	[pos="e,38458,902.7 38458,998.65 38458,998.65 38458,912.7 38458,912.7"];
	moe_agg_s1_l7_gpu7	[height=0.5,
		pos="38652,1012",
		width=1.815];
	moe_agg_s1_l7_gpu7 -> dp_reduce	[pos="e,38479,882 38652,993.89 38652,958.18 38652,882 38652,882 38652,882 38489,882 38489,882"];
}
