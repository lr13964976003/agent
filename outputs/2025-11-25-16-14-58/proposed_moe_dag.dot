// Proposed MoE DAG with EP=16
digraph proposed_moe_dag {
	compound=true rankdir=TB splines=ortho
	node [fontsize=10 margin="0.1,0.05"]
	input [label="Input\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]" fillcolor=lightblue shape=ellipse style=filled]
	subgraph cluster_layer0 {
		color=purple label="Layer 0 (16 Experts, 1 per GPU)" style=dashed
		layer0_mha_qkv [label="MHA QKV Linear\nInput: [batch=128, seq=10000, 4096]\nOutput: [batch=128, seq=10000, 32×128]\nGPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
		layer0_mha_attn [label="MHA Attention\nInput: [batch=128, seq=10000, 32×128]\nOutput: [batch=128, seq=10000, 32×128]\nGPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
		layer0_mha_out [label="MHA Output Linear\nInput: [batch=128, seq=10000, 32×128]\nOutput: [batch=128, seq=10000, 4096]\nGPU: 0" fillcolor=lightgreen shape=rectangle style=filled]
		layer0_mha_res [label="MHA Residual Add\nInput: [batch=128, seq=10000, 4096]×2\nOutput: [batch=128, seq=10000, 4096]" fillcolor=lightcoral shape=rectangle style=filled]
		layer0_gate [label="Expert Gate\nGating (Top-2)\nInput: [batch=128, seq=10000, 4096]\nOutput: [routing=16, per_expert_tokens]\nGPU: 0" fillcolor=lightyellow shape=parallelogram style=filled]
		subgraph cluster_experts {
			label="16 Experts Across GPUs" style=dotted
			layer0_exp0 [label="Expert 0\nInput: [tokens_to_exp0, 4096]\nOutput: [tokens_from_exp0, 4096]\nGPU: 0" fillcolor=lightblue shape=rectangle style=filled]
			layer0_exp1 [label="Expert 1\nInput: [tokens_to_exp1, 4096]\nOutput: [tokens_from_exp1, 4096]\nGPU: 1" fillcolor=lightblue shape=rectangle style=filled]
			layer0_exp8 [label="Expert 8\nInput: [tokens_to_exp8, 4096]\nOutput: [tokens_from_exp8, 4096]\nGPU: 8" fillcolor=lightblue shape=rectangle style=filled]
			layer0_exp15 [label="Expert 15\nInput: [tokens_to_exp15, 4096]\nOutput: [tokens_from_exp15, 4096]\nGPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		}
		layer0_aggregate [label="Token Aggregation\nGather from all experts\nInput: [per_expert_outputs]\nOutput: [batch=128, seq=10000, 4096]\nGPU: 0" fillcolor=lightyellow shape=parallelogram style=filled]
		layer0_exp_res [label="Expert Residual Add\nInput: [batch=128, seq=10000, 4096]×2\nOutput: [batch=128, seq=10000, 4096]" fillcolor=lightcoral shape=rectangle style=filled]
	}
	subgraph cluster_communication {
		color=red label="Cross-GPU Communication (NCCL)" style=dashed
		comm_0_to_1 [label="Token Send\nGPU 0 → GPU 1\n[variable_tokens, 4096]" fillcolor=orange shape=ellipse style=filled]
		comm_0_to_8 [label="Token Send\nGPU 0 → GPU 8\n[variable_tokens, 4096]" fillcolor=orange shape=ellipse style=filled]
		comm_1_to_0 [label="Expert Result\nGPU 1 → GPU 0\n[processed_tokens, 4096]" fillcolor=orange shape=ellipse style=filled]
		comm_8_to_0 [label="Expert Result\nGPU 8 → GPU 0\n[processed_tokens, 4096]" fillcolor=orange shape=ellipse style=filled]
	}
	output [label="Output\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]" fillcolor=lightblue shape=ellipse style=filled]
	input -> layer0_mha_qkv
	layer0_mha_qkv -> layer0_mha_attn
	layer0_mha_attn -> layer0_mha_out
	layer0_mha_out -> layer0_mha_res
	input -> layer0_mha_res [label=residual style=dashed]
	layer0_mha_res -> layer0_gate
	layer0_gate -> layer0_exp0 [label="route tokens" style=dashed]
	layer0_gate -> layer0_exp1 [label="route tokens" style=dashed]
	layer0_gate -> layer0_exp8 [label="route tokens" style=dashed]
	layer0_gate -> layer0_exp15 [label="route tokens" style=dashed]
	layer0_gate -> comm_0_to_1 [style=dotted]
	layer0_gate -> comm_0_to_8 [style=dotted]
	comm_0_to_1 -> layer0_exp1 [label=tokens style=dotted]
	comm_0_to_8 -> layer0_exp8 [label=tokens style=dotted]
	layer0_exp1 -> comm_1_to_0 [style=dotted]
	layer0_exp8 -> comm_8_to_0 [style=dotted]
	comm_1_to_0 -> layer0_aggregate [label=results style=dotted]
	comm_8_to_0 -> layer0_aggregate [label=results style=dotted]
	layer0_exp0 -> layer0_aggregate
	layer0_exp15 -> layer0_aggregate
	layer0_aggregate -> layer0_exp_res
	layer0_mha_res -> layer0_exp_res [label=residual style=dashed]
	layer0_exp_res -> output [label="After 16 similar layers"]
	note1 [label="Note: Layer 1-15\nrepeat similar patterns\nwith different GPU mappings\n(layer_x: GPUs 16-31, etc.)" shape=note style=dashed]
	note2 [label="Key Innovation: One expert per GPU\nMinimizes contention\nEnables async communication" fillcolor=lightgray shape=note style=filled]
}
