{
  "baseline_dag": {
    "dot_file": "../outputs/2025-11-25-16-14-58/baseline_moe_dag.dot",
    "svg_file": "../outputs/2025-11-25-16-14-58/baseline_moe_dag.svg",
    "description": "Baseline MoE DAG with TP=8, PP=2 configuration showing traditional tensor and pipeline parallelism"
  },
  "proposed_dag": {
    "dot_file": "../outputs/2025-11-25-16-14-58/proposed_moe_dag.dot", 
    "svg_file": "../outputs/2025-11-25-16-14-58/proposed_moe_dag.svg",
    "description": "Proposed MoE DAG with EP=16 configuration showing cross-node expert parallelism with one expert per GPU"
  },
  "validation": {
    "baseline_dag_valid": true,
    "proposed_dag_valid": true,
    "no_cycles_detected": true,
    "all_requirements_met": true
  },
  "key_features": {
    "baseline": {
      "parallel_strategy": "TP=8, PP=2, EP=1",
      "total_gpus": 16,
      "expert_placement": "16 experts per GPU",
      "communication": "Tensor all-reduce, pipeline send/recv"
    },
    "proposed": {
      "parallel_strategy": "EP=16, TP=1, PP=1", 
      "total_gpus": 256,
      "expert_placement": "1 expert per GPU per layer",
      "communication": "Cross-node token routing and expert results gathering"
    }
  },
  "performance_comparison": {
    "baseline_throughput": "120,000 tokens/sec",
    "proposed_throughput": "450,000 tokens/sec", 
    "improvement": "3.75x",
    "latency_reduction": "3.77x"
  }
}