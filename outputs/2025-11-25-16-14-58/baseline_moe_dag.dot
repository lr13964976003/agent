// Baseline MoE DAG with TP=8, PP=2
digraph baseline_moe_dag {
	rankdir=TB splines=ortho
	node [fontsize=10 margin="0.1,0.05"]
	input [label="Input\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]" fillcolor=lightblue shape=ellipse style=filled]
	subgraph cluster_stage0 {
		color=blue label="Pipeline Stage 0 (Layers 0-7)" style=dashed
		stage0_input [label="Stage0 Input\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]" fillcolor=lightyellow shape=ellipse style=filled]
		subgraph cluster_layer0 {
			color=black label="Layer 0" style=rounded
			layer0_mha_qkv [label="MHA QKV Linear\nTP=8\nInput: [batch=128, seq=10000, 4096]\nOutput: [batch=128, seq=10000, 32×128]\nGPUs: [0,1,2,3,4,5,6,7]" fillcolor=lightgreen shape=rectangle style=filled]
			layer0_mha_attn [label="MHA Attention\nInput: [batch=128, seq=10000, 32×128]\nOutput: [batch=128, seq=10000, 32×128]\nGPUs: [0,1,2,3,4,5,6,7]" fillcolor=lightgreen shape=rectangle style=filled]
			layer0_mha_out [label="MHA Output Linear\nTP=8\nInput: [batch=128, seq=10000, 32×128]\nOutput: [batch=128, seq=10000, 4096]\nGPUs: [0,1,2,3,4,5,6,7]" fillcolor=lightgreen shape=rectangle style=filled]
			layer0_mha_res [label="MHA Residual Add\nInput: [batch=128, seq=10000, 4096]×2\nOutput: [batch=128, seq=10000, 4096]" fillcolor=lightcoral shape=rectangle style=filled]
			layer0_gate [label="Expert Gate\nInput: [batch=128, seq=10000, 4096]\nOutput: [batch=128, seq=10000, routing=16]\nGPUs: [0,1,2,3,4,5,6,7]" fillcolor=lightyellow shape=parallelogram style=filled]
			layer0_experts [label="Expert Layer\n16 Experts per GPU\nInput: [batch=128, seq=10000, 4096]\nOutput: [batch=128, seq=10000, 4096]\nGPUs: [0,1,2,3,4,5,6,7]" fillcolor=lightgreen shape=rectangle style=filled]
			layer0_exp_res [label="Expert Residual Add\nInput: [batch=128, seq=10000, 4096]×2\nOutput: [batch=128, seq=10000, 4096]" fillcolor=lightcoral shape=rectangle style=filled]
		}
	}
	subgraph cluster_stage1 {
		color=red label="Pipeline Stage 1 (Layers 8-15)" style=dashed
		stage1_input [label="Stage1 Input\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]" fillcolor=lightyellow shape=ellipse style=filled]
		subgraph cluster_layer8 {
			color=black label="Layer 8" style=rounded
			layer8_mha_qkv [label="MHA QKV Linear\nTP=8\nInput: [batch=128, seq=10000, 4096]\nOutput: [batch=128, seq=10000, 32×128]\nGPUs: [8,9,10,11,12,13,14,15]" fillcolor=lightgreen shape=rectangle style=filled]
			layer8_mha_attn [label="MHA Attention\nInput: [batch=128, seq=10000, 32×128]\nOutput: [batch=128, seq=10000, 32×128]\nGPUs: [8,9,10,11,12,13,14,15]" fillcolor=lightgreen shape=rectangle style=filled]
			layer8_mha_out [label="MHA Output Linear\nTP=8\nInput: [batch=128, seq=10000, 32×128]\nOutput: [batch=128, seq=10000, 4096]\nGPUs: [8,9,10,11,12,13,14,15]" fillcolor=lightgreen shape=rectangle style=filled]
			layer8_mha_res [label="MHA Residual Add\nInput: [batch=128, seq=10000, 4096]×2\nOutput: [batch=128, seq=10000, 4096]" fillcolor=lightcoral shape=rectangle style=filled]
			layer8_gate [label="Expert Gate\nInput: [batch=128, seq=10000, 4096]\nOutput: [batch=128, seq=10000, routing=16]\nGPUs: [8,9,10,11,12,13,14,15]" fillcolor=lightyellow shape=parallelogram style=filled]
			layer8_experts [label="Expert Layer\n16 Experts per GPU\nInput: [batch=128, seq=10000, 4096]\nOutput: [batch=128, seq=10000, 4096]\nGPUs: [8,9,10,11,12,13,14,15]" fillcolor=lightgreen shape=rectangle style=filled]
			layer8_exp_res [label="Expert Residual Add\nInput: [batch=128, seq=10000, 4096]×2\nOutput: [batch=128, seq=10000, 4096]" fillcolor=lightcoral shape=rectangle style=filled]
		}
	}
	output [label="Output\nInput: [batch_size=128, seq_len=10000, hidden=4096]\nOutput: [batch_size=128, seq_len=10000, hidden=4096]" fillcolor=lightblue shape=ellipse style=filled]
	input -> stage0_input [label="Initial Input"]
	stage0_input -> layer0_mha_qkv
	layer0_mha_qkv -> layer0_mha_attn
	layer0_mha_attn -> layer0_mha_out
	layer0_mha_out -> layer0_mha_res
	stage0_input -> layer0_mha_res [label=residual style=dashed]
	layer0_mha_res -> layer0_gate
	layer0_gate -> layer0_experts
	layer0_experts -> layer0_exp_res
	layer0_mha_res -> layer0_exp_res [label=residual style=dashed]
	layer0_exp_res -> stage1_input [label="Pipeline Send\n[batch=128, seq=10000, 4096]\nGPU7→GPU8" color=blue style=dotted]
	stage1_input -> layer8_mha_qkv
	layer8_mha_qkv -> layer8_mha_attn
	layer8_mha_attn -> layer8_mha_out
	layer8_mha_out -> layer8_mha_res
	stage1_input -> layer8_mha_res [label=residual style=dashed]
	layer8_mha_res -> layer8_gate
	layer8_gate -> layer8_experts
	layer8_experts -> layer8_exp_res
	layer8_mha_res -> layer8_exp_res [label=residual style=dashed]
	layer8_exp_res -> output
	note [label="Note: Layers 1-7 and 9-15\nrepeat similar patterns\nwith same device mappings" shape=note style=dashed]
}
