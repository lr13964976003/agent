// Proposed Large-Scale Expert Parallelism with EP=16
digraph proposed_moe_fixed {
	graph [bgcolor=white fontname=Arial fontsize=12 rankdir=TB]
	input [label="Model Input\nBatch: 128, Seq: 10000, Dim: 4096" fontname=Arial shape=plaintext]
	global_router [label="Global Token Router\nAll GPUs" fillcolor=lightgreen fontname=Arial shape=parallelogram style=filled]
	layernorm_0 [label="LayerNorm L0\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_qkv_0 [label="MHA QKV Linear L0\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_attn_0 [label="MHA Attention L0\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_out_0 [label="MHA Output Linear L0\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	attn_res_0 [label="Attention Residual L0\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	local_gating_0 [label="Layer Gating L0\nAll GPUs" fillcolor=lightgreen fontname=Arial shape=parallelogram style=filled]
	token_route_0 [label="Token Routing L0\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	expert_0_0 [label="Expert 0 L0\nGPU 0\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_1 [label="Expert 1 L0\nGPU 1\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_2 [label="Expert 2 L0\nGPU 2\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_3 [label="Expert 3 L0\nGPU 3\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_4 [label="Expert 4 L0\nGPU 4\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_5 [label="Expert 5 L0\nGPU 5\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_6 [label="Expert 6 L0\nGPU 6\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_7 [label="Expert 7 L0\nGPU 7\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_8 [label="Expert 8 L0\nGPU 8\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_9 [label="Expert 9 L0\nGPU 9\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_10 [label="Expert 10 L0\nGPU 10\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_11 [label="Expert 11 L0\nGPU 11\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_12 [label="Expert 12 L0\nGPU 12\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_13 [label="Expert 13 L0\nGPU 13\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_14 [label="Expert 14 L0\nGPU 14\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_0_15 [label="Expert 15 L0\nGPU 15\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_agg_0 [label="Expert Aggregation L0\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	moe_combine_0 [label="MoE Output Combine L0\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	moe_res_0 [label="MoE Residual L0\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	layernorm_1 [label="LayerNorm L1\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_qkv_1 [label="MHA QKV Linear L1\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_attn_1 [label="MHA Attention L1\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_out_1 [label="MHA Output Linear L1\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	attn_res_1 [label="Attention Residual L1\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	local_gating_1 [label="Layer Gating L1\nAll GPUs" fillcolor=lightgreen fontname=Arial shape=parallelogram style=filled]
	token_route_1 [label="Token Routing L1\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	expert_1_0 [label="Expert 0 L1\nGPU 0\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_1 [label="Expert 1 L1\nGPU 1\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_2 [label="Expert 2 L1\nGPU 2\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_3 [label="Expert 3 L1\nGPU 3\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_4 [label="Expert 4 L1\nGPU 4\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_5 [label="Expert 5 L1\nGPU 5\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_6 [label="Expert 6 L1\nGPU 6\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_7 [label="Expert 7 L1\nGPU 7\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_8 [label="Expert 8 L1\nGPU 8\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_9 [label="Expert 9 L1\nGPU 9\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_10 [label="Expert 10 L1\nGPU 10\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_11 [label="Expert 11 L1\nGPU 11\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_12 [label="Expert 12 L1\nGPU 12\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_13 [label="Expert 13 L1\nGPU 13\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_14 [label="Expert 14 L1\nGPU 14\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_1_15 [label="Expert 15 L1\nGPU 15\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_agg_1 [label="Expert Aggregation L1\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	moe_combine_1 [label="MoE Output Combine L1\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	moe_res_1 [label="MoE Residual L1\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	layernorm_2 [label="LayerNorm L2\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_qkv_2 [label="MHA QKV Linear L2\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_attn_2 [label="MHA Attention L2\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_out_2 [label="MHA Output Linear L2\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	attn_res_2 [label="Attention Residual L2\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	local_gating_2 [label="Layer Gating L2\nAll GPUs" fillcolor=lightgreen fontname=Arial shape=parallelogram style=filled]
	token_route_2 [label="Token Routing L2\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	expert_2_0 [label="Expert 0 L2\nGPU 0\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_1 [label="Expert 1 L2\nGPU 1\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_2 [label="Expert 2 L2\nGPU 2\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_3 [label="Expert 3 L2\nGPU 3\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_4 [label="Expert 4 L2\nGPU 4\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_5 [label="Expert 5 L2\nGPU 5\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_6 [label="Expert 6 L2\nGPU 6\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_7 [label="Expert 7 L2\nGPU 7\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_8 [label="Expert 8 L2\nGPU 8\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_9 [label="Expert 9 L2\nGPU 9\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_10 [label="Expert 10 L2\nGPU 10\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_11 [label="Expert 11 L2\nGPU 11\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_12 [label="Expert 12 L2\nGPU 12\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_13 [label="Expert 13 L2\nGPU 13\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_14 [label="Expert 14 L2\nGPU 14\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_2_15 [label="Expert 15 L2\nGPU 15\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_agg_2 [label="Expert Aggregation L2\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	moe_combine_2 [label="MoE Output Combine L2\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	moe_res_2 [label="MoE Residual L2\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	layernorm_3 [label="LayerNorm L3\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_qkv_3 [label="MHA QKV Linear L3\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_attn_3 [label="MHA Attention L3\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	mha_out_3 [label="MHA Output Linear L3\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	attn_res_3 [label="Attention Residual L3\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	local_gating_3 [label="Layer Gating L3\nAll GPUs" fillcolor=lightgreen fontname=Arial shape=parallelogram style=filled]
	token_route_3 [label="Token Routing L3\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	expert_3_0 [label="Expert 0 L3\nGPU 0\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_1 [label="Expert 1 L3\nGPU 1\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_2 [label="Expert 2 L3\nGPU 2\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_3 [label="Expert 3 L3\nGPU 3\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_4 [label="Expert 4 L3\nGPU 4\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_5 [label="Expert 5 L3\nGPU 5\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_6 [label="Expert 6 L3\nGPU 6\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_7 [label="Expert 7 L3\nGPU 7\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_8 [label="Expert 8 L3\nGPU 8\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_9 [label="Expert 9 L3\nGPU 9\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_10 [label="Expert 10 L3\nGPU 10\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_11 [label="Expert 11 L3\nGPU 11\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_12 [label="Expert 12 L3\nGPU 12\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_13 [label="Expert 13 L3\nGPU 13\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_14 [label="Expert 14 L3\nGPU 14\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_3_15 [label="Expert 15 L3\nGPU 15\nFFN 4096→32768→4096" fillcolor=lightcoral fontname=Arial shape=rectangle style=filled]
	expert_agg_3 [label="Expert Aggregation L3\nAll GPUs" fillcolor=lightyellow fontname=Arial shape=ellipse style=filled]
	moe_combine_3 [label="MoE Output Combine L3\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	moe_res_3 [label="MoE Residual L3\nAll GPUs" fillcolor=lightblue fontname=Arial shape=rectangle style=filled]
	output [label="Model Output\nBatch: 128, Seq: 10000, Dim: 4096" fontname=Arial shape=plaintext]
	input -> global_router
	global_router -> layernorm_0
	layernorm_0 -> mha_qkv_0
	mha_qkv_0 -> mha_attn_0
	mha_attn_0 -> mha_out_0
	mha_out_0 -> attn_res_0
	layernorm_0 -> attn_res_0
	attn_res_0 -> local_gating_0
	local_gating_0 -> token_route_0
	token_route_0 -> expert_0_0
	expert_0_0 -> expert_agg_0
	token_route_0 -> expert_0_1
	expert_0_1 -> expert_agg_0
	token_route_0 -> expert_0_2
	expert_0_2 -> expert_agg_0
	token_route_0 -> expert_0_3
	expert_0_3 -> expert_agg_0
	token_route_0 -> expert_0_4
	expert_0_4 -> expert_agg_0
	token_route_0 -> expert_0_5
	expert_0_5 -> expert_agg_0
	token_route_0 -> expert_0_6
	expert_0_6 -> expert_agg_0
	token_route_0 -> expert_0_7
	expert_0_7 -> expert_agg_0
	token_route_0 -> expert_0_8
	expert_0_8 -> expert_agg_0
	token_route_0 -> expert_0_9
	expert_0_9 -> expert_agg_0
	token_route_0 -> expert_0_10
	expert_0_10 -> expert_agg_0
	token_route_0 -> expert_0_11
	expert_0_11 -> expert_agg_0
	token_route_0 -> expert_0_12
	expert_0_12 -> expert_agg_0
	token_route_0 -> expert_0_13
	expert_0_13 -> expert_agg_0
	token_route_0 -> expert_0_14
	expert_0_14 -> expert_agg_0
	token_route_0 -> expert_0_15
	expert_0_15 -> expert_agg_0
	expert_agg_0 -> moe_combine_0
	moe_combine_0 -> moe_res_0
	attn_res_0 -> moe_res_0
	moe_res_0 -> layernorm_1
	layernorm_1 -> mha_qkv_1
	mha_qkv_1 -> mha_attn_1
	mha_attn_1 -> mha_out_1
	mha_out_1 -> attn_res_1
	layernorm_1 -> attn_res_1
	attn_res_1 -> local_gating_1
	local_gating_1 -> token_route_1
	token_route_1 -> expert_1_0
	expert_1_0 -> expert_agg_1
	token_route_1 -> expert_1_1
	expert_1_1 -> expert_agg_1
	token_route_1 -> expert_1_2
	expert_1_2 -> expert_agg_1
	token_route_1 -> expert_1_3
	expert_1_3 -> expert_agg_1
	token_route_1 -> expert_1_4
	expert_1_4 -> expert_agg_1
	token_route_1 -> expert_1_5
	expert_1_5 -> expert_agg_1
	token_route_1 -> expert_1_6
	expert_1_6 -> expert_agg_1
	token_route_1 -> expert_1_7
	expert_1_7 -> expert_agg_1
	token_route_1 -> expert_1_8
	expert_1_8 -> expert_agg_1
	token_route_1 -> expert_1_9
	expert_1_9 -> expert_agg_1
	token_route_1 -> expert_1_10
	expert_1_10 -> expert_agg_1
	token_route_1 -> expert_1_11
	expert_1_11 -> expert_agg_1
	token_route_1 -> expert_1_12
	expert_1_12 -> expert_agg_1
	token_route_1 -> expert_1_13
	expert_1_13 -> expert_agg_1
	token_route_1 -> expert_1_14
	expert_1_14 -> expert_agg_1
	token_route_1 -> expert_1_15
	expert_1_15 -> expert_agg_1
	expert_agg_1 -> moe_combine_1
	moe_combine_1 -> moe_res_1
	attn_res_1 -> moe_res_1
	moe_res_1 -> layernorm_2
	layernorm_2 -> mha_qkv_2
	mha_qkv_2 -> mha_attn_2
	mha_attn_2 -> mha_out_2
	mha_out_2 -> attn_res_2
	layernorm_2 -> attn_res_2
	attn_res_2 -> local_gating_2
	local_gating_2 -> token_route_2
	token_route_2 -> expert_2_0
	expert_2_0 -> expert_agg_2
	token_route_2 -> expert_2_1
	expert_2_1 -> expert_agg_2
	token_route_2 -> expert_2_2
	expert_2_2 -> expert_agg_2
	token_route_2 -> expert_2_3
	expert_2_3 -> expert_agg_2
	token_route_2 -> expert_2_4
	expert_2_4 -> expert_agg_2
	token_route_2 -> expert_2_5
	expert_2_5 -> expert_agg_2
	token_route_2 -> expert_2_6
	expert_2_6 -> expert_agg_2
	token_route_2 -> expert_2_7
	expert_2_7 -> expert_agg_2
	token_route_2 -> expert_2_8
	expert_2_8 -> expert_agg_2
	token_route_2 -> expert_2_9
	expert_2_9 -> expert_agg_2
	token_route_2 -> expert_2_10
	expert_2_10 -> expert_agg_2
	token_route_2 -> expert_2_11
	expert_2_11 -> expert_agg_2
	token_route_2 -> expert_2_12
	expert_2_12 -> expert_agg_2
	token_route_2 -> expert_2_13
	expert_2_13 -> expert_agg_2
	token_route_2 -> expert_2_14
	expert_2_14 -> expert_agg_2
	token_route_2 -> expert_2_15
	expert_2_15 -> expert_agg_2
	expert_agg_2 -> moe_combine_2
	moe_combine_2 -> moe_res_2
	attn_res_2 -> moe_res_2
	moe_res_2 -> layernorm_3
	layernorm_3 -> mha_qkv_3
	mha_qkv_3 -> mha_attn_3
	mha_attn_3 -> mha_out_3
	mha_out_3 -> attn_res_3
	layernorm_3 -> attn_res_3
	attn_res_3 -> local_gating_3
	local_gating_3 -> token_route_3
	token_route_3 -> expert_3_0
	expert_3_0 -> expert_agg_3
	token_route_3 -> expert_3_1
	expert_3_1 -> expert_agg_3
	token_route_3 -> expert_3_2
	expert_3_2 -> expert_agg_3
	token_route_3 -> expert_3_3
	expert_3_3 -> expert_agg_3
	token_route_3 -> expert_3_4
	expert_3_4 -> expert_agg_3
	token_route_3 -> expert_3_5
	expert_3_5 -> expert_agg_3
	token_route_3 -> expert_3_6
	expert_3_6 -> expert_agg_3
	token_route_3 -> expert_3_7
	expert_3_7 -> expert_agg_3
	token_route_3 -> expert_3_8
	expert_3_8 -> expert_agg_3
	token_route_3 -> expert_3_9
	expert_3_9 -> expert_agg_3
	token_route_3 -> expert_3_10
	expert_3_10 -> expert_agg_3
	token_route_3 -> expert_3_11
	expert_3_11 -> expert_agg_3
	token_route_3 -> expert_3_12
	expert_3_12 -> expert_agg_3
	token_route_3 -> expert_3_13
	expert_3_13 -> expert_agg_3
	token_route_3 -> expert_3_14
	expert_3_14 -> expert_agg_3
	token_route_3 -> expert_3_15
	expert_3_15 -> expert_agg_3
	expert_agg_3 -> moe_combine_3
	moe_combine_3 -> moe_res_3
	attn_res_3 -> moe_res_3
	moe_res_3 -> output
}
