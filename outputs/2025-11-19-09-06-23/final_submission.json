{
  "generated_dags": {
    "baseline": {
      "dot_file": "../outputs/2025-11-19-09-06-23/baseline_moe_dag.dot",
      "svg_file": "../outputs/2025-11-19-09-06-23/baseline_moe_dag.svg",
      "description": "Baseline MoE deployment with TP=8, PP=2 using expert colocation"
    },
    "proposed": {
      "dot_file": "../outputs/2025-11-19-09-06-23/proposed_moe_dag_fixed.dot",
      "svg_file": "../outputs/2025-11-19-09-06-23/proposed_moe_dag_fixed.svg",
      "description": "Proposed cross-node expert parallelism with EP=16 (one expert per GPU)"
    },
    "supporting_files": {
      "deployment_analysis": "../outputs/2025-11-19-09-06-23/deployment_analysis.md",
      "baseline_generator": "../outputs/2025-11-19-09-06-23/generate_baseline_dag.py",
      "proposed_generator": "../outputs/2025-11-19-09-06-23/generate_proposed_dag_fixed.py"
    }
  },
  "validation": {
    "baseline_dag_valid": true,
    "proposed_dag_valid": true,
    "no_cycles_detected": true,
    "all_nodes_connected": true,
    "expert_partitioning_correct": true
  },
  "deployment_summary": {
    "total_gpus": 16,
    "expert_distribution": {
      "baseline": "8 experts per GPU across 2 layers",
      "proposed": "1 expert per GPU across all 4 layers"
    },
    "parallelism_strategy": {
      "baseline": "TP=8, PP=2, EP=1",
      "proposed": "EP=16, TP=1, PP=1"
    },
    "memory_allocation_per_expert": "512MB"
  }
}