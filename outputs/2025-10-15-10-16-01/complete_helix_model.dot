digraph complete_helix_model {
	rankdir=TB size="30,40"
	node [fillcolor=lightblue shape=ellipse style=filled]
	model_input [label="Model Input\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: all GPUs" fillcolor=lightgreen shape=parallelogram]
	layer0_mha [label="Layer 0 - Multi-Head Attention\nTwo-Level Partitioning (4×4=16 partitions)\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPUs: 0-15 (partitioned)" fillcolor=lightcoral shape=rectangle]
	layer0_mlp [label="Layer 0 - Feed Forward Network\nTensor Parallel (16-way)\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPUs: 0-15 (tensor parallel)" fillcolor=lightcoral shape=rectangle]
	layer1_mha [label="Layer 1 - Multi-Head Attention\nTwo-Level Partitioning (4×4=16 partitions)\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPUs: 0-15 (partitioned)" fillcolor=lightcoral shape=rectangle]
	layer1_mlp [label="Layer 1 - Feed Forward Network\nTensor Parallel (16-way)\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPUs: 0-15 (tensor parallel)" fillcolor=lightcoral shape=rectangle]
	model_output [label="Model Output\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: all GPUs" fillcolor=lightgreen shape=parallelogram]
	model_input -> layer0_mha
	layer0_mha -> layer0_mlp
	layer0_mlp -> layer1_mha
	layer1_mha -> layer1_mlp
	layer1_mlp -> model_output
}
