digraph mlp_layer_0_tensor_parallel {
	rankdir=TB size="20,30"
	node [fillcolor=lightblue shape=ellipse style=filled]
	input [label="MLP Input\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: all GPUs" fillcolor=lightgreen shape=parallelogram]
	ln [label="LayerNorm\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: all GPUs" fillcolor=lightyellow shape=rectangle]
	fc1_device_0 [label="FC1 Linear (Column Parallel)\nDevice 0\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 0" fillcolor=lightcoral shape=rectangle]
	fc1_device_1 [label="FC1 Linear (Column Parallel)\nDevice 1\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 1" fillcolor=lightcoral shape=rectangle]
	fc1_device_2 [label="FC1 Linear (Column Parallel)\nDevice 2\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 2" fillcolor=lightcoral shape=rectangle]
	fc1_device_3 [label="FC1 Linear (Column Parallel)\nDevice 3\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 3" fillcolor=lightcoral shape=rectangle]
	fc1_device_4 [label="FC1 Linear (Column Parallel)\nDevice 4\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 4" fillcolor=lightcoral shape=rectangle]
	fc1_device_5 [label="FC1 Linear (Column Parallel)\nDevice 5\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 5" fillcolor=lightcoral shape=rectangle]
	fc1_device_6 [label="FC1 Linear (Column Parallel)\nDevice 6\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 6" fillcolor=lightcoral shape=rectangle]
	fc1_device_7 [label="FC1 Linear (Column Parallel)\nDevice 7\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 7" fillcolor=lightcoral shape=rectangle]
	fc1_device_8 [label="FC1 Linear (Column Parallel)\nDevice 8\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 8" fillcolor=lightcoral shape=rectangle]
	fc1_device_9 [label="FC1 Linear (Column Parallel)\nDevice 9\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 9" fillcolor=lightcoral shape=rectangle]
	fc1_device_10 [label="FC1 Linear (Column Parallel)\nDevice 10\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 10" fillcolor=lightcoral shape=rectangle]
	fc1_device_11 [label="FC1 Linear (Column Parallel)\nDevice 11\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 11" fillcolor=lightcoral shape=rectangle]
	fc1_device_12 [label="FC1 Linear (Column Parallel)\nDevice 12\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 12" fillcolor=lightcoral shape=rectangle]
	fc1_device_13 [label="FC1 Linear (Column Parallel)\nDevice 13\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 13" fillcolor=lightcoral shape=rectangle]
	fc1_device_14 [label="FC1 Linear (Column Parallel)\nDevice 14\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 14" fillcolor=lightcoral shape=rectangle]
	fc1_device_15 [label="FC1 Linear (Column Parallel)\nDevice 15\nInput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nGPU: 15" fillcolor=lightcoral shape=rectangle]
	fc1_concat [label="Concatenate FC1 Outputs\nInput: 16×[batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=32768]\nGPU: all GPUs" fillcolor=lightsteelblue shape=parallelogram]
	gelu [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, hidden_dim=32768]\nOutput: [batch_size=1024, seq_len=10000, hidden_dim=32768]\nGPU: all GPUs" fillcolor=lightyellow shape=rectangle]
	fc2_device_0 [label="FC2 Linear (Row Parallel)\nDevice 0\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 0" fillcolor=lightcoral shape=rectangle]
	fc2_device_1 [label="FC2 Linear (Row Parallel)\nDevice 1\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 1" fillcolor=lightcoral shape=rectangle]
	fc2_device_2 [label="FC2 Linear (Row Parallel)\nDevice 2\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 2" fillcolor=lightcoral shape=rectangle]
	fc2_device_3 [label="FC2 Linear (Row Parallel)\nDevice 3\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 3" fillcolor=lightcoral shape=rectangle]
	fc2_device_4 [label="FC2 Linear (Row Parallel)\nDevice 4\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 4" fillcolor=lightcoral shape=rectangle]
	fc2_device_5 [label="FC2 Linear (Row Parallel)\nDevice 5\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 5" fillcolor=lightcoral shape=rectangle]
	fc2_device_6 [label="FC2 Linear (Row Parallel)\nDevice 6\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 6" fillcolor=lightcoral shape=rectangle]
	fc2_device_7 [label="FC2 Linear (Row Parallel)\nDevice 7\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 7" fillcolor=lightcoral shape=rectangle]
	fc2_device_8 [label="FC2 Linear (Row Parallel)\nDevice 8\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 8" fillcolor=lightcoral shape=rectangle]
	fc2_device_9 [label="FC2 Linear (Row Parallel)\nDevice 9\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 9" fillcolor=lightcoral shape=rectangle]
	fc2_device_10 [label="FC2 Linear (Row Parallel)\nDevice 10\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 10" fillcolor=lightcoral shape=rectangle]
	fc2_device_11 [label="FC2 Linear (Row Parallel)\nDevice 11\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 11" fillcolor=lightcoral shape=rectangle]
	fc2_device_12 [label="FC2 Linear (Row Parallel)\nDevice 12\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 12" fillcolor=lightcoral shape=rectangle]
	fc2_device_13 [label="FC2 Linear (Row Parallel)\nDevice 13\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 13" fillcolor=lightcoral shape=rectangle]
	fc2_device_14 [label="FC2 Linear (Row Parallel)\nDevice 14\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 14" fillcolor=lightcoral shape=rectangle]
	fc2_device_15 [label="FC2 Linear (Row Parallel)\nDevice 15\nInput: [batch_size=1024, seq_len=10000, hidden_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=2048]\nGPU: 15" fillcolor=lightcoral shape=rectangle]
	fc2_allreduce [label="All-Reduce Sum\nInput: 16×[batch_size=1024, seq_len=10000, embed_dim=2048]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: all GPUs" fillcolor=lightsteelblue shape=parallelogram]
	residual [label="Residual Add\nInput 1: [batch_size=1024, seq_len=10000, embed_dim=8192]\nInput 2: [batch_size=1024, seq_len=10000, embed_dim=8192]\nOutput: [batch_size=1024, seq_len=10000, embed_dim=8192]\nGPU: all GPUs" fillcolor=lightgray shape=rectangle]
	input -> ln
	ln -> fc1_device_0
	ln -> fc1_device_1
	ln -> fc1_device_2
	ln -> fc1_device_3
	ln -> fc1_device_4
	ln -> fc1_device_5
	ln -> fc1_device_6
	ln -> fc1_device_7
	ln -> fc1_device_8
	ln -> fc1_device_9
	ln -> fc1_device_10
	ln -> fc1_device_11
	ln -> fc1_device_12
	ln -> fc1_device_13
	ln -> fc1_device_14
	ln -> fc1_device_15
	fc1_device_0 -> fc1_concat
	fc1_device_1 -> fc1_concat
	fc1_device_2 -> fc1_concat
	fc1_device_3 -> fc1_concat
	fc1_device_4 -> fc1_concat
	fc1_device_5 -> fc1_concat
	fc1_device_6 -> fc1_concat
	fc1_device_7 -> fc1_concat
	fc1_device_8 -> fc1_concat
	fc1_device_9 -> fc1_concat
	fc1_device_10 -> fc1_concat
	fc1_device_11 -> fc1_concat
	fc1_device_12 -> fc1_concat
	fc1_device_13 -> fc1_concat
	fc1_device_14 -> fc1_concat
	fc1_device_15 -> fc1_concat
	fc1_concat -> gelu
	gelu -> fc2_device_0
	gelu -> fc2_device_1
	gelu -> fc2_device_2
	gelu -> fc2_device_3
	gelu -> fc2_device_4
	gelu -> fc2_device_5
	gelu -> fc2_device_6
	gelu -> fc2_device_7
	gelu -> fc2_device_8
	gelu -> fc2_device_9
	gelu -> fc2_device_10
	gelu -> fc2_device_11
	gelu -> fc2_device_12
	gelu -> fc2_device_13
	gelu -> fc2_device_14
	gelu -> fc2_device_15
	fc2_device_0 -> fc2_allreduce
	fc2_device_1 -> fc2_allreduce
	fc2_device_2 -> fc2_allreduce
	fc2_device_3 -> fc2_allreduce
	fc2_device_4 -> fc2_allreduce
	fc2_device_5 -> fc2_allreduce
	fc2_device_6 -> fc2_allreduce
	fc2_device_7 -> fc2_allreduce
	fc2_device_8 -> fc2_allreduce
	fc2_device_9 -> fc2_allreduce
	fc2_device_10 -> fc2_allreduce
	fc2_device_11 -> fc2_allreduce
	fc2_device_12 -> fc2_allreduce
	fc2_device_13 -> fc2_allreduce
	fc2_device_14 -> fc2_allreduce
	fc2_device_15 -> fc2_allreduce
	fc2_allreduce -> residual
	input -> residual
}
