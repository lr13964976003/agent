// EP32-TP16-PP4-DP4 MoE LLM Inference DAG
digraph new_deployment_dag {
	graph [bgcolor=white fontname=Arial rankdir=TB]
	node [fontname=Arial shape=rectangle style=filled]
	input [label="Input\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\nDP=4: Split batch into 4 groups of 32 sequences each" fillcolor=lightblue shape=ellipse]
	dp_split [label="Data Parallelism Split\nGPU: DP0-DP3\nSplit batch_size=128 → 4×32\nNo communication required for inference" fillcolor=lightgreen shape=parallelogram]
	embed_dp0 [label="Embedding Layer DP0\nGPU: DP0_TP0-DP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]\nTP=16: hidden_size=4096 → 256 per TP rank" fillcolor=lightgreen shape=rectangle]
	embed_dp1 [label="Embedding Layer DP1\nGPU: DP1_TP0-DP1_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]\nTP=16: hidden_size=4096 → 256 per TP rank" fillcolor=lightgreen shape=rectangle]
	embed_dp2 [label="Embedding Layer DP2\nGPU: DP2_TP0-DP2_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]\nTP=16: hidden_size=4096 → 256 per TP rank" fillcolor=lightgreen shape=rectangle]
	embed_dp3 [label="Embedding Layer DP3\nGPU: DP3_TP0-DP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]\nTP=16: hidden_size=4096 → 256 per TP rank" fillcolor=lightgreen shape=rectangle]
	hidden_up_dp0 [label="Hidden Upscaling DP0\nGPU: DP0_TP0-DP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\nTP=16: All-Gather operation" fillcolor=lightgreen shape=rectangle]
	hidden_up_dp1 [label="Hidden Upscaling DP1\nGPU: DP1_TP0-DP1_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\nTP=16: All-Gather operation" fillcolor=lightgreen shape=rectangle]
	hidden_up_dp2 [label="Hidden Upscaling DP2\nGPU: DP2_TP0-DP2_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\nTP=16: All-Gather operation" fillcolor=lightgreen shape=rectangle]
	hidden_up_dp3 [label="Hidden Upscaling DP3\nGPU: DP3_TP0-DP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\nTP=16: All-Gather operation" fillcolor=lightgreen shape=rectangle]
	pp0_dp0_start [label="PP Stage 0 DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nLayers 0-3 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp0_dp1_start [label="PP Stage 0 DP1\nGPU: DP1_PP0_TP0-DP1_PP0_TP15\nLayers 0-3 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp0_dp2_start [label="PP Stage 0 DP2\nGPU: DP2_PP0_TP0-DP2_PP0_TP15\nLayers 0-3 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp0_dp3_start [label="PP Stage 0 DP3\nGPU: DP3_PP0_TP0-DP3_PP0_TP15\nLayers 0-3 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	layer0_norm_dp0 [label="Layer 0: RMSNorm DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightcoral]
	layer0_q_proj_dp0 [label="Layer 0: Q Projection DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=64]" fillcolor=lightcoral]
	layer0_k_proj_dp0 [label="Layer 0: K Projection DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=64]" fillcolor=lightcoral]
	layer0_v_proj_dp0 [label="Layer 0: V Projection DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=64]" fillcolor=lightcoral]
	layer0_qk_matmul_dp0 [label="Layer 0: QK^T MatMul DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, heads=32, d_k=64] x2\nOutput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]" fillcolor=lightcoral]
	layer0_attn_scale_dp0 [label="Layer 0: Attention Scale DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\nOutput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]" fillcolor=lightcoral]
	layer0_attn_softmax_dp0 [label="Layer 0: Softmax DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\nOutput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]" fillcolor=lightcoral]
	layer0_attn_v_matmul_dp0 [label="Layer 0: Attention V MatMul DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32] x [batch_size=32, seq_len=1024, heads=32, d_k=64]\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=64]" fillcolor=lightcoral]
	layer0_attn_out_proj_dp0 [label="Layer 0: Attention Output Projection DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, heads=32, d_k=64]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightcoral]
	layer0_attn_residual_dp0 [label="Layer 0: Attention Residual DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096] x2\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightcoral]
	layer0_attn_ar_dp0 [label="All-Reduce\nTP Group: DP0_PP0_TP0-DP0_PP0_TP15\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse style=dashed]
	layer0_norm2_dp0 [label="Layer 0: RMSNorm 2 DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightyellow shape=parallelogram]
	layer0_gate_dp0 [label="Layer 0: MoE Gate DP0\nGPU: DP0_EP0-DP0_EP31\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, num_experts=64]" fillcolor=lightyellow shape=parallelogram]
	layer0_route_dp0 [label="Token Routing DP0\nGPU: DP0_EP0-DP0_EP31\nSelect 2 experts per token\nLoad balance across 64 experts\nEP32: 2 experts per GPU" fillcolor=pink shape=ellipse style=dashed]
	layer0_expert0_dp0 [label="Layer 0: Expert 0 DP0\nGPU: DP0_EP0\nInput: [batch_size=2, seq_len=32, hidden_size=4096]\nOutput: [batch_size=2, seq_len=32, hidden_size=4096]\nUp-proj → Activation → Down-proj" fillcolor=lightsteelblue]
	layer0_expert1_dp0 [label="Layer 0: Expert 1 DP0\nGPU: DP0_EP0\nInput: [batch_size=2, seq_len=32, hidden_size=4096]\nOutput: [batch_size=2, seq_len=32, hidden_size=4096]\nUp-proj → Activation → Down-proj" fillcolor=lightsteelblue]
	layer0_expert2_dp0 [label="Layer 0: Expert 2 DP0\nGPU: DP0_EP1\nInput: [batch_size=2, seq_len=32, hidden_size=4096]\nOutput: [batch_size=2, seq_len=32, hidden_size=4096]\nUp-proj → Activation → Down-proj" fillcolor=lightsteelblue]
	layer0_expert3_dp0 [label="Layer 0: Expert 3 DP0\nGPU: DP0_EP1\nInput: [batch_size=2, seq_len=32, hidden_size=4096]\nOutput: [batch_size=2, seq_len=32, hidden_size=4096]\nUp-proj → Activation → Down-proj" fillcolor=lightsteelblue]
	layer0_ep_a2a_dp0 [label="All-to-All\nEP Group: DP0_EP0-DP0_EP31\nToken dispatch/combine\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	layer0_expert_combine_dp0 [label="Expert Combine DP0\nGPU: DP0_EP0-DP0_EP31\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightyellow shape=parallelogram]
	layer0_moe_residual_dp0 [label="Layer 0: MoE Residual DP0\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096] x2\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightcoral]
	layers1to3_dp0 [label="Layers 1-3 DP0\nSame structure as Layer 0\n4 layers per stage\nTP=16, EP=32 (2 experts/GPU)" fillcolor=lightgray shape=rectangle]
	layers1to3_dp1 [label="Layers 1-3 DP1\nSame structure as Layer 0\n4 layers per stage\nTP=16, EP=32 (2 experts/GPU)" fillcolor=lightgray shape=rectangle]
	layers1to3_dp2 [label="Layers 1-3 DP2\nSame structure as Layer 0\n4 layers per stage\nTP=16, EP=32 (2 experts/GPU)" fillcolor=lightgray shape=rectangle]
	layers1to3_dp3 [label="Layers 1-3 DP3\nSame structure as Layer 0\n4 layers per stage\nTP=16, EP=32 (2 experts/GPU)" fillcolor=lightgray shape=rectangle]
	pp0_to_pp1_dp0 [label="Pipeline Transfer DP0\nPP0 → PP1\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp1_to_pp2_dp0 [label="Pipeline Transfer DP0\nPP1 → PP2\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp2_to_pp3_dp0 [label="Pipeline Transfer DP0\nPP2 → PP3\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp0_to_pp1_dp1 [label="Pipeline Transfer DP1\nPP0 → PP1\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp1_to_pp2_dp1 [label="Pipeline Transfer DP1\nPP1 → PP2\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp2_to_pp3_dp1 [label="Pipeline Transfer DP1\nPP2 → PP3\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp0_to_pp1_dp2 [label="Pipeline Transfer DP2\nPP0 → PP1\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp1_to_pp2_dp2 [label="Pipeline Transfer DP2\nPP1 → PP2\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp2_to_pp3_dp2 [label="Pipeline Transfer DP2\nPP2 → PP3\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp0_to_pp1_dp3 [label="Pipeline Transfer DP3\nPP0 → PP1\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp1_to_pp2_dp3 [label="Pipeline Transfer DP3\nPP1 → PP2\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp2_to_pp3_dp3 [label="Pipeline Transfer DP3\nPP2 → PP3\nActivations transfer\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=orange shape=ellipse]
	pp1_dp0_start [label="PP Stage 1 DP0\nGPU: DP0_PP1_TP0-DP0_PP1_TP15\nLayers 4-7 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp2_dp0_start [label="PP Stage 2 DP0\nGPU: DP0_PP2_TP0-DP0_PP2_TP15\nLayers 8-11 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp3_dp0_start [label="PP Stage 3 DP0\nGPU: DP0_PP3_TP0-DP0_PP3_TP15\nLayers 12-15 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp1_dp1_start [label="PP Stage 1 DP1\nGPU: DP1_PP1_TP0-DP1_PP1_TP15\nLayers 4-7 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp2_dp1_start [label="PP Stage 2 DP1\nGPU: DP1_PP2_TP0-DP1_PP2_TP15\nLayers 8-11 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp3_dp1_start [label="PP Stage 3 DP1\nGPU: DP1_PP3_TP0-DP1_PP3_TP15\nLayers 12-15 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp1_dp2_start [label="PP Stage 1 DP2\nGPU: DP2_PP1_TP0-DP2_PP1_TP15\nLayers 4-7 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp2_dp2_start [label="PP Stage 2 DP2\nGPU: DP2_PP2_TP0-DP2_PP2_TP15\nLayers 8-11 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp3_dp2_start [label="PP Stage 3 DP2\nGPU: DP2_PP3_TP0-DP2_PP3_TP15\nLayers 12-15 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp1_dp3_start [label="PP Stage 1 DP3\nGPU: DP3_PP1_TP0-DP3_PP1_TP15\nLayers 4-7 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp2_dp3_start [label="PP Stage 2 DP3\nGPU: DP3_PP2_TP0-DP3_PP2_TP15\nLayers 8-11 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	pp3_dp3_start [label="PP Stage 3 DP3\nGPU: DP3_PP3_TP0-DP3_PP3_TP15\nLayers 12-15 (4 layers)\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances" fillcolor=yellow shape=parallelogram]
	final_norm_dp0 [label="Final RMSNorm DP0\nGPU: DP0_PP3_TP0-DP0_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	output_proj_dp0 [label="Output Projection DP0\nGPU: DP0_PP3_TP0-DP0_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]" fillcolor=lightgreen shape=rectangle]
	final_norm_dp1 [label="Final RMSNorm DP1\nGPU: DP1_PP3_TP0-DP1_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	output_proj_dp1 [label="Output Projection DP1\nGPU: DP1_PP3_TP0-DP1_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]" fillcolor=lightgreen shape=rectangle]
	final_norm_dp2 [label="Final RMSNorm DP2\nGPU: DP2_PP3_TP0-DP2_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	output_proj_dp2 [label="Output Projection DP2\nGPU: DP2_PP3_TP0-DP2_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]" fillcolor=lightgreen shape=rectangle]
	final_norm_dp3 [label="Final RMSNorm DP3\nGPU: DP3_PP3_TP0-DP3_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]" fillcolor=lightgreen shape=rectangle]
	output_proj_dp3 [label="Output Projection DP3\nGPU: DP3_PP3_TP0-DP3_PP3_TP15\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]" fillcolor=lightgreen shape=rectangle]
	dp_combine [label="Data Parallelism Combine\nGPU: Output aggregation\nInput: 4×[batch_size=32, seq_len=1024, vocab_size=51200]\nOutput: [batch_size=128, seq_len=1024, vocab_size=51200]" fillcolor=orange shape=ellipse]
	output [label="Output\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\nOutput: [batch_size=128, seq_len=1024, vocab_size=51200]" fillcolor=lightblue shape=ellipse]
	input -> dp_split
	dp_split -> embed_dp0
	embed_dp0 -> hidden_up_dp0
	hidden_up_dp0 -> pp0_dp0_start
	pp0_dp0_start -> layer0_norm_dp0
	layer0_norm_dp0 -> layer0_q_proj_dp0
	layer0_norm_dp0 -> layer0_k_proj_dp0
	layer0_norm_dp0 -> layer0_v_proj_dp0
	layer0_q_proj_dp0 -> layer0_qk_matmul_dp0
	layer0_k_proj_dp0 -> layer0_qk_matmul_dp0
	layer0_qk_matmul_dp0 -> layer0_attn_scale_dp0
	layer0_attn_scale_dp0 -> layer0_attn_softmax_dp0
	layer0_attn_softmax_dp0 -> layer0_attn_v_matmul_dp0
	layer0_v_proj_dp0 -> layer0_attn_v_matmul_dp0
	layer0_attn_v_matmul_dp0 -> layer0_attn_out_proj_dp0
	layer0_attn_out_proj_dp0 -> layer0_attn_residual_dp0
	hidden_up_dp0 -> layer0_attn_residual_dp0
	layer0_attn_residual_dp0 -> layer0_attn_ar_dp0
	layer0_attn_ar_dp0 -> layer0_norm2_dp0
	layer0_norm2_dp0 -> layer0_gate_dp0
	layer0_gate_dp0 -> layer0_route_dp0
	layer0_route_dp0 -> layer0_expert0_dp0
	layer0_route_dp0 -> layer0_expert1_dp0
	layer0_route_dp0 -> layer0_expert2_dp0
	layer0_route_dp0 -> layer0_expert3_dp0
	layer0_expert0_dp0 -> layer0_ep_a2a_dp0
	layer0_expert1_dp0 -> layer0_ep_a2a_dp0
	layer0_expert2_dp0 -> layer0_ep_a2a_dp0
	layer0_expert3_dp0 -> layer0_ep_a2a_dp0
	layer0_ep_a2a_dp0 -> layer0_expert_combine_dp0
	layer0_expert_combine_dp0 -> layer0_moe_residual_dp0
	layer0_attn_ar_dp0 -> layer0_moe_residual_dp0
	layer0_moe_residual_dp0 -> layers1to3_dp0
	layers1to3_dp0 -> pp0_to_pp1_dp0
	pp0_to_pp1_dp0 -> pp1_dp0_start
	pp1_dp0_start -> pp1_to_pp2_dp0
	pp1_to_pp2_dp0 -> pp2_dp0_start
	pp2_dp0_start -> pp2_to_pp3_dp0
	pp2_to_pp3_dp0 -> pp3_dp0_start
	pp3_dp0_start -> final_norm_dp0
	final_norm_dp0 -> output_proj_dp0
	output_proj_dp0 -> dp_combine
	dp_split -> embed_dp1
	embed_dp1 -> hidden_up_dp1
	hidden_up_dp1 -> pp0_dp1_start
	pp0_dp1_start -> layers1to3_dp1
	layers1to3_dp1 -> pp0_to_pp1_dp1
	pp0_to_pp1_dp1 -> pp1_dp1_start
	pp1_dp1_start -> pp1_to_pp2_dp1
	pp1_to_pp2_dp1 -> pp2_dp1_start
	pp2_dp1_start -> pp2_to_pp3_dp1
	pp2_to_pp3_dp1 -> pp3_dp1_start
	pp3_dp1_start -> final_norm_dp1
	final_norm_dp1 -> output_proj_dp1
	output_proj_dp1 -> dp_combine
	dp_split -> embed_dp2
	embed_dp2 -> hidden_up_dp2
	hidden_up_dp2 -> pp0_dp2_start
	pp0_dp2_start -> layers1to3_dp2
	layers1to3_dp2 -> pp0_to_pp1_dp2
	pp0_to_pp1_dp2 -> pp1_dp2_start
	pp1_dp2_start -> pp1_to_pp2_dp2
	pp1_to_pp2_dp2 -> pp2_dp2_start
	pp2_dp2_start -> pp2_to_pp3_dp2
	pp2_to_pp3_dp2 -> pp3_dp2_start
	pp3_dp2_start -> final_norm_dp2
	final_norm_dp2 -> output_proj_dp2
	output_proj_dp2 -> dp_combine
	dp_split -> embed_dp3
	embed_dp3 -> hidden_up_dp3
	hidden_up_dp3 -> pp0_dp3_start
	pp0_dp3_start -> layers1to3_dp3
	layers1to3_dp3 -> pp0_to_pp1_dp3
	pp0_to_pp1_dp3 -> pp1_dp3_start
	pp1_dp3_start -> pp1_to_pp2_dp3
	pp1_to_pp2_dp3 -> pp2_dp3_start
	pp2_dp3_start -> pp2_to_pp3_dp3
	pp2_to_pp3_dp3 -> pp3_dp3_start
	pp3_dp3_start -> final_norm_dp3
	final_norm_dp3 -> output_proj_dp3
	output_proj_dp3 -> dp_combine
	dp_combine -> output
}
