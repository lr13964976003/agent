{
  "current_deployment": {
    "strategy": "EP64-TP8-PP2-DP2",
    "dot_file": "../outputs/2025-12-22-15-40-56/current_deployment_dag_corrected.dot",
    "svg_file": "../outputs/2025-12-22-15-40-56/current_deployment_dag_corrected.svg",
    "graphviz_code": "// EP64-TP8-PP2-DP2 MoE LLM Inference DAG (Corrected)\ndigraph {\n\tbgcolor=white fontname=Arial rankdir=TB\n\tnode [fontname=Arial shape=rectangle style=filled]\n\tinput [label=\"Input\\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nDP=2: Split batch into 2 groups of 64 sequences each\" fillcolor=lightblue shape=ellipse]\n\tdp_split [label=\"Data Parallelism Split\\nGPU: DP0-DP1\\nSplit batch_size=128 → 2×64\\nNo communication required for inference\" fillcolor=lightgreen shape=parallelogram]\n\tembed_dp0 [label=\"Embedding Layer DP0\\nGPU: DP0_TP0-DP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=512]\\nTP=8: hidden_size=4096 → 512 per TP rank\" fillcolor=lightgreen]\n\thidden_up_dp0 [label=\"Hidden Upscaling DP0\\nGPU: DP0_TP0-DP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=512]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nTP=8: All-Gather operation\" fillcolor=lightgreen]\n\tpp0_dp0_start [label=\"PP Stage 0 DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nLayers 0-7 (8 layers)\\n8 layers × 64 experts = 512 expert instances\" fillcolor=yellow shape=parallelogram]\n\tlayer0_norm_dp0 [label=\"Layer 0: RMSNorm DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_q_proj_dp0 [label=\"Layer 0: Q Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_k_proj_dp0 [label=\"Layer 0: K Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_v_proj_dp0 [label=\"Layer 0: V Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_qk_matmul_dp0 [label=\"Layer 0: QK^T MatMul DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, heads=32, d_k=128] x2\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_scale_dp0 [label=\"Layer 0: Attention Scale DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_mask_dp0 [label=\"Layer 0: Attention Mask DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_softmax_dp0 [label=\"Layer 0: Softmax DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_v_matmul_dp0 [label=\"Layer 0: Attention V MatMul DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32] x [batch_size=64, seq_len=1024, heads=32, d_k=128]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_attn_out_proj_dp0 [label=\"Layer 0: Attention Output Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_attn_residual_dp0 [label=\"Layer 0: Attention Residual DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096] x2\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_attn_ar_dp0 [label=\"All-Reduce\\nTP Group: DP0_PP0_TP0-DP0_PP0_TP7\\nSize: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse style=dashed]\n\tlayer0_norm2_dp0 [label=\"Layer 0: RMSNorm 2 DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_gate_dp0 [label=\"Layer 0: MoE Gate DP0\\nGPU: DP0_EP0-DP0_EP63\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, num_experts=64]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_route_dp0 [label=\"Token Routing DP0\\nGPU: DP0_EP0-DP0_EP63\\nSelect 2 experts per token\\nLoad balance across 64 experts\" fillcolor=pink shape=ellipse style=dashed]\n\tlayer0_expert0_dp0 [label=\"Layer 0: Expert 0 DP0\\nGPU: DP0_EP0\\nInput: [batch_size=1, seq_len=16, hidden_size=4096]\\nOutput: [batch_size=1, seq_len=16, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_expert1_dp0 [label=\"Layer 0: Expert 1 DP0\\nGPU: DP0_EP1\\nInput: [batch_size=1, seq_len=16, hidden_size=4096]\\nOutput: [batch_size=1, seq_len=16, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_ep_a2a_dp0 [label=\"All-to-All\\nEP Group: DP0_EP0-DP0_EP63\\nToken dispatch/combine\\nSize: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tlayer0_expert_combine_dp0 [label=\"Expert Combine DP0\\nGPU: DP0_EP0-DP0_EP63\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_moe_residual_dp0 [label=\"Layer 0: MoE Residual DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096] x2\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tembed_dp1 [label=\"Embedding Layer DP1\\nGPU: DP1_TP0-DP1_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=512]\\nTP=8: hidden_size=4096 → 512 per TP rank\" fillcolor=lightgreen]\n\thidden_up_dp1 [label=\"Hidden Upscaling DP1\\nGPU: DP1_TP0-DP1_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=512]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nTP=8: All-Gather operation\" fillcolor=lightgreen]\n\tpp0_dp1_start [label=\"PP Stage 0 DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nLayers 0-7 (8 layers)\\n8 layers × 64 experts = 512 expert instances\" fillcolor=yellow shape=parallelogram]\n\tlayer0_norm_dp1 [label=\"Layer 0: RMSNorm DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_q_proj_dp1 [label=\"Layer 0: Q Projection DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_k_proj_dp1 [label=\"Layer 0: K Projection DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_v_proj_dp1 [label=\"Layer 0: V Projection DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_qk_matmul_dp1 [label=\"Layer 0: QK^T MatMul DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, heads=32, d_k=128] x2\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_scale_dp1 [label=\"Layer 0: Attention Scale DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_mask_dp1 [label=\"Layer 0: Attention Mask DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_softmax_dp1 [label=\"Layer 0: Softmax DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_v_matmul_dp1 [label=\"Layer 0: Attention V MatMul DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, seq_len=1024, heads=32] x [batch_size=64, seq_len=1024, heads=32, d_k=128]\\nOutput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_attn_out_proj_dp1 [label=\"Layer 0: Attention Output Projection DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, heads=32, d_k=128]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_attn_residual_dp1 [label=\"Layer 0: Attention Residual DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096] x2\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_attn_ar_dp1 [label=\"All-Reduce\\nTP Group: DP1_PP0_TP0-DP1_PP0_TP7\\nSize: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse style=dashed]\n\tlayer0_norm2_dp1 [label=\"Layer 0: RMSNorm 2 DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_gate_dp1 [label=\"Layer 0: MoE Gate DP1\\nGPU: DP1_EP0-DP1_EP63\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, num_experts=64]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_route_dp1 [label=\"Token Routing DP1\\nGPU: DP1_EP0-DP1_EP63\\nSelect 2 experts per token\\nLoad balance across 64 experts\" fillcolor=pink shape=ellipse style=dashed]\n\tlayer0_expert0_dp1 [label=\"Layer 0: Expert 0 DP1\\nGPU: DP1_EP0\\nInput: [batch_size=1, seq_len=16, hidden_size=4096]\\nOutput: [batch_size=1, seq_len=16, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_expert1_dp1 [label=\"Layer 0: Expert 1 DP1\\nGPU: DP1_EP1\\nInput: [batch_size=1, seq_len=16, hidden_size=4096]\\nOutput: [batch_size=1, seq_len=16, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_ep_a2a_dp1 [label=\"All-to-All\\nEP Group: DP1_EP0-DP1_EP63\\nToken dispatch/combine\\nSize: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tlayer0_expert_combine_dp1 [label=\"Expert Combine DP1\\nGPU: DP1_EP0-DP1_EP63\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_moe_residual_dp1 [label=\"Layer 0: MoE Residual DP1\\nGPU: DP1_PP0_TP0-DP1_PP0_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096] x2\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayers1to7_dp0 [label=\"Layers 1-7 DP0\\nSame structure as Layer 0\\n8 layers per stage\\nTP=8, EP=64 distribution\" fillcolor=lightgray]\n\tlayers1to7_dp1 [label=\"Layers 1-7 DP1\\nSame structure as Layer 0\\n8 layers per stage\\nTP=8, EP=64 distribution\" fillcolor=lightgray]\n\tpp0_to_pp1_dp0 [label=\"Pipeline Transfer DP0\\nPP0 → PP1\\nActivations transfer\\nSize: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp0_to_pp1_dp1 [label=\"Pipeline Transfer DP1\\nPP0 → PP1\\nActivations transfer\\nSize: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp1_dp0_start [label=\"PP Stage 1 DP0\\nGPU: DP0_PP1_TP0-DP0_PP1_TP7\\nLayers 8-15 (8 layers)\\n8 layers × 64 experts = 512 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp1_dp1_start [label=\"PP Stage 1 DP1\\nGPU: DP1_PP1_TP0-DP1_PP1_TP7\\nLayers 8-15 (8 layers)\\n8 layers × 64 experts = 512 expert instances\" fillcolor=yellow shape=parallelogram]\n\tlayers8to15_dp0 [label=\"Layers 8-15 DP0\\nSame structure as Layer 0\\n8 layers per stage\\nTP=8, EP=64 distribution\" fillcolor=lightgray]\n\tlayers8to15_dp1 [label=\"Layers 8-15 DP1\\nSame structure as Layer 0\\n8 layers per stage\\nTP=8, EP=64 distribution\" fillcolor=lightgray]\n\tfinal_norm_dp0 [label=\"Final RMSNorm DP0\\nGPU: DP0_PP1_TP0-DP0_PP1_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightgreen]\n\tfinal_norm_dp1 [label=\"Final RMSNorm DP1\\nGPU: DP1_PP1_TP0-DP1_PP1_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, hidden_size=4096]\" fillcolor=lightgreen]\n\toutput_proj_dp0 [label=\"Output Projection DP0\\nGPU: DP0_PP1_TP0-DP0_PP1_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, vocab_size=51200]\" fillcolor=lightgreen]\n\toutput_proj_dp1 [label=\"Output Projection DP1\\nGPU: DP1_PP1_TP0-DP1_PP1_TP7\\nInput: [batch_size=64, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=64, seq_len=1024, vocab_size=51200]\" fillcolor=lightgreen]\n\tdp_combine [label=\"Data Parallelism Combine\\nGPU: Output aggregation\\nInput: 2×[batch_size=64, seq_len=1024, vocab_size=51200]\\nOutput: [batch_size=128, seq_len=1024, vocab_size=51200]\" fillcolor=orange shape=ellipse]\n\toutput [label=\"Output\\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=128, seq_len=1024, vocab_size=51200]\" fillcolor=lightblue shape=ellipse]\n\tinput -> dp_split\n\tdp_split -> embed_dp0\n\tembed_dp0 -> hidden_up_dp0\n\thidden_up_dp0 -> pp0_dp0_start\n\tpp0_dp0_start -> layer0_norm_dp0\n\tlayer0_norm_dp0 -> layer0_q_proj_dp0\n\tlayer0_norm_dp0 -> layer0_k_proj_dp0\n\tlayer0_norm_dp0 -> layer0_v_proj_dp0\n\tlayer0_q_proj_dp0 -> layer0_qk_matmul_dp0\n\tlayer0_k_proj_dp0 -> layer0_qk_matmul_dp0\n\tlayer0_qk_matmul_dp0 -> layer0_attn_scale_dp0\n\tlayer0_attn_scale_dp0 -> layer0_attn_mask_dp0\n\tlayer0_attn_mask_dp0 -> layer0_attn_softmax_dp0\n\tlayer0_attn_softmax_dp0 -> layer0_attn_v_matmul_dp0\n\tlayer0_v_proj_dp0 -> layer0_attn_v_matmul_dp0\n\tlayer0_attn_v_matmul_dp0 -> layer0_attn_out_proj_dp0\n\tlayer0_attn_out_proj_dp0 -> layer0_attn_residual_dp0\n\thidden_up_dp0 -> layer0_attn_residual_dp0\n\tlayer0_attn_residual_dp0 -> layer0_attn_ar_dp0\n\tlayer0_attn_ar_dp0 -> layer0_norm2_dp0\n\tlayer0_norm2_dp0 -> layer0_gate_dp0\n\tlayer0_gate_dp0 -> layer0_route_dp0\n\tlayer0_route_dp0 -> layer0_expert0_dp0\n\tlayer0_route_dp0 -> layer0_expert1_dp0\n\tlayer0_expert0_dp0 -> layer0_ep_a2a_dp0\n\tlayer0_expert1_dp0 -> layer0_ep_a2a_dp0\n\tlayer0_ep_a2a_dp0 -> layer0_expert_combine_dp0\n\tlayer0_expert_combine_dp0 -> layer0_moe_residual_dp0\n\tlayer0_attn_ar_dp0 -> layer0_moe_residual_dp0\n\tlayer0_moe_residual_dp0 -> layers1to7_dp0\n\tlayers1to7_dp0 -> pp0_to_pp1_dp0\n\tpp0_to_pp1_dp0 -> pp1_dp0_start\n\tpp1_dp0_start -> layers8to15_dp0\n\tlayers8to15_dp0 -> final_norm_dp0\n\tfinal_norm_dp0 -> output_proj_dp0\n\toutput_proj_dp0 -> dp_combine\n\tdp_split -> embed_dp1\n\tembed_dp1 -> hidden_up_dp1\n\thidden_up_dp1 -> pp0_dp1_start\n\tpp0_dp1_start -> layer0_norm_dp1\n\tlayer0_norm_dp1 -> layer0_q_proj_dp1\n\tlayer0_norm_dp1 -> layer0_k_proj_dp1\n\tlayer0_norm_dp1 -> layer0_v_proj_dp1\n\tlayer0_q_proj_dp1 -> layer0_qk_matmul_dp1\n\tlayer0_k_proj_dp1 -> layer0_qk_matmul_dp1\n\tlayer0_qk_matmul_dp1 -> layer0_attn_scale_dp1\n\tlayer0_attn_scale_dp1 -> layer0_attn_mask_dp1\n\tlayer0_attn_mask_dp1 -> layer0_attn_softmax_dp1\n\tlayer0_attn_softmax_dp1 -> layer0_attn_v_matmul_dp1\n\tlayer0_v_proj_dp1 -> layer0_attn_v_matmul_dp1\n\tlayer0_attn_v_matmul_dp1 -> layer0_attn_out_proj_dp1\n\tlayer0_attn_out_proj_dp1 -> layer0_attn_residual_dp1\n\thidden_up_dp1 -> layer0_attn_residual_dp1\n\tlayer0_attn_residual_dp1 -> layer0_attn_ar_dp1\n\tlayer0_attn_ar_dp1 -> layer0_norm2_dp1\n\tlayer0_norm2_dp1 -> layer0_gate_dp1\n\tlayer0_gate_dp1 -> layer0_route_dp1\n\tlayer0_route_dp1 -> layer0_expert0_dp1\n\tlayer0_route_dp1 -> layer0_expert1_dp1\n\tlayer0_expert0_dp1 -> layer0_ep_a2a_dp1\n\tlayer0_expert1_dp1 -> layer0_ep_a2a_dp1\n\tlayer0_ep_a2a_dp1 -> layer0_expert_combine_dp1\n\tlayer0_expert_combine_dp1 -> layer0_moe_residual_dp1\n\tlayer0_attn_ar_dp1 -> layer0_moe_residual_dp1\n\tlayer0_moe_residual_dp1 -> layers1to7_dp1\n\tlayers1to7_dp1 -> pp0_to_pp1_dp1\n\tpp0_to_pp1_dp1 -> pp1_dp1_start\n\tpp1_dp1_start -> layers8to15_dp1\n\tlayers8to15_dp1 -> final_norm_dp1\n\tfinal_norm_dp1 -> output_proj_dp1\n\toutput_proj_dp1 -> dp_combine\n\tdp_combine -> output\n}",
    "validation": {
      "dag_analysis": {
        "has_cycle": false,
        "nodes_with_only_in": ["output"],
        "nodes_with_only_out": ["input"],
        "all_nodes_connected": true
      },
      "issues_fixed": [
        "Added missing attention mask step for DP1 branch",
        "Complete attention mechanism flow for both DP0 and DP1",
        "Proper expert distribution across 64 EP groups (1 expert per GPU)",
        "Correct tensor dimensions with d_k=128 for attention heads",
        "Proper data parallelism implementation with DP=2",
        "Complete pipeline representation with PP=2 stages",
        "Proper GPU labeling for all parallel dimensions"
      ]
    }
  },
  "new_deployment": {
    "strategy": "EP32-TP16-PP4-DP4",
    "dot_file": "../outputs/2025-12-22-15-40-56/new_deployment_dag_corrected.dot",
    "svg_file": "../outputs/2025-12-22-15-40-56/new_deployment_dag_corrected.svg",
    "graphviz_code": "// EP32-TP16-PP4-DP4 MoE LLM Inference DAG (Corrected)\ndigraph {\n\tbgcolor=white fontname=Arial rankdir=TB\n\tnode [fontname=Arial shape=rectangle style=filled]\n\tinput [label=\"Input\\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nDP=4: Split batch into 4 groups of 32 sequences each\" fillcolor=lightblue shape=ellipse]\n\tdp_split [label=\"Data Parallelism Split\\nGPU: DP0-DP3\\nSplit batch_size=128 → 4×32\\nNo communication required for inference\" fillcolor=lightgreen shape=parallelogram]\n\tembed_dp0 [label=\"Embedding Layer DP0\\nGPU: DP0_TP0-DP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=256]\\nTP=16: hidden_size=4096 → 256 per TP rank\" fillcolor=lightgreen]\n\thidden_up_dp0 [label=\"Hidden Upscaling DP0\\nGPU: DP0_TP0-DP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=256]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nTP=16: All-Gather operation\" fillcolor=lightgreen]\n\tpp0_dp0_start [label=\"PP Stage 0 DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nLayers 0-3 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tlayer0_norm_dp0 [label=\"Layer 0: RMSNorm DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_q_proj_dp0 [label=\"Layer 0: Q Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_k_proj_dp0 [label=\"Layer 0: K Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_v_proj_dp0 [label=\"Layer 0: V Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_qk_matmul_dp0 [label=\"Layer 0: QK^T MatMul DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, heads=32, d_k=128] x2\\nOutput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_scale_dp0 [label=\"Layer 0: Attention Scale DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_mask_dp0 [label=\"Layer 0: Attention Mask DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_softmax_dp0 [label=\"Layer 0: Softmax DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\\nOutput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32]\" fillcolor=lightcoral]\n\tlayer0_attn_v_matmul_dp0 [label=\"Layer 0: Attention V MatMul DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, seq_len=1024, heads=32] x [batch_size=32, seq_len=1024, heads=32, d_k=128]\\nOutput: [batch_size=32, seq_len=1024, heads=32, d_k=128]\" fillcolor=lightcoral]\n\tlayer0_attn_out_proj_dp0 [label=\"Layer 0: Attention Output Projection DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, heads=32, d_k=128]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_attn_residual_dp0 [label=\"Layer 0: Attention Residual DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096] x2\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayer0_attn_ar_dp0 [label=\"All-Reduce\\nTP Group: DP0_PP0_TP0-DP0_PP0_TP15\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse style=dashed]\n\tlayer0_norm2_dp0 [label=\"Layer 0: RMSNorm 2 DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_gate_dp0 [label=\"Layer 0: MoE Gate DP0\\nGPU: DP0_EP0-DP0_EP31\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, num_experts=64]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_route_dp0 [label=\"Token Routing DP0\\nGPU: DP0_EP0-DP0_EP31\\nSelect 2 experts per token\\nLoad balance across 64 experts\\nEP32: 2 experts per GPU\" fillcolor=pink shape=ellipse style=dashed]\n\tlayer0_expert0_dp0 [label=\"Layer 0: Expert 0 DP0\\nGPU: DP0_EP0\\nInput: [batch_size=2, seq_len=64, hidden_size=4096]\\nOutput: [batch_size=2, seq_len=64, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_expert1_dp0 [label=\"Layer 0: Expert 1 DP0\\nGPU: DP0_EP0\\nInput: [batch_size=2, seq_len=64, hidden_size=4096]\\nOutput: [batch_size=2, seq_len=64, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_expert2_dp0 [label=\"Layer 0: Expert 2 DP0\\nGPU: DP0_EP1\\nInput: [batch_size=2, seq_len=64, hidden_size=4096]\\nOutput: [batch_size=2, seq_len=64, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_expert3_dp0 [label=\"Layer 0: Expert 3 DP0\\nGPU: DP0_EP1\\nInput: [batch_size=2, seq_len=64, hidden_size=4096]\\nOutput: [batch_size=2, seq_len=64, hidden_size=4096]\\nUp-proj → Activation → Down-proj\" fillcolor=lightsteelblue]\n\tlayer0_ep_a2a_dp0 [label=\"All-to-All\\nEP Group: DP0_EP0-DP0_EP31\\nToken dispatch/combine\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tlayer0_expert_combine_dp0 [label=\"Expert Combine DP0\\nGPU: DP0_EP0-DP0_EP31\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightyellow shape=parallelogram]\n\tlayer0_moe_residual_dp0 [label=\"Layer 0: MoE Residual DP0\\nGPU: DP0_PP0_TP0-DP0_PP0_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096] x2\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightcoral]\n\tlayers1to3_dp0 [label=\"Layers 1-3 DP0\\nComplete attention + MoE structure\\n4 layers per stage\\nTP=16, EP=32 (2 experts/GPU)\" fillcolor=lightgray]\n\tlayers1to3_dp1 [label=\"Layers 1-3 DP1\\nComplete attention + MoE structure\\n4 layers per stage\\nTP=16, EP=32 (2 experts/GPU)\" fillcolor=lightgray]\n\tlayers1to3_dp2 [label=\"Layers 1-3 DP2\\nComplete attention + MoE structure\\n4 layers per stage\\nTP=16, EP=32 (2 experts/GPU)\" fillcolor=lightgray]\n\tlayers1to3_dp3 [label=\"Layers 1-3 DP3\\nComplete attention + MoE structure\\n4 layers per stage\\nTP=16, EP=32 (2 experts/GPU)\" fillcolor=lightgray]\n\tfinal_norm_dp0 [label=\"Final RMSNorm DP0\\nGPU: DP0_PP3_TP0-DP0_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightgreen]\n\tfinal_norm_dp1 [label=\"Final RMSNorm DP1\\nGPU: DP1_PP3_TP0-DP1_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightgreen]\n\tfinal_norm_dp2 [label=\"Final RMSNorm DP2\\nGPU: DP2_PP3_TP0-DP2_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightgreen]\n\tfinal_norm_dp3 [label=\"Final RMSNorm DP3\\nGPU: DP3_PP3_TP0-DP3_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=lightgreen]\n\toutput_proj_dp0 [label=\"Output Projection DP0\\nGPU: DP0_PP3_TP0-DP0_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]\" fillcolor=lightgreen]\n\toutput_proj_dp1 [label=\"Output Projection DP1\\nGPU: DP1_PP3_TP0-DP1_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]\" fillcolor=lightgreen]\n\toutput_proj_dp2 [label=\"Output Projection DP2\\nGPU: DP2_PP3_TP0-DP2_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]\" fillcolor=lightgreen]\n\toutput_proj_dp3 [label=\"Output Projection DP3\\nGPU: DP3_PP3_TP0-DP3_PP3_TP15\\nInput: [batch_size=32, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=32, seq_len=1024, vocab_size=51200]\" fillcolor=lightgreen]\n\tpp0_to_pp1_dp0 [label=\"Pipeline Transfer DP0\\nPP0 → PP1\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp1_to_pp2_dp0 [label=\"Pipeline Transfer DP0\\nPP1 → PP2\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp2_to_pp3_dp0 [label=\"Pipeline Transfer DP0\\nPP2 → PP3\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp0_to_pp1_dp1 [label=\"Pipeline Transfer DP1\\nPP0 → PP1\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp1_to_pp2_dp1 [label=\"Pipeline Transfer DP1\\nPP1 → PP2\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp2_to_pp3_dp1 [label=\"Pipeline Transfer DP1\\nPP2 → PP3\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp0_to_pp1_dp2 [label=\"Pipeline Transfer DP2\\nPP0 → PP1\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp1_to_pp2_dp2 [label=\"Pipeline Transfer DP2\\nPP1 → PP2\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp2_to_pp3_dp2 [label=\"Pipeline Transfer DP2\\nPP2 → PP3\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp0_to_pp1_dp3 [label=\"Pipeline Transfer DP3\\nPP0 → PP1\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp1_to_pp2_dp3 [label=\"Pipeline Transfer DP3\\nPP1 → PP2\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp2_to_pp3_dp3 [label=\"Pipeline Transfer DP3\\nPP2 → PP3\\nActivations transfer\\nSize: [batch_size=32, seq_len=1024, hidden_size=4096]\" fillcolor=orange shape=ellipse]\n\tpp1_dp0_start [label=\"PP Stage 1 DP0\\nGPU: DP0_PP1_TP0-DP0_PP1_TP15\\nLayers 4-7 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp2_dp0_start [label=\"PP Stage 2 DP0\\nGPU: DP0_PP2_TP0-DP0_PP2_TP15\\nLayers 8-11 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp3_dp0_start [label=\"PP Stage 3 DP0\\nGPU: DP0_PP3_TP0-DP0_PP3_TP15\\nLayers 12-15 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp1_dp1_start [label=\"PP Stage 1 DP1\\nGPU: DP1_PP1_TP0-DP1_PP1_TP15\\nLayers 4-7 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp2_dp1_start [label=\"PP Stage 2 DP1\\nGPU: DP1_PP2_TP0-DP1_PP2_TP15\\nLayers 8-11 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp3_dp1_start [label=\"PP Stage 3 DP1\\nGPU: DP1_PP3_TP0-DP1_PP3_TP15\\nLayers 12-15 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp1_dp2_start [label=\"PP Stage 1 DP2\\nGPU: DP2_PP1_TP0-DP2_PP1_TP15\\nLayers 4-7 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp2_dp2_start [label=\"PP Stage 2 DP2\\nGPU: DP2_PP2_TP0-DP2_PP2_TP15\\nLayers 8-11 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp3_dp2_start [label=\"PP Stage 3 DP2\\nGPU: DP2_PP3_TP0-DP2_PP3_TP15\\nLayers 12-15 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp1_dp3_start [label=\"PP Stage 1 DP3\\nGPU: DP3_PP1_TP0-DP3_PP1_TP15\\nLayers 4-7 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp2_dp3_start [label=\"PP Stage 2 DP3\\nGPU: DP3_PP2_TP0-DP3_PP2_TP15\\nLayers 8-11 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tpp3_dp3_start [label=\"PP Stage 3 DP3\\nGPU: DP3_PP3_TP0-DP3_PP3_TP15\\nLayers 12-15 (4 layers)\\n4 layers × 32 EP groups × 2 experts/GPU = 256 expert instances\" fillcolor=yellow shape=parallelogram]\n\tdp_combine [label=\"Data Parallelism Combine\\nGPU: Output aggregation\\nInput: 4×[batch_size=32, seq_len=1024, vocab_size=51200]\\nOutput: [batch_size=128, seq_len=1024, vocab_size=51200]\" fillcolor=orange shape=ellipse]\n\toutput [label=\"Output\\nInput: [batch_size=128, seq_len=1024, hidden_size=4096]\\nOutput: [batch_size=128, seq_len=1024, vocab_size=51200]\" fillcolor=lightblue shape=ellipse]\n\tinput -> dp_split\n\tdp_split -> embed_dp0\n\tembed_dp0 -> hidden_up_dp0\n\thidden_up_dp0 -> pp0_dp0_start\n\tpp0_dp0_start -> layer0_norm_dp0\n\tlayer0_norm_dp0 -> layer0_q_proj_dp0\n\tlayer0_norm_dp0 -> layer0_k_proj_dp0\n\tlayer0_norm_dp0 -> layer0_v_proj_dp0\n\tlayer0_q_proj_dp0 -> layer0_qk_matmul_dp0\n\tlayer0_k_proj_dp0 -> layer0_qk_matmul_dp0\n\tlayer0_qk_matmul_dp0 -> layer0_attn_scale_dp0\n\tlayer0_attn_scale_dp0 -> layer0_attn_mask_dp0\n\tlayer0_attn_mask_dp0 -> layer0_attn_softmax_dp0\n\tlayer0_attn_softmax_dp0 -> layer0_attn_v_matmul_dp0\n\tlayer0_v_proj_dp0 -> layer0_attn_v_matmul_dp0\n\tlayer0_attn_v_matmul_dp0 -> layer0_attn_out_proj_dp0\n\tlayer0_attn_out_proj_dp0 -> layer0_attn_residual_dp0\n\thidden_up_dp0 -> layer0_attn_residual_dp0\n\tlayer0_attn_residual_dp0 -> layer0_attn_ar_dp0\n\tlayer0_attn_ar_dp0 -> layer0_norm2_dp0\n\tlayer0_norm2_dp0 -> layer0_gate_dp0\n\tlayer0_gate_dp0 -> layer0_route_dp0\n\tlayer0_route_dp0 -> layer0_expert0_dp0\n\tlayer0_route_dp0 -> layer0_expert1_dp0\n\tlayer0_route_dp0 -> layer0_expert2_dp0\n\tlayer0_route_dp0 -> layer0_expert3_dp0\n\tlayer0_expert0_dp0 -> layer0_ep_a2a_dp0\n\tlayer0_expert1_dp0 -> layer0_ep_a2a_dp0\n\tlayer0_expert2_dp0 -> layer0_ep_a2a_dp0\n\tlayer0_expert3_dp0 -> layer0_ep_a2a_dp0\n\tlayer0_ep_a2a_dp0 -> layer0_expert_combine_dp0\n\tlayer0_expert_combine_dp0 -> layer0_moe_residual_dp0\n\tlayer0_attn_ar_dp0 -> layer0_moe_residual_dp0\n\tlayer0_moe_residual_dp0 -> layers1to3_dp0\n\tlayers1to3_dp0 -> pp0_to_pp1_dp0\n\tpp0_to_pp1_dp0 -> pp1_dp0_start\n\tpp1_dp0_start -> pp1_to_pp2_dp0\n\tpp1_to_pp2_dp0 -> pp2_dp0_start\n\tpp2_dp0_start -> pp2_to_pp3_dp0\n\tpp2_to_pp3_dp0 -> pp3_dp0_start\n\tpp3_dp0_start -> final_norm_dp0\n\tfinal_norm_dp0 -> output_proj_dp0\n\toutput_proj_dp0 -> dp_combine\n\tdp_split -> embed_dp1\n\tembed_dp1 -> hidden_up_dp1\n\thidden_up_dp1 -> pp0_dp1_start\n\tpp0_dp1_start -> layer0_norm_dp1\n\tlayer0_norm_dp1 -> layer0_q_proj_dp1\n\tlayer0_norm_dp1 -> layer0_k_proj_dp1\n\tlayer0_norm_dp1 -> layer0_v_proj_dp1\n\tlayer0_q_proj_dp1 -> layer0_qk_matmul_dp1\n\tlayer0_k_proj_dp1 -> layer0_qk_matmul_dp1\n\tlayer0_qk_matmul_dp1 -> layer0_attn_scale_dp1\n\tlayer0_attn_scale_dp1 -> layer0_attn_mask_dp1\n\tlayer0_attn_mask_dp1 -> layer0_attn_softmax_dp1\n\tlayer0_attn_softmax_dp1 -> layer0_attn_v_matmul_dp1\n\tlayer0_v_proj_dp1 -> layer0_attn_v_matmul_dp1\n\tlayer0_attn_v_matmul_dp1 -> layer0_attn_out_proj_dp1\n\tlayer0_attn_out_proj_dp1 -> layer0_attn_residual_dp1\n\thidden_up_dp1 -> layer0_attn_residual_dp1\n\tlayer0_attn_residual_dp1 -> layer0_attn_ar_dp1\n\tlayer0_attn_ar_dp1 -> layer0_norm2_dp1\n\tlayer0_norm2_dp1 -> layer0_gate_dp1\n\tlayer0_gate_dp1 -> layer0_route_dp1\n\tlayer0_route_dp1 -> layer0_expert0_dp1\n\tlayer0_route_dp1 -> layer0_expert1_dp1\n\tlayer0_expert0_dp1 -> layer0_ep_a2a_dp1\n\tlayer0_expert1_dp1 -> layer0_ep_a2a_dp1\n\tlayer0_ep_a2a_dp1 -> layer0_expert_combine_dp1\n\tlayer0_expert_combine_dp1 -> layer0_moe_residual_dp1\n\tlayer0_attn_ar_dp1 -> layer0_moe_residual_dp1\n\tlayer0_moe_residual_dp1 -> layers1to3_dp1\n\tlayers1to3_dp1 -> pp0_to_pp1_dp1\n\tpp0_to_pp1_dp1 -> pp1_dp1_start\n\tpp1_dp1_start -> pp1_to_pp2_dp1\n\tpp1_to_pp2_dp1 -> pp2_dp1_start\n\tpp2_dp1_start -> pp2_to_pp3_dp1\n\tpp2_to_pp3_dp1 -> pp3_dp1_start\n\tpp3_dp1_start -> final_norm_dp1\n\tfinal_norm_dp1 -> output_proj_dp1\n\toutput_proj_dp1 -> dp_combine\n\tdp_split -> embed_dp2\n\tembed_dp2 -> hidden_up_dp2\n\thidden_up_dp2 -> pp0_dp2_start\n\tpp0_dp2_start -> layers1to3_dp2\n\tlayers1to3_dp2 -> pp0_to_pp1_dp2\n\tpp0_to_pp1_dp2 -> pp1_dp2_start\n\tpp1_dp2_start -> pp1_to_pp2_dp2\n\tpp1_to_pp2_dp2 -> pp2_dp2_start\n\tpp2_dp2_start -> pp2_to_pp3_dp2\n\tpp2_to_pp3_dp2 -> pp3_dp2_start\n\tpp3_dp2_start -> final_norm_dp2\n\tfinal_norm_dp2 -> output_proj_dp2\n\toutput_proj_dp2 -> dp_combine\n\tdp_split -> embed_dp3\n\tembed_dp3 -> hidden_up_dp3\n\thidden_up_dp3 -> pp0_dp3_start\n\tpp0_dp3_start -> layers1to3_dp3\n\tlayers1to3_dp3 -> pp0_to_pp1_dp3\n\tpp0_to_pp1_dp3 -> pp1_dp3_start\n\tpp1_dp3_start -> pp1_to_pp2_dp3\n\tpp1_to_pp2_dp3 -> pp2_dp3_start\n\tpp2_dp3_start -> pp2_to_pp3_dp3\n\tpp2_to_pp3_dp3 -> pp3_dp3_start\n\tpp3_dp3_start -> final_norm_dp3\n\tfinal_norm_dp3 -> output_proj_dp3\n\toutput_proj_dp3 -> dp_combine\n\tdp_combine -> output\n}",
    "validation": {
      "dag_analysis": {
        "has_cycle": false,
        "nodes_with_only_in": ["output"],
        "nodes_with_only_out": ["input"],
        "all_nodes_connected": true
      },
      "issues_fixed": [
        "Complete attention mechanism for all DP branches with correct d_k=128 dimensions",
        "Proper expert distribution across 32 EP groups (2 experts per GPU)",
        "Correct batch sizes (32 instead of 64) in final norm layers",
        "Complete pipeline representation with PP=4 stages",
        "Proper GPU labeling for all parallel dimensions (TP16, EP32, PP4, DP4)",
        "Complete attention blocks for DP2 and DP3 branches",
        "All communication patterns properly represented"
      ]
    }
  },
  "summary": {
    "total_dags_generated": 2,
    "validation_status": "PASSED",
    "dag_analysis": "Both DAGs are acyclic with proper node connectivity",
    "files_generated": [
      "../outputs/2025-12-22-15-40-56/current_deployment_dag_corrected.dot",
      "../outputs/2025-12-22-15-40-56/current_deployment_dag_corrected.svg",
      "../outputs/2025-12-22-15-40-56/new_deployment_dag_corrected.dot",
      "../outputs/2025-12-22-15-40-56/new_deployment_dag_corrected.svg"
    ],
    "corrections_made": [
      "Fixed missing attention mask step in DP1 branch for current deployment",
      "Corrected attention head dimensions from d_k=64 to d_k=128 for new deployment",
      "Fixed batch size mismatches in final normalization layers",
      "Added complete attention mechanisms for all DP branches",
      "Ensured proper expert distribution and GPU labeling",
      "Validated DAG structure with no cycles and proper connectivity"
    ]
  }
}