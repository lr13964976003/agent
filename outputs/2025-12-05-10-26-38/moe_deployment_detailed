// 30B MoE Model Deployment DAG
digraph {
	bgcolor=white rankdir=TB splines=ortho
	node [fillcolor=lightblue shape=rectangle style=filled]
	input [label="Input\n[batch_size=128, seq_len=1024, hidden=1024]\nGPU: ALL" fillcolor=white shape=ellipse]
	dp_split [label="Data Parallel Split\n[batch_size=64, seq_len=1024, hidden=1024]\nGPU: Routing" fillcolor=lightpink shape=parallelogram]
	layer0_attn_qkv_gpu0 [label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer0_attn_qkv_gpu1 [label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer0_attn_qkv_gpu2 [label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer0_attn_qkv_gpu3 [label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer0_attn_score_gpu0 [label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0" fillcolor=lightblue]
	layer0_attn_score_gpu1 [label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1" fillcolor=lightblue]
	layer0_attn_score_gpu2 [label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2" fillcolor=lightblue]
	layer0_attn_score_gpu3 [label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3" fillcolor=lightblue]
	layer0_attn_out_gpu0 [label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer0_attn_out_gpu1 [label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer0_attn_out_gpu2 [label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer0_attn_out_gpu3 [label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer0_attn_allreduce [label="Layer0 Attention\nAll-Reduce Sum\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1024]\nGPU: 0,1,2,3" fillcolor=lightgray shape=ellipse]
	layer0_moe_route [label="Layer0 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0,1,2,3" fillcolor=lightblue shape=parallelogram]
	layer0_expert0 [label="Layer0 Expert 0_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 0" fillcolor=lightblue]
	layer0_expert1 [label="Layer0 Expert 0_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 1" fillcolor=lightblue]
	layer0_expert2 [label="Layer0 Expert 0_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 2" fillcolor=lightblue]
	layer0_expert3 [label="Layer0 Expert 0_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 3" fillcolor=lightblue]
	layer0_expert4 [label="Layer0 Expert 1_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 4" fillcolor=lightgreen]
	layer0_expert5 [label="Layer0 Expert 1_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 5" fillcolor=lightgreen]
	layer0_expert6 [label="Layer0 Expert 1_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 6" fillcolor=lightgreen]
	layer0_expert7 [label="Layer0 Expert 1_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 7" fillcolor=lightgreen]
	layer0_expert8 [label="Layer0 Expert 2_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 8" fillcolor=lightyellow]
	layer0_expert9 [label="Layer0 Expert 2_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 9" fillcolor=lightyellow]
	layer0_expert10 [label="Layer0 Expert 2_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 10" fillcolor=lightyellow]
	layer0_expert11 [label="Layer0 Expert 2_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 11" fillcolor=lightyellow]
	layer0_expert12 [label="Layer0 Expert 3_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 12" fillcolor=lightcoral]
	layer0_expert13 [label="Layer0 Expert 3_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 13" fillcolor=lightcoral]
	layer0_expert14 [label="Layer0 Expert 3_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 14" fillcolor=lightcoral]
	layer0_expert15 [label="Layer0 Expert 3_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 15" fillcolor=lightcoral]
	layer0_moe_all2all [label="Layer0 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer0_moe_agg [label="Layer0 MoE\nOutput Aggregation\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1024]\nGPU: 0,1,2,3" fillcolor=lightblue shape=parallelogram]
	layer1_attn_qkv_gpu0 [label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer1_attn_score_gpu0 [label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0" fillcolor=lightblue]
	layer1_attn_out_gpu0 [label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer1_moe_route_gpu0 [label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0" fillcolor=lightblue shape=parallelogram]
	layer1_attn_qkv_gpu1 [label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer1_attn_score_gpu1 [label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1" fillcolor=lightblue]
	layer1_attn_out_gpu1 [label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer1_moe_route_gpu1 [label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 1" fillcolor=lightblue shape=parallelogram]
	layer1_attn_qkv_gpu2 [label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer1_attn_score_gpu2 [label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2" fillcolor=lightblue]
	layer1_attn_out_gpu2 [label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer1_moe_route_gpu2 [label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 2" fillcolor=lightblue shape=parallelogram]
	layer1_attn_qkv_gpu3 [label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer1_attn_score_gpu3 [label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3" fillcolor=lightblue]
	layer1_attn_out_gpu3 [label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer1_moe_route_gpu3 [label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 3" fillcolor=lightblue shape=parallelogram]
	layer1_attn_allreduce [label="Layer1 Attention\nAll-Reduce Sum\nGPU: 0,1,2,3" fillcolor=lightgray shape=ellipse]
	layer1_moe_all2all [label="Layer1 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer1_moe_agg [label="Layer1 MoE\nOutput Aggregation\nGPU: 0,1,2,3" fillcolor=lightblue shape=parallelogram]
	layer2_attn_qkv_gpu0 [label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer2_attn_score_gpu0 [label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0" fillcolor=lightblue]
	layer2_attn_out_gpu0 [label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer2_moe_route_gpu0 [label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0" fillcolor=lightblue shape=parallelogram]
	layer2_attn_qkv_gpu1 [label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer2_attn_score_gpu1 [label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1" fillcolor=lightblue]
	layer2_attn_out_gpu1 [label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer2_moe_route_gpu1 [label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 1" fillcolor=lightblue shape=parallelogram]
	layer2_attn_qkv_gpu2 [label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer2_attn_score_gpu2 [label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2" fillcolor=lightblue]
	layer2_attn_out_gpu2 [label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer2_moe_route_gpu2 [label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 2" fillcolor=lightblue shape=parallelogram]
	layer2_attn_qkv_gpu3 [label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer2_attn_score_gpu3 [label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3" fillcolor=lightblue]
	layer2_attn_out_gpu3 [label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer2_moe_route_gpu3 [label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 3" fillcolor=lightblue shape=parallelogram]
	layer2_attn_allreduce [label="Layer2 Attention\nAll-Reduce Sum\nGPU: 0,1,2,3" fillcolor=lightgray shape=ellipse]
	layer2_moe_all2all [label="Layer2 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer2_moe_agg [label="Layer2 MoE\nOutput Aggregation\nGPU: 0,1,2,3" fillcolor=lightblue shape=parallelogram]
	layer3_attn_qkv_gpu0 [label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer3_attn_score_gpu0 [label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0" fillcolor=lightblue]
	layer3_attn_out_gpu0 [label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0" fillcolor=lightblue]
	layer3_moe_route_gpu0 [label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0" fillcolor=lightblue shape=parallelogram]
	layer3_attn_qkv_gpu1 [label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer3_attn_score_gpu1 [label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1" fillcolor=lightblue]
	layer3_attn_out_gpu1 [label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1" fillcolor=lightblue]
	layer3_moe_route_gpu1 [label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 1" fillcolor=lightblue shape=parallelogram]
	layer3_attn_qkv_gpu2 [label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer3_attn_score_gpu2 [label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2" fillcolor=lightblue]
	layer3_attn_out_gpu2 [label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2" fillcolor=lightblue]
	layer3_moe_route_gpu2 [label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 2" fillcolor=lightblue shape=parallelogram]
	layer3_attn_qkv_gpu3 [label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer3_attn_score_gpu3 [label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3" fillcolor=lightblue]
	layer3_attn_out_gpu3 [label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3" fillcolor=lightblue]
	layer3_moe_route_gpu3 [label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 3" fillcolor=lightblue shape=parallelogram]
	layer3_attn_allreduce [label="Layer3 Attention\nAll-Reduce Sum\nGPU: 0,1,2,3" fillcolor=lightgray shape=ellipse]
	layer3_moe_all2all [label="Layer3 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer3_moe_agg [label="Layer3 MoE\nOutput Aggregation\nGPU: 0,1,2,3" fillcolor=lightblue shape=parallelogram]
	layer4_attn_qkv_gpu4 [label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer4_attn_score_gpu4 [label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4" fillcolor=lightgreen]
	layer4_attn_out_gpu4 [label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer4_moe_route_gpu4 [label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4" fillcolor=lightgreen shape=parallelogram]
	layer4_attn_qkv_gpu5 [label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer4_attn_score_gpu5 [label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5" fillcolor=lightgreen]
	layer4_attn_out_gpu5 [label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer4_moe_route_gpu5 [label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5" fillcolor=lightgreen shape=parallelogram]
	layer4_attn_qkv_gpu6 [label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer4_attn_score_gpu6 [label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6" fillcolor=lightgreen]
	layer4_attn_out_gpu6 [label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer4_moe_route_gpu6 [label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6" fillcolor=lightgreen shape=parallelogram]
	layer4_attn_qkv_gpu7 [label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer4_attn_score_gpu7 [label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7" fillcolor=lightgreen]
	layer4_attn_out_gpu7 [label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer4_moe_route_gpu7 [label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7" fillcolor=lightgreen shape=parallelogram]
	layer4_attn_allreduce [label="Layer4 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7" fillcolor=lightgray shape=ellipse]
	layer4_moe_all2all [label="Layer4 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer4_moe_agg [label="Layer4 MoE\nOutput Aggregation\nGPU: 4,5,6,7" fillcolor=lightgreen shape=parallelogram]
	layer5_attn_qkv_gpu4 [label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer5_attn_score_gpu4 [label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4" fillcolor=lightgreen]
	layer5_attn_out_gpu4 [label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer5_moe_route_gpu4 [label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4" fillcolor=lightgreen shape=parallelogram]
	layer5_attn_qkv_gpu5 [label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer5_attn_score_gpu5 [label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5" fillcolor=lightgreen]
	layer5_attn_out_gpu5 [label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer5_moe_route_gpu5 [label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5" fillcolor=lightgreen shape=parallelogram]
	layer5_attn_qkv_gpu6 [label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer5_attn_score_gpu6 [label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6" fillcolor=lightgreen]
	layer5_attn_out_gpu6 [label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer5_moe_route_gpu6 [label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6" fillcolor=lightgreen shape=parallelogram]
	layer5_attn_qkv_gpu7 [label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer5_attn_score_gpu7 [label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7" fillcolor=lightgreen]
	layer5_attn_out_gpu7 [label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer5_moe_route_gpu7 [label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7" fillcolor=lightgreen shape=parallelogram]
	layer5_attn_allreduce [label="Layer5 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7" fillcolor=lightgray shape=ellipse]
	layer5_moe_all2all [label="Layer5 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer5_moe_agg [label="Layer5 MoE\nOutput Aggregation\nGPU: 4,5,6,7" fillcolor=lightgreen shape=parallelogram]
	layer6_attn_qkv_gpu4 [label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer6_attn_score_gpu4 [label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4" fillcolor=lightgreen]
	layer6_attn_out_gpu4 [label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer6_moe_route_gpu4 [label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4" fillcolor=lightgreen shape=parallelogram]
	layer6_attn_qkv_gpu5 [label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer6_attn_score_gpu5 [label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5" fillcolor=lightgreen]
	layer6_attn_out_gpu5 [label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer6_moe_route_gpu5 [label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5" fillcolor=lightgreen shape=parallelogram]
	layer6_attn_qkv_gpu6 [label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer6_attn_score_gpu6 [label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6" fillcolor=lightgreen]
	layer6_attn_out_gpu6 [label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer6_moe_route_gpu6 [label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6" fillcolor=lightgreen shape=parallelogram]
	layer6_attn_qkv_gpu7 [label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer6_attn_score_gpu7 [label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7" fillcolor=lightgreen]
	layer6_attn_out_gpu7 [label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer6_moe_route_gpu7 [label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7" fillcolor=lightgreen shape=parallelogram]
	layer6_attn_allreduce [label="Layer6 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7" fillcolor=lightgray shape=ellipse]
	layer6_moe_all2all [label="Layer6 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer6_moe_agg [label="Layer6 MoE\nOutput Aggregation\nGPU: 4,5,6,7" fillcolor=lightgreen shape=parallelogram]
	layer7_attn_qkv_gpu4 [label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer7_attn_score_gpu4 [label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4" fillcolor=lightgreen]
	layer7_attn_out_gpu4 [label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4" fillcolor=lightgreen]
	layer7_moe_route_gpu4 [label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4" fillcolor=lightgreen shape=parallelogram]
	layer7_attn_qkv_gpu5 [label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer7_attn_score_gpu5 [label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5" fillcolor=lightgreen]
	layer7_attn_out_gpu5 [label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5" fillcolor=lightgreen]
	layer7_moe_route_gpu5 [label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5" fillcolor=lightgreen shape=parallelogram]
	layer7_attn_qkv_gpu6 [label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer7_attn_score_gpu6 [label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6" fillcolor=lightgreen]
	layer7_attn_out_gpu6 [label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6" fillcolor=lightgreen]
	layer7_moe_route_gpu6 [label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6" fillcolor=lightgreen shape=parallelogram]
	layer7_attn_qkv_gpu7 [label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer7_attn_score_gpu7 [label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7" fillcolor=lightgreen]
	layer7_attn_out_gpu7 [label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7" fillcolor=lightgreen]
	layer7_moe_route_gpu7 [label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7" fillcolor=lightgreen shape=parallelogram]
	layer7_attn_allreduce [label="Layer7 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7" fillcolor=lightgray shape=ellipse]
	layer7_moe_all2all [label="Layer7 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer7_moe_agg [label="Layer7 MoE\nOutput Aggregation\nGPU: 4,5,6,7" fillcolor=lightgreen shape=parallelogram]
	layer8_attn_qkv_gpu8 [label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer8_attn_score_gpu8 [label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8" fillcolor=lightyellow]
	layer8_attn_out_gpu8 [label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer8_moe_route_gpu8 [label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8" fillcolor=lightyellow shape=parallelogram]
	layer8_attn_qkv_gpu9 [label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer8_attn_score_gpu9 [label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9" fillcolor=lightyellow]
	layer8_attn_out_gpu9 [label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer8_moe_route_gpu9 [label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9" fillcolor=lightyellow shape=parallelogram]
	layer8_attn_qkv_gpu10 [label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer8_attn_score_gpu10 [label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10" fillcolor=lightyellow]
	layer8_attn_out_gpu10 [label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer8_moe_route_gpu10 [label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10" fillcolor=lightyellow shape=parallelogram]
	layer8_attn_qkv_gpu11 [label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer8_attn_score_gpu11 [label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11" fillcolor=lightyellow]
	layer8_attn_out_gpu11 [label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer8_moe_route_gpu11 [label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
	layer8_attn_allreduce [label="Layer8 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11" fillcolor=lightgray shape=ellipse]
	layer8_moe_all2all [label="Layer8 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer8_moe_agg [label="Layer8 MoE\nOutput Aggregation\nGPU: 8,9,10,11" fillcolor=lightyellow shape=parallelogram]
	layer9_attn_qkv_gpu8 [label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer9_attn_score_gpu8 [label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8" fillcolor=lightyellow]
	layer9_attn_out_gpu8 [label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer9_moe_route_gpu8 [label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8" fillcolor=lightyellow shape=parallelogram]
	layer9_attn_qkv_gpu9 [label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer9_attn_score_gpu9 [label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9" fillcolor=lightyellow]
	layer9_attn_out_gpu9 [label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer9_moe_route_gpu9 [label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9" fillcolor=lightyellow shape=parallelogram]
	layer9_attn_qkv_gpu10 [label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer9_attn_score_gpu10 [label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10" fillcolor=lightyellow]
	layer9_attn_out_gpu10 [label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer9_moe_route_gpu10 [label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10" fillcolor=lightyellow shape=parallelogram]
	layer9_attn_qkv_gpu11 [label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer9_attn_score_gpu11 [label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11" fillcolor=lightyellow]
	layer9_attn_out_gpu11 [label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer9_moe_route_gpu11 [label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
	layer9_attn_allreduce [label="Layer9 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11" fillcolor=lightgray shape=ellipse]
	layer9_moe_all2all [label="Layer9 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer9_moe_agg [label="Layer9 MoE\nOutput Aggregation\nGPU: 8,9,10,11" fillcolor=lightyellow shape=parallelogram]
	layer10_attn_qkv_gpu8 [label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer10_attn_score_gpu8 [label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8" fillcolor=lightyellow]
	layer10_attn_out_gpu8 [label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer10_moe_route_gpu8 [label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8" fillcolor=lightyellow shape=parallelogram]
	layer10_attn_qkv_gpu9 [label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer10_attn_score_gpu9 [label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9" fillcolor=lightyellow]
	layer10_attn_out_gpu9 [label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer10_moe_route_gpu9 [label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9" fillcolor=lightyellow shape=parallelogram]
	layer10_attn_qkv_gpu10 [label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer10_attn_score_gpu10 [label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10" fillcolor=lightyellow]
	layer10_attn_out_gpu10 [label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer10_moe_route_gpu10 [label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10" fillcolor=lightyellow shape=parallelogram]
	layer10_attn_qkv_gpu11 [label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer10_attn_score_gpu11 [label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11" fillcolor=lightyellow]
	layer10_attn_out_gpu11 [label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer10_moe_route_gpu11 [label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
	layer10_attn_allreduce [label="Layer10 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11" fillcolor=lightgray shape=ellipse]
	layer10_moe_all2all [label="Layer10 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer10_moe_agg [label="Layer10 MoE\nOutput Aggregation\nGPU: 8,9,10,11" fillcolor=lightyellow shape=parallelogram]
	layer11_attn_qkv_gpu8 [label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer11_attn_score_gpu8 [label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8" fillcolor=lightyellow]
	layer11_attn_out_gpu8 [label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8" fillcolor=lightyellow]
	layer11_moe_route_gpu8 [label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8" fillcolor=lightyellow shape=parallelogram]
	layer11_attn_qkv_gpu9 [label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer11_attn_score_gpu9 [label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9" fillcolor=lightyellow]
	layer11_attn_out_gpu9 [label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9" fillcolor=lightyellow]
	layer11_moe_route_gpu9 [label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9" fillcolor=lightyellow shape=parallelogram]
	layer11_attn_qkv_gpu10 [label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer11_attn_score_gpu10 [label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10" fillcolor=lightyellow]
	layer11_attn_out_gpu10 [label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10" fillcolor=lightyellow]
	layer11_moe_route_gpu10 [label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10" fillcolor=lightyellow shape=parallelogram]
	layer11_attn_qkv_gpu11 [label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer11_attn_score_gpu11 [label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11" fillcolor=lightyellow]
	layer11_attn_out_gpu11 [label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11" fillcolor=lightyellow]
	layer11_moe_route_gpu11 [label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11" fillcolor=lightyellow shape=parallelogram]
	layer11_attn_allreduce [label="Layer11 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11" fillcolor=lightgray shape=ellipse]
	layer11_moe_all2all [label="Layer11 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer11_moe_agg [label="Layer11 MoE\nOutput Aggregation\nGPU: 8,9,10,11" fillcolor=lightyellow shape=parallelogram]
	layer12_attn_qkv_gpu12 [label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer12_attn_score_gpu12 [label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12" fillcolor=lightcoral]
	layer12_attn_out_gpu12 [label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer12_moe_route_gpu12 [label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12" fillcolor=lightcoral shape=parallelogram]
	layer12_attn_qkv_gpu13 [label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer12_attn_score_gpu13 [label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13" fillcolor=lightcoral]
	layer12_attn_out_gpu13 [label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer12_moe_route_gpu13 [label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13" fillcolor=lightcoral shape=parallelogram]
	layer12_attn_qkv_gpu14 [label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer12_attn_score_gpu14 [label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14" fillcolor=lightcoral]
	layer12_attn_out_gpu14 [label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer12_moe_route_gpu14 [label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14" fillcolor=lightcoral shape=parallelogram]
	layer12_attn_qkv_gpu15 [label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer12_attn_score_gpu15 [label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15" fillcolor=lightcoral]
	layer12_attn_out_gpu15 [label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer12_moe_route_gpu15 [label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15" fillcolor=lightcoral shape=parallelogram]
	layer12_attn_allreduce [label="Layer12 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15" fillcolor=lightgray shape=ellipse]
	layer12_moe_all2all [label="Layer12 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer12_moe_agg [label="Layer12 MoE\nOutput Aggregation\nGPU: 12,13,14,15" fillcolor=lightcoral shape=parallelogram]
	layer13_attn_qkv_gpu12 [label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer13_attn_score_gpu12 [label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12" fillcolor=lightcoral]
	layer13_attn_out_gpu12 [label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer13_moe_route_gpu12 [label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12" fillcolor=lightcoral shape=parallelogram]
	layer13_attn_qkv_gpu13 [label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer13_attn_score_gpu13 [label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13" fillcolor=lightcoral]
	layer13_attn_out_gpu13 [label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer13_moe_route_gpu13 [label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13" fillcolor=lightcoral shape=parallelogram]
	layer13_attn_qkv_gpu14 [label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer13_attn_score_gpu14 [label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14" fillcolor=lightcoral]
	layer13_attn_out_gpu14 [label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer13_moe_route_gpu14 [label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14" fillcolor=lightcoral shape=parallelogram]
	layer13_attn_qkv_gpu15 [label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer13_attn_score_gpu15 [label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15" fillcolor=lightcoral]
	layer13_attn_out_gpu15 [label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer13_moe_route_gpu15 [label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15" fillcolor=lightcoral shape=parallelogram]
	layer13_attn_allreduce [label="Layer13 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15" fillcolor=lightgray shape=ellipse]
	layer13_moe_all2all [label="Layer13 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer13_moe_agg [label="Layer13 MoE\nOutput Aggregation\nGPU: 12,13,14,15" fillcolor=lightcoral shape=parallelogram]
	layer14_attn_qkv_gpu12 [label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer14_attn_score_gpu12 [label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12" fillcolor=lightcoral]
	layer14_attn_out_gpu12 [label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer14_moe_route_gpu12 [label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12" fillcolor=lightcoral shape=parallelogram]
	layer14_attn_qkv_gpu13 [label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer14_attn_score_gpu13 [label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13" fillcolor=lightcoral]
	layer14_attn_out_gpu13 [label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer14_moe_route_gpu13 [label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13" fillcolor=lightcoral shape=parallelogram]
	layer14_attn_qkv_gpu14 [label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer14_attn_score_gpu14 [label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14" fillcolor=lightcoral]
	layer14_attn_out_gpu14 [label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer14_moe_route_gpu14 [label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14" fillcolor=lightcoral shape=parallelogram]
	layer14_attn_qkv_gpu15 [label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer14_attn_score_gpu15 [label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15" fillcolor=lightcoral]
	layer14_attn_out_gpu15 [label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer14_moe_route_gpu15 [label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15" fillcolor=lightcoral shape=parallelogram]
	layer14_attn_allreduce [label="Layer14 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15" fillcolor=lightgray shape=ellipse]
	layer14_moe_all2all [label="Layer14 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer14_moe_agg [label="Layer14 MoE\nOutput Aggregation\nGPU: 12,13,14,15" fillcolor=lightcoral shape=parallelogram]
	layer15_attn_qkv_gpu12 [label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer15_attn_score_gpu12 [label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12" fillcolor=lightcoral]
	layer15_attn_out_gpu12 [label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12" fillcolor=lightcoral]
	layer15_moe_route_gpu12 [label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12" fillcolor=lightcoral shape=parallelogram]
	layer15_attn_qkv_gpu13 [label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer15_attn_score_gpu13 [label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13" fillcolor=lightcoral]
	layer15_attn_out_gpu13 [label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13" fillcolor=lightcoral]
	layer15_moe_route_gpu13 [label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13" fillcolor=lightcoral shape=parallelogram]
	layer15_attn_qkv_gpu14 [label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer15_attn_score_gpu14 [label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14" fillcolor=lightcoral]
	layer15_attn_out_gpu14 [label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14" fillcolor=lightcoral]
	layer15_moe_route_gpu14 [label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14" fillcolor=lightcoral shape=parallelogram]
	layer15_attn_qkv_gpu15 [label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer15_attn_score_gpu15 [label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15" fillcolor=lightcoral]
	layer15_attn_out_gpu15 [label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15" fillcolor=lightcoral]
	layer15_moe_route_gpu15 [label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15" fillcolor=lightcoral shape=parallelogram]
	layer15_attn_allreduce [label="Layer15 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15" fillcolor=lightgray shape=ellipse]
	layer15_moe_all2all [label="Layer15 MoE\nAll-to-All Communication\nGPU: 0-15" fillcolor=lightgray shape=ellipse]
	layer15_moe_agg [label="Layer15 MoE\nOutput Aggregation\nGPU: 12,13,14,15" fillcolor=lightcoral shape=parallelogram]
	output_agg [label="Output Aggregation\nAll-Reduce Sum\nInput: [128, 1024, 1024]\nOutput: [128, 1024, 1024]\nGPU: 12,13,14,15" fillcolor=lightpink shape=parallelogram]
	output [label="Final Output\n[batch_size=128, seq_len=1024, hidden=1024]\nGPU: ALL" fillcolor=white shape=ellipse]
	input -> dp_split
	dp_split -> layer0_attn_qkv_gpu0
	layer0_attn_qkv_gpu0 -> layer0_attn_score_gpu0
	layer0_attn_score_gpu0 -> layer0_attn_out_gpu0
	layer0_attn_out_gpu0 -> layer0_attn_allreduce
	layer0_attn_allreduce -> layer0_moe_route_gpu0
	dp_split -> layer0_attn_qkv_gpu1
	layer0_attn_qkv_gpu1 -> layer0_attn_score_gpu1
	layer0_attn_score_gpu1 -> layer0_attn_out_gpu1
	layer0_attn_out_gpu1 -> layer0_attn_allreduce
	layer0_attn_allreduce -> layer0_moe_route_gpu1
	dp_split -> layer0_attn_qkv_gpu2
	layer0_attn_qkv_gpu2 -> layer0_attn_score_gpu2
	layer0_attn_score_gpu2 -> layer0_attn_out_gpu2
	layer0_attn_out_gpu2 -> layer0_attn_allreduce
	layer0_attn_allreduce -> layer0_moe_route_gpu2
	dp_split -> layer0_attn_qkv_gpu3
	layer0_attn_qkv_gpu3 -> layer0_attn_score_gpu3
	layer0_attn_score_gpu3 -> layer0_attn_out_gpu3
	layer0_attn_out_gpu3 -> layer0_attn_allreduce
	layer0_attn_allreduce -> layer0_moe_route_gpu3
	layer0_moe_route_gpu0 -> layer0_moe_all2all
	layer0_moe_route_gpu1 -> layer0_moe_all2all
	layer0_moe_route_gpu2 -> layer0_moe_all2all
	layer0_moe_route_gpu3 -> layer0_moe_all2all
	layer0_moe_all2all -> layer0_expert0
	layer0_moe_all2all -> layer0_expert1
	layer0_moe_all2all -> layer0_expert2
	layer0_moe_all2all -> layer0_expert3
	layer0_moe_all2all -> layer0_expert4
	layer0_moe_all2all -> layer0_expert5
	layer0_moe_all2all -> layer0_expert6
	layer0_moe_all2all -> layer0_expert7
	layer0_moe_all2all -> layer0_expert8
	layer0_moe_all2all -> layer0_expert9
	layer0_moe_all2all -> layer0_expert10
	layer0_moe_all2all -> layer0_expert11
	layer0_moe_all2all -> layer0_expert12
	layer0_moe_all2all -> layer0_expert13
	layer0_moe_all2all -> layer0_expert14
	layer0_moe_all2all -> layer0_expert15
	layer0_moe_all2all -> layer0_moe_agg
	layer0_moe_all2all -> layer0_moe_agg
	layer0_moe_all2all -> layer0_moe_agg
	layer0_moe_all2all -> layer0_moe_agg
	layer15_moe_agg -> output_agg
	output_agg -> output
}
