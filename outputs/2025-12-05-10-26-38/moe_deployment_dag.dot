// 30B MoE Model Deployment DAG
digraph {
	dpi=300 rankdir=TB size="100,100"
	node [fontname=Arial fontsize=10 shape=rectangle style=filled]
	input_dp0 [label="Input Batch (DP=0)" fillcolor=white fontname=Arial fontsize=9 shape=ellipse style=filled]
	node [shape=rectangle]
	input_dp1 [label="Input Batch (DP=1)" fillcolor=white fontname=Arial fontsize=9 shape=ellipse style=filled]
	node [shape=rectangle]
	split_mb0 [label="Split to Micro-batches\nInput: [batch=64, seq=1024, hidden=1024]\nOutput: 8×[batch=8, seq=1024, hidden=1024]" fillcolor="#DDA0DD" fontsize=9 shape=parallelogram style=filled]
	split_mb1 [label="Split to Micro-batches\nInput: [batch=64, seq=1024, hidden=1024]\nOutput: 8×[batch=8, seq=1024, hidden=1024]" fillcolor="#DDA0DD" fontsize=9 shape=parallelogram style=filled]
	input_dp0 -> split_mb0
	input_dp1 -> split_mb1
	layer0_qkv_dp0_mb0_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb0_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb0_gpu0
	split_mb1 -> layer0_qkv_dp1_mb0_gpu0
	layer0_qkv_dp0_mb0_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb0_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb0_gpu1
	split_mb1 -> layer0_qkv_dp1_mb0_gpu1
	layer0_qkv_dp0_mb0_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb0_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb0_gpu2
	split_mb1 -> layer0_qkv_dp1_mb0_gpu2
	layer0_qkv_dp0_mb0_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb0_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb0_gpu3
	split_mb1 -> layer0_qkv_dp1_mb0_gpu3
	layer0_attn_scores_dp0_mb0_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb0_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb0_gpu0 -> layer0_attn_scores_dp0_mb0_gpu0
	layer0_qkv_dp1_mb0_gpu0 -> layer0_attn_scores_dp1_mb0_gpu0
	layer0_softmax_dp0_mb0_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb0_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb0_gpu0 -> layer0_softmax_dp0_mb0_gpu0
	layer0_attn_scores_dp1_mb0_gpu0 -> layer0_softmax_dp1_mb0_gpu0
	layer0_attn_scores_dp0_mb0_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb0_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb0_gpu1 -> layer0_attn_scores_dp0_mb0_gpu1
	layer0_qkv_dp1_mb0_gpu1 -> layer0_attn_scores_dp1_mb0_gpu1
	layer0_softmax_dp0_mb0_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb0_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb0_gpu1 -> layer0_softmax_dp0_mb0_gpu1
	layer0_attn_scores_dp1_mb0_gpu1 -> layer0_softmax_dp1_mb0_gpu1
	layer0_attn_scores_dp0_mb0_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb0_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb0_gpu2 -> layer0_attn_scores_dp0_mb0_gpu2
	layer0_qkv_dp1_mb0_gpu2 -> layer0_attn_scores_dp1_mb0_gpu2
	layer0_softmax_dp0_mb0_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb0_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb0_gpu2 -> layer0_softmax_dp0_mb0_gpu2
	layer0_attn_scores_dp1_mb0_gpu2 -> layer0_softmax_dp1_mb0_gpu2
	layer0_attn_scores_dp0_mb0_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb0_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb0_gpu3 -> layer0_attn_scores_dp0_mb0_gpu3
	layer0_qkv_dp1_mb0_gpu3 -> layer0_attn_scores_dp1_mb0_gpu3
	layer0_softmax_dp0_mb0_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb0_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb0_gpu3 -> layer0_softmax_dp0_mb0_gpu3
	layer0_attn_scores_dp1_mb0_gpu3 -> layer0_softmax_dp1_mb0_gpu3
	layer0_attn_out_dp0_mb0_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb0_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb0_gpu0 -> layer0_attn_out_dp0_mb0_gpu0
	layer0_softmax_dp1_mb0_gpu0 -> layer0_attn_out_dp1_mb0_gpu0
	layer0_attn_out_dp0_mb0_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb0_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb0_gpu1 -> layer0_attn_out_dp0_mb0_gpu1
	layer0_softmax_dp1_mb0_gpu1 -> layer0_attn_out_dp1_mb0_gpu1
	layer0_attn_out_dp0_mb0_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb0_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb0_gpu2 -> layer0_attn_out_dp0_mb0_gpu2
	layer0_softmax_dp1_mb0_gpu2 -> layer0_attn_out_dp1_mb0_gpu2
	layer0_attn_out_dp0_mb0_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb0_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb0_gpu3 -> layer0_attn_out_dp0_mb0_gpu3
	layer0_softmax_dp1_mb0_gpu3 -> layer0_attn_out_dp1_mb0_gpu3
	layer0_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb0_gpu0 -> layer0_attn_allreduce_dp0_mb0
	layer0_attn_out_dp1_mb0_gpu0 -> layer0_attn_allreduce_dp1_mb0
	layer0_attn_out_dp0_mb0_gpu1 -> layer0_attn_allreduce_dp0_mb0
	layer0_attn_out_dp1_mb0_gpu1 -> layer0_attn_allreduce_dp1_mb0
	layer0_attn_out_dp0_mb0_gpu2 -> layer0_attn_allreduce_dp0_mb0
	layer0_attn_out_dp1_mb0_gpu2 -> layer0_attn_allreduce_dp1_mb0
	layer0_attn_out_dp0_mb0_gpu3 -> layer0_attn_allreduce_dp0_mb0
	layer0_attn_out_dp1_mb0_gpu3 -> layer0_attn_allreduce_dp1_mb0
	layer0_router_dp0_mb0_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb0_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb0 -> layer0_router_dp0_mb0_gpu0
	layer0_attn_allreduce_dp1_mb0 -> layer0_router_dp1_mb0_gpu0
	layer0_expert0_mlp1_dp0_mb0_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb0_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb0_gpu0 -> layer0_expert0_mlp1_dp0_mb0_gpu0
	layer0_router_dp1_mb0_gpu0 -> layer0_expert0_mlp1_dp1_mb0_gpu0
	layer0_expert0_mlp2_dp0_mb0_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb0_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb0_gpu0 -> layer0_expert0_mlp2_dp0_mb0_gpu0
	layer0_expert0_mlp1_dp1_mb0_gpu0 -> layer0_expert0_mlp2_dp1_mb0_gpu0
	layer0_expert1_mlp1_dp0_mb0_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb0_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb0_gpu0 -> layer0_expert1_mlp1_dp0_mb0_gpu1
	layer0_router_dp1_mb0_gpu0 -> layer0_expert1_mlp1_dp1_mb0_gpu1
	layer0_expert1_mlp2_dp0_mb0_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb0_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb0_gpu1 -> layer0_expert1_mlp2_dp0_mb0_gpu1
	layer0_expert1_mlp1_dp1_mb0_gpu1 -> layer0_expert1_mlp2_dp1_mb0_gpu1
	layer0_expert2_mlp1_dp0_mb0_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb0_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb0_gpu0 -> layer0_expert2_mlp1_dp0_mb0_gpu2
	layer0_router_dp1_mb0_gpu0 -> layer0_expert2_mlp1_dp1_mb0_gpu2
	layer0_expert2_mlp2_dp0_mb0_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb0_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb0_gpu2 -> layer0_expert2_mlp2_dp0_mb0_gpu2
	layer0_expert2_mlp1_dp1_mb0_gpu2 -> layer0_expert2_mlp2_dp1_mb0_gpu2
	layer0_expert3_mlp1_dp0_mb0_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb0_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb0_gpu0 -> layer0_expert3_mlp1_dp0_mb0_gpu3
	layer0_router_dp1_mb0_gpu0 -> layer0_expert3_mlp1_dp1_mb0_gpu3
	layer0_expert3_mlp2_dp0_mb0_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb0_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb0_gpu3 -> layer0_expert3_mlp2_dp0_mb0_gpu3
	layer0_expert3_mlp1_dp1_mb0_gpu3 -> layer0_expert3_mlp2_dp1_mb0_gpu3
	layer0_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb0_gpu0 -> layer0_expert_agg_dp0_mb0
	layer0_expert0_mlp2_dp1_mb0_gpu0 -> layer0_expert_agg_dp1_mb0
	layer0_expert1_mlp2_dp0_mb0_gpu1 -> layer0_expert_agg_dp0_mb0
	layer0_expert1_mlp2_dp1_mb0_gpu1 -> layer0_expert_agg_dp1_mb0
	layer0_expert2_mlp2_dp0_mb0_gpu2 -> layer0_expert_agg_dp0_mb0
	layer0_expert2_mlp2_dp1_mb0_gpu2 -> layer0_expert_agg_dp1_mb0
	layer0_expert3_mlp2_dp0_mb0_gpu3 -> layer0_expert_agg_dp0_mb0
	layer0_expert3_mlp2_dp1_mb0_gpu3 -> layer0_expert_agg_dp1_mb0
	layer1_qkv_dp0_mb0_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb0_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb0 -> layer1_qkv_dp0_mb0_gpu0
	layer0_expert_agg_dp1_mb0 -> layer1_qkv_dp1_mb0_gpu0
	layer1_qkv_dp0_mb0_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb0_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb0 -> layer1_qkv_dp0_mb0_gpu1
	layer0_expert_agg_dp1_mb0 -> layer1_qkv_dp1_mb0_gpu1
	layer1_qkv_dp0_mb0_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb0_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb0 -> layer1_qkv_dp0_mb0_gpu2
	layer0_expert_agg_dp1_mb0 -> layer1_qkv_dp1_mb0_gpu2
	layer1_qkv_dp0_mb0_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb0_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb0 -> layer1_qkv_dp0_mb0_gpu3
	layer0_expert_agg_dp1_mb0 -> layer1_qkv_dp1_mb0_gpu3
	layer1_attn_scores_dp0_mb0_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb0_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb0_gpu0 -> layer1_attn_scores_dp0_mb0_gpu0
	layer1_qkv_dp1_mb0_gpu0 -> layer1_attn_scores_dp1_mb0_gpu0
	layer1_softmax_dp0_mb0_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb0_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb0_gpu0 -> layer1_softmax_dp0_mb0_gpu0
	layer1_attn_scores_dp1_mb0_gpu0 -> layer1_softmax_dp1_mb0_gpu0
	layer1_attn_scores_dp0_mb0_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb0_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb0_gpu1 -> layer1_attn_scores_dp0_mb0_gpu1
	layer1_qkv_dp1_mb0_gpu1 -> layer1_attn_scores_dp1_mb0_gpu1
	layer1_softmax_dp0_mb0_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb0_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb0_gpu1 -> layer1_softmax_dp0_mb0_gpu1
	layer1_attn_scores_dp1_mb0_gpu1 -> layer1_softmax_dp1_mb0_gpu1
	layer1_attn_scores_dp0_mb0_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb0_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb0_gpu2 -> layer1_attn_scores_dp0_mb0_gpu2
	layer1_qkv_dp1_mb0_gpu2 -> layer1_attn_scores_dp1_mb0_gpu2
	layer1_softmax_dp0_mb0_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb0_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb0_gpu2 -> layer1_softmax_dp0_mb0_gpu2
	layer1_attn_scores_dp1_mb0_gpu2 -> layer1_softmax_dp1_mb0_gpu2
	layer1_attn_scores_dp0_mb0_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb0_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb0_gpu3 -> layer1_attn_scores_dp0_mb0_gpu3
	layer1_qkv_dp1_mb0_gpu3 -> layer1_attn_scores_dp1_mb0_gpu3
	layer1_softmax_dp0_mb0_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb0_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb0_gpu3 -> layer1_softmax_dp0_mb0_gpu3
	layer1_attn_scores_dp1_mb0_gpu3 -> layer1_softmax_dp1_mb0_gpu3
	layer1_attn_out_dp0_mb0_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb0_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb0_gpu0 -> layer1_attn_out_dp0_mb0_gpu0
	layer1_softmax_dp1_mb0_gpu0 -> layer1_attn_out_dp1_mb0_gpu0
	layer1_attn_out_dp0_mb0_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb0_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb0_gpu1 -> layer1_attn_out_dp0_mb0_gpu1
	layer1_softmax_dp1_mb0_gpu1 -> layer1_attn_out_dp1_mb0_gpu1
	layer1_attn_out_dp0_mb0_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb0_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb0_gpu2 -> layer1_attn_out_dp0_mb0_gpu2
	layer1_softmax_dp1_mb0_gpu2 -> layer1_attn_out_dp1_mb0_gpu2
	layer1_attn_out_dp0_mb0_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb0_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb0_gpu3 -> layer1_attn_out_dp0_mb0_gpu3
	layer1_softmax_dp1_mb0_gpu3 -> layer1_attn_out_dp1_mb0_gpu3
	layer1_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb0_gpu0 -> layer1_attn_allreduce_dp0_mb0
	layer1_attn_out_dp1_mb0_gpu0 -> layer1_attn_allreduce_dp1_mb0
	layer1_attn_out_dp0_mb0_gpu1 -> layer1_attn_allreduce_dp0_mb0
	layer1_attn_out_dp1_mb0_gpu1 -> layer1_attn_allreduce_dp1_mb0
	layer1_attn_out_dp0_mb0_gpu2 -> layer1_attn_allreduce_dp0_mb0
	layer1_attn_out_dp1_mb0_gpu2 -> layer1_attn_allreduce_dp1_mb0
	layer1_attn_out_dp0_mb0_gpu3 -> layer1_attn_allreduce_dp0_mb0
	layer1_attn_out_dp1_mb0_gpu3 -> layer1_attn_allreduce_dp1_mb0
	layer1_router_dp0_mb0_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb0_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb0 -> layer1_router_dp0_mb0_gpu0
	layer1_attn_allreduce_dp1_mb0 -> layer1_router_dp1_mb0_gpu0
	layer1_expert0_mlp1_dp0_mb0_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb0_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb0_gpu0 -> layer1_expert0_mlp1_dp0_mb0_gpu0
	layer1_router_dp1_mb0_gpu0 -> layer1_expert0_mlp1_dp1_mb0_gpu0
	layer1_expert0_mlp2_dp0_mb0_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb0_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb0_gpu0 -> layer1_expert0_mlp2_dp0_mb0_gpu0
	layer1_expert0_mlp1_dp1_mb0_gpu0 -> layer1_expert0_mlp2_dp1_mb0_gpu0
	layer1_expert1_mlp1_dp0_mb0_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb0_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb0_gpu0 -> layer1_expert1_mlp1_dp0_mb0_gpu1
	layer1_router_dp1_mb0_gpu0 -> layer1_expert1_mlp1_dp1_mb0_gpu1
	layer1_expert1_mlp2_dp0_mb0_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb0_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb0_gpu1 -> layer1_expert1_mlp2_dp0_mb0_gpu1
	layer1_expert1_mlp1_dp1_mb0_gpu1 -> layer1_expert1_mlp2_dp1_mb0_gpu1
	layer1_expert2_mlp1_dp0_mb0_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb0_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb0_gpu0 -> layer1_expert2_mlp1_dp0_mb0_gpu2
	layer1_router_dp1_mb0_gpu0 -> layer1_expert2_mlp1_dp1_mb0_gpu2
	layer1_expert2_mlp2_dp0_mb0_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb0_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb0_gpu2 -> layer1_expert2_mlp2_dp0_mb0_gpu2
	layer1_expert2_mlp1_dp1_mb0_gpu2 -> layer1_expert2_mlp2_dp1_mb0_gpu2
	layer1_expert3_mlp1_dp0_mb0_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb0_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb0_gpu0 -> layer1_expert3_mlp1_dp0_mb0_gpu3
	layer1_router_dp1_mb0_gpu0 -> layer1_expert3_mlp1_dp1_mb0_gpu3
	layer1_expert3_mlp2_dp0_mb0_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb0_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb0_gpu3 -> layer1_expert3_mlp2_dp0_mb0_gpu3
	layer1_expert3_mlp1_dp1_mb0_gpu3 -> layer1_expert3_mlp2_dp1_mb0_gpu3
	layer1_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb0_gpu0 -> layer1_expert_agg_dp0_mb0
	layer1_expert0_mlp2_dp1_mb0_gpu0 -> layer1_expert_agg_dp1_mb0
	layer1_expert1_mlp2_dp0_mb0_gpu1 -> layer1_expert_agg_dp0_mb0
	layer1_expert1_mlp2_dp1_mb0_gpu1 -> layer1_expert_agg_dp1_mb0
	layer1_expert2_mlp2_dp0_mb0_gpu2 -> layer1_expert_agg_dp0_mb0
	layer1_expert2_mlp2_dp1_mb0_gpu2 -> layer1_expert_agg_dp1_mb0
	layer1_expert3_mlp2_dp0_mb0_gpu3 -> layer1_expert_agg_dp0_mb0
	layer1_expert3_mlp2_dp1_mb0_gpu3 -> layer1_expert_agg_dp1_mb0
	layer2_qkv_dp0_mb0_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb0_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb0 -> layer2_qkv_dp0_mb0_gpu0
	layer1_expert_agg_dp1_mb0 -> layer2_qkv_dp1_mb0_gpu0
	layer2_qkv_dp0_mb0_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb0_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb0 -> layer2_qkv_dp0_mb0_gpu1
	layer1_expert_agg_dp1_mb0 -> layer2_qkv_dp1_mb0_gpu1
	layer2_qkv_dp0_mb0_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb0_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb0 -> layer2_qkv_dp0_mb0_gpu2
	layer1_expert_agg_dp1_mb0 -> layer2_qkv_dp1_mb0_gpu2
	layer2_qkv_dp0_mb0_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb0_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb0 -> layer2_qkv_dp0_mb0_gpu3
	layer1_expert_agg_dp1_mb0 -> layer2_qkv_dp1_mb0_gpu3
	layer2_attn_scores_dp0_mb0_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb0_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb0_gpu0 -> layer2_attn_scores_dp0_mb0_gpu0
	layer2_qkv_dp1_mb0_gpu0 -> layer2_attn_scores_dp1_mb0_gpu0
	layer2_softmax_dp0_mb0_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb0_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb0_gpu0 -> layer2_softmax_dp0_mb0_gpu0
	layer2_attn_scores_dp1_mb0_gpu0 -> layer2_softmax_dp1_mb0_gpu0
	layer2_attn_scores_dp0_mb0_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb0_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb0_gpu1 -> layer2_attn_scores_dp0_mb0_gpu1
	layer2_qkv_dp1_mb0_gpu1 -> layer2_attn_scores_dp1_mb0_gpu1
	layer2_softmax_dp0_mb0_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb0_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb0_gpu1 -> layer2_softmax_dp0_mb0_gpu1
	layer2_attn_scores_dp1_mb0_gpu1 -> layer2_softmax_dp1_mb0_gpu1
	layer2_attn_scores_dp0_mb0_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb0_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb0_gpu2 -> layer2_attn_scores_dp0_mb0_gpu2
	layer2_qkv_dp1_mb0_gpu2 -> layer2_attn_scores_dp1_mb0_gpu2
	layer2_softmax_dp0_mb0_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb0_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb0_gpu2 -> layer2_softmax_dp0_mb0_gpu2
	layer2_attn_scores_dp1_mb0_gpu2 -> layer2_softmax_dp1_mb0_gpu2
	layer2_attn_scores_dp0_mb0_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb0_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb0_gpu3 -> layer2_attn_scores_dp0_mb0_gpu3
	layer2_qkv_dp1_mb0_gpu3 -> layer2_attn_scores_dp1_mb0_gpu3
	layer2_softmax_dp0_mb0_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb0_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb0_gpu3 -> layer2_softmax_dp0_mb0_gpu3
	layer2_attn_scores_dp1_mb0_gpu3 -> layer2_softmax_dp1_mb0_gpu3
	layer2_attn_out_dp0_mb0_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb0_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb0_gpu0 -> layer2_attn_out_dp0_mb0_gpu0
	layer2_softmax_dp1_mb0_gpu0 -> layer2_attn_out_dp1_mb0_gpu0
	layer2_attn_out_dp0_mb0_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb0_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb0_gpu1 -> layer2_attn_out_dp0_mb0_gpu1
	layer2_softmax_dp1_mb0_gpu1 -> layer2_attn_out_dp1_mb0_gpu1
	layer2_attn_out_dp0_mb0_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb0_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb0_gpu2 -> layer2_attn_out_dp0_mb0_gpu2
	layer2_softmax_dp1_mb0_gpu2 -> layer2_attn_out_dp1_mb0_gpu2
	layer2_attn_out_dp0_mb0_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb0_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb0_gpu3 -> layer2_attn_out_dp0_mb0_gpu3
	layer2_softmax_dp1_mb0_gpu3 -> layer2_attn_out_dp1_mb0_gpu3
	layer2_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb0_gpu0 -> layer2_attn_allreduce_dp0_mb0
	layer2_attn_out_dp1_mb0_gpu0 -> layer2_attn_allreduce_dp1_mb0
	layer2_attn_out_dp0_mb0_gpu1 -> layer2_attn_allreduce_dp0_mb0
	layer2_attn_out_dp1_mb0_gpu1 -> layer2_attn_allreduce_dp1_mb0
	layer2_attn_out_dp0_mb0_gpu2 -> layer2_attn_allreduce_dp0_mb0
	layer2_attn_out_dp1_mb0_gpu2 -> layer2_attn_allreduce_dp1_mb0
	layer2_attn_out_dp0_mb0_gpu3 -> layer2_attn_allreduce_dp0_mb0
	layer2_attn_out_dp1_mb0_gpu3 -> layer2_attn_allreduce_dp1_mb0
	layer2_router_dp0_mb0_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb0_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb0 -> layer2_router_dp0_mb0_gpu0
	layer2_attn_allreduce_dp1_mb0 -> layer2_router_dp1_mb0_gpu0
	layer2_expert0_mlp1_dp0_mb0_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb0_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb0_gpu0 -> layer2_expert0_mlp1_dp0_mb0_gpu0
	layer2_router_dp1_mb0_gpu0 -> layer2_expert0_mlp1_dp1_mb0_gpu0
	layer2_expert0_mlp2_dp0_mb0_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb0_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb0_gpu0 -> layer2_expert0_mlp2_dp0_mb0_gpu0
	layer2_expert0_mlp1_dp1_mb0_gpu0 -> layer2_expert0_mlp2_dp1_mb0_gpu0
	layer2_expert1_mlp1_dp0_mb0_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb0_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb0_gpu0 -> layer2_expert1_mlp1_dp0_mb0_gpu1
	layer2_router_dp1_mb0_gpu0 -> layer2_expert1_mlp1_dp1_mb0_gpu1
	layer2_expert1_mlp2_dp0_mb0_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb0_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb0_gpu1 -> layer2_expert1_mlp2_dp0_mb0_gpu1
	layer2_expert1_mlp1_dp1_mb0_gpu1 -> layer2_expert1_mlp2_dp1_mb0_gpu1
	layer2_expert2_mlp1_dp0_mb0_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb0_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb0_gpu0 -> layer2_expert2_mlp1_dp0_mb0_gpu2
	layer2_router_dp1_mb0_gpu0 -> layer2_expert2_mlp1_dp1_mb0_gpu2
	layer2_expert2_mlp2_dp0_mb0_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb0_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb0_gpu2 -> layer2_expert2_mlp2_dp0_mb0_gpu2
	layer2_expert2_mlp1_dp1_mb0_gpu2 -> layer2_expert2_mlp2_dp1_mb0_gpu2
	layer2_expert3_mlp1_dp0_mb0_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb0_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb0_gpu0 -> layer2_expert3_mlp1_dp0_mb0_gpu3
	layer2_router_dp1_mb0_gpu0 -> layer2_expert3_mlp1_dp1_mb0_gpu3
	layer2_expert3_mlp2_dp0_mb0_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb0_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb0_gpu3 -> layer2_expert3_mlp2_dp0_mb0_gpu3
	layer2_expert3_mlp1_dp1_mb0_gpu3 -> layer2_expert3_mlp2_dp1_mb0_gpu3
	layer2_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb0_gpu0 -> layer2_expert_agg_dp0_mb0
	layer2_expert0_mlp2_dp1_mb0_gpu0 -> layer2_expert_agg_dp1_mb0
	layer2_expert1_mlp2_dp0_mb0_gpu1 -> layer2_expert_agg_dp0_mb0
	layer2_expert1_mlp2_dp1_mb0_gpu1 -> layer2_expert_agg_dp1_mb0
	layer2_expert2_mlp2_dp0_mb0_gpu2 -> layer2_expert_agg_dp0_mb0
	layer2_expert2_mlp2_dp1_mb0_gpu2 -> layer2_expert_agg_dp1_mb0
	layer2_expert3_mlp2_dp0_mb0_gpu3 -> layer2_expert_agg_dp0_mb0
	layer2_expert3_mlp2_dp1_mb0_gpu3 -> layer2_expert_agg_dp1_mb0
	layer3_qkv_dp0_mb0_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb0_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb0 -> layer3_qkv_dp0_mb0_gpu0
	layer2_expert_agg_dp1_mb0 -> layer3_qkv_dp1_mb0_gpu0
	layer3_qkv_dp0_mb0_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb0_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb0 -> layer3_qkv_dp0_mb0_gpu1
	layer2_expert_agg_dp1_mb0 -> layer3_qkv_dp1_mb0_gpu1
	layer3_qkv_dp0_mb0_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb0_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb0 -> layer3_qkv_dp0_mb0_gpu2
	layer2_expert_agg_dp1_mb0 -> layer3_qkv_dp1_mb0_gpu2
	layer3_qkv_dp0_mb0_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb0_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb0 -> layer3_qkv_dp0_mb0_gpu3
	layer2_expert_agg_dp1_mb0 -> layer3_qkv_dp1_mb0_gpu3
	layer3_attn_scores_dp0_mb0_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb0_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb0_gpu0 -> layer3_attn_scores_dp0_mb0_gpu0
	layer3_qkv_dp1_mb0_gpu0 -> layer3_attn_scores_dp1_mb0_gpu0
	layer3_softmax_dp0_mb0_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb0_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb0_gpu0 -> layer3_softmax_dp0_mb0_gpu0
	layer3_attn_scores_dp1_mb0_gpu0 -> layer3_softmax_dp1_mb0_gpu0
	layer3_attn_scores_dp0_mb0_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb0_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb0_gpu1 -> layer3_attn_scores_dp0_mb0_gpu1
	layer3_qkv_dp1_mb0_gpu1 -> layer3_attn_scores_dp1_mb0_gpu1
	layer3_softmax_dp0_mb0_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb0_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb0_gpu1 -> layer3_softmax_dp0_mb0_gpu1
	layer3_attn_scores_dp1_mb0_gpu1 -> layer3_softmax_dp1_mb0_gpu1
	layer3_attn_scores_dp0_mb0_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb0_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb0_gpu2 -> layer3_attn_scores_dp0_mb0_gpu2
	layer3_qkv_dp1_mb0_gpu2 -> layer3_attn_scores_dp1_mb0_gpu2
	layer3_softmax_dp0_mb0_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb0_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb0_gpu2 -> layer3_softmax_dp0_mb0_gpu2
	layer3_attn_scores_dp1_mb0_gpu2 -> layer3_softmax_dp1_mb0_gpu2
	layer3_attn_scores_dp0_mb0_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb0_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb0_gpu3 -> layer3_attn_scores_dp0_mb0_gpu3
	layer3_qkv_dp1_mb0_gpu3 -> layer3_attn_scores_dp1_mb0_gpu3
	layer3_softmax_dp0_mb0_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb0_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb0_gpu3 -> layer3_softmax_dp0_mb0_gpu3
	layer3_attn_scores_dp1_mb0_gpu3 -> layer3_softmax_dp1_mb0_gpu3
	layer3_attn_out_dp0_mb0_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb0_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb0_gpu0 -> layer3_attn_out_dp0_mb0_gpu0
	layer3_softmax_dp1_mb0_gpu0 -> layer3_attn_out_dp1_mb0_gpu0
	layer3_attn_out_dp0_mb0_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb0_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb0_gpu1 -> layer3_attn_out_dp0_mb0_gpu1
	layer3_softmax_dp1_mb0_gpu1 -> layer3_attn_out_dp1_mb0_gpu1
	layer3_attn_out_dp0_mb0_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb0_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb0_gpu2 -> layer3_attn_out_dp0_mb0_gpu2
	layer3_softmax_dp1_mb0_gpu2 -> layer3_attn_out_dp1_mb0_gpu2
	layer3_attn_out_dp0_mb0_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb0_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb0_gpu3 -> layer3_attn_out_dp0_mb0_gpu3
	layer3_softmax_dp1_mb0_gpu3 -> layer3_attn_out_dp1_mb0_gpu3
	layer3_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb0_gpu0 -> layer3_attn_allreduce_dp0_mb0
	layer3_attn_out_dp1_mb0_gpu0 -> layer3_attn_allreduce_dp1_mb0
	layer3_attn_out_dp0_mb0_gpu1 -> layer3_attn_allreduce_dp0_mb0
	layer3_attn_out_dp1_mb0_gpu1 -> layer3_attn_allreduce_dp1_mb0
	layer3_attn_out_dp0_mb0_gpu2 -> layer3_attn_allreduce_dp0_mb0
	layer3_attn_out_dp1_mb0_gpu2 -> layer3_attn_allreduce_dp1_mb0
	layer3_attn_out_dp0_mb0_gpu3 -> layer3_attn_allreduce_dp0_mb0
	layer3_attn_out_dp1_mb0_gpu3 -> layer3_attn_allreduce_dp1_mb0
	layer3_router_dp0_mb0_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb0_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb0 -> layer3_router_dp0_mb0_gpu0
	layer3_attn_allreduce_dp1_mb0 -> layer3_router_dp1_mb0_gpu0
	layer3_expert0_mlp1_dp0_mb0_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb0_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb0_gpu0 -> layer3_expert0_mlp1_dp0_mb0_gpu0
	layer3_router_dp1_mb0_gpu0 -> layer3_expert0_mlp1_dp1_mb0_gpu0
	layer3_expert0_mlp2_dp0_mb0_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb0_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb0_gpu0 -> layer3_expert0_mlp2_dp0_mb0_gpu0
	layer3_expert0_mlp1_dp1_mb0_gpu0 -> layer3_expert0_mlp2_dp1_mb0_gpu0
	layer3_expert1_mlp1_dp0_mb0_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb0_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb0_gpu0 -> layer3_expert1_mlp1_dp0_mb0_gpu1
	layer3_router_dp1_mb0_gpu0 -> layer3_expert1_mlp1_dp1_mb0_gpu1
	layer3_expert1_mlp2_dp0_mb0_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb0_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb0_gpu1 -> layer3_expert1_mlp2_dp0_mb0_gpu1
	layer3_expert1_mlp1_dp1_mb0_gpu1 -> layer3_expert1_mlp2_dp1_mb0_gpu1
	layer3_expert2_mlp1_dp0_mb0_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb0_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb0_gpu0 -> layer3_expert2_mlp1_dp0_mb0_gpu2
	layer3_router_dp1_mb0_gpu0 -> layer3_expert2_mlp1_dp1_mb0_gpu2
	layer3_expert2_mlp2_dp0_mb0_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb0_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb0_gpu2 -> layer3_expert2_mlp2_dp0_mb0_gpu2
	layer3_expert2_mlp1_dp1_mb0_gpu2 -> layer3_expert2_mlp2_dp1_mb0_gpu2
	layer3_expert3_mlp1_dp0_mb0_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb0_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb0_gpu0 -> layer3_expert3_mlp1_dp0_mb0_gpu3
	layer3_router_dp1_mb0_gpu0 -> layer3_expert3_mlp1_dp1_mb0_gpu3
	layer3_expert3_mlp2_dp0_mb0_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb0_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb0_gpu3 -> layer3_expert3_mlp2_dp0_mb0_gpu3
	layer3_expert3_mlp1_dp1_mb0_gpu3 -> layer3_expert3_mlp2_dp1_mb0_gpu3
	layer3_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb0_gpu0 -> layer3_expert_agg_dp0_mb0
	layer3_expert0_mlp2_dp1_mb0_gpu0 -> layer3_expert_agg_dp1_mb0
	layer3_expert1_mlp2_dp0_mb0_gpu1 -> layer3_expert_agg_dp0_mb0
	layer3_expert1_mlp2_dp1_mb0_gpu1 -> layer3_expert_agg_dp1_mb0
	layer3_expert2_mlp2_dp0_mb0_gpu2 -> layer3_expert_agg_dp0_mb0
	layer3_expert2_mlp2_dp1_mb0_gpu2 -> layer3_expert_agg_dp1_mb0
	layer3_expert3_mlp2_dp0_mb0_gpu3 -> layer3_expert_agg_dp0_mb0
	layer3_expert3_mlp2_dp1_mb0_gpu3 -> layer3_expert_agg_dp1_mb0
	layer0_qkv_dp0_mb1_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb1_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb1_gpu0
	split_mb1 -> layer0_qkv_dp1_mb1_gpu0
	layer0_qkv_dp0_mb1_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb1_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb1_gpu1
	split_mb1 -> layer0_qkv_dp1_mb1_gpu1
	layer0_qkv_dp0_mb1_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb1_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb1_gpu2
	split_mb1 -> layer0_qkv_dp1_mb1_gpu2
	layer0_qkv_dp0_mb1_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb1_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb1_gpu3
	split_mb1 -> layer0_qkv_dp1_mb1_gpu3
	layer0_attn_scores_dp0_mb1_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb1_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb1_gpu0 -> layer0_attn_scores_dp0_mb1_gpu0
	layer0_qkv_dp1_mb1_gpu0 -> layer0_attn_scores_dp1_mb1_gpu0
	layer0_softmax_dp0_mb1_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb1_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb1_gpu0 -> layer0_softmax_dp0_mb1_gpu0
	layer0_attn_scores_dp1_mb1_gpu0 -> layer0_softmax_dp1_mb1_gpu0
	layer0_attn_scores_dp0_mb1_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb1_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb1_gpu1 -> layer0_attn_scores_dp0_mb1_gpu1
	layer0_qkv_dp1_mb1_gpu1 -> layer0_attn_scores_dp1_mb1_gpu1
	layer0_softmax_dp0_mb1_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb1_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb1_gpu1 -> layer0_softmax_dp0_mb1_gpu1
	layer0_attn_scores_dp1_mb1_gpu1 -> layer0_softmax_dp1_mb1_gpu1
	layer0_attn_scores_dp0_mb1_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb1_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb1_gpu2 -> layer0_attn_scores_dp0_mb1_gpu2
	layer0_qkv_dp1_mb1_gpu2 -> layer0_attn_scores_dp1_mb1_gpu2
	layer0_softmax_dp0_mb1_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb1_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb1_gpu2 -> layer0_softmax_dp0_mb1_gpu2
	layer0_attn_scores_dp1_mb1_gpu2 -> layer0_softmax_dp1_mb1_gpu2
	layer0_attn_scores_dp0_mb1_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb1_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb1_gpu3 -> layer0_attn_scores_dp0_mb1_gpu3
	layer0_qkv_dp1_mb1_gpu3 -> layer0_attn_scores_dp1_mb1_gpu3
	layer0_softmax_dp0_mb1_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb1_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb1_gpu3 -> layer0_softmax_dp0_mb1_gpu3
	layer0_attn_scores_dp1_mb1_gpu3 -> layer0_softmax_dp1_mb1_gpu3
	layer0_attn_out_dp0_mb1_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb1_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb1_gpu0 -> layer0_attn_out_dp0_mb1_gpu0
	layer0_softmax_dp1_mb1_gpu0 -> layer0_attn_out_dp1_mb1_gpu0
	layer0_attn_out_dp0_mb1_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb1_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb1_gpu1 -> layer0_attn_out_dp0_mb1_gpu1
	layer0_softmax_dp1_mb1_gpu1 -> layer0_attn_out_dp1_mb1_gpu1
	layer0_attn_out_dp0_mb1_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb1_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb1_gpu2 -> layer0_attn_out_dp0_mb1_gpu2
	layer0_softmax_dp1_mb1_gpu2 -> layer0_attn_out_dp1_mb1_gpu2
	layer0_attn_out_dp0_mb1_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb1_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb1_gpu3 -> layer0_attn_out_dp0_mb1_gpu3
	layer0_softmax_dp1_mb1_gpu3 -> layer0_attn_out_dp1_mb1_gpu3
	layer0_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb1_gpu0 -> layer0_attn_allreduce_dp0_mb1
	layer0_attn_out_dp1_mb1_gpu0 -> layer0_attn_allreduce_dp1_mb1
	layer0_attn_out_dp0_mb1_gpu1 -> layer0_attn_allreduce_dp0_mb1
	layer0_attn_out_dp1_mb1_gpu1 -> layer0_attn_allreduce_dp1_mb1
	layer0_attn_out_dp0_mb1_gpu2 -> layer0_attn_allreduce_dp0_mb1
	layer0_attn_out_dp1_mb1_gpu2 -> layer0_attn_allreduce_dp1_mb1
	layer0_attn_out_dp0_mb1_gpu3 -> layer0_attn_allreduce_dp0_mb1
	layer0_attn_out_dp1_mb1_gpu3 -> layer0_attn_allreduce_dp1_mb1
	layer0_router_dp0_mb1_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb1_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb1 -> layer0_router_dp0_mb1_gpu0
	layer0_attn_allreduce_dp1_mb1 -> layer0_router_dp1_mb1_gpu0
	layer0_expert0_mlp1_dp0_mb1_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb1_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb1_gpu0 -> layer0_expert0_mlp1_dp0_mb1_gpu0
	layer0_router_dp1_mb1_gpu0 -> layer0_expert0_mlp1_dp1_mb1_gpu0
	layer0_expert0_mlp2_dp0_mb1_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb1_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb1_gpu0 -> layer0_expert0_mlp2_dp0_mb1_gpu0
	layer0_expert0_mlp1_dp1_mb1_gpu0 -> layer0_expert0_mlp2_dp1_mb1_gpu0
	layer0_expert1_mlp1_dp0_mb1_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb1_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb1_gpu0 -> layer0_expert1_mlp1_dp0_mb1_gpu1
	layer0_router_dp1_mb1_gpu0 -> layer0_expert1_mlp1_dp1_mb1_gpu1
	layer0_expert1_mlp2_dp0_mb1_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb1_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb1_gpu1 -> layer0_expert1_mlp2_dp0_mb1_gpu1
	layer0_expert1_mlp1_dp1_mb1_gpu1 -> layer0_expert1_mlp2_dp1_mb1_gpu1
	layer0_expert2_mlp1_dp0_mb1_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb1_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb1_gpu0 -> layer0_expert2_mlp1_dp0_mb1_gpu2
	layer0_router_dp1_mb1_gpu0 -> layer0_expert2_mlp1_dp1_mb1_gpu2
	layer0_expert2_mlp2_dp0_mb1_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb1_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb1_gpu2 -> layer0_expert2_mlp2_dp0_mb1_gpu2
	layer0_expert2_mlp1_dp1_mb1_gpu2 -> layer0_expert2_mlp2_dp1_mb1_gpu2
	layer0_expert3_mlp1_dp0_mb1_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb1_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb1_gpu0 -> layer0_expert3_mlp1_dp0_mb1_gpu3
	layer0_router_dp1_mb1_gpu0 -> layer0_expert3_mlp1_dp1_mb1_gpu3
	layer0_expert3_mlp2_dp0_mb1_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb1_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb1_gpu3 -> layer0_expert3_mlp2_dp0_mb1_gpu3
	layer0_expert3_mlp1_dp1_mb1_gpu3 -> layer0_expert3_mlp2_dp1_mb1_gpu3
	layer0_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb1_gpu0 -> layer0_expert_agg_dp0_mb1
	layer0_expert0_mlp2_dp1_mb1_gpu0 -> layer0_expert_agg_dp1_mb1
	layer0_expert1_mlp2_dp0_mb1_gpu1 -> layer0_expert_agg_dp0_mb1
	layer0_expert1_mlp2_dp1_mb1_gpu1 -> layer0_expert_agg_dp1_mb1
	layer0_expert2_mlp2_dp0_mb1_gpu2 -> layer0_expert_agg_dp0_mb1
	layer0_expert2_mlp2_dp1_mb1_gpu2 -> layer0_expert_agg_dp1_mb1
	layer0_expert3_mlp2_dp0_mb1_gpu3 -> layer0_expert_agg_dp0_mb1
	layer0_expert3_mlp2_dp1_mb1_gpu3 -> layer0_expert_agg_dp1_mb1
	layer1_qkv_dp0_mb1_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb1_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb1 -> layer1_qkv_dp0_mb1_gpu0
	layer0_expert_agg_dp1_mb1 -> layer1_qkv_dp1_mb1_gpu0
	layer1_qkv_dp0_mb1_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb1_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb1 -> layer1_qkv_dp0_mb1_gpu1
	layer0_expert_agg_dp1_mb1 -> layer1_qkv_dp1_mb1_gpu1
	layer1_qkv_dp0_mb1_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb1_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb1 -> layer1_qkv_dp0_mb1_gpu2
	layer0_expert_agg_dp1_mb1 -> layer1_qkv_dp1_mb1_gpu2
	layer1_qkv_dp0_mb1_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb1_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb1 -> layer1_qkv_dp0_mb1_gpu3
	layer0_expert_agg_dp1_mb1 -> layer1_qkv_dp1_mb1_gpu3
	layer1_attn_scores_dp0_mb1_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb1_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb1_gpu0 -> layer1_attn_scores_dp0_mb1_gpu0
	layer1_qkv_dp1_mb1_gpu0 -> layer1_attn_scores_dp1_mb1_gpu0
	layer1_softmax_dp0_mb1_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb1_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb1_gpu0 -> layer1_softmax_dp0_mb1_gpu0
	layer1_attn_scores_dp1_mb1_gpu0 -> layer1_softmax_dp1_mb1_gpu0
	layer1_attn_scores_dp0_mb1_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb1_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb1_gpu1 -> layer1_attn_scores_dp0_mb1_gpu1
	layer1_qkv_dp1_mb1_gpu1 -> layer1_attn_scores_dp1_mb1_gpu1
	layer1_softmax_dp0_mb1_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb1_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb1_gpu1 -> layer1_softmax_dp0_mb1_gpu1
	layer1_attn_scores_dp1_mb1_gpu1 -> layer1_softmax_dp1_mb1_gpu1
	layer1_attn_scores_dp0_mb1_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb1_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb1_gpu2 -> layer1_attn_scores_dp0_mb1_gpu2
	layer1_qkv_dp1_mb1_gpu2 -> layer1_attn_scores_dp1_mb1_gpu2
	layer1_softmax_dp0_mb1_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb1_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb1_gpu2 -> layer1_softmax_dp0_mb1_gpu2
	layer1_attn_scores_dp1_mb1_gpu2 -> layer1_softmax_dp1_mb1_gpu2
	layer1_attn_scores_dp0_mb1_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb1_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb1_gpu3 -> layer1_attn_scores_dp0_mb1_gpu3
	layer1_qkv_dp1_mb1_gpu3 -> layer1_attn_scores_dp1_mb1_gpu3
	layer1_softmax_dp0_mb1_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb1_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb1_gpu3 -> layer1_softmax_dp0_mb1_gpu3
	layer1_attn_scores_dp1_mb1_gpu3 -> layer1_softmax_dp1_mb1_gpu3
	layer1_attn_out_dp0_mb1_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb1_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb1_gpu0 -> layer1_attn_out_dp0_mb1_gpu0
	layer1_softmax_dp1_mb1_gpu0 -> layer1_attn_out_dp1_mb1_gpu0
	layer1_attn_out_dp0_mb1_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb1_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb1_gpu1 -> layer1_attn_out_dp0_mb1_gpu1
	layer1_softmax_dp1_mb1_gpu1 -> layer1_attn_out_dp1_mb1_gpu1
	layer1_attn_out_dp0_mb1_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb1_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb1_gpu2 -> layer1_attn_out_dp0_mb1_gpu2
	layer1_softmax_dp1_mb1_gpu2 -> layer1_attn_out_dp1_mb1_gpu2
	layer1_attn_out_dp0_mb1_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb1_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb1_gpu3 -> layer1_attn_out_dp0_mb1_gpu3
	layer1_softmax_dp1_mb1_gpu3 -> layer1_attn_out_dp1_mb1_gpu3
	layer1_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb1_gpu0 -> layer1_attn_allreduce_dp0_mb1
	layer1_attn_out_dp1_mb1_gpu0 -> layer1_attn_allreduce_dp1_mb1
	layer1_attn_out_dp0_mb1_gpu1 -> layer1_attn_allreduce_dp0_mb1
	layer1_attn_out_dp1_mb1_gpu1 -> layer1_attn_allreduce_dp1_mb1
	layer1_attn_out_dp0_mb1_gpu2 -> layer1_attn_allreduce_dp0_mb1
	layer1_attn_out_dp1_mb1_gpu2 -> layer1_attn_allreduce_dp1_mb1
	layer1_attn_out_dp0_mb1_gpu3 -> layer1_attn_allreduce_dp0_mb1
	layer1_attn_out_dp1_mb1_gpu3 -> layer1_attn_allreduce_dp1_mb1
	layer1_router_dp0_mb1_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb1_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb1 -> layer1_router_dp0_mb1_gpu0
	layer1_attn_allreduce_dp1_mb1 -> layer1_router_dp1_mb1_gpu0
	layer1_expert0_mlp1_dp0_mb1_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb1_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb1_gpu0 -> layer1_expert0_mlp1_dp0_mb1_gpu0
	layer1_router_dp1_mb1_gpu0 -> layer1_expert0_mlp1_dp1_mb1_gpu0
	layer1_expert0_mlp2_dp0_mb1_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb1_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb1_gpu0 -> layer1_expert0_mlp2_dp0_mb1_gpu0
	layer1_expert0_mlp1_dp1_mb1_gpu0 -> layer1_expert0_mlp2_dp1_mb1_gpu0
	layer1_expert1_mlp1_dp0_mb1_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb1_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb1_gpu0 -> layer1_expert1_mlp1_dp0_mb1_gpu1
	layer1_router_dp1_mb1_gpu0 -> layer1_expert1_mlp1_dp1_mb1_gpu1
	layer1_expert1_mlp2_dp0_mb1_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb1_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb1_gpu1 -> layer1_expert1_mlp2_dp0_mb1_gpu1
	layer1_expert1_mlp1_dp1_mb1_gpu1 -> layer1_expert1_mlp2_dp1_mb1_gpu1
	layer1_expert2_mlp1_dp0_mb1_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb1_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb1_gpu0 -> layer1_expert2_mlp1_dp0_mb1_gpu2
	layer1_router_dp1_mb1_gpu0 -> layer1_expert2_mlp1_dp1_mb1_gpu2
	layer1_expert2_mlp2_dp0_mb1_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb1_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb1_gpu2 -> layer1_expert2_mlp2_dp0_mb1_gpu2
	layer1_expert2_mlp1_dp1_mb1_gpu2 -> layer1_expert2_mlp2_dp1_mb1_gpu2
	layer1_expert3_mlp1_dp0_mb1_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb1_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb1_gpu0 -> layer1_expert3_mlp1_dp0_mb1_gpu3
	layer1_router_dp1_mb1_gpu0 -> layer1_expert3_mlp1_dp1_mb1_gpu3
	layer1_expert3_mlp2_dp0_mb1_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb1_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb1_gpu3 -> layer1_expert3_mlp2_dp0_mb1_gpu3
	layer1_expert3_mlp1_dp1_mb1_gpu3 -> layer1_expert3_mlp2_dp1_mb1_gpu3
	layer1_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb1_gpu0 -> layer1_expert_agg_dp0_mb1
	layer1_expert0_mlp2_dp1_mb1_gpu0 -> layer1_expert_agg_dp1_mb1
	layer1_expert1_mlp2_dp0_mb1_gpu1 -> layer1_expert_agg_dp0_mb1
	layer1_expert1_mlp2_dp1_mb1_gpu1 -> layer1_expert_agg_dp1_mb1
	layer1_expert2_mlp2_dp0_mb1_gpu2 -> layer1_expert_agg_dp0_mb1
	layer1_expert2_mlp2_dp1_mb1_gpu2 -> layer1_expert_agg_dp1_mb1
	layer1_expert3_mlp2_dp0_mb1_gpu3 -> layer1_expert_agg_dp0_mb1
	layer1_expert3_mlp2_dp1_mb1_gpu3 -> layer1_expert_agg_dp1_mb1
	layer2_qkv_dp0_mb1_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb1_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb1 -> layer2_qkv_dp0_mb1_gpu0
	layer1_expert_agg_dp1_mb1 -> layer2_qkv_dp1_mb1_gpu0
	layer2_qkv_dp0_mb1_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb1_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb1 -> layer2_qkv_dp0_mb1_gpu1
	layer1_expert_agg_dp1_mb1 -> layer2_qkv_dp1_mb1_gpu1
	layer2_qkv_dp0_mb1_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb1_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb1 -> layer2_qkv_dp0_mb1_gpu2
	layer1_expert_agg_dp1_mb1 -> layer2_qkv_dp1_mb1_gpu2
	layer2_qkv_dp0_mb1_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb1_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb1 -> layer2_qkv_dp0_mb1_gpu3
	layer1_expert_agg_dp1_mb1 -> layer2_qkv_dp1_mb1_gpu3
	layer2_attn_scores_dp0_mb1_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb1_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb1_gpu0 -> layer2_attn_scores_dp0_mb1_gpu0
	layer2_qkv_dp1_mb1_gpu0 -> layer2_attn_scores_dp1_mb1_gpu0
	layer2_softmax_dp0_mb1_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb1_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb1_gpu0 -> layer2_softmax_dp0_mb1_gpu0
	layer2_attn_scores_dp1_mb1_gpu0 -> layer2_softmax_dp1_mb1_gpu0
	layer2_attn_scores_dp0_mb1_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb1_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb1_gpu1 -> layer2_attn_scores_dp0_mb1_gpu1
	layer2_qkv_dp1_mb1_gpu1 -> layer2_attn_scores_dp1_mb1_gpu1
	layer2_softmax_dp0_mb1_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb1_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb1_gpu1 -> layer2_softmax_dp0_mb1_gpu1
	layer2_attn_scores_dp1_mb1_gpu1 -> layer2_softmax_dp1_mb1_gpu1
	layer2_attn_scores_dp0_mb1_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb1_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb1_gpu2 -> layer2_attn_scores_dp0_mb1_gpu2
	layer2_qkv_dp1_mb1_gpu2 -> layer2_attn_scores_dp1_mb1_gpu2
	layer2_softmax_dp0_mb1_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb1_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb1_gpu2 -> layer2_softmax_dp0_mb1_gpu2
	layer2_attn_scores_dp1_mb1_gpu2 -> layer2_softmax_dp1_mb1_gpu2
	layer2_attn_scores_dp0_mb1_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb1_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb1_gpu3 -> layer2_attn_scores_dp0_mb1_gpu3
	layer2_qkv_dp1_mb1_gpu3 -> layer2_attn_scores_dp1_mb1_gpu3
	layer2_softmax_dp0_mb1_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb1_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb1_gpu3 -> layer2_softmax_dp0_mb1_gpu3
	layer2_attn_scores_dp1_mb1_gpu3 -> layer2_softmax_dp1_mb1_gpu3
	layer2_attn_out_dp0_mb1_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb1_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb1_gpu0 -> layer2_attn_out_dp0_mb1_gpu0
	layer2_softmax_dp1_mb1_gpu0 -> layer2_attn_out_dp1_mb1_gpu0
	layer2_attn_out_dp0_mb1_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb1_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb1_gpu1 -> layer2_attn_out_dp0_mb1_gpu1
	layer2_softmax_dp1_mb1_gpu1 -> layer2_attn_out_dp1_mb1_gpu1
	layer2_attn_out_dp0_mb1_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb1_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb1_gpu2 -> layer2_attn_out_dp0_mb1_gpu2
	layer2_softmax_dp1_mb1_gpu2 -> layer2_attn_out_dp1_mb1_gpu2
	layer2_attn_out_dp0_mb1_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb1_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb1_gpu3 -> layer2_attn_out_dp0_mb1_gpu3
	layer2_softmax_dp1_mb1_gpu3 -> layer2_attn_out_dp1_mb1_gpu3
	layer2_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb1_gpu0 -> layer2_attn_allreduce_dp0_mb1
	layer2_attn_out_dp1_mb1_gpu0 -> layer2_attn_allreduce_dp1_mb1
	layer2_attn_out_dp0_mb1_gpu1 -> layer2_attn_allreduce_dp0_mb1
	layer2_attn_out_dp1_mb1_gpu1 -> layer2_attn_allreduce_dp1_mb1
	layer2_attn_out_dp0_mb1_gpu2 -> layer2_attn_allreduce_dp0_mb1
	layer2_attn_out_dp1_mb1_gpu2 -> layer2_attn_allreduce_dp1_mb1
	layer2_attn_out_dp0_mb1_gpu3 -> layer2_attn_allreduce_dp0_mb1
	layer2_attn_out_dp1_mb1_gpu3 -> layer2_attn_allreduce_dp1_mb1
	layer2_router_dp0_mb1_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb1_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb1 -> layer2_router_dp0_mb1_gpu0
	layer2_attn_allreduce_dp1_mb1 -> layer2_router_dp1_mb1_gpu0
	layer2_expert0_mlp1_dp0_mb1_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb1_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb1_gpu0 -> layer2_expert0_mlp1_dp0_mb1_gpu0
	layer2_router_dp1_mb1_gpu0 -> layer2_expert0_mlp1_dp1_mb1_gpu0
	layer2_expert0_mlp2_dp0_mb1_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb1_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb1_gpu0 -> layer2_expert0_mlp2_dp0_mb1_gpu0
	layer2_expert0_mlp1_dp1_mb1_gpu0 -> layer2_expert0_mlp2_dp1_mb1_gpu0
	layer2_expert1_mlp1_dp0_mb1_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb1_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb1_gpu0 -> layer2_expert1_mlp1_dp0_mb1_gpu1
	layer2_router_dp1_mb1_gpu0 -> layer2_expert1_mlp1_dp1_mb1_gpu1
	layer2_expert1_mlp2_dp0_mb1_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb1_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb1_gpu1 -> layer2_expert1_mlp2_dp0_mb1_gpu1
	layer2_expert1_mlp1_dp1_mb1_gpu1 -> layer2_expert1_mlp2_dp1_mb1_gpu1
	layer2_expert2_mlp1_dp0_mb1_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb1_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb1_gpu0 -> layer2_expert2_mlp1_dp0_mb1_gpu2
	layer2_router_dp1_mb1_gpu0 -> layer2_expert2_mlp1_dp1_mb1_gpu2
	layer2_expert2_mlp2_dp0_mb1_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb1_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb1_gpu2 -> layer2_expert2_mlp2_dp0_mb1_gpu2
	layer2_expert2_mlp1_dp1_mb1_gpu2 -> layer2_expert2_mlp2_dp1_mb1_gpu2
	layer2_expert3_mlp1_dp0_mb1_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb1_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb1_gpu0 -> layer2_expert3_mlp1_dp0_mb1_gpu3
	layer2_router_dp1_mb1_gpu0 -> layer2_expert3_mlp1_dp1_mb1_gpu3
	layer2_expert3_mlp2_dp0_mb1_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb1_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb1_gpu3 -> layer2_expert3_mlp2_dp0_mb1_gpu3
	layer2_expert3_mlp1_dp1_mb1_gpu3 -> layer2_expert3_mlp2_dp1_mb1_gpu3
	layer2_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb1_gpu0 -> layer2_expert_agg_dp0_mb1
	layer2_expert0_mlp2_dp1_mb1_gpu0 -> layer2_expert_agg_dp1_mb1
	layer2_expert1_mlp2_dp0_mb1_gpu1 -> layer2_expert_agg_dp0_mb1
	layer2_expert1_mlp2_dp1_mb1_gpu1 -> layer2_expert_agg_dp1_mb1
	layer2_expert2_mlp2_dp0_mb1_gpu2 -> layer2_expert_agg_dp0_mb1
	layer2_expert2_mlp2_dp1_mb1_gpu2 -> layer2_expert_agg_dp1_mb1
	layer2_expert3_mlp2_dp0_mb1_gpu3 -> layer2_expert_agg_dp0_mb1
	layer2_expert3_mlp2_dp1_mb1_gpu3 -> layer2_expert_agg_dp1_mb1
	layer3_qkv_dp0_mb1_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb1_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb1 -> layer3_qkv_dp0_mb1_gpu0
	layer2_expert_agg_dp1_mb1 -> layer3_qkv_dp1_mb1_gpu0
	layer3_qkv_dp0_mb1_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb1_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb1 -> layer3_qkv_dp0_mb1_gpu1
	layer2_expert_agg_dp1_mb1 -> layer3_qkv_dp1_mb1_gpu1
	layer3_qkv_dp0_mb1_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb1_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb1 -> layer3_qkv_dp0_mb1_gpu2
	layer2_expert_agg_dp1_mb1 -> layer3_qkv_dp1_mb1_gpu2
	layer3_qkv_dp0_mb1_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb1_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb1 -> layer3_qkv_dp0_mb1_gpu3
	layer2_expert_agg_dp1_mb1 -> layer3_qkv_dp1_mb1_gpu3
	layer3_attn_scores_dp0_mb1_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb1_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb1_gpu0 -> layer3_attn_scores_dp0_mb1_gpu0
	layer3_qkv_dp1_mb1_gpu0 -> layer3_attn_scores_dp1_mb1_gpu0
	layer3_softmax_dp0_mb1_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb1_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb1_gpu0 -> layer3_softmax_dp0_mb1_gpu0
	layer3_attn_scores_dp1_mb1_gpu0 -> layer3_softmax_dp1_mb1_gpu0
	layer3_attn_scores_dp0_mb1_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb1_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb1_gpu1 -> layer3_attn_scores_dp0_mb1_gpu1
	layer3_qkv_dp1_mb1_gpu1 -> layer3_attn_scores_dp1_mb1_gpu1
	layer3_softmax_dp0_mb1_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb1_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb1_gpu1 -> layer3_softmax_dp0_mb1_gpu1
	layer3_attn_scores_dp1_mb1_gpu1 -> layer3_softmax_dp1_mb1_gpu1
	layer3_attn_scores_dp0_mb1_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb1_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb1_gpu2 -> layer3_attn_scores_dp0_mb1_gpu2
	layer3_qkv_dp1_mb1_gpu2 -> layer3_attn_scores_dp1_mb1_gpu2
	layer3_softmax_dp0_mb1_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb1_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb1_gpu2 -> layer3_softmax_dp0_mb1_gpu2
	layer3_attn_scores_dp1_mb1_gpu2 -> layer3_softmax_dp1_mb1_gpu2
	layer3_attn_scores_dp0_mb1_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb1_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb1_gpu3 -> layer3_attn_scores_dp0_mb1_gpu3
	layer3_qkv_dp1_mb1_gpu3 -> layer3_attn_scores_dp1_mb1_gpu3
	layer3_softmax_dp0_mb1_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb1_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb1_gpu3 -> layer3_softmax_dp0_mb1_gpu3
	layer3_attn_scores_dp1_mb1_gpu3 -> layer3_softmax_dp1_mb1_gpu3
	layer3_attn_out_dp0_mb1_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb1_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb1_gpu0 -> layer3_attn_out_dp0_mb1_gpu0
	layer3_softmax_dp1_mb1_gpu0 -> layer3_attn_out_dp1_mb1_gpu0
	layer3_attn_out_dp0_mb1_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb1_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb1_gpu1 -> layer3_attn_out_dp0_mb1_gpu1
	layer3_softmax_dp1_mb1_gpu1 -> layer3_attn_out_dp1_mb1_gpu1
	layer3_attn_out_dp0_mb1_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb1_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb1_gpu2 -> layer3_attn_out_dp0_mb1_gpu2
	layer3_softmax_dp1_mb1_gpu2 -> layer3_attn_out_dp1_mb1_gpu2
	layer3_attn_out_dp0_mb1_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb1_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb1_gpu3 -> layer3_attn_out_dp0_mb1_gpu3
	layer3_softmax_dp1_mb1_gpu3 -> layer3_attn_out_dp1_mb1_gpu3
	layer3_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb1_gpu0 -> layer3_attn_allreduce_dp0_mb1
	layer3_attn_out_dp1_mb1_gpu0 -> layer3_attn_allreduce_dp1_mb1
	layer3_attn_out_dp0_mb1_gpu1 -> layer3_attn_allreduce_dp0_mb1
	layer3_attn_out_dp1_mb1_gpu1 -> layer3_attn_allreduce_dp1_mb1
	layer3_attn_out_dp0_mb1_gpu2 -> layer3_attn_allreduce_dp0_mb1
	layer3_attn_out_dp1_mb1_gpu2 -> layer3_attn_allreduce_dp1_mb1
	layer3_attn_out_dp0_mb1_gpu3 -> layer3_attn_allreduce_dp0_mb1
	layer3_attn_out_dp1_mb1_gpu3 -> layer3_attn_allreduce_dp1_mb1
	layer3_router_dp0_mb1_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb1_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb1 -> layer3_router_dp0_mb1_gpu0
	layer3_attn_allreduce_dp1_mb1 -> layer3_router_dp1_mb1_gpu0
	layer3_expert0_mlp1_dp0_mb1_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb1_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb1_gpu0 -> layer3_expert0_mlp1_dp0_mb1_gpu0
	layer3_router_dp1_mb1_gpu0 -> layer3_expert0_mlp1_dp1_mb1_gpu0
	layer3_expert0_mlp2_dp0_mb1_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb1_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb1_gpu0 -> layer3_expert0_mlp2_dp0_mb1_gpu0
	layer3_expert0_mlp1_dp1_mb1_gpu0 -> layer3_expert0_mlp2_dp1_mb1_gpu0
	layer3_expert1_mlp1_dp0_mb1_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb1_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb1_gpu0 -> layer3_expert1_mlp1_dp0_mb1_gpu1
	layer3_router_dp1_mb1_gpu0 -> layer3_expert1_mlp1_dp1_mb1_gpu1
	layer3_expert1_mlp2_dp0_mb1_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb1_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb1_gpu1 -> layer3_expert1_mlp2_dp0_mb1_gpu1
	layer3_expert1_mlp1_dp1_mb1_gpu1 -> layer3_expert1_mlp2_dp1_mb1_gpu1
	layer3_expert2_mlp1_dp0_mb1_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb1_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb1_gpu0 -> layer3_expert2_mlp1_dp0_mb1_gpu2
	layer3_router_dp1_mb1_gpu0 -> layer3_expert2_mlp1_dp1_mb1_gpu2
	layer3_expert2_mlp2_dp0_mb1_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb1_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb1_gpu2 -> layer3_expert2_mlp2_dp0_mb1_gpu2
	layer3_expert2_mlp1_dp1_mb1_gpu2 -> layer3_expert2_mlp2_dp1_mb1_gpu2
	layer3_expert3_mlp1_dp0_mb1_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb1_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb1_gpu0 -> layer3_expert3_mlp1_dp0_mb1_gpu3
	layer3_router_dp1_mb1_gpu0 -> layer3_expert3_mlp1_dp1_mb1_gpu3
	layer3_expert3_mlp2_dp0_mb1_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb1_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb1_gpu3 -> layer3_expert3_mlp2_dp0_mb1_gpu3
	layer3_expert3_mlp1_dp1_mb1_gpu3 -> layer3_expert3_mlp2_dp1_mb1_gpu3
	layer3_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb1_gpu0 -> layer3_expert_agg_dp0_mb1
	layer3_expert0_mlp2_dp1_mb1_gpu0 -> layer3_expert_agg_dp1_mb1
	layer3_expert1_mlp2_dp0_mb1_gpu1 -> layer3_expert_agg_dp0_mb1
	layer3_expert1_mlp2_dp1_mb1_gpu1 -> layer3_expert_agg_dp1_mb1
	layer3_expert2_mlp2_dp0_mb1_gpu2 -> layer3_expert_agg_dp0_mb1
	layer3_expert2_mlp2_dp1_mb1_gpu2 -> layer3_expert_agg_dp1_mb1
	layer3_expert3_mlp2_dp0_mb1_gpu3 -> layer3_expert_agg_dp0_mb1
	layer3_expert3_mlp2_dp1_mb1_gpu3 -> layer3_expert_agg_dp1_mb1
	layer0_qkv_dp0_mb2_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb2_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb2_gpu0
	split_mb1 -> layer0_qkv_dp1_mb2_gpu0
	layer0_qkv_dp0_mb2_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb2_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb2_gpu1
	split_mb1 -> layer0_qkv_dp1_mb2_gpu1
	layer0_qkv_dp0_mb2_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb2_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb2_gpu2
	split_mb1 -> layer0_qkv_dp1_mb2_gpu2
	layer0_qkv_dp0_mb2_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb2_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb2_gpu3
	split_mb1 -> layer0_qkv_dp1_mb2_gpu3
	layer0_attn_scores_dp0_mb2_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb2_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb2_gpu0 -> layer0_attn_scores_dp0_mb2_gpu0
	layer0_qkv_dp1_mb2_gpu0 -> layer0_attn_scores_dp1_mb2_gpu0
	layer0_softmax_dp0_mb2_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb2_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb2_gpu0 -> layer0_softmax_dp0_mb2_gpu0
	layer0_attn_scores_dp1_mb2_gpu0 -> layer0_softmax_dp1_mb2_gpu0
	layer0_attn_scores_dp0_mb2_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb2_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb2_gpu1 -> layer0_attn_scores_dp0_mb2_gpu1
	layer0_qkv_dp1_mb2_gpu1 -> layer0_attn_scores_dp1_mb2_gpu1
	layer0_softmax_dp0_mb2_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb2_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb2_gpu1 -> layer0_softmax_dp0_mb2_gpu1
	layer0_attn_scores_dp1_mb2_gpu1 -> layer0_softmax_dp1_mb2_gpu1
	layer0_attn_scores_dp0_mb2_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb2_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb2_gpu2 -> layer0_attn_scores_dp0_mb2_gpu2
	layer0_qkv_dp1_mb2_gpu2 -> layer0_attn_scores_dp1_mb2_gpu2
	layer0_softmax_dp0_mb2_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb2_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb2_gpu2 -> layer0_softmax_dp0_mb2_gpu2
	layer0_attn_scores_dp1_mb2_gpu2 -> layer0_softmax_dp1_mb2_gpu2
	layer0_attn_scores_dp0_mb2_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb2_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb2_gpu3 -> layer0_attn_scores_dp0_mb2_gpu3
	layer0_qkv_dp1_mb2_gpu3 -> layer0_attn_scores_dp1_mb2_gpu3
	layer0_softmax_dp0_mb2_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb2_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb2_gpu3 -> layer0_softmax_dp0_mb2_gpu3
	layer0_attn_scores_dp1_mb2_gpu3 -> layer0_softmax_dp1_mb2_gpu3
	layer0_attn_out_dp0_mb2_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb2_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb2_gpu0 -> layer0_attn_out_dp0_mb2_gpu0
	layer0_softmax_dp1_mb2_gpu0 -> layer0_attn_out_dp1_mb2_gpu0
	layer0_attn_out_dp0_mb2_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb2_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb2_gpu1 -> layer0_attn_out_dp0_mb2_gpu1
	layer0_softmax_dp1_mb2_gpu1 -> layer0_attn_out_dp1_mb2_gpu1
	layer0_attn_out_dp0_mb2_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb2_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb2_gpu2 -> layer0_attn_out_dp0_mb2_gpu2
	layer0_softmax_dp1_mb2_gpu2 -> layer0_attn_out_dp1_mb2_gpu2
	layer0_attn_out_dp0_mb2_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb2_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb2_gpu3 -> layer0_attn_out_dp0_mb2_gpu3
	layer0_softmax_dp1_mb2_gpu3 -> layer0_attn_out_dp1_mb2_gpu3
	layer0_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb2_gpu0 -> layer0_attn_allreduce_dp0_mb2
	layer0_attn_out_dp1_mb2_gpu0 -> layer0_attn_allreduce_dp1_mb2
	layer0_attn_out_dp0_mb2_gpu1 -> layer0_attn_allreduce_dp0_mb2
	layer0_attn_out_dp1_mb2_gpu1 -> layer0_attn_allreduce_dp1_mb2
	layer0_attn_out_dp0_mb2_gpu2 -> layer0_attn_allreduce_dp0_mb2
	layer0_attn_out_dp1_mb2_gpu2 -> layer0_attn_allreduce_dp1_mb2
	layer0_attn_out_dp0_mb2_gpu3 -> layer0_attn_allreduce_dp0_mb2
	layer0_attn_out_dp1_mb2_gpu3 -> layer0_attn_allreduce_dp1_mb2
	layer0_router_dp0_mb2_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb2_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb2 -> layer0_router_dp0_mb2_gpu0
	layer0_attn_allreduce_dp1_mb2 -> layer0_router_dp1_mb2_gpu0
	layer0_expert0_mlp1_dp0_mb2_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb2_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb2_gpu0 -> layer0_expert0_mlp1_dp0_mb2_gpu0
	layer0_router_dp1_mb2_gpu0 -> layer0_expert0_mlp1_dp1_mb2_gpu0
	layer0_expert0_mlp2_dp0_mb2_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb2_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb2_gpu0 -> layer0_expert0_mlp2_dp0_mb2_gpu0
	layer0_expert0_mlp1_dp1_mb2_gpu0 -> layer0_expert0_mlp2_dp1_mb2_gpu0
	layer0_expert1_mlp1_dp0_mb2_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb2_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb2_gpu0 -> layer0_expert1_mlp1_dp0_mb2_gpu1
	layer0_router_dp1_mb2_gpu0 -> layer0_expert1_mlp1_dp1_mb2_gpu1
	layer0_expert1_mlp2_dp0_mb2_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb2_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb2_gpu1 -> layer0_expert1_mlp2_dp0_mb2_gpu1
	layer0_expert1_mlp1_dp1_mb2_gpu1 -> layer0_expert1_mlp2_dp1_mb2_gpu1
	layer0_expert2_mlp1_dp0_mb2_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb2_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb2_gpu0 -> layer0_expert2_mlp1_dp0_mb2_gpu2
	layer0_router_dp1_mb2_gpu0 -> layer0_expert2_mlp1_dp1_mb2_gpu2
	layer0_expert2_mlp2_dp0_mb2_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb2_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb2_gpu2 -> layer0_expert2_mlp2_dp0_mb2_gpu2
	layer0_expert2_mlp1_dp1_mb2_gpu2 -> layer0_expert2_mlp2_dp1_mb2_gpu2
	layer0_expert3_mlp1_dp0_mb2_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb2_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb2_gpu0 -> layer0_expert3_mlp1_dp0_mb2_gpu3
	layer0_router_dp1_mb2_gpu0 -> layer0_expert3_mlp1_dp1_mb2_gpu3
	layer0_expert3_mlp2_dp0_mb2_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb2_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb2_gpu3 -> layer0_expert3_mlp2_dp0_mb2_gpu3
	layer0_expert3_mlp1_dp1_mb2_gpu3 -> layer0_expert3_mlp2_dp1_mb2_gpu3
	layer0_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb2_gpu0 -> layer0_expert_agg_dp0_mb2
	layer0_expert0_mlp2_dp1_mb2_gpu0 -> layer0_expert_agg_dp1_mb2
	layer0_expert1_mlp2_dp0_mb2_gpu1 -> layer0_expert_agg_dp0_mb2
	layer0_expert1_mlp2_dp1_mb2_gpu1 -> layer0_expert_agg_dp1_mb2
	layer0_expert2_mlp2_dp0_mb2_gpu2 -> layer0_expert_agg_dp0_mb2
	layer0_expert2_mlp2_dp1_mb2_gpu2 -> layer0_expert_agg_dp1_mb2
	layer0_expert3_mlp2_dp0_mb2_gpu3 -> layer0_expert_agg_dp0_mb2
	layer0_expert3_mlp2_dp1_mb2_gpu3 -> layer0_expert_agg_dp1_mb2
	layer1_qkv_dp0_mb2_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb2_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb2 -> layer1_qkv_dp0_mb2_gpu0
	layer0_expert_agg_dp1_mb2 -> layer1_qkv_dp1_mb2_gpu0
	layer1_qkv_dp0_mb2_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb2_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb2 -> layer1_qkv_dp0_mb2_gpu1
	layer0_expert_agg_dp1_mb2 -> layer1_qkv_dp1_mb2_gpu1
	layer1_qkv_dp0_mb2_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb2_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb2 -> layer1_qkv_dp0_mb2_gpu2
	layer0_expert_agg_dp1_mb2 -> layer1_qkv_dp1_mb2_gpu2
	layer1_qkv_dp0_mb2_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb2_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb2 -> layer1_qkv_dp0_mb2_gpu3
	layer0_expert_agg_dp1_mb2 -> layer1_qkv_dp1_mb2_gpu3
	layer1_attn_scores_dp0_mb2_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb2_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb2_gpu0 -> layer1_attn_scores_dp0_mb2_gpu0
	layer1_qkv_dp1_mb2_gpu0 -> layer1_attn_scores_dp1_mb2_gpu0
	layer1_softmax_dp0_mb2_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb2_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb2_gpu0 -> layer1_softmax_dp0_mb2_gpu0
	layer1_attn_scores_dp1_mb2_gpu0 -> layer1_softmax_dp1_mb2_gpu0
	layer1_attn_scores_dp0_mb2_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb2_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb2_gpu1 -> layer1_attn_scores_dp0_mb2_gpu1
	layer1_qkv_dp1_mb2_gpu1 -> layer1_attn_scores_dp1_mb2_gpu1
	layer1_softmax_dp0_mb2_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb2_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb2_gpu1 -> layer1_softmax_dp0_mb2_gpu1
	layer1_attn_scores_dp1_mb2_gpu1 -> layer1_softmax_dp1_mb2_gpu1
	layer1_attn_scores_dp0_mb2_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb2_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb2_gpu2 -> layer1_attn_scores_dp0_mb2_gpu2
	layer1_qkv_dp1_mb2_gpu2 -> layer1_attn_scores_dp1_mb2_gpu2
	layer1_softmax_dp0_mb2_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb2_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb2_gpu2 -> layer1_softmax_dp0_mb2_gpu2
	layer1_attn_scores_dp1_mb2_gpu2 -> layer1_softmax_dp1_mb2_gpu2
	layer1_attn_scores_dp0_mb2_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb2_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb2_gpu3 -> layer1_attn_scores_dp0_mb2_gpu3
	layer1_qkv_dp1_mb2_gpu3 -> layer1_attn_scores_dp1_mb2_gpu3
	layer1_softmax_dp0_mb2_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb2_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb2_gpu3 -> layer1_softmax_dp0_mb2_gpu3
	layer1_attn_scores_dp1_mb2_gpu3 -> layer1_softmax_dp1_mb2_gpu3
	layer1_attn_out_dp0_mb2_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb2_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb2_gpu0 -> layer1_attn_out_dp0_mb2_gpu0
	layer1_softmax_dp1_mb2_gpu0 -> layer1_attn_out_dp1_mb2_gpu0
	layer1_attn_out_dp0_mb2_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb2_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb2_gpu1 -> layer1_attn_out_dp0_mb2_gpu1
	layer1_softmax_dp1_mb2_gpu1 -> layer1_attn_out_dp1_mb2_gpu1
	layer1_attn_out_dp0_mb2_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb2_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb2_gpu2 -> layer1_attn_out_dp0_mb2_gpu2
	layer1_softmax_dp1_mb2_gpu2 -> layer1_attn_out_dp1_mb2_gpu2
	layer1_attn_out_dp0_mb2_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb2_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb2_gpu3 -> layer1_attn_out_dp0_mb2_gpu3
	layer1_softmax_dp1_mb2_gpu3 -> layer1_attn_out_dp1_mb2_gpu3
	layer1_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb2_gpu0 -> layer1_attn_allreduce_dp0_mb2
	layer1_attn_out_dp1_mb2_gpu0 -> layer1_attn_allreduce_dp1_mb2
	layer1_attn_out_dp0_mb2_gpu1 -> layer1_attn_allreduce_dp0_mb2
	layer1_attn_out_dp1_mb2_gpu1 -> layer1_attn_allreduce_dp1_mb2
	layer1_attn_out_dp0_mb2_gpu2 -> layer1_attn_allreduce_dp0_mb2
	layer1_attn_out_dp1_mb2_gpu2 -> layer1_attn_allreduce_dp1_mb2
	layer1_attn_out_dp0_mb2_gpu3 -> layer1_attn_allreduce_dp0_mb2
	layer1_attn_out_dp1_mb2_gpu3 -> layer1_attn_allreduce_dp1_mb2
	layer1_router_dp0_mb2_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb2_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb2 -> layer1_router_dp0_mb2_gpu0
	layer1_attn_allreduce_dp1_mb2 -> layer1_router_dp1_mb2_gpu0
	layer1_expert0_mlp1_dp0_mb2_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb2_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb2_gpu0 -> layer1_expert0_mlp1_dp0_mb2_gpu0
	layer1_router_dp1_mb2_gpu0 -> layer1_expert0_mlp1_dp1_mb2_gpu0
	layer1_expert0_mlp2_dp0_mb2_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb2_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb2_gpu0 -> layer1_expert0_mlp2_dp0_mb2_gpu0
	layer1_expert0_mlp1_dp1_mb2_gpu0 -> layer1_expert0_mlp2_dp1_mb2_gpu0
	layer1_expert1_mlp1_dp0_mb2_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb2_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb2_gpu0 -> layer1_expert1_mlp1_dp0_mb2_gpu1
	layer1_router_dp1_mb2_gpu0 -> layer1_expert1_mlp1_dp1_mb2_gpu1
	layer1_expert1_mlp2_dp0_mb2_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb2_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb2_gpu1 -> layer1_expert1_mlp2_dp0_mb2_gpu1
	layer1_expert1_mlp1_dp1_mb2_gpu1 -> layer1_expert1_mlp2_dp1_mb2_gpu1
	layer1_expert2_mlp1_dp0_mb2_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb2_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb2_gpu0 -> layer1_expert2_mlp1_dp0_mb2_gpu2
	layer1_router_dp1_mb2_gpu0 -> layer1_expert2_mlp1_dp1_mb2_gpu2
	layer1_expert2_mlp2_dp0_mb2_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb2_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb2_gpu2 -> layer1_expert2_mlp2_dp0_mb2_gpu2
	layer1_expert2_mlp1_dp1_mb2_gpu2 -> layer1_expert2_mlp2_dp1_mb2_gpu2
	layer1_expert3_mlp1_dp0_mb2_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb2_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb2_gpu0 -> layer1_expert3_mlp1_dp0_mb2_gpu3
	layer1_router_dp1_mb2_gpu0 -> layer1_expert3_mlp1_dp1_mb2_gpu3
	layer1_expert3_mlp2_dp0_mb2_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb2_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb2_gpu3 -> layer1_expert3_mlp2_dp0_mb2_gpu3
	layer1_expert3_mlp1_dp1_mb2_gpu3 -> layer1_expert3_mlp2_dp1_mb2_gpu3
	layer1_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb2_gpu0 -> layer1_expert_agg_dp0_mb2
	layer1_expert0_mlp2_dp1_mb2_gpu0 -> layer1_expert_agg_dp1_mb2
	layer1_expert1_mlp2_dp0_mb2_gpu1 -> layer1_expert_agg_dp0_mb2
	layer1_expert1_mlp2_dp1_mb2_gpu1 -> layer1_expert_agg_dp1_mb2
	layer1_expert2_mlp2_dp0_mb2_gpu2 -> layer1_expert_agg_dp0_mb2
	layer1_expert2_mlp2_dp1_mb2_gpu2 -> layer1_expert_agg_dp1_mb2
	layer1_expert3_mlp2_dp0_mb2_gpu3 -> layer1_expert_agg_dp0_mb2
	layer1_expert3_mlp2_dp1_mb2_gpu3 -> layer1_expert_agg_dp1_mb2
	layer2_qkv_dp0_mb2_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb2_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb2 -> layer2_qkv_dp0_mb2_gpu0
	layer1_expert_agg_dp1_mb2 -> layer2_qkv_dp1_mb2_gpu0
	layer2_qkv_dp0_mb2_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb2_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb2 -> layer2_qkv_dp0_mb2_gpu1
	layer1_expert_agg_dp1_mb2 -> layer2_qkv_dp1_mb2_gpu1
	layer2_qkv_dp0_mb2_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb2_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb2 -> layer2_qkv_dp0_mb2_gpu2
	layer1_expert_agg_dp1_mb2 -> layer2_qkv_dp1_mb2_gpu2
	layer2_qkv_dp0_mb2_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb2_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb2 -> layer2_qkv_dp0_mb2_gpu3
	layer1_expert_agg_dp1_mb2 -> layer2_qkv_dp1_mb2_gpu3
	layer2_attn_scores_dp0_mb2_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb2_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb2_gpu0 -> layer2_attn_scores_dp0_mb2_gpu0
	layer2_qkv_dp1_mb2_gpu0 -> layer2_attn_scores_dp1_mb2_gpu0
	layer2_softmax_dp0_mb2_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb2_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb2_gpu0 -> layer2_softmax_dp0_mb2_gpu0
	layer2_attn_scores_dp1_mb2_gpu0 -> layer2_softmax_dp1_mb2_gpu0
	layer2_attn_scores_dp0_mb2_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb2_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb2_gpu1 -> layer2_attn_scores_dp0_mb2_gpu1
	layer2_qkv_dp1_mb2_gpu1 -> layer2_attn_scores_dp1_mb2_gpu1
	layer2_softmax_dp0_mb2_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb2_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb2_gpu1 -> layer2_softmax_dp0_mb2_gpu1
	layer2_attn_scores_dp1_mb2_gpu1 -> layer2_softmax_dp1_mb2_gpu1
	layer2_attn_scores_dp0_mb2_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb2_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb2_gpu2 -> layer2_attn_scores_dp0_mb2_gpu2
	layer2_qkv_dp1_mb2_gpu2 -> layer2_attn_scores_dp1_mb2_gpu2
	layer2_softmax_dp0_mb2_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb2_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb2_gpu2 -> layer2_softmax_dp0_mb2_gpu2
	layer2_attn_scores_dp1_mb2_gpu2 -> layer2_softmax_dp1_mb2_gpu2
	layer2_attn_scores_dp0_mb2_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb2_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb2_gpu3 -> layer2_attn_scores_dp0_mb2_gpu3
	layer2_qkv_dp1_mb2_gpu3 -> layer2_attn_scores_dp1_mb2_gpu3
	layer2_softmax_dp0_mb2_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb2_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb2_gpu3 -> layer2_softmax_dp0_mb2_gpu3
	layer2_attn_scores_dp1_mb2_gpu3 -> layer2_softmax_dp1_mb2_gpu3
	layer2_attn_out_dp0_mb2_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb2_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb2_gpu0 -> layer2_attn_out_dp0_mb2_gpu0
	layer2_softmax_dp1_mb2_gpu0 -> layer2_attn_out_dp1_mb2_gpu0
	layer2_attn_out_dp0_mb2_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb2_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb2_gpu1 -> layer2_attn_out_dp0_mb2_gpu1
	layer2_softmax_dp1_mb2_gpu1 -> layer2_attn_out_dp1_mb2_gpu1
	layer2_attn_out_dp0_mb2_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb2_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb2_gpu2 -> layer2_attn_out_dp0_mb2_gpu2
	layer2_softmax_dp1_mb2_gpu2 -> layer2_attn_out_dp1_mb2_gpu2
	layer2_attn_out_dp0_mb2_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb2_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb2_gpu3 -> layer2_attn_out_dp0_mb2_gpu3
	layer2_softmax_dp1_mb2_gpu3 -> layer2_attn_out_dp1_mb2_gpu3
	layer2_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb2_gpu0 -> layer2_attn_allreduce_dp0_mb2
	layer2_attn_out_dp1_mb2_gpu0 -> layer2_attn_allreduce_dp1_mb2
	layer2_attn_out_dp0_mb2_gpu1 -> layer2_attn_allreduce_dp0_mb2
	layer2_attn_out_dp1_mb2_gpu1 -> layer2_attn_allreduce_dp1_mb2
	layer2_attn_out_dp0_mb2_gpu2 -> layer2_attn_allreduce_dp0_mb2
	layer2_attn_out_dp1_mb2_gpu2 -> layer2_attn_allreduce_dp1_mb2
	layer2_attn_out_dp0_mb2_gpu3 -> layer2_attn_allreduce_dp0_mb2
	layer2_attn_out_dp1_mb2_gpu3 -> layer2_attn_allreduce_dp1_mb2
	layer2_router_dp0_mb2_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb2_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb2 -> layer2_router_dp0_mb2_gpu0
	layer2_attn_allreduce_dp1_mb2 -> layer2_router_dp1_mb2_gpu0
	layer2_expert0_mlp1_dp0_mb2_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb2_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb2_gpu0 -> layer2_expert0_mlp1_dp0_mb2_gpu0
	layer2_router_dp1_mb2_gpu0 -> layer2_expert0_mlp1_dp1_mb2_gpu0
	layer2_expert0_mlp2_dp0_mb2_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb2_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb2_gpu0 -> layer2_expert0_mlp2_dp0_mb2_gpu0
	layer2_expert0_mlp1_dp1_mb2_gpu0 -> layer2_expert0_mlp2_dp1_mb2_gpu0
	layer2_expert1_mlp1_dp0_mb2_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb2_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb2_gpu0 -> layer2_expert1_mlp1_dp0_mb2_gpu1
	layer2_router_dp1_mb2_gpu0 -> layer2_expert1_mlp1_dp1_mb2_gpu1
	layer2_expert1_mlp2_dp0_mb2_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb2_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb2_gpu1 -> layer2_expert1_mlp2_dp0_mb2_gpu1
	layer2_expert1_mlp1_dp1_mb2_gpu1 -> layer2_expert1_mlp2_dp1_mb2_gpu1
	layer2_expert2_mlp1_dp0_mb2_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb2_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb2_gpu0 -> layer2_expert2_mlp1_dp0_mb2_gpu2
	layer2_router_dp1_mb2_gpu0 -> layer2_expert2_mlp1_dp1_mb2_gpu2
	layer2_expert2_mlp2_dp0_mb2_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb2_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb2_gpu2 -> layer2_expert2_mlp2_dp0_mb2_gpu2
	layer2_expert2_mlp1_dp1_mb2_gpu2 -> layer2_expert2_mlp2_dp1_mb2_gpu2
	layer2_expert3_mlp1_dp0_mb2_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb2_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb2_gpu0 -> layer2_expert3_mlp1_dp0_mb2_gpu3
	layer2_router_dp1_mb2_gpu0 -> layer2_expert3_mlp1_dp1_mb2_gpu3
	layer2_expert3_mlp2_dp0_mb2_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb2_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb2_gpu3 -> layer2_expert3_mlp2_dp0_mb2_gpu3
	layer2_expert3_mlp1_dp1_mb2_gpu3 -> layer2_expert3_mlp2_dp1_mb2_gpu3
	layer2_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb2_gpu0 -> layer2_expert_agg_dp0_mb2
	layer2_expert0_mlp2_dp1_mb2_gpu0 -> layer2_expert_agg_dp1_mb2
	layer2_expert1_mlp2_dp0_mb2_gpu1 -> layer2_expert_agg_dp0_mb2
	layer2_expert1_mlp2_dp1_mb2_gpu1 -> layer2_expert_agg_dp1_mb2
	layer2_expert2_mlp2_dp0_mb2_gpu2 -> layer2_expert_agg_dp0_mb2
	layer2_expert2_mlp2_dp1_mb2_gpu2 -> layer2_expert_agg_dp1_mb2
	layer2_expert3_mlp2_dp0_mb2_gpu3 -> layer2_expert_agg_dp0_mb2
	layer2_expert3_mlp2_dp1_mb2_gpu3 -> layer2_expert_agg_dp1_mb2
	layer3_qkv_dp0_mb2_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb2_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb2 -> layer3_qkv_dp0_mb2_gpu0
	layer2_expert_agg_dp1_mb2 -> layer3_qkv_dp1_mb2_gpu0
	layer3_qkv_dp0_mb2_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb2_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb2 -> layer3_qkv_dp0_mb2_gpu1
	layer2_expert_agg_dp1_mb2 -> layer3_qkv_dp1_mb2_gpu1
	layer3_qkv_dp0_mb2_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb2_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb2 -> layer3_qkv_dp0_mb2_gpu2
	layer2_expert_agg_dp1_mb2 -> layer3_qkv_dp1_mb2_gpu2
	layer3_qkv_dp0_mb2_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb2_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb2 -> layer3_qkv_dp0_mb2_gpu3
	layer2_expert_agg_dp1_mb2 -> layer3_qkv_dp1_mb2_gpu3
	layer3_attn_scores_dp0_mb2_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb2_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb2_gpu0 -> layer3_attn_scores_dp0_mb2_gpu0
	layer3_qkv_dp1_mb2_gpu0 -> layer3_attn_scores_dp1_mb2_gpu0
	layer3_softmax_dp0_mb2_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb2_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb2_gpu0 -> layer3_softmax_dp0_mb2_gpu0
	layer3_attn_scores_dp1_mb2_gpu0 -> layer3_softmax_dp1_mb2_gpu0
	layer3_attn_scores_dp0_mb2_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb2_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb2_gpu1 -> layer3_attn_scores_dp0_mb2_gpu1
	layer3_qkv_dp1_mb2_gpu1 -> layer3_attn_scores_dp1_mb2_gpu1
	layer3_softmax_dp0_mb2_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb2_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb2_gpu1 -> layer3_softmax_dp0_mb2_gpu1
	layer3_attn_scores_dp1_mb2_gpu1 -> layer3_softmax_dp1_mb2_gpu1
	layer3_attn_scores_dp0_mb2_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb2_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb2_gpu2 -> layer3_attn_scores_dp0_mb2_gpu2
	layer3_qkv_dp1_mb2_gpu2 -> layer3_attn_scores_dp1_mb2_gpu2
	layer3_softmax_dp0_mb2_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb2_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb2_gpu2 -> layer3_softmax_dp0_mb2_gpu2
	layer3_attn_scores_dp1_mb2_gpu2 -> layer3_softmax_dp1_mb2_gpu2
	layer3_attn_scores_dp0_mb2_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb2_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb2_gpu3 -> layer3_attn_scores_dp0_mb2_gpu3
	layer3_qkv_dp1_mb2_gpu3 -> layer3_attn_scores_dp1_mb2_gpu3
	layer3_softmax_dp0_mb2_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb2_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb2_gpu3 -> layer3_softmax_dp0_mb2_gpu3
	layer3_attn_scores_dp1_mb2_gpu3 -> layer3_softmax_dp1_mb2_gpu3
	layer3_attn_out_dp0_mb2_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb2_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb2_gpu0 -> layer3_attn_out_dp0_mb2_gpu0
	layer3_softmax_dp1_mb2_gpu0 -> layer3_attn_out_dp1_mb2_gpu0
	layer3_attn_out_dp0_mb2_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb2_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb2_gpu1 -> layer3_attn_out_dp0_mb2_gpu1
	layer3_softmax_dp1_mb2_gpu1 -> layer3_attn_out_dp1_mb2_gpu1
	layer3_attn_out_dp0_mb2_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb2_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb2_gpu2 -> layer3_attn_out_dp0_mb2_gpu2
	layer3_softmax_dp1_mb2_gpu2 -> layer3_attn_out_dp1_mb2_gpu2
	layer3_attn_out_dp0_mb2_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb2_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb2_gpu3 -> layer3_attn_out_dp0_mb2_gpu3
	layer3_softmax_dp1_mb2_gpu3 -> layer3_attn_out_dp1_mb2_gpu3
	layer3_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb2_gpu0 -> layer3_attn_allreduce_dp0_mb2
	layer3_attn_out_dp1_mb2_gpu0 -> layer3_attn_allreduce_dp1_mb2
	layer3_attn_out_dp0_mb2_gpu1 -> layer3_attn_allreduce_dp0_mb2
	layer3_attn_out_dp1_mb2_gpu1 -> layer3_attn_allreduce_dp1_mb2
	layer3_attn_out_dp0_mb2_gpu2 -> layer3_attn_allreduce_dp0_mb2
	layer3_attn_out_dp1_mb2_gpu2 -> layer3_attn_allreduce_dp1_mb2
	layer3_attn_out_dp0_mb2_gpu3 -> layer3_attn_allreduce_dp0_mb2
	layer3_attn_out_dp1_mb2_gpu3 -> layer3_attn_allreduce_dp1_mb2
	layer3_router_dp0_mb2_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb2_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb2 -> layer3_router_dp0_mb2_gpu0
	layer3_attn_allreduce_dp1_mb2 -> layer3_router_dp1_mb2_gpu0
	layer3_expert0_mlp1_dp0_mb2_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb2_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb2_gpu0 -> layer3_expert0_mlp1_dp0_mb2_gpu0
	layer3_router_dp1_mb2_gpu0 -> layer3_expert0_mlp1_dp1_mb2_gpu0
	layer3_expert0_mlp2_dp0_mb2_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb2_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb2_gpu0 -> layer3_expert0_mlp2_dp0_mb2_gpu0
	layer3_expert0_mlp1_dp1_mb2_gpu0 -> layer3_expert0_mlp2_dp1_mb2_gpu0
	layer3_expert1_mlp1_dp0_mb2_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb2_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb2_gpu0 -> layer3_expert1_mlp1_dp0_mb2_gpu1
	layer3_router_dp1_mb2_gpu0 -> layer3_expert1_mlp1_dp1_mb2_gpu1
	layer3_expert1_mlp2_dp0_mb2_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb2_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb2_gpu1 -> layer3_expert1_mlp2_dp0_mb2_gpu1
	layer3_expert1_mlp1_dp1_mb2_gpu1 -> layer3_expert1_mlp2_dp1_mb2_gpu1
	layer3_expert2_mlp1_dp0_mb2_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb2_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb2_gpu0 -> layer3_expert2_mlp1_dp0_mb2_gpu2
	layer3_router_dp1_mb2_gpu0 -> layer3_expert2_mlp1_dp1_mb2_gpu2
	layer3_expert2_mlp2_dp0_mb2_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb2_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb2_gpu2 -> layer3_expert2_mlp2_dp0_mb2_gpu2
	layer3_expert2_mlp1_dp1_mb2_gpu2 -> layer3_expert2_mlp2_dp1_mb2_gpu2
	layer3_expert3_mlp1_dp0_mb2_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb2_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb2_gpu0 -> layer3_expert3_mlp1_dp0_mb2_gpu3
	layer3_router_dp1_mb2_gpu0 -> layer3_expert3_mlp1_dp1_mb2_gpu3
	layer3_expert3_mlp2_dp0_mb2_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb2_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb2_gpu3 -> layer3_expert3_mlp2_dp0_mb2_gpu3
	layer3_expert3_mlp1_dp1_mb2_gpu3 -> layer3_expert3_mlp2_dp1_mb2_gpu3
	layer3_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb2_gpu0 -> layer3_expert_agg_dp0_mb2
	layer3_expert0_mlp2_dp1_mb2_gpu0 -> layer3_expert_agg_dp1_mb2
	layer3_expert1_mlp2_dp0_mb2_gpu1 -> layer3_expert_agg_dp0_mb2
	layer3_expert1_mlp2_dp1_mb2_gpu1 -> layer3_expert_agg_dp1_mb2
	layer3_expert2_mlp2_dp0_mb2_gpu2 -> layer3_expert_agg_dp0_mb2
	layer3_expert2_mlp2_dp1_mb2_gpu2 -> layer3_expert_agg_dp1_mb2
	layer3_expert3_mlp2_dp0_mb2_gpu3 -> layer3_expert_agg_dp0_mb2
	layer3_expert3_mlp2_dp1_mb2_gpu3 -> layer3_expert_agg_dp1_mb2
	layer0_qkv_dp0_mb3_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb3_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb3_gpu0
	split_mb1 -> layer0_qkv_dp1_mb3_gpu0
	layer0_qkv_dp0_mb3_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb3_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb3_gpu1
	split_mb1 -> layer0_qkv_dp1_mb3_gpu1
	layer0_qkv_dp0_mb3_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb3_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb3_gpu2
	split_mb1 -> layer0_qkv_dp1_mb3_gpu2
	layer0_qkv_dp0_mb3_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb3_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb3_gpu3
	split_mb1 -> layer0_qkv_dp1_mb3_gpu3
	layer0_attn_scores_dp0_mb3_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb3_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb3_gpu0 -> layer0_attn_scores_dp0_mb3_gpu0
	layer0_qkv_dp1_mb3_gpu0 -> layer0_attn_scores_dp1_mb3_gpu0
	layer0_softmax_dp0_mb3_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb3_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb3_gpu0 -> layer0_softmax_dp0_mb3_gpu0
	layer0_attn_scores_dp1_mb3_gpu0 -> layer0_softmax_dp1_mb3_gpu0
	layer0_attn_scores_dp0_mb3_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb3_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb3_gpu1 -> layer0_attn_scores_dp0_mb3_gpu1
	layer0_qkv_dp1_mb3_gpu1 -> layer0_attn_scores_dp1_mb3_gpu1
	layer0_softmax_dp0_mb3_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb3_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb3_gpu1 -> layer0_softmax_dp0_mb3_gpu1
	layer0_attn_scores_dp1_mb3_gpu1 -> layer0_softmax_dp1_mb3_gpu1
	layer0_attn_scores_dp0_mb3_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb3_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb3_gpu2 -> layer0_attn_scores_dp0_mb3_gpu2
	layer0_qkv_dp1_mb3_gpu2 -> layer0_attn_scores_dp1_mb3_gpu2
	layer0_softmax_dp0_mb3_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb3_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb3_gpu2 -> layer0_softmax_dp0_mb3_gpu2
	layer0_attn_scores_dp1_mb3_gpu2 -> layer0_softmax_dp1_mb3_gpu2
	layer0_attn_scores_dp0_mb3_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb3_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb3_gpu3 -> layer0_attn_scores_dp0_mb3_gpu3
	layer0_qkv_dp1_mb3_gpu3 -> layer0_attn_scores_dp1_mb3_gpu3
	layer0_softmax_dp0_mb3_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb3_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb3_gpu3 -> layer0_softmax_dp0_mb3_gpu3
	layer0_attn_scores_dp1_mb3_gpu3 -> layer0_softmax_dp1_mb3_gpu3
	layer0_attn_out_dp0_mb3_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb3_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb3_gpu0 -> layer0_attn_out_dp0_mb3_gpu0
	layer0_softmax_dp1_mb3_gpu0 -> layer0_attn_out_dp1_mb3_gpu0
	layer0_attn_out_dp0_mb3_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb3_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb3_gpu1 -> layer0_attn_out_dp0_mb3_gpu1
	layer0_softmax_dp1_mb3_gpu1 -> layer0_attn_out_dp1_mb3_gpu1
	layer0_attn_out_dp0_mb3_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb3_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb3_gpu2 -> layer0_attn_out_dp0_mb3_gpu2
	layer0_softmax_dp1_mb3_gpu2 -> layer0_attn_out_dp1_mb3_gpu2
	layer0_attn_out_dp0_mb3_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb3_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb3_gpu3 -> layer0_attn_out_dp0_mb3_gpu3
	layer0_softmax_dp1_mb3_gpu3 -> layer0_attn_out_dp1_mb3_gpu3
	layer0_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb3_gpu0 -> layer0_attn_allreduce_dp0_mb3
	layer0_attn_out_dp1_mb3_gpu0 -> layer0_attn_allreduce_dp1_mb3
	layer0_attn_out_dp0_mb3_gpu1 -> layer0_attn_allreduce_dp0_mb3
	layer0_attn_out_dp1_mb3_gpu1 -> layer0_attn_allreduce_dp1_mb3
	layer0_attn_out_dp0_mb3_gpu2 -> layer0_attn_allreduce_dp0_mb3
	layer0_attn_out_dp1_mb3_gpu2 -> layer0_attn_allreduce_dp1_mb3
	layer0_attn_out_dp0_mb3_gpu3 -> layer0_attn_allreduce_dp0_mb3
	layer0_attn_out_dp1_mb3_gpu3 -> layer0_attn_allreduce_dp1_mb3
	layer0_router_dp0_mb3_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb3_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb3 -> layer0_router_dp0_mb3_gpu0
	layer0_attn_allreduce_dp1_mb3 -> layer0_router_dp1_mb3_gpu0
	layer0_expert0_mlp1_dp0_mb3_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb3_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb3_gpu0 -> layer0_expert0_mlp1_dp0_mb3_gpu0
	layer0_router_dp1_mb3_gpu0 -> layer0_expert0_mlp1_dp1_mb3_gpu0
	layer0_expert0_mlp2_dp0_mb3_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb3_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb3_gpu0 -> layer0_expert0_mlp2_dp0_mb3_gpu0
	layer0_expert0_mlp1_dp1_mb3_gpu0 -> layer0_expert0_mlp2_dp1_mb3_gpu0
	layer0_expert1_mlp1_dp0_mb3_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb3_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb3_gpu0 -> layer0_expert1_mlp1_dp0_mb3_gpu1
	layer0_router_dp1_mb3_gpu0 -> layer0_expert1_mlp1_dp1_mb3_gpu1
	layer0_expert1_mlp2_dp0_mb3_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb3_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb3_gpu1 -> layer0_expert1_mlp2_dp0_mb3_gpu1
	layer0_expert1_mlp1_dp1_mb3_gpu1 -> layer0_expert1_mlp2_dp1_mb3_gpu1
	layer0_expert2_mlp1_dp0_mb3_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb3_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb3_gpu0 -> layer0_expert2_mlp1_dp0_mb3_gpu2
	layer0_router_dp1_mb3_gpu0 -> layer0_expert2_mlp1_dp1_mb3_gpu2
	layer0_expert2_mlp2_dp0_mb3_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb3_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb3_gpu2 -> layer0_expert2_mlp2_dp0_mb3_gpu2
	layer0_expert2_mlp1_dp1_mb3_gpu2 -> layer0_expert2_mlp2_dp1_mb3_gpu2
	layer0_expert3_mlp1_dp0_mb3_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb3_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb3_gpu0 -> layer0_expert3_mlp1_dp0_mb3_gpu3
	layer0_router_dp1_mb3_gpu0 -> layer0_expert3_mlp1_dp1_mb3_gpu3
	layer0_expert3_mlp2_dp0_mb3_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb3_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb3_gpu3 -> layer0_expert3_mlp2_dp0_mb3_gpu3
	layer0_expert3_mlp1_dp1_mb3_gpu3 -> layer0_expert3_mlp2_dp1_mb3_gpu3
	layer0_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb3_gpu0 -> layer0_expert_agg_dp0_mb3
	layer0_expert0_mlp2_dp1_mb3_gpu0 -> layer0_expert_agg_dp1_mb3
	layer0_expert1_mlp2_dp0_mb3_gpu1 -> layer0_expert_agg_dp0_mb3
	layer0_expert1_mlp2_dp1_mb3_gpu1 -> layer0_expert_agg_dp1_mb3
	layer0_expert2_mlp2_dp0_mb3_gpu2 -> layer0_expert_agg_dp0_mb3
	layer0_expert2_mlp2_dp1_mb3_gpu2 -> layer0_expert_agg_dp1_mb3
	layer0_expert3_mlp2_dp0_mb3_gpu3 -> layer0_expert_agg_dp0_mb3
	layer0_expert3_mlp2_dp1_mb3_gpu3 -> layer0_expert_agg_dp1_mb3
	layer1_qkv_dp0_mb3_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb3_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb3 -> layer1_qkv_dp0_mb3_gpu0
	layer0_expert_agg_dp1_mb3 -> layer1_qkv_dp1_mb3_gpu0
	layer1_qkv_dp0_mb3_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb3_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb3 -> layer1_qkv_dp0_mb3_gpu1
	layer0_expert_agg_dp1_mb3 -> layer1_qkv_dp1_mb3_gpu1
	layer1_qkv_dp0_mb3_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb3_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb3 -> layer1_qkv_dp0_mb3_gpu2
	layer0_expert_agg_dp1_mb3 -> layer1_qkv_dp1_mb3_gpu2
	layer1_qkv_dp0_mb3_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb3_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb3 -> layer1_qkv_dp0_mb3_gpu3
	layer0_expert_agg_dp1_mb3 -> layer1_qkv_dp1_mb3_gpu3
	layer1_attn_scores_dp0_mb3_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb3_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb3_gpu0 -> layer1_attn_scores_dp0_mb3_gpu0
	layer1_qkv_dp1_mb3_gpu0 -> layer1_attn_scores_dp1_mb3_gpu0
	layer1_softmax_dp0_mb3_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb3_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb3_gpu0 -> layer1_softmax_dp0_mb3_gpu0
	layer1_attn_scores_dp1_mb3_gpu0 -> layer1_softmax_dp1_mb3_gpu0
	layer1_attn_scores_dp0_mb3_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb3_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb3_gpu1 -> layer1_attn_scores_dp0_mb3_gpu1
	layer1_qkv_dp1_mb3_gpu1 -> layer1_attn_scores_dp1_mb3_gpu1
	layer1_softmax_dp0_mb3_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb3_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb3_gpu1 -> layer1_softmax_dp0_mb3_gpu1
	layer1_attn_scores_dp1_mb3_gpu1 -> layer1_softmax_dp1_mb3_gpu1
	layer1_attn_scores_dp0_mb3_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb3_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb3_gpu2 -> layer1_attn_scores_dp0_mb3_gpu2
	layer1_qkv_dp1_mb3_gpu2 -> layer1_attn_scores_dp1_mb3_gpu2
	layer1_softmax_dp0_mb3_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb3_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb3_gpu2 -> layer1_softmax_dp0_mb3_gpu2
	layer1_attn_scores_dp1_mb3_gpu2 -> layer1_softmax_dp1_mb3_gpu2
	layer1_attn_scores_dp0_mb3_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb3_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb3_gpu3 -> layer1_attn_scores_dp0_mb3_gpu3
	layer1_qkv_dp1_mb3_gpu3 -> layer1_attn_scores_dp1_mb3_gpu3
	layer1_softmax_dp0_mb3_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb3_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb3_gpu3 -> layer1_softmax_dp0_mb3_gpu3
	layer1_attn_scores_dp1_mb3_gpu3 -> layer1_softmax_dp1_mb3_gpu3
	layer1_attn_out_dp0_mb3_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb3_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb3_gpu0 -> layer1_attn_out_dp0_mb3_gpu0
	layer1_softmax_dp1_mb3_gpu0 -> layer1_attn_out_dp1_mb3_gpu0
	layer1_attn_out_dp0_mb3_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb3_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb3_gpu1 -> layer1_attn_out_dp0_mb3_gpu1
	layer1_softmax_dp1_mb3_gpu1 -> layer1_attn_out_dp1_mb3_gpu1
	layer1_attn_out_dp0_mb3_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb3_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb3_gpu2 -> layer1_attn_out_dp0_mb3_gpu2
	layer1_softmax_dp1_mb3_gpu2 -> layer1_attn_out_dp1_mb3_gpu2
	layer1_attn_out_dp0_mb3_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb3_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb3_gpu3 -> layer1_attn_out_dp0_mb3_gpu3
	layer1_softmax_dp1_mb3_gpu3 -> layer1_attn_out_dp1_mb3_gpu3
	layer1_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb3_gpu0 -> layer1_attn_allreduce_dp0_mb3
	layer1_attn_out_dp1_mb3_gpu0 -> layer1_attn_allreduce_dp1_mb3
	layer1_attn_out_dp0_mb3_gpu1 -> layer1_attn_allreduce_dp0_mb3
	layer1_attn_out_dp1_mb3_gpu1 -> layer1_attn_allreduce_dp1_mb3
	layer1_attn_out_dp0_mb3_gpu2 -> layer1_attn_allreduce_dp0_mb3
	layer1_attn_out_dp1_mb3_gpu2 -> layer1_attn_allreduce_dp1_mb3
	layer1_attn_out_dp0_mb3_gpu3 -> layer1_attn_allreduce_dp0_mb3
	layer1_attn_out_dp1_mb3_gpu3 -> layer1_attn_allreduce_dp1_mb3
	layer1_router_dp0_mb3_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb3_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb3 -> layer1_router_dp0_mb3_gpu0
	layer1_attn_allreduce_dp1_mb3 -> layer1_router_dp1_mb3_gpu0
	layer1_expert0_mlp1_dp0_mb3_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb3_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb3_gpu0 -> layer1_expert0_mlp1_dp0_mb3_gpu0
	layer1_router_dp1_mb3_gpu0 -> layer1_expert0_mlp1_dp1_mb3_gpu0
	layer1_expert0_mlp2_dp0_mb3_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb3_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb3_gpu0 -> layer1_expert0_mlp2_dp0_mb3_gpu0
	layer1_expert0_mlp1_dp1_mb3_gpu0 -> layer1_expert0_mlp2_dp1_mb3_gpu0
	layer1_expert1_mlp1_dp0_mb3_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb3_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb3_gpu0 -> layer1_expert1_mlp1_dp0_mb3_gpu1
	layer1_router_dp1_mb3_gpu0 -> layer1_expert1_mlp1_dp1_mb3_gpu1
	layer1_expert1_mlp2_dp0_mb3_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb3_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb3_gpu1 -> layer1_expert1_mlp2_dp0_mb3_gpu1
	layer1_expert1_mlp1_dp1_mb3_gpu1 -> layer1_expert1_mlp2_dp1_mb3_gpu1
	layer1_expert2_mlp1_dp0_mb3_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb3_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb3_gpu0 -> layer1_expert2_mlp1_dp0_mb3_gpu2
	layer1_router_dp1_mb3_gpu0 -> layer1_expert2_mlp1_dp1_mb3_gpu2
	layer1_expert2_mlp2_dp0_mb3_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb3_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb3_gpu2 -> layer1_expert2_mlp2_dp0_mb3_gpu2
	layer1_expert2_mlp1_dp1_mb3_gpu2 -> layer1_expert2_mlp2_dp1_mb3_gpu2
	layer1_expert3_mlp1_dp0_mb3_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb3_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb3_gpu0 -> layer1_expert3_mlp1_dp0_mb3_gpu3
	layer1_router_dp1_mb3_gpu0 -> layer1_expert3_mlp1_dp1_mb3_gpu3
	layer1_expert3_mlp2_dp0_mb3_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb3_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb3_gpu3 -> layer1_expert3_mlp2_dp0_mb3_gpu3
	layer1_expert3_mlp1_dp1_mb3_gpu3 -> layer1_expert3_mlp2_dp1_mb3_gpu3
	layer1_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb3_gpu0 -> layer1_expert_agg_dp0_mb3
	layer1_expert0_mlp2_dp1_mb3_gpu0 -> layer1_expert_agg_dp1_mb3
	layer1_expert1_mlp2_dp0_mb3_gpu1 -> layer1_expert_agg_dp0_mb3
	layer1_expert1_mlp2_dp1_mb3_gpu1 -> layer1_expert_agg_dp1_mb3
	layer1_expert2_mlp2_dp0_mb3_gpu2 -> layer1_expert_agg_dp0_mb3
	layer1_expert2_mlp2_dp1_mb3_gpu2 -> layer1_expert_agg_dp1_mb3
	layer1_expert3_mlp2_dp0_mb3_gpu3 -> layer1_expert_agg_dp0_mb3
	layer1_expert3_mlp2_dp1_mb3_gpu3 -> layer1_expert_agg_dp1_mb3
	layer2_qkv_dp0_mb3_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb3_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb3 -> layer2_qkv_dp0_mb3_gpu0
	layer1_expert_agg_dp1_mb3 -> layer2_qkv_dp1_mb3_gpu0
	layer2_qkv_dp0_mb3_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb3_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb3 -> layer2_qkv_dp0_mb3_gpu1
	layer1_expert_agg_dp1_mb3 -> layer2_qkv_dp1_mb3_gpu1
	layer2_qkv_dp0_mb3_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb3_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb3 -> layer2_qkv_dp0_mb3_gpu2
	layer1_expert_agg_dp1_mb3 -> layer2_qkv_dp1_mb3_gpu2
	layer2_qkv_dp0_mb3_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb3_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb3 -> layer2_qkv_dp0_mb3_gpu3
	layer1_expert_agg_dp1_mb3 -> layer2_qkv_dp1_mb3_gpu3
	layer2_attn_scores_dp0_mb3_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb3_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb3_gpu0 -> layer2_attn_scores_dp0_mb3_gpu0
	layer2_qkv_dp1_mb3_gpu0 -> layer2_attn_scores_dp1_mb3_gpu0
	layer2_softmax_dp0_mb3_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb3_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb3_gpu0 -> layer2_softmax_dp0_mb3_gpu0
	layer2_attn_scores_dp1_mb3_gpu0 -> layer2_softmax_dp1_mb3_gpu0
	layer2_attn_scores_dp0_mb3_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb3_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb3_gpu1 -> layer2_attn_scores_dp0_mb3_gpu1
	layer2_qkv_dp1_mb3_gpu1 -> layer2_attn_scores_dp1_mb3_gpu1
	layer2_softmax_dp0_mb3_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb3_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb3_gpu1 -> layer2_softmax_dp0_mb3_gpu1
	layer2_attn_scores_dp1_mb3_gpu1 -> layer2_softmax_dp1_mb3_gpu1
	layer2_attn_scores_dp0_mb3_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb3_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb3_gpu2 -> layer2_attn_scores_dp0_mb3_gpu2
	layer2_qkv_dp1_mb3_gpu2 -> layer2_attn_scores_dp1_mb3_gpu2
	layer2_softmax_dp0_mb3_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb3_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb3_gpu2 -> layer2_softmax_dp0_mb3_gpu2
	layer2_attn_scores_dp1_mb3_gpu2 -> layer2_softmax_dp1_mb3_gpu2
	layer2_attn_scores_dp0_mb3_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb3_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb3_gpu3 -> layer2_attn_scores_dp0_mb3_gpu3
	layer2_qkv_dp1_mb3_gpu3 -> layer2_attn_scores_dp1_mb3_gpu3
	layer2_softmax_dp0_mb3_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb3_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb3_gpu3 -> layer2_softmax_dp0_mb3_gpu3
	layer2_attn_scores_dp1_mb3_gpu3 -> layer2_softmax_dp1_mb3_gpu3
	layer2_attn_out_dp0_mb3_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb3_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb3_gpu0 -> layer2_attn_out_dp0_mb3_gpu0
	layer2_softmax_dp1_mb3_gpu0 -> layer2_attn_out_dp1_mb3_gpu0
	layer2_attn_out_dp0_mb3_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb3_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb3_gpu1 -> layer2_attn_out_dp0_mb3_gpu1
	layer2_softmax_dp1_mb3_gpu1 -> layer2_attn_out_dp1_mb3_gpu1
	layer2_attn_out_dp0_mb3_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb3_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb3_gpu2 -> layer2_attn_out_dp0_mb3_gpu2
	layer2_softmax_dp1_mb3_gpu2 -> layer2_attn_out_dp1_mb3_gpu2
	layer2_attn_out_dp0_mb3_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb3_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb3_gpu3 -> layer2_attn_out_dp0_mb3_gpu3
	layer2_softmax_dp1_mb3_gpu3 -> layer2_attn_out_dp1_mb3_gpu3
	layer2_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb3_gpu0 -> layer2_attn_allreduce_dp0_mb3
	layer2_attn_out_dp1_mb3_gpu0 -> layer2_attn_allreduce_dp1_mb3
	layer2_attn_out_dp0_mb3_gpu1 -> layer2_attn_allreduce_dp0_mb3
	layer2_attn_out_dp1_mb3_gpu1 -> layer2_attn_allreduce_dp1_mb3
	layer2_attn_out_dp0_mb3_gpu2 -> layer2_attn_allreduce_dp0_mb3
	layer2_attn_out_dp1_mb3_gpu2 -> layer2_attn_allreduce_dp1_mb3
	layer2_attn_out_dp0_mb3_gpu3 -> layer2_attn_allreduce_dp0_mb3
	layer2_attn_out_dp1_mb3_gpu3 -> layer2_attn_allreduce_dp1_mb3
	layer2_router_dp0_mb3_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb3_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb3 -> layer2_router_dp0_mb3_gpu0
	layer2_attn_allreduce_dp1_mb3 -> layer2_router_dp1_mb3_gpu0
	layer2_expert0_mlp1_dp0_mb3_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb3_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb3_gpu0 -> layer2_expert0_mlp1_dp0_mb3_gpu0
	layer2_router_dp1_mb3_gpu0 -> layer2_expert0_mlp1_dp1_mb3_gpu0
	layer2_expert0_mlp2_dp0_mb3_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb3_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb3_gpu0 -> layer2_expert0_mlp2_dp0_mb3_gpu0
	layer2_expert0_mlp1_dp1_mb3_gpu0 -> layer2_expert0_mlp2_dp1_mb3_gpu0
	layer2_expert1_mlp1_dp0_mb3_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb3_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb3_gpu0 -> layer2_expert1_mlp1_dp0_mb3_gpu1
	layer2_router_dp1_mb3_gpu0 -> layer2_expert1_mlp1_dp1_mb3_gpu1
	layer2_expert1_mlp2_dp0_mb3_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb3_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb3_gpu1 -> layer2_expert1_mlp2_dp0_mb3_gpu1
	layer2_expert1_mlp1_dp1_mb3_gpu1 -> layer2_expert1_mlp2_dp1_mb3_gpu1
	layer2_expert2_mlp1_dp0_mb3_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb3_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb3_gpu0 -> layer2_expert2_mlp1_dp0_mb3_gpu2
	layer2_router_dp1_mb3_gpu0 -> layer2_expert2_mlp1_dp1_mb3_gpu2
	layer2_expert2_mlp2_dp0_mb3_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb3_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb3_gpu2 -> layer2_expert2_mlp2_dp0_mb3_gpu2
	layer2_expert2_mlp1_dp1_mb3_gpu2 -> layer2_expert2_mlp2_dp1_mb3_gpu2
	layer2_expert3_mlp1_dp0_mb3_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb3_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb3_gpu0 -> layer2_expert3_mlp1_dp0_mb3_gpu3
	layer2_router_dp1_mb3_gpu0 -> layer2_expert3_mlp1_dp1_mb3_gpu3
	layer2_expert3_mlp2_dp0_mb3_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb3_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb3_gpu3 -> layer2_expert3_mlp2_dp0_mb3_gpu3
	layer2_expert3_mlp1_dp1_mb3_gpu3 -> layer2_expert3_mlp2_dp1_mb3_gpu3
	layer2_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb3_gpu0 -> layer2_expert_agg_dp0_mb3
	layer2_expert0_mlp2_dp1_mb3_gpu0 -> layer2_expert_agg_dp1_mb3
	layer2_expert1_mlp2_dp0_mb3_gpu1 -> layer2_expert_agg_dp0_mb3
	layer2_expert1_mlp2_dp1_mb3_gpu1 -> layer2_expert_agg_dp1_mb3
	layer2_expert2_mlp2_dp0_mb3_gpu2 -> layer2_expert_agg_dp0_mb3
	layer2_expert2_mlp2_dp1_mb3_gpu2 -> layer2_expert_agg_dp1_mb3
	layer2_expert3_mlp2_dp0_mb3_gpu3 -> layer2_expert_agg_dp0_mb3
	layer2_expert3_mlp2_dp1_mb3_gpu3 -> layer2_expert_agg_dp1_mb3
	layer3_qkv_dp0_mb3_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb3_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb3 -> layer3_qkv_dp0_mb3_gpu0
	layer2_expert_agg_dp1_mb3 -> layer3_qkv_dp1_mb3_gpu0
	layer3_qkv_dp0_mb3_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb3_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb3 -> layer3_qkv_dp0_mb3_gpu1
	layer2_expert_agg_dp1_mb3 -> layer3_qkv_dp1_mb3_gpu1
	layer3_qkv_dp0_mb3_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb3_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb3 -> layer3_qkv_dp0_mb3_gpu2
	layer2_expert_agg_dp1_mb3 -> layer3_qkv_dp1_mb3_gpu2
	layer3_qkv_dp0_mb3_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb3_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb3 -> layer3_qkv_dp0_mb3_gpu3
	layer2_expert_agg_dp1_mb3 -> layer3_qkv_dp1_mb3_gpu3
	layer3_attn_scores_dp0_mb3_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb3_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb3_gpu0 -> layer3_attn_scores_dp0_mb3_gpu0
	layer3_qkv_dp1_mb3_gpu0 -> layer3_attn_scores_dp1_mb3_gpu0
	layer3_softmax_dp0_mb3_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb3_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb3_gpu0 -> layer3_softmax_dp0_mb3_gpu0
	layer3_attn_scores_dp1_mb3_gpu0 -> layer3_softmax_dp1_mb3_gpu0
	layer3_attn_scores_dp0_mb3_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb3_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb3_gpu1 -> layer3_attn_scores_dp0_mb3_gpu1
	layer3_qkv_dp1_mb3_gpu1 -> layer3_attn_scores_dp1_mb3_gpu1
	layer3_softmax_dp0_mb3_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb3_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb3_gpu1 -> layer3_softmax_dp0_mb3_gpu1
	layer3_attn_scores_dp1_mb3_gpu1 -> layer3_softmax_dp1_mb3_gpu1
	layer3_attn_scores_dp0_mb3_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb3_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb3_gpu2 -> layer3_attn_scores_dp0_mb3_gpu2
	layer3_qkv_dp1_mb3_gpu2 -> layer3_attn_scores_dp1_mb3_gpu2
	layer3_softmax_dp0_mb3_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb3_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb3_gpu2 -> layer3_softmax_dp0_mb3_gpu2
	layer3_attn_scores_dp1_mb3_gpu2 -> layer3_softmax_dp1_mb3_gpu2
	layer3_attn_scores_dp0_mb3_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb3_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb3_gpu3 -> layer3_attn_scores_dp0_mb3_gpu3
	layer3_qkv_dp1_mb3_gpu3 -> layer3_attn_scores_dp1_mb3_gpu3
	layer3_softmax_dp0_mb3_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb3_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb3_gpu3 -> layer3_softmax_dp0_mb3_gpu3
	layer3_attn_scores_dp1_mb3_gpu3 -> layer3_softmax_dp1_mb3_gpu3
	layer3_attn_out_dp0_mb3_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb3_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb3_gpu0 -> layer3_attn_out_dp0_mb3_gpu0
	layer3_softmax_dp1_mb3_gpu0 -> layer3_attn_out_dp1_mb3_gpu0
	layer3_attn_out_dp0_mb3_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb3_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb3_gpu1 -> layer3_attn_out_dp0_mb3_gpu1
	layer3_softmax_dp1_mb3_gpu1 -> layer3_attn_out_dp1_mb3_gpu1
	layer3_attn_out_dp0_mb3_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb3_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb3_gpu2 -> layer3_attn_out_dp0_mb3_gpu2
	layer3_softmax_dp1_mb3_gpu2 -> layer3_attn_out_dp1_mb3_gpu2
	layer3_attn_out_dp0_mb3_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb3_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb3_gpu3 -> layer3_attn_out_dp0_mb3_gpu3
	layer3_softmax_dp1_mb3_gpu3 -> layer3_attn_out_dp1_mb3_gpu3
	layer3_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb3_gpu0 -> layer3_attn_allreduce_dp0_mb3
	layer3_attn_out_dp1_mb3_gpu0 -> layer3_attn_allreduce_dp1_mb3
	layer3_attn_out_dp0_mb3_gpu1 -> layer3_attn_allreduce_dp0_mb3
	layer3_attn_out_dp1_mb3_gpu1 -> layer3_attn_allreduce_dp1_mb3
	layer3_attn_out_dp0_mb3_gpu2 -> layer3_attn_allreduce_dp0_mb3
	layer3_attn_out_dp1_mb3_gpu2 -> layer3_attn_allreduce_dp1_mb3
	layer3_attn_out_dp0_mb3_gpu3 -> layer3_attn_allreduce_dp0_mb3
	layer3_attn_out_dp1_mb3_gpu3 -> layer3_attn_allreduce_dp1_mb3
	layer3_router_dp0_mb3_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb3_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb3 -> layer3_router_dp0_mb3_gpu0
	layer3_attn_allreduce_dp1_mb3 -> layer3_router_dp1_mb3_gpu0
	layer3_expert0_mlp1_dp0_mb3_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb3_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb3_gpu0 -> layer3_expert0_mlp1_dp0_mb3_gpu0
	layer3_router_dp1_mb3_gpu0 -> layer3_expert0_mlp1_dp1_mb3_gpu0
	layer3_expert0_mlp2_dp0_mb3_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb3_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb3_gpu0 -> layer3_expert0_mlp2_dp0_mb3_gpu0
	layer3_expert0_mlp1_dp1_mb3_gpu0 -> layer3_expert0_mlp2_dp1_mb3_gpu0
	layer3_expert1_mlp1_dp0_mb3_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb3_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb3_gpu0 -> layer3_expert1_mlp1_dp0_mb3_gpu1
	layer3_router_dp1_mb3_gpu0 -> layer3_expert1_mlp1_dp1_mb3_gpu1
	layer3_expert1_mlp2_dp0_mb3_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb3_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb3_gpu1 -> layer3_expert1_mlp2_dp0_mb3_gpu1
	layer3_expert1_mlp1_dp1_mb3_gpu1 -> layer3_expert1_mlp2_dp1_mb3_gpu1
	layer3_expert2_mlp1_dp0_mb3_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb3_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb3_gpu0 -> layer3_expert2_mlp1_dp0_mb3_gpu2
	layer3_router_dp1_mb3_gpu0 -> layer3_expert2_mlp1_dp1_mb3_gpu2
	layer3_expert2_mlp2_dp0_mb3_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb3_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb3_gpu2 -> layer3_expert2_mlp2_dp0_mb3_gpu2
	layer3_expert2_mlp1_dp1_mb3_gpu2 -> layer3_expert2_mlp2_dp1_mb3_gpu2
	layer3_expert3_mlp1_dp0_mb3_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb3_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb3_gpu0 -> layer3_expert3_mlp1_dp0_mb3_gpu3
	layer3_router_dp1_mb3_gpu0 -> layer3_expert3_mlp1_dp1_mb3_gpu3
	layer3_expert3_mlp2_dp0_mb3_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb3_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb3_gpu3 -> layer3_expert3_mlp2_dp0_mb3_gpu3
	layer3_expert3_mlp1_dp1_mb3_gpu3 -> layer3_expert3_mlp2_dp1_mb3_gpu3
	layer3_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb3_gpu0 -> layer3_expert_agg_dp0_mb3
	layer3_expert0_mlp2_dp1_mb3_gpu0 -> layer3_expert_agg_dp1_mb3
	layer3_expert1_mlp2_dp0_mb3_gpu1 -> layer3_expert_agg_dp0_mb3
	layer3_expert1_mlp2_dp1_mb3_gpu1 -> layer3_expert_agg_dp1_mb3
	layer3_expert2_mlp2_dp0_mb3_gpu2 -> layer3_expert_agg_dp0_mb3
	layer3_expert2_mlp2_dp1_mb3_gpu2 -> layer3_expert_agg_dp1_mb3
	layer3_expert3_mlp2_dp0_mb3_gpu3 -> layer3_expert_agg_dp0_mb3
	layer3_expert3_mlp2_dp1_mb3_gpu3 -> layer3_expert_agg_dp1_mb3
	layer0_qkv_dp0_mb4_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb4_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb4_gpu0
	split_mb1 -> layer0_qkv_dp1_mb4_gpu0
	layer0_qkv_dp0_mb4_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb4_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb4_gpu1
	split_mb1 -> layer0_qkv_dp1_mb4_gpu1
	layer0_qkv_dp0_mb4_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb4_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb4_gpu2
	split_mb1 -> layer0_qkv_dp1_mb4_gpu2
	layer0_qkv_dp0_mb4_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb4_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb4_gpu3
	split_mb1 -> layer0_qkv_dp1_mb4_gpu3
	layer0_attn_scores_dp0_mb4_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb4_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb4_gpu0 -> layer0_attn_scores_dp0_mb4_gpu0
	layer0_qkv_dp1_mb4_gpu0 -> layer0_attn_scores_dp1_mb4_gpu0
	layer0_softmax_dp0_mb4_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb4_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb4_gpu0 -> layer0_softmax_dp0_mb4_gpu0
	layer0_attn_scores_dp1_mb4_gpu0 -> layer0_softmax_dp1_mb4_gpu0
	layer0_attn_scores_dp0_mb4_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb4_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb4_gpu1 -> layer0_attn_scores_dp0_mb4_gpu1
	layer0_qkv_dp1_mb4_gpu1 -> layer0_attn_scores_dp1_mb4_gpu1
	layer0_softmax_dp0_mb4_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb4_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb4_gpu1 -> layer0_softmax_dp0_mb4_gpu1
	layer0_attn_scores_dp1_mb4_gpu1 -> layer0_softmax_dp1_mb4_gpu1
	layer0_attn_scores_dp0_mb4_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb4_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb4_gpu2 -> layer0_attn_scores_dp0_mb4_gpu2
	layer0_qkv_dp1_mb4_gpu2 -> layer0_attn_scores_dp1_mb4_gpu2
	layer0_softmax_dp0_mb4_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb4_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb4_gpu2 -> layer0_softmax_dp0_mb4_gpu2
	layer0_attn_scores_dp1_mb4_gpu2 -> layer0_softmax_dp1_mb4_gpu2
	layer0_attn_scores_dp0_mb4_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb4_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb4_gpu3 -> layer0_attn_scores_dp0_mb4_gpu3
	layer0_qkv_dp1_mb4_gpu3 -> layer0_attn_scores_dp1_mb4_gpu3
	layer0_softmax_dp0_mb4_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb4_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb4_gpu3 -> layer0_softmax_dp0_mb4_gpu3
	layer0_attn_scores_dp1_mb4_gpu3 -> layer0_softmax_dp1_mb4_gpu3
	layer0_attn_out_dp0_mb4_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb4_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb4_gpu0 -> layer0_attn_out_dp0_mb4_gpu0
	layer0_softmax_dp1_mb4_gpu0 -> layer0_attn_out_dp1_mb4_gpu0
	layer0_attn_out_dp0_mb4_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb4_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb4_gpu1 -> layer0_attn_out_dp0_mb4_gpu1
	layer0_softmax_dp1_mb4_gpu1 -> layer0_attn_out_dp1_mb4_gpu1
	layer0_attn_out_dp0_mb4_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb4_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb4_gpu2 -> layer0_attn_out_dp0_mb4_gpu2
	layer0_softmax_dp1_mb4_gpu2 -> layer0_attn_out_dp1_mb4_gpu2
	layer0_attn_out_dp0_mb4_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb4_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb4_gpu3 -> layer0_attn_out_dp0_mb4_gpu3
	layer0_softmax_dp1_mb4_gpu3 -> layer0_attn_out_dp1_mb4_gpu3
	layer0_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb4_gpu0 -> layer0_attn_allreduce_dp0_mb4
	layer0_attn_out_dp1_mb4_gpu0 -> layer0_attn_allreduce_dp1_mb4
	layer0_attn_out_dp0_mb4_gpu1 -> layer0_attn_allreduce_dp0_mb4
	layer0_attn_out_dp1_mb4_gpu1 -> layer0_attn_allreduce_dp1_mb4
	layer0_attn_out_dp0_mb4_gpu2 -> layer0_attn_allreduce_dp0_mb4
	layer0_attn_out_dp1_mb4_gpu2 -> layer0_attn_allreduce_dp1_mb4
	layer0_attn_out_dp0_mb4_gpu3 -> layer0_attn_allreduce_dp0_mb4
	layer0_attn_out_dp1_mb4_gpu3 -> layer0_attn_allreduce_dp1_mb4
	layer0_router_dp0_mb4_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb4_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb4 -> layer0_router_dp0_mb4_gpu0
	layer0_attn_allreduce_dp1_mb4 -> layer0_router_dp1_mb4_gpu0
	layer0_expert0_mlp1_dp0_mb4_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb4_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb4_gpu0 -> layer0_expert0_mlp1_dp0_mb4_gpu0
	layer0_router_dp1_mb4_gpu0 -> layer0_expert0_mlp1_dp1_mb4_gpu0
	layer0_expert0_mlp2_dp0_mb4_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb4_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb4_gpu0 -> layer0_expert0_mlp2_dp0_mb4_gpu0
	layer0_expert0_mlp1_dp1_mb4_gpu0 -> layer0_expert0_mlp2_dp1_mb4_gpu0
	layer0_expert1_mlp1_dp0_mb4_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb4_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb4_gpu0 -> layer0_expert1_mlp1_dp0_mb4_gpu1
	layer0_router_dp1_mb4_gpu0 -> layer0_expert1_mlp1_dp1_mb4_gpu1
	layer0_expert1_mlp2_dp0_mb4_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb4_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb4_gpu1 -> layer0_expert1_mlp2_dp0_mb4_gpu1
	layer0_expert1_mlp1_dp1_mb4_gpu1 -> layer0_expert1_mlp2_dp1_mb4_gpu1
	layer0_expert2_mlp1_dp0_mb4_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb4_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb4_gpu0 -> layer0_expert2_mlp1_dp0_mb4_gpu2
	layer0_router_dp1_mb4_gpu0 -> layer0_expert2_mlp1_dp1_mb4_gpu2
	layer0_expert2_mlp2_dp0_mb4_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb4_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb4_gpu2 -> layer0_expert2_mlp2_dp0_mb4_gpu2
	layer0_expert2_mlp1_dp1_mb4_gpu2 -> layer0_expert2_mlp2_dp1_mb4_gpu2
	layer0_expert3_mlp1_dp0_mb4_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb4_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb4_gpu0 -> layer0_expert3_mlp1_dp0_mb4_gpu3
	layer0_router_dp1_mb4_gpu0 -> layer0_expert3_mlp1_dp1_mb4_gpu3
	layer0_expert3_mlp2_dp0_mb4_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb4_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb4_gpu3 -> layer0_expert3_mlp2_dp0_mb4_gpu3
	layer0_expert3_mlp1_dp1_mb4_gpu3 -> layer0_expert3_mlp2_dp1_mb4_gpu3
	layer0_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb4_gpu0 -> layer0_expert_agg_dp0_mb4
	layer0_expert0_mlp2_dp1_mb4_gpu0 -> layer0_expert_agg_dp1_mb4
	layer0_expert1_mlp2_dp0_mb4_gpu1 -> layer0_expert_agg_dp0_mb4
	layer0_expert1_mlp2_dp1_mb4_gpu1 -> layer0_expert_agg_dp1_mb4
	layer0_expert2_mlp2_dp0_mb4_gpu2 -> layer0_expert_agg_dp0_mb4
	layer0_expert2_mlp2_dp1_mb4_gpu2 -> layer0_expert_agg_dp1_mb4
	layer0_expert3_mlp2_dp0_mb4_gpu3 -> layer0_expert_agg_dp0_mb4
	layer0_expert3_mlp2_dp1_mb4_gpu3 -> layer0_expert_agg_dp1_mb4
	layer1_qkv_dp0_mb4_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb4_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb4 -> layer1_qkv_dp0_mb4_gpu0
	layer0_expert_agg_dp1_mb4 -> layer1_qkv_dp1_mb4_gpu0
	layer1_qkv_dp0_mb4_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb4_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb4 -> layer1_qkv_dp0_mb4_gpu1
	layer0_expert_agg_dp1_mb4 -> layer1_qkv_dp1_mb4_gpu1
	layer1_qkv_dp0_mb4_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb4_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb4 -> layer1_qkv_dp0_mb4_gpu2
	layer0_expert_agg_dp1_mb4 -> layer1_qkv_dp1_mb4_gpu2
	layer1_qkv_dp0_mb4_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb4_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb4 -> layer1_qkv_dp0_mb4_gpu3
	layer0_expert_agg_dp1_mb4 -> layer1_qkv_dp1_mb4_gpu3
	layer1_attn_scores_dp0_mb4_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb4_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb4_gpu0 -> layer1_attn_scores_dp0_mb4_gpu0
	layer1_qkv_dp1_mb4_gpu0 -> layer1_attn_scores_dp1_mb4_gpu0
	layer1_softmax_dp0_mb4_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb4_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb4_gpu0 -> layer1_softmax_dp0_mb4_gpu0
	layer1_attn_scores_dp1_mb4_gpu0 -> layer1_softmax_dp1_mb4_gpu0
	layer1_attn_scores_dp0_mb4_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb4_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb4_gpu1 -> layer1_attn_scores_dp0_mb4_gpu1
	layer1_qkv_dp1_mb4_gpu1 -> layer1_attn_scores_dp1_mb4_gpu1
	layer1_softmax_dp0_mb4_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb4_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb4_gpu1 -> layer1_softmax_dp0_mb4_gpu1
	layer1_attn_scores_dp1_mb4_gpu1 -> layer1_softmax_dp1_mb4_gpu1
	layer1_attn_scores_dp0_mb4_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb4_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb4_gpu2 -> layer1_attn_scores_dp0_mb4_gpu2
	layer1_qkv_dp1_mb4_gpu2 -> layer1_attn_scores_dp1_mb4_gpu2
	layer1_softmax_dp0_mb4_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb4_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb4_gpu2 -> layer1_softmax_dp0_mb4_gpu2
	layer1_attn_scores_dp1_mb4_gpu2 -> layer1_softmax_dp1_mb4_gpu2
	layer1_attn_scores_dp0_mb4_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb4_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb4_gpu3 -> layer1_attn_scores_dp0_mb4_gpu3
	layer1_qkv_dp1_mb4_gpu3 -> layer1_attn_scores_dp1_mb4_gpu3
	layer1_softmax_dp0_mb4_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb4_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb4_gpu3 -> layer1_softmax_dp0_mb4_gpu3
	layer1_attn_scores_dp1_mb4_gpu3 -> layer1_softmax_dp1_mb4_gpu3
	layer1_attn_out_dp0_mb4_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb4_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb4_gpu0 -> layer1_attn_out_dp0_mb4_gpu0
	layer1_softmax_dp1_mb4_gpu0 -> layer1_attn_out_dp1_mb4_gpu0
	layer1_attn_out_dp0_mb4_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb4_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb4_gpu1 -> layer1_attn_out_dp0_mb4_gpu1
	layer1_softmax_dp1_mb4_gpu1 -> layer1_attn_out_dp1_mb4_gpu1
	layer1_attn_out_dp0_mb4_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb4_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb4_gpu2 -> layer1_attn_out_dp0_mb4_gpu2
	layer1_softmax_dp1_mb4_gpu2 -> layer1_attn_out_dp1_mb4_gpu2
	layer1_attn_out_dp0_mb4_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb4_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb4_gpu3 -> layer1_attn_out_dp0_mb4_gpu3
	layer1_softmax_dp1_mb4_gpu3 -> layer1_attn_out_dp1_mb4_gpu3
	layer1_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb4_gpu0 -> layer1_attn_allreduce_dp0_mb4
	layer1_attn_out_dp1_mb4_gpu0 -> layer1_attn_allreduce_dp1_mb4
	layer1_attn_out_dp0_mb4_gpu1 -> layer1_attn_allreduce_dp0_mb4
	layer1_attn_out_dp1_mb4_gpu1 -> layer1_attn_allreduce_dp1_mb4
	layer1_attn_out_dp0_mb4_gpu2 -> layer1_attn_allreduce_dp0_mb4
	layer1_attn_out_dp1_mb4_gpu2 -> layer1_attn_allreduce_dp1_mb4
	layer1_attn_out_dp0_mb4_gpu3 -> layer1_attn_allreduce_dp0_mb4
	layer1_attn_out_dp1_mb4_gpu3 -> layer1_attn_allreduce_dp1_mb4
	layer1_router_dp0_mb4_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb4_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb4 -> layer1_router_dp0_mb4_gpu0
	layer1_attn_allreduce_dp1_mb4 -> layer1_router_dp1_mb4_gpu0
	layer1_expert0_mlp1_dp0_mb4_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb4_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb4_gpu0 -> layer1_expert0_mlp1_dp0_mb4_gpu0
	layer1_router_dp1_mb4_gpu0 -> layer1_expert0_mlp1_dp1_mb4_gpu0
	layer1_expert0_mlp2_dp0_mb4_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb4_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb4_gpu0 -> layer1_expert0_mlp2_dp0_mb4_gpu0
	layer1_expert0_mlp1_dp1_mb4_gpu0 -> layer1_expert0_mlp2_dp1_mb4_gpu0
	layer1_expert1_mlp1_dp0_mb4_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb4_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb4_gpu0 -> layer1_expert1_mlp1_dp0_mb4_gpu1
	layer1_router_dp1_mb4_gpu0 -> layer1_expert1_mlp1_dp1_mb4_gpu1
	layer1_expert1_mlp2_dp0_mb4_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb4_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb4_gpu1 -> layer1_expert1_mlp2_dp0_mb4_gpu1
	layer1_expert1_mlp1_dp1_mb4_gpu1 -> layer1_expert1_mlp2_dp1_mb4_gpu1
	layer1_expert2_mlp1_dp0_mb4_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb4_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb4_gpu0 -> layer1_expert2_mlp1_dp0_mb4_gpu2
	layer1_router_dp1_mb4_gpu0 -> layer1_expert2_mlp1_dp1_mb4_gpu2
	layer1_expert2_mlp2_dp0_mb4_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb4_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb4_gpu2 -> layer1_expert2_mlp2_dp0_mb4_gpu2
	layer1_expert2_mlp1_dp1_mb4_gpu2 -> layer1_expert2_mlp2_dp1_mb4_gpu2
	layer1_expert3_mlp1_dp0_mb4_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb4_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb4_gpu0 -> layer1_expert3_mlp1_dp0_mb4_gpu3
	layer1_router_dp1_mb4_gpu0 -> layer1_expert3_mlp1_dp1_mb4_gpu3
	layer1_expert3_mlp2_dp0_mb4_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb4_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb4_gpu3 -> layer1_expert3_mlp2_dp0_mb4_gpu3
	layer1_expert3_mlp1_dp1_mb4_gpu3 -> layer1_expert3_mlp2_dp1_mb4_gpu3
	layer1_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb4_gpu0 -> layer1_expert_agg_dp0_mb4
	layer1_expert0_mlp2_dp1_mb4_gpu0 -> layer1_expert_agg_dp1_mb4
	layer1_expert1_mlp2_dp0_mb4_gpu1 -> layer1_expert_agg_dp0_mb4
	layer1_expert1_mlp2_dp1_mb4_gpu1 -> layer1_expert_agg_dp1_mb4
	layer1_expert2_mlp2_dp0_mb4_gpu2 -> layer1_expert_agg_dp0_mb4
	layer1_expert2_mlp2_dp1_mb4_gpu2 -> layer1_expert_agg_dp1_mb4
	layer1_expert3_mlp2_dp0_mb4_gpu3 -> layer1_expert_agg_dp0_mb4
	layer1_expert3_mlp2_dp1_mb4_gpu3 -> layer1_expert_agg_dp1_mb4
	layer2_qkv_dp0_mb4_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb4_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb4 -> layer2_qkv_dp0_mb4_gpu0
	layer1_expert_agg_dp1_mb4 -> layer2_qkv_dp1_mb4_gpu0
	layer2_qkv_dp0_mb4_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb4_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb4 -> layer2_qkv_dp0_mb4_gpu1
	layer1_expert_agg_dp1_mb4 -> layer2_qkv_dp1_mb4_gpu1
	layer2_qkv_dp0_mb4_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb4_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb4 -> layer2_qkv_dp0_mb4_gpu2
	layer1_expert_agg_dp1_mb4 -> layer2_qkv_dp1_mb4_gpu2
	layer2_qkv_dp0_mb4_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb4_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb4 -> layer2_qkv_dp0_mb4_gpu3
	layer1_expert_agg_dp1_mb4 -> layer2_qkv_dp1_mb4_gpu3
	layer2_attn_scores_dp0_mb4_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb4_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb4_gpu0 -> layer2_attn_scores_dp0_mb4_gpu0
	layer2_qkv_dp1_mb4_gpu0 -> layer2_attn_scores_dp1_mb4_gpu0
	layer2_softmax_dp0_mb4_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb4_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb4_gpu0 -> layer2_softmax_dp0_mb4_gpu0
	layer2_attn_scores_dp1_mb4_gpu0 -> layer2_softmax_dp1_mb4_gpu0
	layer2_attn_scores_dp0_mb4_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb4_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb4_gpu1 -> layer2_attn_scores_dp0_mb4_gpu1
	layer2_qkv_dp1_mb4_gpu1 -> layer2_attn_scores_dp1_mb4_gpu1
	layer2_softmax_dp0_mb4_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb4_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb4_gpu1 -> layer2_softmax_dp0_mb4_gpu1
	layer2_attn_scores_dp1_mb4_gpu1 -> layer2_softmax_dp1_mb4_gpu1
	layer2_attn_scores_dp0_mb4_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb4_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb4_gpu2 -> layer2_attn_scores_dp0_mb4_gpu2
	layer2_qkv_dp1_mb4_gpu2 -> layer2_attn_scores_dp1_mb4_gpu2
	layer2_softmax_dp0_mb4_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb4_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb4_gpu2 -> layer2_softmax_dp0_mb4_gpu2
	layer2_attn_scores_dp1_mb4_gpu2 -> layer2_softmax_dp1_mb4_gpu2
	layer2_attn_scores_dp0_mb4_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb4_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb4_gpu3 -> layer2_attn_scores_dp0_mb4_gpu3
	layer2_qkv_dp1_mb4_gpu3 -> layer2_attn_scores_dp1_mb4_gpu3
	layer2_softmax_dp0_mb4_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb4_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb4_gpu3 -> layer2_softmax_dp0_mb4_gpu3
	layer2_attn_scores_dp1_mb4_gpu3 -> layer2_softmax_dp1_mb4_gpu3
	layer2_attn_out_dp0_mb4_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb4_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb4_gpu0 -> layer2_attn_out_dp0_mb4_gpu0
	layer2_softmax_dp1_mb4_gpu0 -> layer2_attn_out_dp1_mb4_gpu0
	layer2_attn_out_dp0_mb4_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb4_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb4_gpu1 -> layer2_attn_out_dp0_mb4_gpu1
	layer2_softmax_dp1_mb4_gpu1 -> layer2_attn_out_dp1_mb4_gpu1
	layer2_attn_out_dp0_mb4_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb4_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb4_gpu2 -> layer2_attn_out_dp0_mb4_gpu2
	layer2_softmax_dp1_mb4_gpu2 -> layer2_attn_out_dp1_mb4_gpu2
	layer2_attn_out_dp0_mb4_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb4_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb4_gpu3 -> layer2_attn_out_dp0_mb4_gpu3
	layer2_softmax_dp1_mb4_gpu3 -> layer2_attn_out_dp1_mb4_gpu3
	layer2_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb4_gpu0 -> layer2_attn_allreduce_dp0_mb4
	layer2_attn_out_dp1_mb4_gpu0 -> layer2_attn_allreduce_dp1_mb4
	layer2_attn_out_dp0_mb4_gpu1 -> layer2_attn_allreduce_dp0_mb4
	layer2_attn_out_dp1_mb4_gpu1 -> layer2_attn_allreduce_dp1_mb4
	layer2_attn_out_dp0_mb4_gpu2 -> layer2_attn_allreduce_dp0_mb4
	layer2_attn_out_dp1_mb4_gpu2 -> layer2_attn_allreduce_dp1_mb4
	layer2_attn_out_dp0_mb4_gpu3 -> layer2_attn_allreduce_dp0_mb4
	layer2_attn_out_dp1_mb4_gpu3 -> layer2_attn_allreduce_dp1_mb4
	layer2_router_dp0_mb4_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb4_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb4 -> layer2_router_dp0_mb4_gpu0
	layer2_attn_allreduce_dp1_mb4 -> layer2_router_dp1_mb4_gpu0
	layer2_expert0_mlp1_dp0_mb4_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb4_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb4_gpu0 -> layer2_expert0_mlp1_dp0_mb4_gpu0
	layer2_router_dp1_mb4_gpu0 -> layer2_expert0_mlp1_dp1_mb4_gpu0
	layer2_expert0_mlp2_dp0_mb4_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb4_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb4_gpu0 -> layer2_expert0_mlp2_dp0_mb4_gpu0
	layer2_expert0_mlp1_dp1_mb4_gpu0 -> layer2_expert0_mlp2_dp1_mb4_gpu0
	layer2_expert1_mlp1_dp0_mb4_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb4_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb4_gpu0 -> layer2_expert1_mlp1_dp0_mb4_gpu1
	layer2_router_dp1_mb4_gpu0 -> layer2_expert1_mlp1_dp1_mb4_gpu1
	layer2_expert1_mlp2_dp0_mb4_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb4_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb4_gpu1 -> layer2_expert1_mlp2_dp0_mb4_gpu1
	layer2_expert1_mlp1_dp1_mb4_gpu1 -> layer2_expert1_mlp2_dp1_mb4_gpu1
	layer2_expert2_mlp1_dp0_mb4_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb4_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb4_gpu0 -> layer2_expert2_mlp1_dp0_mb4_gpu2
	layer2_router_dp1_mb4_gpu0 -> layer2_expert2_mlp1_dp1_mb4_gpu2
	layer2_expert2_mlp2_dp0_mb4_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb4_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb4_gpu2 -> layer2_expert2_mlp2_dp0_mb4_gpu2
	layer2_expert2_mlp1_dp1_mb4_gpu2 -> layer2_expert2_mlp2_dp1_mb4_gpu2
	layer2_expert3_mlp1_dp0_mb4_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb4_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb4_gpu0 -> layer2_expert3_mlp1_dp0_mb4_gpu3
	layer2_router_dp1_mb4_gpu0 -> layer2_expert3_mlp1_dp1_mb4_gpu3
	layer2_expert3_mlp2_dp0_mb4_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb4_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb4_gpu3 -> layer2_expert3_mlp2_dp0_mb4_gpu3
	layer2_expert3_mlp1_dp1_mb4_gpu3 -> layer2_expert3_mlp2_dp1_mb4_gpu3
	layer2_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb4_gpu0 -> layer2_expert_agg_dp0_mb4
	layer2_expert0_mlp2_dp1_mb4_gpu0 -> layer2_expert_agg_dp1_mb4
	layer2_expert1_mlp2_dp0_mb4_gpu1 -> layer2_expert_agg_dp0_mb4
	layer2_expert1_mlp2_dp1_mb4_gpu1 -> layer2_expert_agg_dp1_mb4
	layer2_expert2_mlp2_dp0_mb4_gpu2 -> layer2_expert_agg_dp0_mb4
	layer2_expert2_mlp2_dp1_mb4_gpu2 -> layer2_expert_agg_dp1_mb4
	layer2_expert3_mlp2_dp0_mb4_gpu3 -> layer2_expert_agg_dp0_mb4
	layer2_expert3_mlp2_dp1_mb4_gpu3 -> layer2_expert_agg_dp1_mb4
	layer3_qkv_dp0_mb4_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb4_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb4 -> layer3_qkv_dp0_mb4_gpu0
	layer2_expert_agg_dp1_mb4 -> layer3_qkv_dp1_mb4_gpu0
	layer3_qkv_dp0_mb4_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb4_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb4 -> layer3_qkv_dp0_mb4_gpu1
	layer2_expert_agg_dp1_mb4 -> layer3_qkv_dp1_mb4_gpu1
	layer3_qkv_dp0_mb4_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb4_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb4 -> layer3_qkv_dp0_mb4_gpu2
	layer2_expert_agg_dp1_mb4 -> layer3_qkv_dp1_mb4_gpu2
	layer3_qkv_dp0_mb4_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb4_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb4 -> layer3_qkv_dp0_mb4_gpu3
	layer2_expert_agg_dp1_mb4 -> layer3_qkv_dp1_mb4_gpu3
	layer3_attn_scores_dp0_mb4_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb4_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb4_gpu0 -> layer3_attn_scores_dp0_mb4_gpu0
	layer3_qkv_dp1_mb4_gpu0 -> layer3_attn_scores_dp1_mb4_gpu0
	layer3_softmax_dp0_mb4_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb4_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb4_gpu0 -> layer3_softmax_dp0_mb4_gpu0
	layer3_attn_scores_dp1_mb4_gpu0 -> layer3_softmax_dp1_mb4_gpu0
	layer3_attn_scores_dp0_mb4_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb4_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb4_gpu1 -> layer3_attn_scores_dp0_mb4_gpu1
	layer3_qkv_dp1_mb4_gpu1 -> layer3_attn_scores_dp1_mb4_gpu1
	layer3_softmax_dp0_mb4_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb4_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb4_gpu1 -> layer3_softmax_dp0_mb4_gpu1
	layer3_attn_scores_dp1_mb4_gpu1 -> layer3_softmax_dp1_mb4_gpu1
	layer3_attn_scores_dp0_mb4_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb4_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb4_gpu2 -> layer3_attn_scores_dp0_mb4_gpu2
	layer3_qkv_dp1_mb4_gpu2 -> layer3_attn_scores_dp1_mb4_gpu2
	layer3_softmax_dp0_mb4_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb4_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb4_gpu2 -> layer3_softmax_dp0_mb4_gpu2
	layer3_attn_scores_dp1_mb4_gpu2 -> layer3_softmax_dp1_mb4_gpu2
	layer3_attn_scores_dp0_mb4_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb4_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb4_gpu3 -> layer3_attn_scores_dp0_mb4_gpu3
	layer3_qkv_dp1_mb4_gpu3 -> layer3_attn_scores_dp1_mb4_gpu3
	layer3_softmax_dp0_mb4_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb4_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb4_gpu3 -> layer3_softmax_dp0_mb4_gpu3
	layer3_attn_scores_dp1_mb4_gpu3 -> layer3_softmax_dp1_mb4_gpu3
	layer3_attn_out_dp0_mb4_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb4_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb4_gpu0 -> layer3_attn_out_dp0_mb4_gpu0
	layer3_softmax_dp1_mb4_gpu0 -> layer3_attn_out_dp1_mb4_gpu0
	layer3_attn_out_dp0_mb4_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb4_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb4_gpu1 -> layer3_attn_out_dp0_mb4_gpu1
	layer3_softmax_dp1_mb4_gpu1 -> layer3_attn_out_dp1_mb4_gpu1
	layer3_attn_out_dp0_mb4_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb4_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb4_gpu2 -> layer3_attn_out_dp0_mb4_gpu2
	layer3_softmax_dp1_mb4_gpu2 -> layer3_attn_out_dp1_mb4_gpu2
	layer3_attn_out_dp0_mb4_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb4_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb4_gpu3 -> layer3_attn_out_dp0_mb4_gpu3
	layer3_softmax_dp1_mb4_gpu3 -> layer3_attn_out_dp1_mb4_gpu3
	layer3_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb4_gpu0 -> layer3_attn_allreduce_dp0_mb4
	layer3_attn_out_dp1_mb4_gpu0 -> layer3_attn_allreduce_dp1_mb4
	layer3_attn_out_dp0_mb4_gpu1 -> layer3_attn_allreduce_dp0_mb4
	layer3_attn_out_dp1_mb4_gpu1 -> layer3_attn_allreduce_dp1_mb4
	layer3_attn_out_dp0_mb4_gpu2 -> layer3_attn_allreduce_dp0_mb4
	layer3_attn_out_dp1_mb4_gpu2 -> layer3_attn_allreduce_dp1_mb4
	layer3_attn_out_dp0_mb4_gpu3 -> layer3_attn_allreduce_dp0_mb4
	layer3_attn_out_dp1_mb4_gpu3 -> layer3_attn_allreduce_dp1_mb4
	layer3_router_dp0_mb4_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb4_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb4 -> layer3_router_dp0_mb4_gpu0
	layer3_attn_allreduce_dp1_mb4 -> layer3_router_dp1_mb4_gpu0
	layer3_expert0_mlp1_dp0_mb4_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb4_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb4_gpu0 -> layer3_expert0_mlp1_dp0_mb4_gpu0
	layer3_router_dp1_mb4_gpu0 -> layer3_expert0_mlp1_dp1_mb4_gpu0
	layer3_expert0_mlp2_dp0_mb4_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb4_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb4_gpu0 -> layer3_expert0_mlp2_dp0_mb4_gpu0
	layer3_expert0_mlp1_dp1_mb4_gpu0 -> layer3_expert0_mlp2_dp1_mb4_gpu0
	layer3_expert1_mlp1_dp0_mb4_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb4_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb4_gpu0 -> layer3_expert1_mlp1_dp0_mb4_gpu1
	layer3_router_dp1_mb4_gpu0 -> layer3_expert1_mlp1_dp1_mb4_gpu1
	layer3_expert1_mlp2_dp0_mb4_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb4_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb4_gpu1 -> layer3_expert1_mlp2_dp0_mb4_gpu1
	layer3_expert1_mlp1_dp1_mb4_gpu1 -> layer3_expert1_mlp2_dp1_mb4_gpu1
	layer3_expert2_mlp1_dp0_mb4_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb4_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb4_gpu0 -> layer3_expert2_mlp1_dp0_mb4_gpu2
	layer3_router_dp1_mb4_gpu0 -> layer3_expert2_mlp1_dp1_mb4_gpu2
	layer3_expert2_mlp2_dp0_mb4_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb4_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb4_gpu2 -> layer3_expert2_mlp2_dp0_mb4_gpu2
	layer3_expert2_mlp1_dp1_mb4_gpu2 -> layer3_expert2_mlp2_dp1_mb4_gpu2
	layer3_expert3_mlp1_dp0_mb4_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb4_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb4_gpu0 -> layer3_expert3_mlp1_dp0_mb4_gpu3
	layer3_router_dp1_mb4_gpu0 -> layer3_expert3_mlp1_dp1_mb4_gpu3
	layer3_expert3_mlp2_dp0_mb4_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb4_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb4_gpu3 -> layer3_expert3_mlp2_dp0_mb4_gpu3
	layer3_expert3_mlp1_dp1_mb4_gpu3 -> layer3_expert3_mlp2_dp1_mb4_gpu3
	layer3_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb4_gpu0 -> layer3_expert_agg_dp0_mb4
	layer3_expert0_mlp2_dp1_mb4_gpu0 -> layer3_expert_agg_dp1_mb4
	layer3_expert1_mlp2_dp0_mb4_gpu1 -> layer3_expert_agg_dp0_mb4
	layer3_expert1_mlp2_dp1_mb4_gpu1 -> layer3_expert_agg_dp1_mb4
	layer3_expert2_mlp2_dp0_mb4_gpu2 -> layer3_expert_agg_dp0_mb4
	layer3_expert2_mlp2_dp1_mb4_gpu2 -> layer3_expert_agg_dp1_mb4
	layer3_expert3_mlp2_dp0_mb4_gpu3 -> layer3_expert_agg_dp0_mb4
	layer3_expert3_mlp2_dp1_mb4_gpu3 -> layer3_expert_agg_dp1_mb4
	layer0_qkv_dp0_mb5_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb5_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb5_gpu0
	split_mb1 -> layer0_qkv_dp1_mb5_gpu0
	layer0_qkv_dp0_mb5_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb5_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb5_gpu1
	split_mb1 -> layer0_qkv_dp1_mb5_gpu1
	layer0_qkv_dp0_mb5_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb5_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb5_gpu2
	split_mb1 -> layer0_qkv_dp1_mb5_gpu2
	layer0_qkv_dp0_mb5_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb5_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb5_gpu3
	split_mb1 -> layer0_qkv_dp1_mb5_gpu3
	layer0_attn_scores_dp0_mb5_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb5_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb5_gpu0 -> layer0_attn_scores_dp0_mb5_gpu0
	layer0_qkv_dp1_mb5_gpu0 -> layer0_attn_scores_dp1_mb5_gpu0
	layer0_softmax_dp0_mb5_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb5_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb5_gpu0 -> layer0_softmax_dp0_mb5_gpu0
	layer0_attn_scores_dp1_mb5_gpu0 -> layer0_softmax_dp1_mb5_gpu0
	layer0_attn_scores_dp0_mb5_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb5_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb5_gpu1 -> layer0_attn_scores_dp0_mb5_gpu1
	layer0_qkv_dp1_mb5_gpu1 -> layer0_attn_scores_dp1_mb5_gpu1
	layer0_softmax_dp0_mb5_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb5_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb5_gpu1 -> layer0_softmax_dp0_mb5_gpu1
	layer0_attn_scores_dp1_mb5_gpu1 -> layer0_softmax_dp1_mb5_gpu1
	layer0_attn_scores_dp0_mb5_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb5_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb5_gpu2 -> layer0_attn_scores_dp0_mb5_gpu2
	layer0_qkv_dp1_mb5_gpu2 -> layer0_attn_scores_dp1_mb5_gpu2
	layer0_softmax_dp0_mb5_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb5_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb5_gpu2 -> layer0_softmax_dp0_mb5_gpu2
	layer0_attn_scores_dp1_mb5_gpu2 -> layer0_softmax_dp1_mb5_gpu2
	layer0_attn_scores_dp0_mb5_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb5_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb5_gpu3 -> layer0_attn_scores_dp0_mb5_gpu3
	layer0_qkv_dp1_mb5_gpu3 -> layer0_attn_scores_dp1_mb5_gpu3
	layer0_softmax_dp0_mb5_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb5_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb5_gpu3 -> layer0_softmax_dp0_mb5_gpu3
	layer0_attn_scores_dp1_mb5_gpu3 -> layer0_softmax_dp1_mb5_gpu3
	layer0_attn_out_dp0_mb5_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb5_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb5_gpu0 -> layer0_attn_out_dp0_mb5_gpu0
	layer0_softmax_dp1_mb5_gpu0 -> layer0_attn_out_dp1_mb5_gpu0
	layer0_attn_out_dp0_mb5_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb5_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb5_gpu1 -> layer0_attn_out_dp0_mb5_gpu1
	layer0_softmax_dp1_mb5_gpu1 -> layer0_attn_out_dp1_mb5_gpu1
	layer0_attn_out_dp0_mb5_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb5_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb5_gpu2 -> layer0_attn_out_dp0_mb5_gpu2
	layer0_softmax_dp1_mb5_gpu2 -> layer0_attn_out_dp1_mb5_gpu2
	layer0_attn_out_dp0_mb5_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb5_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb5_gpu3 -> layer0_attn_out_dp0_mb5_gpu3
	layer0_softmax_dp1_mb5_gpu3 -> layer0_attn_out_dp1_mb5_gpu3
	layer0_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb5_gpu0 -> layer0_attn_allreduce_dp0_mb5
	layer0_attn_out_dp1_mb5_gpu0 -> layer0_attn_allreduce_dp1_mb5
	layer0_attn_out_dp0_mb5_gpu1 -> layer0_attn_allreduce_dp0_mb5
	layer0_attn_out_dp1_mb5_gpu1 -> layer0_attn_allreduce_dp1_mb5
	layer0_attn_out_dp0_mb5_gpu2 -> layer0_attn_allreduce_dp0_mb5
	layer0_attn_out_dp1_mb5_gpu2 -> layer0_attn_allreduce_dp1_mb5
	layer0_attn_out_dp0_mb5_gpu3 -> layer0_attn_allreduce_dp0_mb5
	layer0_attn_out_dp1_mb5_gpu3 -> layer0_attn_allreduce_dp1_mb5
	layer0_router_dp0_mb5_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb5_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb5 -> layer0_router_dp0_mb5_gpu0
	layer0_attn_allreduce_dp1_mb5 -> layer0_router_dp1_mb5_gpu0
	layer0_expert0_mlp1_dp0_mb5_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb5_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb5_gpu0 -> layer0_expert0_mlp1_dp0_mb5_gpu0
	layer0_router_dp1_mb5_gpu0 -> layer0_expert0_mlp1_dp1_mb5_gpu0
	layer0_expert0_mlp2_dp0_mb5_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb5_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb5_gpu0 -> layer0_expert0_mlp2_dp0_mb5_gpu0
	layer0_expert0_mlp1_dp1_mb5_gpu0 -> layer0_expert0_mlp2_dp1_mb5_gpu0
	layer0_expert1_mlp1_dp0_mb5_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb5_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb5_gpu0 -> layer0_expert1_mlp1_dp0_mb5_gpu1
	layer0_router_dp1_mb5_gpu0 -> layer0_expert1_mlp1_dp1_mb5_gpu1
	layer0_expert1_mlp2_dp0_mb5_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb5_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb5_gpu1 -> layer0_expert1_mlp2_dp0_mb5_gpu1
	layer0_expert1_mlp1_dp1_mb5_gpu1 -> layer0_expert1_mlp2_dp1_mb5_gpu1
	layer0_expert2_mlp1_dp0_mb5_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb5_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb5_gpu0 -> layer0_expert2_mlp1_dp0_mb5_gpu2
	layer0_router_dp1_mb5_gpu0 -> layer0_expert2_mlp1_dp1_mb5_gpu2
	layer0_expert2_mlp2_dp0_mb5_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb5_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb5_gpu2 -> layer0_expert2_mlp2_dp0_mb5_gpu2
	layer0_expert2_mlp1_dp1_mb5_gpu2 -> layer0_expert2_mlp2_dp1_mb5_gpu2
	layer0_expert3_mlp1_dp0_mb5_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb5_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb5_gpu0 -> layer0_expert3_mlp1_dp0_mb5_gpu3
	layer0_router_dp1_mb5_gpu0 -> layer0_expert3_mlp1_dp1_mb5_gpu3
	layer0_expert3_mlp2_dp0_mb5_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb5_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb5_gpu3 -> layer0_expert3_mlp2_dp0_mb5_gpu3
	layer0_expert3_mlp1_dp1_mb5_gpu3 -> layer0_expert3_mlp2_dp1_mb5_gpu3
	layer0_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb5_gpu0 -> layer0_expert_agg_dp0_mb5
	layer0_expert0_mlp2_dp1_mb5_gpu0 -> layer0_expert_agg_dp1_mb5
	layer0_expert1_mlp2_dp0_mb5_gpu1 -> layer0_expert_agg_dp0_mb5
	layer0_expert1_mlp2_dp1_mb5_gpu1 -> layer0_expert_agg_dp1_mb5
	layer0_expert2_mlp2_dp0_mb5_gpu2 -> layer0_expert_agg_dp0_mb5
	layer0_expert2_mlp2_dp1_mb5_gpu2 -> layer0_expert_agg_dp1_mb5
	layer0_expert3_mlp2_dp0_mb5_gpu3 -> layer0_expert_agg_dp0_mb5
	layer0_expert3_mlp2_dp1_mb5_gpu3 -> layer0_expert_agg_dp1_mb5
	layer1_qkv_dp0_mb5_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb5_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb5 -> layer1_qkv_dp0_mb5_gpu0
	layer0_expert_agg_dp1_mb5 -> layer1_qkv_dp1_mb5_gpu0
	layer1_qkv_dp0_mb5_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb5_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb5 -> layer1_qkv_dp0_mb5_gpu1
	layer0_expert_agg_dp1_mb5 -> layer1_qkv_dp1_mb5_gpu1
	layer1_qkv_dp0_mb5_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb5_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb5 -> layer1_qkv_dp0_mb5_gpu2
	layer0_expert_agg_dp1_mb5 -> layer1_qkv_dp1_mb5_gpu2
	layer1_qkv_dp0_mb5_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb5_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb5 -> layer1_qkv_dp0_mb5_gpu3
	layer0_expert_agg_dp1_mb5 -> layer1_qkv_dp1_mb5_gpu3
	layer1_attn_scores_dp0_mb5_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb5_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb5_gpu0 -> layer1_attn_scores_dp0_mb5_gpu0
	layer1_qkv_dp1_mb5_gpu0 -> layer1_attn_scores_dp1_mb5_gpu0
	layer1_softmax_dp0_mb5_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb5_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb5_gpu0 -> layer1_softmax_dp0_mb5_gpu0
	layer1_attn_scores_dp1_mb5_gpu0 -> layer1_softmax_dp1_mb5_gpu0
	layer1_attn_scores_dp0_mb5_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb5_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb5_gpu1 -> layer1_attn_scores_dp0_mb5_gpu1
	layer1_qkv_dp1_mb5_gpu1 -> layer1_attn_scores_dp1_mb5_gpu1
	layer1_softmax_dp0_mb5_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb5_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb5_gpu1 -> layer1_softmax_dp0_mb5_gpu1
	layer1_attn_scores_dp1_mb5_gpu1 -> layer1_softmax_dp1_mb5_gpu1
	layer1_attn_scores_dp0_mb5_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb5_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb5_gpu2 -> layer1_attn_scores_dp0_mb5_gpu2
	layer1_qkv_dp1_mb5_gpu2 -> layer1_attn_scores_dp1_mb5_gpu2
	layer1_softmax_dp0_mb5_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb5_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb5_gpu2 -> layer1_softmax_dp0_mb5_gpu2
	layer1_attn_scores_dp1_mb5_gpu2 -> layer1_softmax_dp1_mb5_gpu2
	layer1_attn_scores_dp0_mb5_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb5_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb5_gpu3 -> layer1_attn_scores_dp0_mb5_gpu3
	layer1_qkv_dp1_mb5_gpu3 -> layer1_attn_scores_dp1_mb5_gpu3
	layer1_softmax_dp0_mb5_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb5_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb5_gpu3 -> layer1_softmax_dp0_mb5_gpu3
	layer1_attn_scores_dp1_mb5_gpu3 -> layer1_softmax_dp1_mb5_gpu3
	layer1_attn_out_dp0_mb5_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb5_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb5_gpu0 -> layer1_attn_out_dp0_mb5_gpu0
	layer1_softmax_dp1_mb5_gpu0 -> layer1_attn_out_dp1_mb5_gpu0
	layer1_attn_out_dp0_mb5_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb5_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb5_gpu1 -> layer1_attn_out_dp0_mb5_gpu1
	layer1_softmax_dp1_mb5_gpu1 -> layer1_attn_out_dp1_mb5_gpu1
	layer1_attn_out_dp0_mb5_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb5_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb5_gpu2 -> layer1_attn_out_dp0_mb5_gpu2
	layer1_softmax_dp1_mb5_gpu2 -> layer1_attn_out_dp1_mb5_gpu2
	layer1_attn_out_dp0_mb5_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb5_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb5_gpu3 -> layer1_attn_out_dp0_mb5_gpu3
	layer1_softmax_dp1_mb5_gpu3 -> layer1_attn_out_dp1_mb5_gpu3
	layer1_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb5_gpu0 -> layer1_attn_allreduce_dp0_mb5
	layer1_attn_out_dp1_mb5_gpu0 -> layer1_attn_allreduce_dp1_mb5
	layer1_attn_out_dp0_mb5_gpu1 -> layer1_attn_allreduce_dp0_mb5
	layer1_attn_out_dp1_mb5_gpu1 -> layer1_attn_allreduce_dp1_mb5
	layer1_attn_out_dp0_mb5_gpu2 -> layer1_attn_allreduce_dp0_mb5
	layer1_attn_out_dp1_mb5_gpu2 -> layer1_attn_allreduce_dp1_mb5
	layer1_attn_out_dp0_mb5_gpu3 -> layer1_attn_allreduce_dp0_mb5
	layer1_attn_out_dp1_mb5_gpu3 -> layer1_attn_allreduce_dp1_mb5
	layer1_router_dp0_mb5_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb5_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb5 -> layer1_router_dp0_mb5_gpu0
	layer1_attn_allreduce_dp1_mb5 -> layer1_router_dp1_mb5_gpu0
	layer1_expert0_mlp1_dp0_mb5_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb5_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb5_gpu0 -> layer1_expert0_mlp1_dp0_mb5_gpu0
	layer1_router_dp1_mb5_gpu0 -> layer1_expert0_mlp1_dp1_mb5_gpu0
	layer1_expert0_mlp2_dp0_mb5_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb5_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb5_gpu0 -> layer1_expert0_mlp2_dp0_mb5_gpu0
	layer1_expert0_mlp1_dp1_mb5_gpu0 -> layer1_expert0_mlp2_dp1_mb5_gpu0
	layer1_expert1_mlp1_dp0_mb5_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb5_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb5_gpu0 -> layer1_expert1_mlp1_dp0_mb5_gpu1
	layer1_router_dp1_mb5_gpu0 -> layer1_expert1_mlp1_dp1_mb5_gpu1
	layer1_expert1_mlp2_dp0_mb5_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb5_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb5_gpu1 -> layer1_expert1_mlp2_dp0_mb5_gpu1
	layer1_expert1_mlp1_dp1_mb5_gpu1 -> layer1_expert1_mlp2_dp1_mb5_gpu1
	layer1_expert2_mlp1_dp0_mb5_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb5_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb5_gpu0 -> layer1_expert2_mlp1_dp0_mb5_gpu2
	layer1_router_dp1_mb5_gpu0 -> layer1_expert2_mlp1_dp1_mb5_gpu2
	layer1_expert2_mlp2_dp0_mb5_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb5_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb5_gpu2 -> layer1_expert2_mlp2_dp0_mb5_gpu2
	layer1_expert2_mlp1_dp1_mb5_gpu2 -> layer1_expert2_mlp2_dp1_mb5_gpu2
	layer1_expert3_mlp1_dp0_mb5_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb5_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb5_gpu0 -> layer1_expert3_mlp1_dp0_mb5_gpu3
	layer1_router_dp1_mb5_gpu0 -> layer1_expert3_mlp1_dp1_mb5_gpu3
	layer1_expert3_mlp2_dp0_mb5_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb5_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb5_gpu3 -> layer1_expert3_mlp2_dp0_mb5_gpu3
	layer1_expert3_mlp1_dp1_mb5_gpu3 -> layer1_expert3_mlp2_dp1_mb5_gpu3
	layer1_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb5_gpu0 -> layer1_expert_agg_dp0_mb5
	layer1_expert0_mlp2_dp1_mb5_gpu0 -> layer1_expert_agg_dp1_mb5
	layer1_expert1_mlp2_dp0_mb5_gpu1 -> layer1_expert_agg_dp0_mb5
	layer1_expert1_mlp2_dp1_mb5_gpu1 -> layer1_expert_agg_dp1_mb5
	layer1_expert2_mlp2_dp0_mb5_gpu2 -> layer1_expert_agg_dp0_mb5
	layer1_expert2_mlp2_dp1_mb5_gpu2 -> layer1_expert_agg_dp1_mb5
	layer1_expert3_mlp2_dp0_mb5_gpu3 -> layer1_expert_agg_dp0_mb5
	layer1_expert3_mlp2_dp1_mb5_gpu3 -> layer1_expert_agg_dp1_mb5
	layer2_qkv_dp0_mb5_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb5_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb5 -> layer2_qkv_dp0_mb5_gpu0
	layer1_expert_agg_dp1_mb5 -> layer2_qkv_dp1_mb5_gpu0
	layer2_qkv_dp0_mb5_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb5_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb5 -> layer2_qkv_dp0_mb5_gpu1
	layer1_expert_agg_dp1_mb5 -> layer2_qkv_dp1_mb5_gpu1
	layer2_qkv_dp0_mb5_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb5_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb5 -> layer2_qkv_dp0_mb5_gpu2
	layer1_expert_agg_dp1_mb5 -> layer2_qkv_dp1_mb5_gpu2
	layer2_qkv_dp0_mb5_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb5_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb5 -> layer2_qkv_dp0_mb5_gpu3
	layer1_expert_agg_dp1_mb5 -> layer2_qkv_dp1_mb5_gpu3
	layer2_attn_scores_dp0_mb5_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb5_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb5_gpu0 -> layer2_attn_scores_dp0_mb5_gpu0
	layer2_qkv_dp1_mb5_gpu0 -> layer2_attn_scores_dp1_mb5_gpu0
	layer2_softmax_dp0_mb5_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb5_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb5_gpu0 -> layer2_softmax_dp0_mb5_gpu0
	layer2_attn_scores_dp1_mb5_gpu0 -> layer2_softmax_dp1_mb5_gpu0
	layer2_attn_scores_dp0_mb5_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb5_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb5_gpu1 -> layer2_attn_scores_dp0_mb5_gpu1
	layer2_qkv_dp1_mb5_gpu1 -> layer2_attn_scores_dp1_mb5_gpu1
	layer2_softmax_dp0_mb5_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb5_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb5_gpu1 -> layer2_softmax_dp0_mb5_gpu1
	layer2_attn_scores_dp1_mb5_gpu1 -> layer2_softmax_dp1_mb5_gpu1
	layer2_attn_scores_dp0_mb5_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb5_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb5_gpu2 -> layer2_attn_scores_dp0_mb5_gpu2
	layer2_qkv_dp1_mb5_gpu2 -> layer2_attn_scores_dp1_mb5_gpu2
	layer2_softmax_dp0_mb5_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb5_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb5_gpu2 -> layer2_softmax_dp0_mb5_gpu2
	layer2_attn_scores_dp1_mb5_gpu2 -> layer2_softmax_dp1_mb5_gpu2
	layer2_attn_scores_dp0_mb5_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb5_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb5_gpu3 -> layer2_attn_scores_dp0_mb5_gpu3
	layer2_qkv_dp1_mb5_gpu3 -> layer2_attn_scores_dp1_mb5_gpu3
	layer2_softmax_dp0_mb5_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb5_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb5_gpu3 -> layer2_softmax_dp0_mb5_gpu3
	layer2_attn_scores_dp1_mb5_gpu3 -> layer2_softmax_dp1_mb5_gpu3
	layer2_attn_out_dp0_mb5_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb5_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb5_gpu0 -> layer2_attn_out_dp0_mb5_gpu0
	layer2_softmax_dp1_mb5_gpu0 -> layer2_attn_out_dp1_mb5_gpu0
	layer2_attn_out_dp0_mb5_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb5_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb5_gpu1 -> layer2_attn_out_dp0_mb5_gpu1
	layer2_softmax_dp1_mb5_gpu1 -> layer2_attn_out_dp1_mb5_gpu1
	layer2_attn_out_dp0_mb5_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb5_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb5_gpu2 -> layer2_attn_out_dp0_mb5_gpu2
	layer2_softmax_dp1_mb5_gpu2 -> layer2_attn_out_dp1_mb5_gpu2
	layer2_attn_out_dp0_mb5_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb5_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb5_gpu3 -> layer2_attn_out_dp0_mb5_gpu3
	layer2_softmax_dp1_mb5_gpu3 -> layer2_attn_out_dp1_mb5_gpu3
	layer2_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb5_gpu0 -> layer2_attn_allreduce_dp0_mb5
	layer2_attn_out_dp1_mb5_gpu0 -> layer2_attn_allreduce_dp1_mb5
	layer2_attn_out_dp0_mb5_gpu1 -> layer2_attn_allreduce_dp0_mb5
	layer2_attn_out_dp1_mb5_gpu1 -> layer2_attn_allreduce_dp1_mb5
	layer2_attn_out_dp0_mb5_gpu2 -> layer2_attn_allreduce_dp0_mb5
	layer2_attn_out_dp1_mb5_gpu2 -> layer2_attn_allreduce_dp1_mb5
	layer2_attn_out_dp0_mb5_gpu3 -> layer2_attn_allreduce_dp0_mb5
	layer2_attn_out_dp1_mb5_gpu3 -> layer2_attn_allreduce_dp1_mb5
	layer2_router_dp0_mb5_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb5_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb5 -> layer2_router_dp0_mb5_gpu0
	layer2_attn_allreduce_dp1_mb5 -> layer2_router_dp1_mb5_gpu0
	layer2_expert0_mlp1_dp0_mb5_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb5_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb5_gpu0 -> layer2_expert0_mlp1_dp0_mb5_gpu0
	layer2_router_dp1_mb5_gpu0 -> layer2_expert0_mlp1_dp1_mb5_gpu0
	layer2_expert0_mlp2_dp0_mb5_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb5_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb5_gpu0 -> layer2_expert0_mlp2_dp0_mb5_gpu0
	layer2_expert0_mlp1_dp1_mb5_gpu0 -> layer2_expert0_mlp2_dp1_mb5_gpu0
	layer2_expert1_mlp1_dp0_mb5_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb5_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb5_gpu0 -> layer2_expert1_mlp1_dp0_mb5_gpu1
	layer2_router_dp1_mb5_gpu0 -> layer2_expert1_mlp1_dp1_mb5_gpu1
	layer2_expert1_mlp2_dp0_mb5_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb5_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb5_gpu1 -> layer2_expert1_mlp2_dp0_mb5_gpu1
	layer2_expert1_mlp1_dp1_mb5_gpu1 -> layer2_expert1_mlp2_dp1_mb5_gpu1
	layer2_expert2_mlp1_dp0_mb5_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb5_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb5_gpu0 -> layer2_expert2_mlp1_dp0_mb5_gpu2
	layer2_router_dp1_mb5_gpu0 -> layer2_expert2_mlp1_dp1_mb5_gpu2
	layer2_expert2_mlp2_dp0_mb5_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb5_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb5_gpu2 -> layer2_expert2_mlp2_dp0_mb5_gpu2
	layer2_expert2_mlp1_dp1_mb5_gpu2 -> layer2_expert2_mlp2_dp1_mb5_gpu2
	layer2_expert3_mlp1_dp0_mb5_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb5_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb5_gpu0 -> layer2_expert3_mlp1_dp0_mb5_gpu3
	layer2_router_dp1_mb5_gpu0 -> layer2_expert3_mlp1_dp1_mb5_gpu3
	layer2_expert3_mlp2_dp0_mb5_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb5_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb5_gpu3 -> layer2_expert3_mlp2_dp0_mb5_gpu3
	layer2_expert3_mlp1_dp1_mb5_gpu3 -> layer2_expert3_mlp2_dp1_mb5_gpu3
	layer2_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb5_gpu0 -> layer2_expert_agg_dp0_mb5
	layer2_expert0_mlp2_dp1_mb5_gpu0 -> layer2_expert_agg_dp1_mb5
	layer2_expert1_mlp2_dp0_mb5_gpu1 -> layer2_expert_agg_dp0_mb5
	layer2_expert1_mlp2_dp1_mb5_gpu1 -> layer2_expert_agg_dp1_mb5
	layer2_expert2_mlp2_dp0_mb5_gpu2 -> layer2_expert_agg_dp0_mb5
	layer2_expert2_mlp2_dp1_mb5_gpu2 -> layer2_expert_agg_dp1_mb5
	layer2_expert3_mlp2_dp0_mb5_gpu3 -> layer2_expert_agg_dp0_mb5
	layer2_expert3_mlp2_dp1_mb5_gpu3 -> layer2_expert_agg_dp1_mb5
	layer3_qkv_dp0_mb5_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb5_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb5 -> layer3_qkv_dp0_mb5_gpu0
	layer2_expert_agg_dp1_mb5 -> layer3_qkv_dp1_mb5_gpu0
	layer3_qkv_dp0_mb5_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb5_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb5 -> layer3_qkv_dp0_mb5_gpu1
	layer2_expert_agg_dp1_mb5 -> layer3_qkv_dp1_mb5_gpu1
	layer3_qkv_dp0_mb5_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb5_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb5 -> layer3_qkv_dp0_mb5_gpu2
	layer2_expert_agg_dp1_mb5 -> layer3_qkv_dp1_mb5_gpu2
	layer3_qkv_dp0_mb5_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb5_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb5 -> layer3_qkv_dp0_mb5_gpu3
	layer2_expert_agg_dp1_mb5 -> layer3_qkv_dp1_mb5_gpu3
	layer3_attn_scores_dp0_mb5_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb5_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb5_gpu0 -> layer3_attn_scores_dp0_mb5_gpu0
	layer3_qkv_dp1_mb5_gpu0 -> layer3_attn_scores_dp1_mb5_gpu0
	layer3_softmax_dp0_mb5_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb5_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb5_gpu0 -> layer3_softmax_dp0_mb5_gpu0
	layer3_attn_scores_dp1_mb5_gpu0 -> layer3_softmax_dp1_mb5_gpu0
	layer3_attn_scores_dp0_mb5_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb5_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb5_gpu1 -> layer3_attn_scores_dp0_mb5_gpu1
	layer3_qkv_dp1_mb5_gpu1 -> layer3_attn_scores_dp1_mb5_gpu1
	layer3_softmax_dp0_mb5_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb5_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb5_gpu1 -> layer3_softmax_dp0_mb5_gpu1
	layer3_attn_scores_dp1_mb5_gpu1 -> layer3_softmax_dp1_mb5_gpu1
	layer3_attn_scores_dp0_mb5_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb5_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb5_gpu2 -> layer3_attn_scores_dp0_mb5_gpu2
	layer3_qkv_dp1_mb5_gpu2 -> layer3_attn_scores_dp1_mb5_gpu2
	layer3_softmax_dp0_mb5_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb5_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb5_gpu2 -> layer3_softmax_dp0_mb5_gpu2
	layer3_attn_scores_dp1_mb5_gpu2 -> layer3_softmax_dp1_mb5_gpu2
	layer3_attn_scores_dp0_mb5_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb5_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb5_gpu3 -> layer3_attn_scores_dp0_mb5_gpu3
	layer3_qkv_dp1_mb5_gpu3 -> layer3_attn_scores_dp1_mb5_gpu3
	layer3_softmax_dp0_mb5_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb5_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb5_gpu3 -> layer3_softmax_dp0_mb5_gpu3
	layer3_attn_scores_dp1_mb5_gpu3 -> layer3_softmax_dp1_mb5_gpu3
	layer3_attn_out_dp0_mb5_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb5_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb5_gpu0 -> layer3_attn_out_dp0_mb5_gpu0
	layer3_softmax_dp1_mb5_gpu0 -> layer3_attn_out_dp1_mb5_gpu0
	layer3_attn_out_dp0_mb5_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb5_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb5_gpu1 -> layer3_attn_out_dp0_mb5_gpu1
	layer3_softmax_dp1_mb5_gpu1 -> layer3_attn_out_dp1_mb5_gpu1
	layer3_attn_out_dp0_mb5_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb5_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb5_gpu2 -> layer3_attn_out_dp0_mb5_gpu2
	layer3_softmax_dp1_mb5_gpu2 -> layer3_attn_out_dp1_mb5_gpu2
	layer3_attn_out_dp0_mb5_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb5_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb5_gpu3 -> layer3_attn_out_dp0_mb5_gpu3
	layer3_softmax_dp1_mb5_gpu3 -> layer3_attn_out_dp1_mb5_gpu3
	layer3_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb5_gpu0 -> layer3_attn_allreduce_dp0_mb5
	layer3_attn_out_dp1_mb5_gpu0 -> layer3_attn_allreduce_dp1_mb5
	layer3_attn_out_dp0_mb5_gpu1 -> layer3_attn_allreduce_dp0_mb5
	layer3_attn_out_dp1_mb5_gpu1 -> layer3_attn_allreduce_dp1_mb5
	layer3_attn_out_dp0_mb5_gpu2 -> layer3_attn_allreduce_dp0_mb5
	layer3_attn_out_dp1_mb5_gpu2 -> layer3_attn_allreduce_dp1_mb5
	layer3_attn_out_dp0_mb5_gpu3 -> layer3_attn_allreduce_dp0_mb5
	layer3_attn_out_dp1_mb5_gpu3 -> layer3_attn_allreduce_dp1_mb5
	layer3_router_dp0_mb5_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb5_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb5 -> layer3_router_dp0_mb5_gpu0
	layer3_attn_allreduce_dp1_mb5 -> layer3_router_dp1_mb5_gpu0
	layer3_expert0_mlp1_dp0_mb5_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb5_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb5_gpu0 -> layer3_expert0_mlp1_dp0_mb5_gpu0
	layer3_router_dp1_mb5_gpu0 -> layer3_expert0_mlp1_dp1_mb5_gpu0
	layer3_expert0_mlp2_dp0_mb5_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb5_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb5_gpu0 -> layer3_expert0_mlp2_dp0_mb5_gpu0
	layer3_expert0_mlp1_dp1_mb5_gpu0 -> layer3_expert0_mlp2_dp1_mb5_gpu0
	layer3_expert1_mlp1_dp0_mb5_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb5_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb5_gpu0 -> layer3_expert1_mlp1_dp0_mb5_gpu1
	layer3_router_dp1_mb5_gpu0 -> layer3_expert1_mlp1_dp1_mb5_gpu1
	layer3_expert1_mlp2_dp0_mb5_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb5_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb5_gpu1 -> layer3_expert1_mlp2_dp0_mb5_gpu1
	layer3_expert1_mlp1_dp1_mb5_gpu1 -> layer3_expert1_mlp2_dp1_mb5_gpu1
	layer3_expert2_mlp1_dp0_mb5_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb5_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb5_gpu0 -> layer3_expert2_mlp1_dp0_mb5_gpu2
	layer3_router_dp1_mb5_gpu0 -> layer3_expert2_mlp1_dp1_mb5_gpu2
	layer3_expert2_mlp2_dp0_mb5_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb5_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb5_gpu2 -> layer3_expert2_mlp2_dp0_mb5_gpu2
	layer3_expert2_mlp1_dp1_mb5_gpu2 -> layer3_expert2_mlp2_dp1_mb5_gpu2
	layer3_expert3_mlp1_dp0_mb5_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb5_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb5_gpu0 -> layer3_expert3_mlp1_dp0_mb5_gpu3
	layer3_router_dp1_mb5_gpu0 -> layer3_expert3_mlp1_dp1_mb5_gpu3
	layer3_expert3_mlp2_dp0_mb5_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb5_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb5_gpu3 -> layer3_expert3_mlp2_dp0_mb5_gpu3
	layer3_expert3_mlp1_dp1_mb5_gpu3 -> layer3_expert3_mlp2_dp1_mb5_gpu3
	layer3_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb5_gpu0 -> layer3_expert_agg_dp0_mb5
	layer3_expert0_mlp2_dp1_mb5_gpu0 -> layer3_expert_agg_dp1_mb5
	layer3_expert1_mlp2_dp0_mb5_gpu1 -> layer3_expert_agg_dp0_mb5
	layer3_expert1_mlp2_dp1_mb5_gpu1 -> layer3_expert_agg_dp1_mb5
	layer3_expert2_mlp2_dp0_mb5_gpu2 -> layer3_expert_agg_dp0_mb5
	layer3_expert2_mlp2_dp1_mb5_gpu2 -> layer3_expert_agg_dp1_mb5
	layer3_expert3_mlp2_dp0_mb5_gpu3 -> layer3_expert_agg_dp0_mb5
	layer3_expert3_mlp2_dp1_mb5_gpu3 -> layer3_expert_agg_dp1_mb5
	layer0_qkv_dp0_mb6_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb6_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb6_gpu0
	split_mb1 -> layer0_qkv_dp1_mb6_gpu0
	layer0_qkv_dp0_mb6_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb6_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb6_gpu1
	split_mb1 -> layer0_qkv_dp1_mb6_gpu1
	layer0_qkv_dp0_mb6_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb6_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb6_gpu2
	split_mb1 -> layer0_qkv_dp1_mb6_gpu2
	layer0_qkv_dp0_mb6_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb6_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb6_gpu3
	split_mb1 -> layer0_qkv_dp1_mb6_gpu3
	layer0_attn_scores_dp0_mb6_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb6_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb6_gpu0 -> layer0_attn_scores_dp0_mb6_gpu0
	layer0_qkv_dp1_mb6_gpu0 -> layer0_attn_scores_dp1_mb6_gpu0
	layer0_softmax_dp0_mb6_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb6_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb6_gpu0 -> layer0_softmax_dp0_mb6_gpu0
	layer0_attn_scores_dp1_mb6_gpu0 -> layer0_softmax_dp1_mb6_gpu0
	layer0_attn_scores_dp0_mb6_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb6_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb6_gpu1 -> layer0_attn_scores_dp0_mb6_gpu1
	layer0_qkv_dp1_mb6_gpu1 -> layer0_attn_scores_dp1_mb6_gpu1
	layer0_softmax_dp0_mb6_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb6_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb6_gpu1 -> layer0_softmax_dp0_mb6_gpu1
	layer0_attn_scores_dp1_mb6_gpu1 -> layer0_softmax_dp1_mb6_gpu1
	layer0_attn_scores_dp0_mb6_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb6_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb6_gpu2 -> layer0_attn_scores_dp0_mb6_gpu2
	layer0_qkv_dp1_mb6_gpu2 -> layer0_attn_scores_dp1_mb6_gpu2
	layer0_softmax_dp0_mb6_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb6_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb6_gpu2 -> layer0_softmax_dp0_mb6_gpu2
	layer0_attn_scores_dp1_mb6_gpu2 -> layer0_softmax_dp1_mb6_gpu2
	layer0_attn_scores_dp0_mb6_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb6_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb6_gpu3 -> layer0_attn_scores_dp0_mb6_gpu3
	layer0_qkv_dp1_mb6_gpu3 -> layer0_attn_scores_dp1_mb6_gpu3
	layer0_softmax_dp0_mb6_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb6_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb6_gpu3 -> layer0_softmax_dp0_mb6_gpu3
	layer0_attn_scores_dp1_mb6_gpu3 -> layer0_softmax_dp1_mb6_gpu3
	layer0_attn_out_dp0_mb6_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb6_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb6_gpu0 -> layer0_attn_out_dp0_mb6_gpu0
	layer0_softmax_dp1_mb6_gpu0 -> layer0_attn_out_dp1_mb6_gpu0
	layer0_attn_out_dp0_mb6_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb6_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb6_gpu1 -> layer0_attn_out_dp0_mb6_gpu1
	layer0_softmax_dp1_mb6_gpu1 -> layer0_attn_out_dp1_mb6_gpu1
	layer0_attn_out_dp0_mb6_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb6_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb6_gpu2 -> layer0_attn_out_dp0_mb6_gpu2
	layer0_softmax_dp1_mb6_gpu2 -> layer0_attn_out_dp1_mb6_gpu2
	layer0_attn_out_dp0_mb6_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb6_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb6_gpu3 -> layer0_attn_out_dp0_mb6_gpu3
	layer0_softmax_dp1_mb6_gpu3 -> layer0_attn_out_dp1_mb6_gpu3
	layer0_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb6_gpu0 -> layer0_attn_allreduce_dp0_mb6
	layer0_attn_out_dp1_mb6_gpu0 -> layer0_attn_allreduce_dp1_mb6
	layer0_attn_out_dp0_mb6_gpu1 -> layer0_attn_allreduce_dp0_mb6
	layer0_attn_out_dp1_mb6_gpu1 -> layer0_attn_allreduce_dp1_mb6
	layer0_attn_out_dp0_mb6_gpu2 -> layer0_attn_allreduce_dp0_mb6
	layer0_attn_out_dp1_mb6_gpu2 -> layer0_attn_allreduce_dp1_mb6
	layer0_attn_out_dp0_mb6_gpu3 -> layer0_attn_allreduce_dp0_mb6
	layer0_attn_out_dp1_mb6_gpu3 -> layer0_attn_allreduce_dp1_mb6
	layer0_router_dp0_mb6_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb6_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb6 -> layer0_router_dp0_mb6_gpu0
	layer0_attn_allreduce_dp1_mb6 -> layer0_router_dp1_mb6_gpu0
	layer0_expert0_mlp1_dp0_mb6_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb6_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb6_gpu0 -> layer0_expert0_mlp1_dp0_mb6_gpu0
	layer0_router_dp1_mb6_gpu0 -> layer0_expert0_mlp1_dp1_mb6_gpu0
	layer0_expert0_mlp2_dp0_mb6_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb6_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb6_gpu0 -> layer0_expert0_mlp2_dp0_mb6_gpu0
	layer0_expert0_mlp1_dp1_mb6_gpu0 -> layer0_expert0_mlp2_dp1_mb6_gpu0
	layer0_expert1_mlp1_dp0_mb6_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb6_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb6_gpu0 -> layer0_expert1_mlp1_dp0_mb6_gpu1
	layer0_router_dp1_mb6_gpu0 -> layer0_expert1_mlp1_dp1_mb6_gpu1
	layer0_expert1_mlp2_dp0_mb6_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb6_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb6_gpu1 -> layer0_expert1_mlp2_dp0_mb6_gpu1
	layer0_expert1_mlp1_dp1_mb6_gpu1 -> layer0_expert1_mlp2_dp1_mb6_gpu1
	layer0_expert2_mlp1_dp0_mb6_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb6_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb6_gpu0 -> layer0_expert2_mlp1_dp0_mb6_gpu2
	layer0_router_dp1_mb6_gpu0 -> layer0_expert2_mlp1_dp1_mb6_gpu2
	layer0_expert2_mlp2_dp0_mb6_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb6_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb6_gpu2 -> layer0_expert2_mlp2_dp0_mb6_gpu2
	layer0_expert2_mlp1_dp1_mb6_gpu2 -> layer0_expert2_mlp2_dp1_mb6_gpu2
	layer0_expert3_mlp1_dp0_mb6_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb6_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb6_gpu0 -> layer0_expert3_mlp1_dp0_mb6_gpu3
	layer0_router_dp1_mb6_gpu0 -> layer0_expert3_mlp1_dp1_mb6_gpu3
	layer0_expert3_mlp2_dp0_mb6_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb6_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb6_gpu3 -> layer0_expert3_mlp2_dp0_mb6_gpu3
	layer0_expert3_mlp1_dp1_mb6_gpu3 -> layer0_expert3_mlp2_dp1_mb6_gpu3
	layer0_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb6_gpu0 -> layer0_expert_agg_dp0_mb6
	layer0_expert0_mlp2_dp1_mb6_gpu0 -> layer0_expert_agg_dp1_mb6
	layer0_expert1_mlp2_dp0_mb6_gpu1 -> layer0_expert_agg_dp0_mb6
	layer0_expert1_mlp2_dp1_mb6_gpu1 -> layer0_expert_agg_dp1_mb6
	layer0_expert2_mlp2_dp0_mb6_gpu2 -> layer0_expert_agg_dp0_mb6
	layer0_expert2_mlp2_dp1_mb6_gpu2 -> layer0_expert_agg_dp1_mb6
	layer0_expert3_mlp2_dp0_mb6_gpu3 -> layer0_expert_agg_dp0_mb6
	layer0_expert3_mlp2_dp1_mb6_gpu3 -> layer0_expert_agg_dp1_mb6
	layer1_qkv_dp0_mb6_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb6_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb6 -> layer1_qkv_dp0_mb6_gpu0
	layer0_expert_agg_dp1_mb6 -> layer1_qkv_dp1_mb6_gpu0
	layer1_qkv_dp0_mb6_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb6_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb6 -> layer1_qkv_dp0_mb6_gpu1
	layer0_expert_agg_dp1_mb6 -> layer1_qkv_dp1_mb6_gpu1
	layer1_qkv_dp0_mb6_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb6_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb6 -> layer1_qkv_dp0_mb6_gpu2
	layer0_expert_agg_dp1_mb6 -> layer1_qkv_dp1_mb6_gpu2
	layer1_qkv_dp0_mb6_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb6_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb6 -> layer1_qkv_dp0_mb6_gpu3
	layer0_expert_agg_dp1_mb6 -> layer1_qkv_dp1_mb6_gpu3
	layer1_attn_scores_dp0_mb6_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb6_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb6_gpu0 -> layer1_attn_scores_dp0_mb6_gpu0
	layer1_qkv_dp1_mb6_gpu0 -> layer1_attn_scores_dp1_mb6_gpu0
	layer1_softmax_dp0_mb6_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb6_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb6_gpu0 -> layer1_softmax_dp0_mb6_gpu0
	layer1_attn_scores_dp1_mb6_gpu0 -> layer1_softmax_dp1_mb6_gpu0
	layer1_attn_scores_dp0_mb6_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb6_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb6_gpu1 -> layer1_attn_scores_dp0_mb6_gpu1
	layer1_qkv_dp1_mb6_gpu1 -> layer1_attn_scores_dp1_mb6_gpu1
	layer1_softmax_dp0_mb6_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb6_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb6_gpu1 -> layer1_softmax_dp0_mb6_gpu1
	layer1_attn_scores_dp1_mb6_gpu1 -> layer1_softmax_dp1_mb6_gpu1
	layer1_attn_scores_dp0_mb6_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb6_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb6_gpu2 -> layer1_attn_scores_dp0_mb6_gpu2
	layer1_qkv_dp1_mb6_gpu2 -> layer1_attn_scores_dp1_mb6_gpu2
	layer1_softmax_dp0_mb6_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb6_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb6_gpu2 -> layer1_softmax_dp0_mb6_gpu2
	layer1_attn_scores_dp1_mb6_gpu2 -> layer1_softmax_dp1_mb6_gpu2
	layer1_attn_scores_dp0_mb6_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb6_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb6_gpu3 -> layer1_attn_scores_dp0_mb6_gpu3
	layer1_qkv_dp1_mb6_gpu3 -> layer1_attn_scores_dp1_mb6_gpu3
	layer1_softmax_dp0_mb6_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb6_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb6_gpu3 -> layer1_softmax_dp0_mb6_gpu3
	layer1_attn_scores_dp1_mb6_gpu3 -> layer1_softmax_dp1_mb6_gpu3
	layer1_attn_out_dp0_mb6_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb6_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb6_gpu0 -> layer1_attn_out_dp0_mb6_gpu0
	layer1_softmax_dp1_mb6_gpu0 -> layer1_attn_out_dp1_mb6_gpu0
	layer1_attn_out_dp0_mb6_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb6_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb6_gpu1 -> layer1_attn_out_dp0_mb6_gpu1
	layer1_softmax_dp1_mb6_gpu1 -> layer1_attn_out_dp1_mb6_gpu1
	layer1_attn_out_dp0_mb6_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb6_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb6_gpu2 -> layer1_attn_out_dp0_mb6_gpu2
	layer1_softmax_dp1_mb6_gpu2 -> layer1_attn_out_dp1_mb6_gpu2
	layer1_attn_out_dp0_mb6_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb6_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb6_gpu3 -> layer1_attn_out_dp0_mb6_gpu3
	layer1_softmax_dp1_mb6_gpu3 -> layer1_attn_out_dp1_mb6_gpu3
	layer1_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb6_gpu0 -> layer1_attn_allreduce_dp0_mb6
	layer1_attn_out_dp1_mb6_gpu0 -> layer1_attn_allreduce_dp1_mb6
	layer1_attn_out_dp0_mb6_gpu1 -> layer1_attn_allreduce_dp0_mb6
	layer1_attn_out_dp1_mb6_gpu1 -> layer1_attn_allreduce_dp1_mb6
	layer1_attn_out_dp0_mb6_gpu2 -> layer1_attn_allreduce_dp0_mb6
	layer1_attn_out_dp1_mb6_gpu2 -> layer1_attn_allreduce_dp1_mb6
	layer1_attn_out_dp0_mb6_gpu3 -> layer1_attn_allreduce_dp0_mb6
	layer1_attn_out_dp1_mb6_gpu3 -> layer1_attn_allreduce_dp1_mb6
	layer1_router_dp0_mb6_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb6_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb6 -> layer1_router_dp0_mb6_gpu0
	layer1_attn_allreduce_dp1_mb6 -> layer1_router_dp1_mb6_gpu0
	layer1_expert0_mlp1_dp0_mb6_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb6_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb6_gpu0 -> layer1_expert0_mlp1_dp0_mb6_gpu0
	layer1_router_dp1_mb6_gpu0 -> layer1_expert0_mlp1_dp1_mb6_gpu0
	layer1_expert0_mlp2_dp0_mb6_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb6_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb6_gpu0 -> layer1_expert0_mlp2_dp0_mb6_gpu0
	layer1_expert0_mlp1_dp1_mb6_gpu0 -> layer1_expert0_mlp2_dp1_mb6_gpu0
	layer1_expert1_mlp1_dp0_mb6_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb6_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb6_gpu0 -> layer1_expert1_mlp1_dp0_mb6_gpu1
	layer1_router_dp1_mb6_gpu0 -> layer1_expert1_mlp1_dp1_mb6_gpu1
	layer1_expert1_mlp2_dp0_mb6_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb6_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb6_gpu1 -> layer1_expert1_mlp2_dp0_mb6_gpu1
	layer1_expert1_mlp1_dp1_mb6_gpu1 -> layer1_expert1_mlp2_dp1_mb6_gpu1
	layer1_expert2_mlp1_dp0_mb6_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb6_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb6_gpu0 -> layer1_expert2_mlp1_dp0_mb6_gpu2
	layer1_router_dp1_mb6_gpu0 -> layer1_expert2_mlp1_dp1_mb6_gpu2
	layer1_expert2_mlp2_dp0_mb6_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb6_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb6_gpu2 -> layer1_expert2_mlp2_dp0_mb6_gpu2
	layer1_expert2_mlp1_dp1_mb6_gpu2 -> layer1_expert2_mlp2_dp1_mb6_gpu2
	layer1_expert3_mlp1_dp0_mb6_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb6_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb6_gpu0 -> layer1_expert3_mlp1_dp0_mb6_gpu3
	layer1_router_dp1_mb6_gpu0 -> layer1_expert3_mlp1_dp1_mb6_gpu3
	layer1_expert3_mlp2_dp0_mb6_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb6_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb6_gpu3 -> layer1_expert3_mlp2_dp0_mb6_gpu3
	layer1_expert3_mlp1_dp1_mb6_gpu3 -> layer1_expert3_mlp2_dp1_mb6_gpu3
	layer1_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb6_gpu0 -> layer1_expert_agg_dp0_mb6
	layer1_expert0_mlp2_dp1_mb6_gpu0 -> layer1_expert_agg_dp1_mb6
	layer1_expert1_mlp2_dp0_mb6_gpu1 -> layer1_expert_agg_dp0_mb6
	layer1_expert1_mlp2_dp1_mb6_gpu1 -> layer1_expert_agg_dp1_mb6
	layer1_expert2_mlp2_dp0_mb6_gpu2 -> layer1_expert_agg_dp0_mb6
	layer1_expert2_mlp2_dp1_mb6_gpu2 -> layer1_expert_agg_dp1_mb6
	layer1_expert3_mlp2_dp0_mb6_gpu3 -> layer1_expert_agg_dp0_mb6
	layer1_expert3_mlp2_dp1_mb6_gpu3 -> layer1_expert_agg_dp1_mb6
	layer2_qkv_dp0_mb6_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb6_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb6 -> layer2_qkv_dp0_mb6_gpu0
	layer1_expert_agg_dp1_mb6 -> layer2_qkv_dp1_mb6_gpu0
	layer2_qkv_dp0_mb6_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb6_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb6 -> layer2_qkv_dp0_mb6_gpu1
	layer1_expert_agg_dp1_mb6 -> layer2_qkv_dp1_mb6_gpu1
	layer2_qkv_dp0_mb6_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb6_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb6 -> layer2_qkv_dp0_mb6_gpu2
	layer1_expert_agg_dp1_mb6 -> layer2_qkv_dp1_mb6_gpu2
	layer2_qkv_dp0_mb6_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb6_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb6 -> layer2_qkv_dp0_mb6_gpu3
	layer1_expert_agg_dp1_mb6 -> layer2_qkv_dp1_mb6_gpu3
	layer2_attn_scores_dp0_mb6_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb6_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb6_gpu0 -> layer2_attn_scores_dp0_mb6_gpu0
	layer2_qkv_dp1_mb6_gpu0 -> layer2_attn_scores_dp1_mb6_gpu0
	layer2_softmax_dp0_mb6_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb6_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb6_gpu0 -> layer2_softmax_dp0_mb6_gpu0
	layer2_attn_scores_dp1_mb6_gpu0 -> layer2_softmax_dp1_mb6_gpu0
	layer2_attn_scores_dp0_mb6_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb6_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb6_gpu1 -> layer2_attn_scores_dp0_mb6_gpu1
	layer2_qkv_dp1_mb6_gpu1 -> layer2_attn_scores_dp1_mb6_gpu1
	layer2_softmax_dp0_mb6_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb6_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb6_gpu1 -> layer2_softmax_dp0_mb6_gpu1
	layer2_attn_scores_dp1_mb6_gpu1 -> layer2_softmax_dp1_mb6_gpu1
	layer2_attn_scores_dp0_mb6_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb6_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb6_gpu2 -> layer2_attn_scores_dp0_mb6_gpu2
	layer2_qkv_dp1_mb6_gpu2 -> layer2_attn_scores_dp1_mb6_gpu2
	layer2_softmax_dp0_mb6_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb6_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb6_gpu2 -> layer2_softmax_dp0_mb6_gpu2
	layer2_attn_scores_dp1_mb6_gpu2 -> layer2_softmax_dp1_mb6_gpu2
	layer2_attn_scores_dp0_mb6_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb6_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb6_gpu3 -> layer2_attn_scores_dp0_mb6_gpu3
	layer2_qkv_dp1_mb6_gpu3 -> layer2_attn_scores_dp1_mb6_gpu3
	layer2_softmax_dp0_mb6_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb6_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb6_gpu3 -> layer2_softmax_dp0_mb6_gpu3
	layer2_attn_scores_dp1_mb6_gpu3 -> layer2_softmax_dp1_mb6_gpu3
	layer2_attn_out_dp0_mb6_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb6_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb6_gpu0 -> layer2_attn_out_dp0_mb6_gpu0
	layer2_softmax_dp1_mb6_gpu0 -> layer2_attn_out_dp1_mb6_gpu0
	layer2_attn_out_dp0_mb6_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb6_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb6_gpu1 -> layer2_attn_out_dp0_mb6_gpu1
	layer2_softmax_dp1_mb6_gpu1 -> layer2_attn_out_dp1_mb6_gpu1
	layer2_attn_out_dp0_mb6_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb6_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb6_gpu2 -> layer2_attn_out_dp0_mb6_gpu2
	layer2_softmax_dp1_mb6_gpu2 -> layer2_attn_out_dp1_mb6_gpu2
	layer2_attn_out_dp0_mb6_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb6_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb6_gpu3 -> layer2_attn_out_dp0_mb6_gpu3
	layer2_softmax_dp1_mb6_gpu3 -> layer2_attn_out_dp1_mb6_gpu3
	layer2_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb6_gpu0 -> layer2_attn_allreduce_dp0_mb6
	layer2_attn_out_dp1_mb6_gpu0 -> layer2_attn_allreduce_dp1_mb6
	layer2_attn_out_dp0_mb6_gpu1 -> layer2_attn_allreduce_dp0_mb6
	layer2_attn_out_dp1_mb6_gpu1 -> layer2_attn_allreduce_dp1_mb6
	layer2_attn_out_dp0_mb6_gpu2 -> layer2_attn_allreduce_dp0_mb6
	layer2_attn_out_dp1_mb6_gpu2 -> layer2_attn_allreduce_dp1_mb6
	layer2_attn_out_dp0_mb6_gpu3 -> layer2_attn_allreduce_dp0_mb6
	layer2_attn_out_dp1_mb6_gpu3 -> layer2_attn_allreduce_dp1_mb6
	layer2_router_dp0_mb6_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb6_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb6 -> layer2_router_dp0_mb6_gpu0
	layer2_attn_allreduce_dp1_mb6 -> layer2_router_dp1_mb6_gpu0
	layer2_expert0_mlp1_dp0_mb6_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb6_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb6_gpu0 -> layer2_expert0_mlp1_dp0_mb6_gpu0
	layer2_router_dp1_mb6_gpu0 -> layer2_expert0_mlp1_dp1_mb6_gpu0
	layer2_expert0_mlp2_dp0_mb6_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb6_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb6_gpu0 -> layer2_expert0_mlp2_dp0_mb6_gpu0
	layer2_expert0_mlp1_dp1_mb6_gpu0 -> layer2_expert0_mlp2_dp1_mb6_gpu0
	layer2_expert1_mlp1_dp0_mb6_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb6_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb6_gpu0 -> layer2_expert1_mlp1_dp0_mb6_gpu1
	layer2_router_dp1_mb6_gpu0 -> layer2_expert1_mlp1_dp1_mb6_gpu1
	layer2_expert1_mlp2_dp0_mb6_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb6_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb6_gpu1 -> layer2_expert1_mlp2_dp0_mb6_gpu1
	layer2_expert1_mlp1_dp1_mb6_gpu1 -> layer2_expert1_mlp2_dp1_mb6_gpu1
	layer2_expert2_mlp1_dp0_mb6_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb6_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb6_gpu0 -> layer2_expert2_mlp1_dp0_mb6_gpu2
	layer2_router_dp1_mb6_gpu0 -> layer2_expert2_mlp1_dp1_mb6_gpu2
	layer2_expert2_mlp2_dp0_mb6_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb6_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb6_gpu2 -> layer2_expert2_mlp2_dp0_mb6_gpu2
	layer2_expert2_mlp1_dp1_mb6_gpu2 -> layer2_expert2_mlp2_dp1_mb6_gpu2
	layer2_expert3_mlp1_dp0_mb6_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb6_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb6_gpu0 -> layer2_expert3_mlp1_dp0_mb6_gpu3
	layer2_router_dp1_mb6_gpu0 -> layer2_expert3_mlp1_dp1_mb6_gpu3
	layer2_expert3_mlp2_dp0_mb6_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb6_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb6_gpu3 -> layer2_expert3_mlp2_dp0_mb6_gpu3
	layer2_expert3_mlp1_dp1_mb6_gpu3 -> layer2_expert3_mlp2_dp1_mb6_gpu3
	layer2_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb6_gpu0 -> layer2_expert_agg_dp0_mb6
	layer2_expert0_mlp2_dp1_mb6_gpu0 -> layer2_expert_agg_dp1_mb6
	layer2_expert1_mlp2_dp0_mb6_gpu1 -> layer2_expert_agg_dp0_mb6
	layer2_expert1_mlp2_dp1_mb6_gpu1 -> layer2_expert_agg_dp1_mb6
	layer2_expert2_mlp2_dp0_mb6_gpu2 -> layer2_expert_agg_dp0_mb6
	layer2_expert2_mlp2_dp1_mb6_gpu2 -> layer2_expert_agg_dp1_mb6
	layer2_expert3_mlp2_dp0_mb6_gpu3 -> layer2_expert_agg_dp0_mb6
	layer2_expert3_mlp2_dp1_mb6_gpu3 -> layer2_expert_agg_dp1_mb6
	layer3_qkv_dp0_mb6_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb6_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb6 -> layer3_qkv_dp0_mb6_gpu0
	layer2_expert_agg_dp1_mb6 -> layer3_qkv_dp1_mb6_gpu0
	layer3_qkv_dp0_mb6_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb6_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb6 -> layer3_qkv_dp0_mb6_gpu1
	layer2_expert_agg_dp1_mb6 -> layer3_qkv_dp1_mb6_gpu1
	layer3_qkv_dp0_mb6_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb6_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb6 -> layer3_qkv_dp0_mb6_gpu2
	layer2_expert_agg_dp1_mb6 -> layer3_qkv_dp1_mb6_gpu2
	layer3_qkv_dp0_mb6_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb6_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb6 -> layer3_qkv_dp0_mb6_gpu3
	layer2_expert_agg_dp1_mb6 -> layer3_qkv_dp1_mb6_gpu3
	layer3_attn_scores_dp0_mb6_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb6_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb6_gpu0 -> layer3_attn_scores_dp0_mb6_gpu0
	layer3_qkv_dp1_mb6_gpu0 -> layer3_attn_scores_dp1_mb6_gpu0
	layer3_softmax_dp0_mb6_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb6_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb6_gpu0 -> layer3_softmax_dp0_mb6_gpu0
	layer3_attn_scores_dp1_mb6_gpu0 -> layer3_softmax_dp1_mb6_gpu0
	layer3_attn_scores_dp0_mb6_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb6_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb6_gpu1 -> layer3_attn_scores_dp0_mb6_gpu1
	layer3_qkv_dp1_mb6_gpu1 -> layer3_attn_scores_dp1_mb6_gpu1
	layer3_softmax_dp0_mb6_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb6_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb6_gpu1 -> layer3_softmax_dp0_mb6_gpu1
	layer3_attn_scores_dp1_mb6_gpu1 -> layer3_softmax_dp1_mb6_gpu1
	layer3_attn_scores_dp0_mb6_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb6_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb6_gpu2 -> layer3_attn_scores_dp0_mb6_gpu2
	layer3_qkv_dp1_mb6_gpu2 -> layer3_attn_scores_dp1_mb6_gpu2
	layer3_softmax_dp0_mb6_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb6_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb6_gpu2 -> layer3_softmax_dp0_mb6_gpu2
	layer3_attn_scores_dp1_mb6_gpu2 -> layer3_softmax_dp1_mb6_gpu2
	layer3_attn_scores_dp0_mb6_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb6_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb6_gpu3 -> layer3_attn_scores_dp0_mb6_gpu3
	layer3_qkv_dp1_mb6_gpu3 -> layer3_attn_scores_dp1_mb6_gpu3
	layer3_softmax_dp0_mb6_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb6_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb6_gpu3 -> layer3_softmax_dp0_mb6_gpu3
	layer3_attn_scores_dp1_mb6_gpu3 -> layer3_softmax_dp1_mb6_gpu3
	layer3_attn_out_dp0_mb6_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb6_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb6_gpu0 -> layer3_attn_out_dp0_mb6_gpu0
	layer3_softmax_dp1_mb6_gpu0 -> layer3_attn_out_dp1_mb6_gpu0
	layer3_attn_out_dp0_mb6_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb6_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb6_gpu1 -> layer3_attn_out_dp0_mb6_gpu1
	layer3_softmax_dp1_mb6_gpu1 -> layer3_attn_out_dp1_mb6_gpu1
	layer3_attn_out_dp0_mb6_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb6_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb6_gpu2 -> layer3_attn_out_dp0_mb6_gpu2
	layer3_softmax_dp1_mb6_gpu2 -> layer3_attn_out_dp1_mb6_gpu2
	layer3_attn_out_dp0_mb6_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb6_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb6_gpu3 -> layer3_attn_out_dp0_mb6_gpu3
	layer3_softmax_dp1_mb6_gpu3 -> layer3_attn_out_dp1_mb6_gpu3
	layer3_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb6_gpu0 -> layer3_attn_allreduce_dp0_mb6
	layer3_attn_out_dp1_mb6_gpu0 -> layer3_attn_allreduce_dp1_mb6
	layer3_attn_out_dp0_mb6_gpu1 -> layer3_attn_allreduce_dp0_mb6
	layer3_attn_out_dp1_mb6_gpu1 -> layer3_attn_allreduce_dp1_mb6
	layer3_attn_out_dp0_mb6_gpu2 -> layer3_attn_allreduce_dp0_mb6
	layer3_attn_out_dp1_mb6_gpu2 -> layer3_attn_allreduce_dp1_mb6
	layer3_attn_out_dp0_mb6_gpu3 -> layer3_attn_allreduce_dp0_mb6
	layer3_attn_out_dp1_mb6_gpu3 -> layer3_attn_allreduce_dp1_mb6
	layer3_router_dp0_mb6_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb6_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb6 -> layer3_router_dp0_mb6_gpu0
	layer3_attn_allreduce_dp1_mb6 -> layer3_router_dp1_mb6_gpu0
	layer3_expert0_mlp1_dp0_mb6_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb6_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb6_gpu0 -> layer3_expert0_mlp1_dp0_mb6_gpu0
	layer3_router_dp1_mb6_gpu0 -> layer3_expert0_mlp1_dp1_mb6_gpu0
	layer3_expert0_mlp2_dp0_mb6_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb6_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb6_gpu0 -> layer3_expert0_mlp2_dp0_mb6_gpu0
	layer3_expert0_mlp1_dp1_mb6_gpu0 -> layer3_expert0_mlp2_dp1_mb6_gpu0
	layer3_expert1_mlp1_dp0_mb6_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb6_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb6_gpu0 -> layer3_expert1_mlp1_dp0_mb6_gpu1
	layer3_router_dp1_mb6_gpu0 -> layer3_expert1_mlp1_dp1_mb6_gpu1
	layer3_expert1_mlp2_dp0_mb6_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb6_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb6_gpu1 -> layer3_expert1_mlp2_dp0_mb6_gpu1
	layer3_expert1_mlp1_dp1_mb6_gpu1 -> layer3_expert1_mlp2_dp1_mb6_gpu1
	layer3_expert2_mlp1_dp0_mb6_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb6_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb6_gpu0 -> layer3_expert2_mlp1_dp0_mb6_gpu2
	layer3_router_dp1_mb6_gpu0 -> layer3_expert2_mlp1_dp1_mb6_gpu2
	layer3_expert2_mlp2_dp0_mb6_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb6_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb6_gpu2 -> layer3_expert2_mlp2_dp0_mb6_gpu2
	layer3_expert2_mlp1_dp1_mb6_gpu2 -> layer3_expert2_mlp2_dp1_mb6_gpu2
	layer3_expert3_mlp1_dp0_mb6_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb6_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb6_gpu0 -> layer3_expert3_mlp1_dp0_mb6_gpu3
	layer3_router_dp1_mb6_gpu0 -> layer3_expert3_mlp1_dp1_mb6_gpu3
	layer3_expert3_mlp2_dp0_mb6_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb6_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb6_gpu3 -> layer3_expert3_mlp2_dp0_mb6_gpu3
	layer3_expert3_mlp1_dp1_mb6_gpu3 -> layer3_expert3_mlp2_dp1_mb6_gpu3
	layer3_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb6_gpu0 -> layer3_expert_agg_dp0_mb6
	layer3_expert0_mlp2_dp1_mb6_gpu0 -> layer3_expert_agg_dp1_mb6
	layer3_expert1_mlp2_dp0_mb6_gpu1 -> layer3_expert_agg_dp0_mb6
	layer3_expert1_mlp2_dp1_mb6_gpu1 -> layer3_expert_agg_dp1_mb6
	layer3_expert2_mlp2_dp0_mb6_gpu2 -> layer3_expert_agg_dp0_mb6
	layer3_expert2_mlp2_dp1_mb6_gpu2 -> layer3_expert_agg_dp1_mb6
	layer3_expert3_mlp2_dp0_mb6_gpu3 -> layer3_expert_agg_dp0_mb6
	layer3_expert3_mlp2_dp1_mb6_gpu3 -> layer3_expert_agg_dp1_mb6
	layer0_qkv_dp0_mb7_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb7_gpu0 [label="Layer 0 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb7_gpu0
	split_mb1 -> layer0_qkv_dp1_mb7_gpu0
	layer0_qkv_dp0_mb7_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb7_gpu1 [label="Layer 0 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb7_gpu1
	split_mb1 -> layer0_qkv_dp1_mb7_gpu1
	layer0_qkv_dp0_mb7_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb7_gpu2 [label="Layer 0 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb7_gpu2
	split_mb1 -> layer0_qkv_dp1_mb7_gpu2
	layer0_qkv_dp0_mb7_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp1_mb7_gpu3 [label="Layer 0 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	split_mb0 -> layer0_qkv_dp0_mb7_gpu3
	split_mb1 -> layer0_qkv_dp1_mb7_gpu3
	layer0_attn_scores_dp0_mb7_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb7_gpu0 [label="Layer 0 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb7_gpu0 -> layer0_attn_scores_dp0_mb7_gpu0
	layer0_qkv_dp1_mb7_gpu0 -> layer0_attn_scores_dp1_mb7_gpu0
	layer0_softmax_dp0_mb7_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb7_gpu0 [label="Layer 0 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb7_gpu0 -> layer0_softmax_dp0_mb7_gpu0
	layer0_attn_scores_dp1_mb7_gpu0 -> layer0_softmax_dp1_mb7_gpu0
	layer0_attn_scores_dp0_mb7_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb7_gpu1 [label="Layer 0 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb7_gpu1 -> layer0_attn_scores_dp0_mb7_gpu1
	layer0_qkv_dp1_mb7_gpu1 -> layer0_attn_scores_dp1_mb7_gpu1
	layer0_softmax_dp0_mb7_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb7_gpu1 [label="Layer 0 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb7_gpu1 -> layer0_softmax_dp0_mb7_gpu1
	layer0_attn_scores_dp1_mb7_gpu1 -> layer0_softmax_dp1_mb7_gpu1
	layer0_attn_scores_dp0_mb7_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb7_gpu2 [label="Layer 0 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb7_gpu2 -> layer0_attn_scores_dp0_mb7_gpu2
	layer0_qkv_dp1_mb7_gpu2 -> layer0_attn_scores_dp1_mb7_gpu2
	layer0_softmax_dp0_mb7_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb7_gpu2 [label="Layer 0 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb7_gpu2 -> layer0_softmax_dp0_mb7_gpu2
	layer0_attn_scores_dp1_mb7_gpu2 -> layer0_softmax_dp1_mb7_gpu2
	layer0_attn_scores_dp0_mb7_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp1_mb7_gpu3 [label="Layer 0 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_qkv_dp0_mb7_gpu3 -> layer0_attn_scores_dp0_mb7_gpu3
	layer0_qkv_dp1_mb7_gpu3 -> layer0_attn_scores_dp1_mb7_gpu3
	layer0_softmax_dp0_mb7_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp1_mb7_gpu3 [label="Layer 0 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_scores_dp0_mb7_gpu3 -> layer0_softmax_dp0_mb7_gpu3
	layer0_attn_scores_dp1_mb7_gpu3 -> layer0_softmax_dp1_mb7_gpu3
	layer0_attn_out_dp0_mb7_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb7_gpu0 [label="Layer 0 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb7_gpu0 -> layer0_attn_out_dp0_mb7_gpu0
	layer0_softmax_dp1_mb7_gpu0 -> layer0_attn_out_dp1_mb7_gpu0
	layer0_attn_out_dp0_mb7_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb7_gpu1 [label="Layer 0 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb7_gpu1 -> layer0_attn_out_dp0_mb7_gpu1
	layer0_softmax_dp1_mb7_gpu1 -> layer0_attn_out_dp1_mb7_gpu1
	layer0_attn_out_dp0_mb7_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb7_gpu2 [label="Layer 0 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb7_gpu2 -> layer0_attn_out_dp0_mb7_gpu2
	layer0_softmax_dp1_mb7_gpu2 -> layer0_attn_out_dp1_mb7_gpu2
	layer0_attn_out_dp0_mb7_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_attn_out_dp1_mb7_gpu3 [label="Layer 0 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_softmax_dp0_mb7_gpu3 -> layer0_attn_out_dp0_mb7_gpu3
	layer0_softmax_dp1_mb7_gpu3 -> layer0_attn_out_dp1_mb7_gpu3
	layer0_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_attn_out_dp0_mb7_gpu0 -> layer0_attn_allreduce_dp0_mb7
	layer0_attn_out_dp1_mb7_gpu0 -> layer0_attn_allreduce_dp1_mb7
	layer0_attn_out_dp0_mb7_gpu1 -> layer0_attn_allreduce_dp0_mb7
	layer0_attn_out_dp1_mb7_gpu1 -> layer0_attn_allreduce_dp1_mb7
	layer0_attn_out_dp0_mb7_gpu2 -> layer0_attn_allreduce_dp0_mb7
	layer0_attn_out_dp1_mb7_gpu2 -> layer0_attn_allreduce_dp1_mb7
	layer0_attn_out_dp0_mb7_gpu3 -> layer0_attn_allreduce_dp0_mb7
	layer0_attn_out_dp1_mb7_gpu3 -> layer0_attn_allreduce_dp1_mb7
	layer0_router_dp0_mb7_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_router_dp1_mb7_gpu0 [label="Layer 0 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer0_attn_allreduce_dp0_mb7 -> layer0_router_dp0_mb7_gpu0
	layer0_attn_allreduce_dp1_mb7 -> layer0_router_dp1_mb7_gpu0
	layer0_expert0_mlp1_dp0_mb7_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp1_mb7_gpu0 [label="Layer 0 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb7_gpu0 -> layer0_expert0_mlp1_dp0_mb7_gpu0
	layer0_router_dp1_mb7_gpu0 -> layer0_expert0_mlp1_dp1_mb7_gpu0
	layer0_expert0_mlp2_dp0_mb7_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp2_dp1_mb7_gpu0 [label="Layer 0 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert0_mlp1_dp0_mb7_gpu0 -> layer0_expert0_mlp2_dp0_mb7_gpu0
	layer0_expert0_mlp1_dp1_mb7_gpu0 -> layer0_expert0_mlp2_dp1_mb7_gpu0
	layer0_expert1_mlp1_dp0_mb7_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp1_mb7_gpu1 [label="Layer 0 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb7_gpu0 -> layer0_expert1_mlp1_dp0_mb7_gpu1
	layer0_router_dp1_mb7_gpu0 -> layer0_expert1_mlp1_dp1_mb7_gpu1
	layer0_expert1_mlp2_dp0_mb7_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp2_dp1_mb7_gpu1 [label="Layer 0 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert1_mlp1_dp0_mb7_gpu1 -> layer0_expert1_mlp2_dp0_mb7_gpu1
	layer0_expert1_mlp1_dp1_mb7_gpu1 -> layer0_expert1_mlp2_dp1_mb7_gpu1
	layer0_expert2_mlp1_dp0_mb7_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp1_mb7_gpu2 [label="Layer 0 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb7_gpu0 -> layer0_expert2_mlp1_dp0_mb7_gpu2
	layer0_router_dp1_mb7_gpu0 -> layer0_expert2_mlp1_dp1_mb7_gpu2
	layer0_expert2_mlp2_dp0_mb7_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp2_dp1_mb7_gpu2 [label="Layer 0 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert2_mlp1_dp0_mb7_gpu2 -> layer0_expert2_mlp2_dp0_mb7_gpu2
	layer0_expert2_mlp1_dp1_mb7_gpu2 -> layer0_expert2_mlp2_dp1_mb7_gpu2
	layer0_expert3_mlp1_dp0_mb7_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp1_mb7_gpu3 [label="Layer 0 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_router_dp0_mb7_gpu0 -> layer0_expert3_mlp1_dp0_mb7_gpu3
	layer0_router_dp1_mb7_gpu0 -> layer0_expert3_mlp1_dp1_mb7_gpu3
	layer0_expert3_mlp2_dp0_mb7_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp2_dp1_mb7_gpu3 [label="Layer 0 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert3_mlp1_dp0_mb7_gpu3 -> layer0_expert3_mlp2_dp0_mb7_gpu3
	layer0_expert3_mlp1_dp1_mb7_gpu3 -> layer0_expert3_mlp2_dp1_mb7_gpu3
	layer0_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer0_expert0_mlp2_dp0_mb7_gpu0 -> layer0_expert_agg_dp0_mb7
	layer0_expert0_mlp2_dp1_mb7_gpu0 -> layer0_expert_agg_dp1_mb7
	layer0_expert1_mlp2_dp0_mb7_gpu1 -> layer0_expert_agg_dp0_mb7
	layer0_expert1_mlp2_dp1_mb7_gpu1 -> layer0_expert_agg_dp1_mb7
	layer0_expert2_mlp2_dp0_mb7_gpu2 -> layer0_expert_agg_dp0_mb7
	layer0_expert2_mlp2_dp1_mb7_gpu2 -> layer0_expert_agg_dp1_mb7
	layer0_expert3_mlp2_dp0_mb7_gpu3 -> layer0_expert_agg_dp0_mb7
	layer0_expert3_mlp2_dp1_mb7_gpu3 -> layer0_expert_agg_dp1_mb7
	layer1_qkv_dp0_mb7_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb7_gpu0 [label="Layer 1 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb7 -> layer1_qkv_dp0_mb7_gpu0
	layer0_expert_agg_dp1_mb7 -> layer1_qkv_dp1_mb7_gpu0
	layer1_qkv_dp0_mb7_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb7_gpu1 [label="Layer 1 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb7 -> layer1_qkv_dp0_mb7_gpu1
	layer0_expert_agg_dp1_mb7 -> layer1_qkv_dp1_mb7_gpu1
	layer1_qkv_dp0_mb7_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb7_gpu2 [label="Layer 1 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb7 -> layer1_qkv_dp0_mb7_gpu2
	layer0_expert_agg_dp1_mb7 -> layer1_qkv_dp1_mb7_gpu2
	layer1_qkv_dp0_mb7_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp1_mb7_gpu3 [label="Layer 1 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer0_expert_agg_dp0_mb7 -> layer1_qkv_dp0_mb7_gpu3
	layer0_expert_agg_dp1_mb7 -> layer1_qkv_dp1_mb7_gpu3
	layer1_attn_scores_dp0_mb7_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb7_gpu0 [label="Layer 1 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb7_gpu0 -> layer1_attn_scores_dp0_mb7_gpu0
	layer1_qkv_dp1_mb7_gpu0 -> layer1_attn_scores_dp1_mb7_gpu0
	layer1_softmax_dp0_mb7_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb7_gpu0 [label="Layer 1 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb7_gpu0 -> layer1_softmax_dp0_mb7_gpu0
	layer1_attn_scores_dp1_mb7_gpu0 -> layer1_softmax_dp1_mb7_gpu0
	layer1_attn_scores_dp0_mb7_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb7_gpu1 [label="Layer 1 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb7_gpu1 -> layer1_attn_scores_dp0_mb7_gpu1
	layer1_qkv_dp1_mb7_gpu1 -> layer1_attn_scores_dp1_mb7_gpu1
	layer1_softmax_dp0_mb7_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb7_gpu1 [label="Layer 1 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb7_gpu1 -> layer1_softmax_dp0_mb7_gpu1
	layer1_attn_scores_dp1_mb7_gpu1 -> layer1_softmax_dp1_mb7_gpu1
	layer1_attn_scores_dp0_mb7_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb7_gpu2 [label="Layer 1 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb7_gpu2 -> layer1_attn_scores_dp0_mb7_gpu2
	layer1_qkv_dp1_mb7_gpu2 -> layer1_attn_scores_dp1_mb7_gpu2
	layer1_softmax_dp0_mb7_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb7_gpu2 [label="Layer 1 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb7_gpu2 -> layer1_softmax_dp0_mb7_gpu2
	layer1_attn_scores_dp1_mb7_gpu2 -> layer1_softmax_dp1_mb7_gpu2
	layer1_attn_scores_dp0_mb7_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp1_mb7_gpu3 [label="Layer 1 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_qkv_dp0_mb7_gpu3 -> layer1_attn_scores_dp0_mb7_gpu3
	layer1_qkv_dp1_mb7_gpu3 -> layer1_attn_scores_dp1_mb7_gpu3
	layer1_softmax_dp0_mb7_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp1_mb7_gpu3 [label="Layer 1 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_scores_dp0_mb7_gpu3 -> layer1_softmax_dp0_mb7_gpu3
	layer1_attn_scores_dp1_mb7_gpu3 -> layer1_softmax_dp1_mb7_gpu3
	layer1_attn_out_dp0_mb7_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb7_gpu0 [label="Layer 1 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb7_gpu0 -> layer1_attn_out_dp0_mb7_gpu0
	layer1_softmax_dp1_mb7_gpu0 -> layer1_attn_out_dp1_mb7_gpu0
	layer1_attn_out_dp0_mb7_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb7_gpu1 [label="Layer 1 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb7_gpu1 -> layer1_attn_out_dp0_mb7_gpu1
	layer1_softmax_dp1_mb7_gpu1 -> layer1_attn_out_dp1_mb7_gpu1
	layer1_attn_out_dp0_mb7_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb7_gpu2 [label="Layer 1 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb7_gpu2 -> layer1_attn_out_dp0_mb7_gpu2
	layer1_softmax_dp1_mb7_gpu2 -> layer1_attn_out_dp1_mb7_gpu2
	layer1_attn_out_dp0_mb7_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_attn_out_dp1_mb7_gpu3 [label="Layer 1 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_softmax_dp0_mb7_gpu3 -> layer1_attn_out_dp0_mb7_gpu3
	layer1_softmax_dp1_mb7_gpu3 -> layer1_attn_out_dp1_mb7_gpu3
	layer1_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_attn_out_dp0_mb7_gpu0 -> layer1_attn_allreduce_dp0_mb7
	layer1_attn_out_dp1_mb7_gpu0 -> layer1_attn_allreduce_dp1_mb7
	layer1_attn_out_dp0_mb7_gpu1 -> layer1_attn_allreduce_dp0_mb7
	layer1_attn_out_dp1_mb7_gpu1 -> layer1_attn_allreduce_dp1_mb7
	layer1_attn_out_dp0_mb7_gpu2 -> layer1_attn_allreduce_dp0_mb7
	layer1_attn_out_dp1_mb7_gpu2 -> layer1_attn_allreduce_dp1_mb7
	layer1_attn_out_dp0_mb7_gpu3 -> layer1_attn_allreduce_dp0_mb7
	layer1_attn_out_dp1_mb7_gpu3 -> layer1_attn_allreduce_dp1_mb7
	layer1_router_dp0_mb7_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_router_dp1_mb7_gpu0 [label="Layer 1 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer1_attn_allreduce_dp0_mb7 -> layer1_router_dp0_mb7_gpu0
	layer1_attn_allreduce_dp1_mb7 -> layer1_router_dp1_mb7_gpu0
	layer1_expert0_mlp1_dp0_mb7_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp1_mb7_gpu0 [label="Layer 1 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb7_gpu0 -> layer1_expert0_mlp1_dp0_mb7_gpu0
	layer1_router_dp1_mb7_gpu0 -> layer1_expert0_mlp1_dp1_mb7_gpu0
	layer1_expert0_mlp2_dp0_mb7_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp2_dp1_mb7_gpu0 [label="Layer 1 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert0_mlp1_dp0_mb7_gpu0 -> layer1_expert0_mlp2_dp0_mb7_gpu0
	layer1_expert0_mlp1_dp1_mb7_gpu0 -> layer1_expert0_mlp2_dp1_mb7_gpu0
	layer1_expert1_mlp1_dp0_mb7_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp1_mb7_gpu1 [label="Layer 1 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb7_gpu0 -> layer1_expert1_mlp1_dp0_mb7_gpu1
	layer1_router_dp1_mb7_gpu0 -> layer1_expert1_mlp1_dp1_mb7_gpu1
	layer1_expert1_mlp2_dp0_mb7_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp2_dp1_mb7_gpu1 [label="Layer 1 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert1_mlp1_dp0_mb7_gpu1 -> layer1_expert1_mlp2_dp0_mb7_gpu1
	layer1_expert1_mlp1_dp1_mb7_gpu1 -> layer1_expert1_mlp2_dp1_mb7_gpu1
	layer1_expert2_mlp1_dp0_mb7_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp1_mb7_gpu2 [label="Layer 1 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb7_gpu0 -> layer1_expert2_mlp1_dp0_mb7_gpu2
	layer1_router_dp1_mb7_gpu0 -> layer1_expert2_mlp1_dp1_mb7_gpu2
	layer1_expert2_mlp2_dp0_mb7_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp2_dp1_mb7_gpu2 [label="Layer 1 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert2_mlp1_dp0_mb7_gpu2 -> layer1_expert2_mlp2_dp0_mb7_gpu2
	layer1_expert2_mlp1_dp1_mb7_gpu2 -> layer1_expert2_mlp2_dp1_mb7_gpu2
	layer1_expert3_mlp1_dp0_mb7_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp1_mb7_gpu3 [label="Layer 1 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_router_dp0_mb7_gpu0 -> layer1_expert3_mlp1_dp0_mb7_gpu3
	layer1_router_dp1_mb7_gpu0 -> layer1_expert3_mlp1_dp1_mb7_gpu3
	layer1_expert3_mlp2_dp0_mb7_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp2_dp1_mb7_gpu3 [label="Layer 1 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert3_mlp1_dp0_mb7_gpu3 -> layer1_expert3_mlp2_dp0_mb7_gpu3
	layer1_expert3_mlp1_dp1_mb7_gpu3 -> layer1_expert3_mlp2_dp1_mb7_gpu3
	layer1_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer1_expert0_mlp2_dp0_mb7_gpu0 -> layer1_expert_agg_dp0_mb7
	layer1_expert0_mlp2_dp1_mb7_gpu0 -> layer1_expert_agg_dp1_mb7
	layer1_expert1_mlp2_dp0_mb7_gpu1 -> layer1_expert_agg_dp0_mb7
	layer1_expert1_mlp2_dp1_mb7_gpu1 -> layer1_expert_agg_dp1_mb7
	layer1_expert2_mlp2_dp0_mb7_gpu2 -> layer1_expert_agg_dp0_mb7
	layer1_expert2_mlp2_dp1_mb7_gpu2 -> layer1_expert_agg_dp1_mb7
	layer1_expert3_mlp2_dp0_mb7_gpu3 -> layer1_expert_agg_dp0_mb7
	layer1_expert3_mlp2_dp1_mb7_gpu3 -> layer1_expert_agg_dp1_mb7
	layer2_qkv_dp0_mb7_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb7_gpu0 [label="Layer 2 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb7 -> layer2_qkv_dp0_mb7_gpu0
	layer1_expert_agg_dp1_mb7 -> layer2_qkv_dp1_mb7_gpu0
	layer2_qkv_dp0_mb7_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb7_gpu1 [label="Layer 2 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb7 -> layer2_qkv_dp0_mb7_gpu1
	layer1_expert_agg_dp1_mb7 -> layer2_qkv_dp1_mb7_gpu1
	layer2_qkv_dp0_mb7_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb7_gpu2 [label="Layer 2 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb7 -> layer2_qkv_dp0_mb7_gpu2
	layer1_expert_agg_dp1_mb7 -> layer2_qkv_dp1_mb7_gpu2
	layer2_qkv_dp0_mb7_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp1_mb7_gpu3 [label="Layer 2 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer1_expert_agg_dp0_mb7 -> layer2_qkv_dp0_mb7_gpu3
	layer1_expert_agg_dp1_mb7 -> layer2_qkv_dp1_mb7_gpu3
	layer2_attn_scores_dp0_mb7_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb7_gpu0 [label="Layer 2 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb7_gpu0 -> layer2_attn_scores_dp0_mb7_gpu0
	layer2_qkv_dp1_mb7_gpu0 -> layer2_attn_scores_dp1_mb7_gpu0
	layer2_softmax_dp0_mb7_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb7_gpu0 [label="Layer 2 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb7_gpu0 -> layer2_softmax_dp0_mb7_gpu0
	layer2_attn_scores_dp1_mb7_gpu0 -> layer2_softmax_dp1_mb7_gpu0
	layer2_attn_scores_dp0_mb7_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb7_gpu1 [label="Layer 2 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb7_gpu1 -> layer2_attn_scores_dp0_mb7_gpu1
	layer2_qkv_dp1_mb7_gpu1 -> layer2_attn_scores_dp1_mb7_gpu1
	layer2_softmax_dp0_mb7_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb7_gpu1 [label="Layer 2 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb7_gpu1 -> layer2_softmax_dp0_mb7_gpu1
	layer2_attn_scores_dp1_mb7_gpu1 -> layer2_softmax_dp1_mb7_gpu1
	layer2_attn_scores_dp0_mb7_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb7_gpu2 [label="Layer 2 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb7_gpu2 -> layer2_attn_scores_dp0_mb7_gpu2
	layer2_qkv_dp1_mb7_gpu2 -> layer2_attn_scores_dp1_mb7_gpu2
	layer2_softmax_dp0_mb7_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb7_gpu2 [label="Layer 2 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb7_gpu2 -> layer2_softmax_dp0_mb7_gpu2
	layer2_attn_scores_dp1_mb7_gpu2 -> layer2_softmax_dp1_mb7_gpu2
	layer2_attn_scores_dp0_mb7_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp1_mb7_gpu3 [label="Layer 2 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_qkv_dp0_mb7_gpu3 -> layer2_attn_scores_dp0_mb7_gpu3
	layer2_qkv_dp1_mb7_gpu3 -> layer2_attn_scores_dp1_mb7_gpu3
	layer2_softmax_dp0_mb7_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp1_mb7_gpu3 [label="Layer 2 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_scores_dp0_mb7_gpu3 -> layer2_softmax_dp0_mb7_gpu3
	layer2_attn_scores_dp1_mb7_gpu3 -> layer2_softmax_dp1_mb7_gpu3
	layer2_attn_out_dp0_mb7_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb7_gpu0 [label="Layer 2 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb7_gpu0 -> layer2_attn_out_dp0_mb7_gpu0
	layer2_softmax_dp1_mb7_gpu0 -> layer2_attn_out_dp1_mb7_gpu0
	layer2_attn_out_dp0_mb7_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb7_gpu1 [label="Layer 2 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb7_gpu1 -> layer2_attn_out_dp0_mb7_gpu1
	layer2_softmax_dp1_mb7_gpu1 -> layer2_attn_out_dp1_mb7_gpu1
	layer2_attn_out_dp0_mb7_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb7_gpu2 [label="Layer 2 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb7_gpu2 -> layer2_attn_out_dp0_mb7_gpu2
	layer2_softmax_dp1_mb7_gpu2 -> layer2_attn_out_dp1_mb7_gpu2
	layer2_attn_out_dp0_mb7_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_attn_out_dp1_mb7_gpu3 [label="Layer 2 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_softmax_dp0_mb7_gpu3 -> layer2_attn_out_dp0_mb7_gpu3
	layer2_softmax_dp1_mb7_gpu3 -> layer2_attn_out_dp1_mb7_gpu3
	layer2_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_attn_out_dp0_mb7_gpu0 -> layer2_attn_allreduce_dp0_mb7
	layer2_attn_out_dp1_mb7_gpu0 -> layer2_attn_allreduce_dp1_mb7
	layer2_attn_out_dp0_mb7_gpu1 -> layer2_attn_allreduce_dp0_mb7
	layer2_attn_out_dp1_mb7_gpu1 -> layer2_attn_allreduce_dp1_mb7
	layer2_attn_out_dp0_mb7_gpu2 -> layer2_attn_allreduce_dp0_mb7
	layer2_attn_out_dp1_mb7_gpu2 -> layer2_attn_allreduce_dp1_mb7
	layer2_attn_out_dp0_mb7_gpu3 -> layer2_attn_allreduce_dp0_mb7
	layer2_attn_out_dp1_mb7_gpu3 -> layer2_attn_allreduce_dp1_mb7
	layer2_router_dp0_mb7_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_router_dp1_mb7_gpu0 [label="Layer 2 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer2_attn_allreduce_dp0_mb7 -> layer2_router_dp0_mb7_gpu0
	layer2_attn_allreduce_dp1_mb7 -> layer2_router_dp1_mb7_gpu0
	layer2_expert0_mlp1_dp0_mb7_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp1_mb7_gpu0 [label="Layer 2 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb7_gpu0 -> layer2_expert0_mlp1_dp0_mb7_gpu0
	layer2_router_dp1_mb7_gpu0 -> layer2_expert0_mlp1_dp1_mb7_gpu0
	layer2_expert0_mlp2_dp0_mb7_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp2_dp1_mb7_gpu0 [label="Layer 2 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert0_mlp1_dp0_mb7_gpu0 -> layer2_expert0_mlp2_dp0_mb7_gpu0
	layer2_expert0_mlp1_dp1_mb7_gpu0 -> layer2_expert0_mlp2_dp1_mb7_gpu0
	layer2_expert1_mlp1_dp0_mb7_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp1_mb7_gpu1 [label="Layer 2 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb7_gpu0 -> layer2_expert1_mlp1_dp0_mb7_gpu1
	layer2_router_dp1_mb7_gpu0 -> layer2_expert1_mlp1_dp1_mb7_gpu1
	layer2_expert1_mlp2_dp0_mb7_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp2_dp1_mb7_gpu1 [label="Layer 2 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert1_mlp1_dp0_mb7_gpu1 -> layer2_expert1_mlp2_dp0_mb7_gpu1
	layer2_expert1_mlp1_dp1_mb7_gpu1 -> layer2_expert1_mlp2_dp1_mb7_gpu1
	layer2_expert2_mlp1_dp0_mb7_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp1_mb7_gpu2 [label="Layer 2 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb7_gpu0 -> layer2_expert2_mlp1_dp0_mb7_gpu2
	layer2_router_dp1_mb7_gpu0 -> layer2_expert2_mlp1_dp1_mb7_gpu2
	layer2_expert2_mlp2_dp0_mb7_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp2_dp1_mb7_gpu2 [label="Layer 2 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert2_mlp1_dp0_mb7_gpu2 -> layer2_expert2_mlp2_dp0_mb7_gpu2
	layer2_expert2_mlp1_dp1_mb7_gpu2 -> layer2_expert2_mlp2_dp1_mb7_gpu2
	layer2_expert3_mlp1_dp0_mb7_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp1_mb7_gpu3 [label="Layer 2 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_router_dp0_mb7_gpu0 -> layer2_expert3_mlp1_dp0_mb7_gpu3
	layer2_router_dp1_mb7_gpu0 -> layer2_expert3_mlp1_dp1_mb7_gpu3
	layer2_expert3_mlp2_dp0_mb7_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp2_dp1_mb7_gpu3 [label="Layer 2 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert3_mlp1_dp0_mb7_gpu3 -> layer2_expert3_mlp2_dp0_mb7_gpu3
	layer2_expert3_mlp1_dp1_mb7_gpu3 -> layer2_expert3_mlp2_dp1_mb7_gpu3
	layer2_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer2_expert0_mlp2_dp0_mb7_gpu0 -> layer2_expert_agg_dp0_mb7
	layer2_expert0_mlp2_dp1_mb7_gpu0 -> layer2_expert_agg_dp1_mb7
	layer2_expert1_mlp2_dp0_mb7_gpu1 -> layer2_expert_agg_dp0_mb7
	layer2_expert1_mlp2_dp1_mb7_gpu1 -> layer2_expert_agg_dp1_mb7
	layer2_expert2_mlp2_dp0_mb7_gpu2 -> layer2_expert_agg_dp0_mb7
	layer2_expert2_mlp2_dp1_mb7_gpu2 -> layer2_expert_agg_dp1_mb7
	layer2_expert3_mlp2_dp0_mb7_gpu3 -> layer2_expert_agg_dp0_mb7
	layer2_expert3_mlp2_dp1_mb7_gpu3 -> layer2_expert_agg_dp1_mb7
	layer3_qkv_dp0_mb7_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb7_gpu0 [label="Layer 3 QKV Projection\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb7 -> layer3_qkv_dp0_mb7_gpu0
	layer2_expert_agg_dp1_mb7 -> layer3_qkv_dp1_mb7_gpu0
	layer3_qkv_dp0_mb7_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb7_gpu1 [label="Layer 3 QKV Projection\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb7 -> layer3_qkv_dp0_mb7_gpu1
	layer2_expert_agg_dp1_mb7 -> layer3_qkv_dp1_mb7_gpu1
	layer3_qkv_dp0_mb7_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb7_gpu2 [label="Layer 3 QKV Projection\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb7 -> layer3_qkv_dp0_mb7_gpu2
	layer2_expert_agg_dp1_mb7 -> layer3_qkv_dp1_mb7_gpu2
	layer3_qkv_dp0_mb7_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp1_mb7_gpu3 [label="Layer 3 QKV Projection\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer2_expert_agg_dp0_mb7 -> layer3_qkv_dp0_mb7_gpu3
	layer2_expert_agg_dp1_mb7 -> layer3_qkv_dp1_mb7_gpu3
	layer3_attn_scores_dp0_mb7_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb7_gpu0 [label="Layer 3 Attention Scores\nGPU 0\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb7_gpu0 -> layer3_attn_scores_dp0_mb7_gpu0
	layer3_qkv_dp1_mb7_gpu0 -> layer3_attn_scores_dp1_mb7_gpu0
	layer3_softmax_dp0_mb7_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb7_gpu0 [label="Layer 3 Attention Softmax\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb7_gpu0 -> layer3_softmax_dp0_mb7_gpu0
	layer3_attn_scores_dp1_mb7_gpu0 -> layer3_softmax_dp1_mb7_gpu0
	layer3_attn_scores_dp0_mb7_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb7_gpu1 [label="Layer 3 Attention Scores\nGPU 1\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb7_gpu1 -> layer3_attn_scores_dp0_mb7_gpu1
	layer3_qkv_dp1_mb7_gpu1 -> layer3_attn_scores_dp1_mb7_gpu1
	layer3_softmax_dp0_mb7_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb7_gpu1 [label="Layer 3 Attention Softmax\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb7_gpu1 -> layer3_softmax_dp0_mb7_gpu1
	layer3_attn_scores_dp1_mb7_gpu1 -> layer3_softmax_dp1_mb7_gpu1
	layer3_attn_scores_dp0_mb7_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb7_gpu2 [label="Layer 3 Attention Scores\nGPU 2\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb7_gpu2 -> layer3_attn_scores_dp0_mb7_gpu2
	layer3_qkv_dp1_mb7_gpu2 -> layer3_attn_scores_dp1_mb7_gpu2
	layer3_softmax_dp0_mb7_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb7_gpu2 [label="Layer 3 Attention Softmax\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb7_gpu2 -> layer3_softmax_dp0_mb7_gpu2
	layer3_attn_scores_dp1_mb7_gpu2 -> layer3_softmax_dp1_mb7_gpu2
	layer3_attn_scores_dp0_mb7_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp1_mb7_gpu3 [label="Layer 3 Attention Scores\nGPU 3\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_qkv_dp0_mb7_gpu3 -> layer3_attn_scores_dp0_mb7_gpu3
	layer3_qkv_dp1_mb7_gpu3 -> layer3_attn_scores_dp1_mb7_gpu3
	layer3_softmax_dp0_mb7_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp1_mb7_gpu3 [label="Layer 3 Attention Softmax\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_scores_dp0_mb7_gpu3 -> layer3_softmax_dp0_mb7_gpu3
	layer3_attn_scores_dp1_mb7_gpu3 -> layer3_softmax_dp1_mb7_gpu3
	layer3_attn_out_dp0_mb7_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb7_gpu0 [label="Layer 3 Attention Output\nGPU 0\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb7_gpu0 -> layer3_attn_out_dp0_mb7_gpu0
	layer3_softmax_dp1_mb7_gpu0 -> layer3_attn_out_dp1_mb7_gpu0
	layer3_attn_out_dp0_mb7_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb7_gpu1 [label="Layer 3 Attention Output\nGPU 1\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb7_gpu1 -> layer3_attn_out_dp0_mb7_gpu1
	layer3_softmax_dp1_mb7_gpu1 -> layer3_attn_out_dp1_mb7_gpu1
	layer3_attn_out_dp0_mb7_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb7_gpu2 [label="Layer 3 Attention Output\nGPU 2\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb7_gpu2 -> layer3_attn_out_dp0_mb7_gpu2
	layer3_softmax_dp1_mb7_gpu2 -> layer3_attn_out_dp1_mb7_gpu2
	layer3_attn_out_dp0_mb7_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_attn_out_dp1_mb7_gpu3 [label="Layer 3 Attention Output\nGPU 3\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_softmax_dp0_mb7_gpu3 -> layer3_attn_out_dp0_mb7_gpu3
	layer3_softmax_dp1_mb7_gpu3 -> layer3_attn_out_dp1_mb7_gpu3
	layer3_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_attn_out_dp0_mb7_gpu0 -> layer3_attn_allreduce_dp0_mb7
	layer3_attn_out_dp1_mb7_gpu0 -> layer3_attn_allreduce_dp1_mb7
	layer3_attn_out_dp0_mb7_gpu1 -> layer3_attn_allreduce_dp0_mb7
	layer3_attn_out_dp1_mb7_gpu1 -> layer3_attn_allreduce_dp1_mb7
	layer3_attn_out_dp0_mb7_gpu2 -> layer3_attn_allreduce_dp0_mb7
	layer3_attn_out_dp1_mb7_gpu2 -> layer3_attn_allreduce_dp1_mb7
	layer3_attn_out_dp0_mb7_gpu3 -> layer3_attn_allreduce_dp0_mb7
	layer3_attn_out_dp1_mb7_gpu3 -> layer3_attn_allreduce_dp1_mb7
	layer3_router_dp0_mb7_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_router_dp1_mb7_gpu0 [label="Layer 3 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer3_attn_allreduce_dp0_mb7 -> layer3_router_dp0_mb7_gpu0
	layer3_attn_allreduce_dp1_mb7 -> layer3_router_dp1_mb7_gpu0
	layer3_expert0_mlp1_dp0_mb7_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp1_mb7_gpu0 [label="Layer 3 Expert 0 MLP1\nGPU 0\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb7_gpu0 -> layer3_expert0_mlp1_dp0_mb7_gpu0
	layer3_router_dp1_mb7_gpu0 -> layer3_expert0_mlp1_dp1_mb7_gpu0
	layer3_expert0_mlp2_dp0_mb7_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp2_dp1_mb7_gpu0 [label="Layer 3 Expert 0 MLP2\nGPU 0\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert0_mlp1_dp0_mb7_gpu0 -> layer3_expert0_mlp2_dp0_mb7_gpu0
	layer3_expert0_mlp1_dp1_mb7_gpu0 -> layer3_expert0_mlp2_dp1_mb7_gpu0
	layer3_expert1_mlp1_dp0_mb7_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp1_mb7_gpu1 [label="Layer 3 Expert 1 MLP1\nGPU 1\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb7_gpu0 -> layer3_expert1_mlp1_dp0_mb7_gpu1
	layer3_router_dp1_mb7_gpu0 -> layer3_expert1_mlp1_dp1_mb7_gpu1
	layer3_expert1_mlp2_dp0_mb7_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp2_dp1_mb7_gpu1 [label="Layer 3 Expert 1 MLP2\nGPU 1\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert1_mlp1_dp0_mb7_gpu1 -> layer3_expert1_mlp2_dp0_mb7_gpu1
	layer3_expert1_mlp1_dp1_mb7_gpu1 -> layer3_expert1_mlp2_dp1_mb7_gpu1
	layer3_expert2_mlp1_dp0_mb7_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp1_mb7_gpu2 [label="Layer 3 Expert 2 MLP1\nGPU 2\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb7_gpu0 -> layer3_expert2_mlp1_dp0_mb7_gpu2
	layer3_router_dp1_mb7_gpu0 -> layer3_expert2_mlp1_dp1_mb7_gpu2
	layer3_expert2_mlp2_dp0_mb7_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp2_dp1_mb7_gpu2 [label="Layer 3 Expert 2 MLP2\nGPU 2\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert2_mlp1_dp0_mb7_gpu2 -> layer3_expert2_mlp2_dp0_mb7_gpu2
	layer3_expert2_mlp1_dp1_mb7_gpu2 -> layer3_expert2_mlp2_dp1_mb7_gpu2
	layer3_expert3_mlp1_dp0_mb7_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp1_mb7_gpu3 [label="Layer 3 Expert 3 MLP1\nGPU 3\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_router_dp0_mb7_gpu0 -> layer3_expert3_mlp1_dp0_mb7_gpu3
	layer3_router_dp1_mb7_gpu0 -> layer3_expert3_mlp1_dp1_mb7_gpu3
	layer3_expert3_mlp2_dp0_mb7_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp2_dp1_mb7_gpu3 [label="Layer 3 Expert 3 MLP2\nGPU 3\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFE5CC" fontsize=9 style=filled]
	layer3_expert3_mlp1_dp0_mb7_gpu3 -> layer3_expert3_mlp2_dp0_mb7_gpu3
	layer3_expert3_mlp1_dp1_mb7_gpu3 -> layer3_expert3_mlp2_dp1_mb7_gpu3
	layer3_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer3_expert0_mlp2_dp0_mb7_gpu0 -> layer3_expert_agg_dp0_mb7
	layer3_expert0_mlp2_dp1_mb7_gpu0 -> layer3_expert_agg_dp1_mb7
	layer3_expert1_mlp2_dp0_mb7_gpu1 -> layer3_expert_agg_dp0_mb7
	layer3_expert1_mlp2_dp1_mb7_gpu1 -> layer3_expert_agg_dp1_mb7
	layer3_expert2_mlp2_dp0_mb7_gpu2 -> layer3_expert_agg_dp0_mb7
	layer3_expert2_mlp2_dp1_mb7_gpu2 -> layer3_expert_agg_dp1_mb7
	layer3_expert3_mlp2_dp0_mb7_gpu3 -> layer3_expert_agg_dp0_mb7
	layer3_expert3_mlp2_dp1_mb7_gpu3 -> layer3_expert_agg_dp1_mb7
	layer4_qkv_dp0_mb0_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb0_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb0 -> layer4_qkv_dp0_mb0_gpu4
	layer3_expert_agg_dp1_mb0 -> layer4_qkv_dp1_mb0_gpu4
	layer4_attn_scores_dp0_mb0_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb0_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb0_gpu4 -> layer4_attn_scores_dp0_mb0_gpu4
	layer4_qkv_dp1_mb0_gpu4 -> layer4_attn_scores_dp1_mb0_gpu4
	layer4_softmax_dp0_mb0_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb0_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb0_gpu4 -> layer4_softmax_dp0_mb0_gpu4
	layer4_attn_scores_dp1_mb0_gpu4 -> layer4_softmax_dp1_mb0_gpu4
	layer4_attn_out_dp0_mb0_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb0_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb0_gpu4 -> layer4_attn_out_dp0_mb0_gpu4
	layer4_softmax_dp1_mb0_gpu4 -> layer4_attn_out_dp1_mb0_gpu4
	layer4_qkv_dp0_mb0_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb0_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb0 -> layer4_qkv_dp0_mb0_gpu5
	layer3_expert_agg_dp1_mb0 -> layer4_qkv_dp1_mb0_gpu5
	layer4_attn_scores_dp0_mb0_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb0_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb0_gpu5 -> layer4_attn_scores_dp0_mb0_gpu5
	layer4_qkv_dp1_mb0_gpu5 -> layer4_attn_scores_dp1_mb0_gpu5
	layer4_softmax_dp0_mb0_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb0_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb0_gpu5 -> layer4_softmax_dp0_mb0_gpu5
	layer4_attn_scores_dp1_mb0_gpu5 -> layer4_softmax_dp1_mb0_gpu5
	layer4_attn_out_dp0_mb0_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb0_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb0_gpu5 -> layer4_attn_out_dp0_mb0_gpu5
	layer4_softmax_dp1_mb0_gpu5 -> layer4_attn_out_dp1_mb0_gpu5
	layer4_qkv_dp0_mb0_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb0_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb0 -> layer4_qkv_dp0_mb0_gpu6
	layer3_expert_agg_dp1_mb0 -> layer4_qkv_dp1_mb0_gpu6
	layer4_attn_scores_dp0_mb0_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb0_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb0_gpu6 -> layer4_attn_scores_dp0_mb0_gpu6
	layer4_qkv_dp1_mb0_gpu6 -> layer4_attn_scores_dp1_mb0_gpu6
	layer4_softmax_dp0_mb0_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb0_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb0_gpu6 -> layer4_softmax_dp0_mb0_gpu6
	layer4_attn_scores_dp1_mb0_gpu6 -> layer4_softmax_dp1_mb0_gpu6
	layer4_attn_out_dp0_mb0_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb0_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb0_gpu6 -> layer4_attn_out_dp0_mb0_gpu6
	layer4_softmax_dp1_mb0_gpu6 -> layer4_attn_out_dp1_mb0_gpu6
	layer4_qkv_dp0_mb0_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb0_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb0 -> layer4_qkv_dp0_mb0_gpu7
	layer3_expert_agg_dp1_mb0 -> layer4_qkv_dp1_mb0_gpu7
	layer4_attn_scores_dp0_mb0_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb0_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb0_gpu7 -> layer4_attn_scores_dp0_mb0_gpu7
	layer4_qkv_dp1_mb0_gpu7 -> layer4_attn_scores_dp1_mb0_gpu7
	layer4_softmax_dp0_mb0_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb0_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb0_gpu7 -> layer4_softmax_dp0_mb0_gpu7
	layer4_attn_scores_dp1_mb0_gpu7 -> layer4_softmax_dp1_mb0_gpu7
	layer4_attn_out_dp0_mb0_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb0_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb0_gpu7 -> layer4_attn_out_dp0_mb0_gpu7
	layer4_softmax_dp1_mb0_gpu7 -> layer4_attn_out_dp1_mb0_gpu7
	layer4_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb0_gpu4 -> layer4_attn_allreduce_dp0_mb0
	layer4_attn_out_dp1_mb0_gpu4 -> layer4_attn_allreduce_dp1_mb0
	layer4_attn_out_dp0_mb0_gpu5 -> layer4_attn_allreduce_dp0_mb0
	layer4_attn_out_dp1_mb0_gpu5 -> layer4_attn_allreduce_dp1_mb0
	layer4_attn_out_dp0_mb0_gpu6 -> layer4_attn_allreduce_dp0_mb0
	layer4_attn_out_dp1_mb0_gpu6 -> layer4_attn_allreduce_dp1_mb0
	layer4_attn_out_dp0_mb0_gpu7 -> layer4_attn_allreduce_dp0_mb0
	layer4_attn_out_dp1_mb0_gpu7 -> layer4_attn_allreduce_dp1_mb0
	layer4_router_dp0_mb0_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb0_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb0 -> layer4_router_dp0_mb0_gpu4
	layer4_attn_allreduce_dp1_mb0 -> layer4_router_dp1_mb0_gpu4
	layer4_expert0_mlp1_dp0_mb0_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb0_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb0_gpu4 -> layer4_expert0_mlp1_dp0_mb0_gpu4
	layer4_router_dp1_mb0_gpu4 -> layer4_expert0_mlp1_dp1_mb0_gpu4
	layer4_expert0_mlp2_dp0_mb0_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb0_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb0_gpu4 -> layer4_expert0_mlp2_dp0_mb0_gpu4
	layer4_expert0_mlp1_dp1_mb0_gpu4 -> layer4_expert0_mlp2_dp1_mb0_gpu4
	layer4_expert1_mlp1_dp0_mb0_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb0_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb0_gpu4 -> layer4_expert1_mlp1_dp0_mb0_gpu5
	layer4_router_dp1_mb0_gpu4 -> layer4_expert1_mlp1_dp1_mb0_gpu5
	layer4_expert1_mlp2_dp0_mb0_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb0_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb0_gpu5 -> layer4_expert1_mlp2_dp0_mb0_gpu5
	layer4_expert1_mlp1_dp1_mb0_gpu5 -> layer4_expert1_mlp2_dp1_mb0_gpu5
	layer4_expert2_mlp1_dp0_mb0_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb0_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb0_gpu4 -> layer4_expert2_mlp1_dp0_mb0_gpu6
	layer4_router_dp1_mb0_gpu4 -> layer4_expert2_mlp1_dp1_mb0_gpu6
	layer4_expert2_mlp2_dp0_mb0_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb0_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb0_gpu6 -> layer4_expert2_mlp2_dp0_mb0_gpu6
	layer4_expert2_mlp1_dp1_mb0_gpu6 -> layer4_expert2_mlp2_dp1_mb0_gpu6
	layer4_expert3_mlp1_dp0_mb0_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb0_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb0_gpu4 -> layer4_expert3_mlp1_dp0_mb0_gpu7
	layer4_router_dp1_mb0_gpu4 -> layer4_expert3_mlp1_dp1_mb0_gpu7
	layer4_expert3_mlp2_dp0_mb0_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb0_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb0_gpu7 -> layer4_expert3_mlp2_dp0_mb0_gpu7
	layer4_expert3_mlp1_dp1_mb0_gpu7 -> layer4_expert3_mlp2_dp1_mb0_gpu7
	layer4_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb0_gpu4 -> layer4_expert_agg_dp0_mb0
	layer4_expert0_mlp2_dp1_mb0_gpu4 -> layer4_expert_agg_dp1_mb0
	layer4_expert1_mlp2_dp0_mb0_gpu5 -> layer4_expert_agg_dp0_mb0
	layer4_expert1_mlp2_dp1_mb0_gpu5 -> layer4_expert_agg_dp1_mb0
	layer4_expert2_mlp2_dp0_mb0_gpu6 -> layer4_expert_agg_dp0_mb0
	layer4_expert2_mlp2_dp1_mb0_gpu6 -> layer4_expert_agg_dp1_mb0
	layer4_expert3_mlp2_dp0_mb0_gpu7 -> layer4_expert_agg_dp0_mb0
	layer4_expert3_mlp2_dp1_mb0_gpu7 -> layer4_expert_agg_dp1_mb0
	layer5_qkv_dp0_mb0_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb0_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb0 -> layer5_qkv_dp0_mb0_gpu4
	layer4_expert_agg_dp1_mb0 -> layer5_qkv_dp1_mb0_gpu4
	layer5_attn_scores_dp0_mb0_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb0_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb0_gpu4 -> layer5_attn_scores_dp0_mb0_gpu4
	layer5_qkv_dp1_mb0_gpu4 -> layer5_attn_scores_dp1_mb0_gpu4
	layer5_softmax_dp0_mb0_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb0_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb0_gpu4 -> layer5_softmax_dp0_mb0_gpu4
	layer5_attn_scores_dp1_mb0_gpu4 -> layer5_softmax_dp1_mb0_gpu4
	layer5_attn_out_dp0_mb0_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb0_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb0_gpu4 -> layer5_attn_out_dp0_mb0_gpu4
	layer5_softmax_dp1_mb0_gpu4 -> layer5_attn_out_dp1_mb0_gpu4
	layer5_qkv_dp0_mb0_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb0_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb0 -> layer5_qkv_dp0_mb0_gpu5
	layer4_expert_agg_dp1_mb0 -> layer5_qkv_dp1_mb0_gpu5
	layer5_attn_scores_dp0_mb0_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb0_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb0_gpu5 -> layer5_attn_scores_dp0_mb0_gpu5
	layer5_qkv_dp1_mb0_gpu5 -> layer5_attn_scores_dp1_mb0_gpu5
	layer5_softmax_dp0_mb0_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb0_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb0_gpu5 -> layer5_softmax_dp0_mb0_gpu5
	layer5_attn_scores_dp1_mb0_gpu5 -> layer5_softmax_dp1_mb0_gpu5
	layer5_attn_out_dp0_mb0_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb0_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb0_gpu5 -> layer5_attn_out_dp0_mb0_gpu5
	layer5_softmax_dp1_mb0_gpu5 -> layer5_attn_out_dp1_mb0_gpu5
	layer5_qkv_dp0_mb0_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb0_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb0 -> layer5_qkv_dp0_mb0_gpu6
	layer4_expert_agg_dp1_mb0 -> layer5_qkv_dp1_mb0_gpu6
	layer5_attn_scores_dp0_mb0_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb0_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb0_gpu6 -> layer5_attn_scores_dp0_mb0_gpu6
	layer5_qkv_dp1_mb0_gpu6 -> layer5_attn_scores_dp1_mb0_gpu6
	layer5_softmax_dp0_mb0_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb0_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb0_gpu6 -> layer5_softmax_dp0_mb0_gpu6
	layer5_attn_scores_dp1_mb0_gpu6 -> layer5_softmax_dp1_mb0_gpu6
	layer5_attn_out_dp0_mb0_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb0_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb0_gpu6 -> layer5_attn_out_dp0_mb0_gpu6
	layer5_softmax_dp1_mb0_gpu6 -> layer5_attn_out_dp1_mb0_gpu6
	layer5_qkv_dp0_mb0_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb0_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb0 -> layer5_qkv_dp0_mb0_gpu7
	layer4_expert_agg_dp1_mb0 -> layer5_qkv_dp1_mb0_gpu7
	layer5_attn_scores_dp0_mb0_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb0_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb0_gpu7 -> layer5_attn_scores_dp0_mb0_gpu7
	layer5_qkv_dp1_mb0_gpu7 -> layer5_attn_scores_dp1_mb0_gpu7
	layer5_softmax_dp0_mb0_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb0_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb0_gpu7 -> layer5_softmax_dp0_mb0_gpu7
	layer5_attn_scores_dp1_mb0_gpu7 -> layer5_softmax_dp1_mb0_gpu7
	layer5_attn_out_dp0_mb0_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb0_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb0_gpu7 -> layer5_attn_out_dp0_mb0_gpu7
	layer5_softmax_dp1_mb0_gpu7 -> layer5_attn_out_dp1_mb0_gpu7
	layer5_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb0_gpu4 -> layer5_attn_allreduce_dp0_mb0
	layer5_attn_out_dp1_mb0_gpu4 -> layer5_attn_allreduce_dp1_mb0
	layer5_attn_out_dp0_mb0_gpu5 -> layer5_attn_allreduce_dp0_mb0
	layer5_attn_out_dp1_mb0_gpu5 -> layer5_attn_allreduce_dp1_mb0
	layer5_attn_out_dp0_mb0_gpu6 -> layer5_attn_allreduce_dp0_mb0
	layer5_attn_out_dp1_mb0_gpu6 -> layer5_attn_allreduce_dp1_mb0
	layer5_attn_out_dp0_mb0_gpu7 -> layer5_attn_allreduce_dp0_mb0
	layer5_attn_out_dp1_mb0_gpu7 -> layer5_attn_allreduce_dp1_mb0
	layer5_router_dp0_mb0_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb0_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb0 -> layer5_router_dp0_mb0_gpu4
	layer5_attn_allreduce_dp1_mb0 -> layer5_router_dp1_mb0_gpu4
	layer5_expert0_mlp1_dp0_mb0_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb0_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb0_gpu4 -> layer5_expert0_mlp1_dp0_mb0_gpu4
	layer5_router_dp1_mb0_gpu4 -> layer5_expert0_mlp1_dp1_mb0_gpu4
	layer5_expert0_mlp2_dp0_mb0_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb0_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb0_gpu4 -> layer5_expert0_mlp2_dp0_mb0_gpu4
	layer5_expert0_mlp1_dp1_mb0_gpu4 -> layer5_expert0_mlp2_dp1_mb0_gpu4
	layer5_expert1_mlp1_dp0_mb0_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb0_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb0_gpu4 -> layer5_expert1_mlp1_dp0_mb0_gpu5
	layer5_router_dp1_mb0_gpu4 -> layer5_expert1_mlp1_dp1_mb0_gpu5
	layer5_expert1_mlp2_dp0_mb0_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb0_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb0_gpu5 -> layer5_expert1_mlp2_dp0_mb0_gpu5
	layer5_expert1_mlp1_dp1_mb0_gpu5 -> layer5_expert1_mlp2_dp1_mb0_gpu5
	layer5_expert2_mlp1_dp0_mb0_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb0_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb0_gpu4 -> layer5_expert2_mlp1_dp0_mb0_gpu6
	layer5_router_dp1_mb0_gpu4 -> layer5_expert2_mlp1_dp1_mb0_gpu6
	layer5_expert2_mlp2_dp0_mb0_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb0_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb0_gpu6 -> layer5_expert2_mlp2_dp0_mb0_gpu6
	layer5_expert2_mlp1_dp1_mb0_gpu6 -> layer5_expert2_mlp2_dp1_mb0_gpu6
	layer5_expert3_mlp1_dp0_mb0_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb0_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb0_gpu4 -> layer5_expert3_mlp1_dp0_mb0_gpu7
	layer5_router_dp1_mb0_gpu4 -> layer5_expert3_mlp1_dp1_mb0_gpu7
	layer5_expert3_mlp2_dp0_mb0_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb0_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb0_gpu7 -> layer5_expert3_mlp2_dp0_mb0_gpu7
	layer5_expert3_mlp1_dp1_mb0_gpu7 -> layer5_expert3_mlp2_dp1_mb0_gpu7
	layer5_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb0_gpu4 -> layer5_expert_agg_dp0_mb0
	layer5_expert0_mlp2_dp1_mb0_gpu4 -> layer5_expert_agg_dp1_mb0
	layer5_expert1_mlp2_dp0_mb0_gpu5 -> layer5_expert_agg_dp0_mb0
	layer5_expert1_mlp2_dp1_mb0_gpu5 -> layer5_expert_agg_dp1_mb0
	layer5_expert2_mlp2_dp0_mb0_gpu6 -> layer5_expert_agg_dp0_mb0
	layer5_expert2_mlp2_dp1_mb0_gpu6 -> layer5_expert_agg_dp1_mb0
	layer5_expert3_mlp2_dp0_mb0_gpu7 -> layer5_expert_agg_dp0_mb0
	layer5_expert3_mlp2_dp1_mb0_gpu7 -> layer5_expert_agg_dp1_mb0
	layer6_qkv_dp0_mb0_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb0_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb0 -> layer6_qkv_dp0_mb0_gpu4
	layer5_expert_agg_dp1_mb0 -> layer6_qkv_dp1_mb0_gpu4
	layer6_attn_scores_dp0_mb0_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb0_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb0_gpu4 -> layer6_attn_scores_dp0_mb0_gpu4
	layer6_qkv_dp1_mb0_gpu4 -> layer6_attn_scores_dp1_mb0_gpu4
	layer6_softmax_dp0_mb0_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb0_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb0_gpu4 -> layer6_softmax_dp0_mb0_gpu4
	layer6_attn_scores_dp1_mb0_gpu4 -> layer6_softmax_dp1_mb0_gpu4
	layer6_attn_out_dp0_mb0_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb0_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb0_gpu4 -> layer6_attn_out_dp0_mb0_gpu4
	layer6_softmax_dp1_mb0_gpu4 -> layer6_attn_out_dp1_mb0_gpu4
	layer6_qkv_dp0_mb0_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb0_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb0 -> layer6_qkv_dp0_mb0_gpu5
	layer5_expert_agg_dp1_mb0 -> layer6_qkv_dp1_mb0_gpu5
	layer6_attn_scores_dp0_mb0_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb0_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb0_gpu5 -> layer6_attn_scores_dp0_mb0_gpu5
	layer6_qkv_dp1_mb0_gpu5 -> layer6_attn_scores_dp1_mb0_gpu5
	layer6_softmax_dp0_mb0_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb0_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb0_gpu5 -> layer6_softmax_dp0_mb0_gpu5
	layer6_attn_scores_dp1_mb0_gpu5 -> layer6_softmax_dp1_mb0_gpu5
	layer6_attn_out_dp0_mb0_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb0_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb0_gpu5 -> layer6_attn_out_dp0_mb0_gpu5
	layer6_softmax_dp1_mb0_gpu5 -> layer6_attn_out_dp1_mb0_gpu5
	layer6_qkv_dp0_mb0_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb0_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb0 -> layer6_qkv_dp0_mb0_gpu6
	layer5_expert_agg_dp1_mb0 -> layer6_qkv_dp1_mb0_gpu6
	layer6_attn_scores_dp0_mb0_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb0_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb0_gpu6 -> layer6_attn_scores_dp0_mb0_gpu6
	layer6_qkv_dp1_mb0_gpu6 -> layer6_attn_scores_dp1_mb0_gpu6
	layer6_softmax_dp0_mb0_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb0_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb0_gpu6 -> layer6_softmax_dp0_mb0_gpu6
	layer6_attn_scores_dp1_mb0_gpu6 -> layer6_softmax_dp1_mb0_gpu6
	layer6_attn_out_dp0_mb0_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb0_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb0_gpu6 -> layer6_attn_out_dp0_mb0_gpu6
	layer6_softmax_dp1_mb0_gpu6 -> layer6_attn_out_dp1_mb0_gpu6
	layer6_qkv_dp0_mb0_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb0_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb0 -> layer6_qkv_dp0_mb0_gpu7
	layer5_expert_agg_dp1_mb0 -> layer6_qkv_dp1_mb0_gpu7
	layer6_attn_scores_dp0_mb0_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb0_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb0_gpu7 -> layer6_attn_scores_dp0_mb0_gpu7
	layer6_qkv_dp1_mb0_gpu7 -> layer6_attn_scores_dp1_mb0_gpu7
	layer6_softmax_dp0_mb0_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb0_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb0_gpu7 -> layer6_softmax_dp0_mb0_gpu7
	layer6_attn_scores_dp1_mb0_gpu7 -> layer6_softmax_dp1_mb0_gpu7
	layer6_attn_out_dp0_mb0_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb0_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb0_gpu7 -> layer6_attn_out_dp0_mb0_gpu7
	layer6_softmax_dp1_mb0_gpu7 -> layer6_attn_out_dp1_mb0_gpu7
	layer6_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb0_gpu4 -> layer6_attn_allreduce_dp0_mb0
	layer6_attn_out_dp1_mb0_gpu4 -> layer6_attn_allreduce_dp1_mb0
	layer6_attn_out_dp0_mb0_gpu5 -> layer6_attn_allreduce_dp0_mb0
	layer6_attn_out_dp1_mb0_gpu5 -> layer6_attn_allreduce_dp1_mb0
	layer6_attn_out_dp0_mb0_gpu6 -> layer6_attn_allreduce_dp0_mb0
	layer6_attn_out_dp1_mb0_gpu6 -> layer6_attn_allreduce_dp1_mb0
	layer6_attn_out_dp0_mb0_gpu7 -> layer6_attn_allreduce_dp0_mb0
	layer6_attn_out_dp1_mb0_gpu7 -> layer6_attn_allreduce_dp1_mb0
	layer6_router_dp0_mb0_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb0_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb0 -> layer6_router_dp0_mb0_gpu4
	layer6_attn_allreduce_dp1_mb0 -> layer6_router_dp1_mb0_gpu4
	layer6_expert0_mlp1_dp0_mb0_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb0_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb0_gpu4 -> layer6_expert0_mlp1_dp0_mb0_gpu4
	layer6_router_dp1_mb0_gpu4 -> layer6_expert0_mlp1_dp1_mb0_gpu4
	layer6_expert0_mlp2_dp0_mb0_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb0_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb0_gpu4 -> layer6_expert0_mlp2_dp0_mb0_gpu4
	layer6_expert0_mlp1_dp1_mb0_gpu4 -> layer6_expert0_mlp2_dp1_mb0_gpu4
	layer6_expert1_mlp1_dp0_mb0_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb0_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb0_gpu4 -> layer6_expert1_mlp1_dp0_mb0_gpu5
	layer6_router_dp1_mb0_gpu4 -> layer6_expert1_mlp1_dp1_mb0_gpu5
	layer6_expert1_mlp2_dp0_mb0_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb0_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb0_gpu5 -> layer6_expert1_mlp2_dp0_mb0_gpu5
	layer6_expert1_mlp1_dp1_mb0_gpu5 -> layer6_expert1_mlp2_dp1_mb0_gpu5
	layer6_expert2_mlp1_dp0_mb0_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb0_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb0_gpu4 -> layer6_expert2_mlp1_dp0_mb0_gpu6
	layer6_router_dp1_mb0_gpu4 -> layer6_expert2_mlp1_dp1_mb0_gpu6
	layer6_expert2_mlp2_dp0_mb0_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb0_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb0_gpu6 -> layer6_expert2_mlp2_dp0_mb0_gpu6
	layer6_expert2_mlp1_dp1_mb0_gpu6 -> layer6_expert2_mlp2_dp1_mb0_gpu6
	layer6_expert3_mlp1_dp0_mb0_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb0_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb0_gpu4 -> layer6_expert3_mlp1_dp0_mb0_gpu7
	layer6_router_dp1_mb0_gpu4 -> layer6_expert3_mlp1_dp1_mb0_gpu7
	layer6_expert3_mlp2_dp0_mb0_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb0_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb0_gpu7 -> layer6_expert3_mlp2_dp0_mb0_gpu7
	layer6_expert3_mlp1_dp1_mb0_gpu7 -> layer6_expert3_mlp2_dp1_mb0_gpu7
	layer6_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb0_gpu4 -> layer6_expert_agg_dp0_mb0
	layer6_expert0_mlp2_dp1_mb0_gpu4 -> layer6_expert_agg_dp1_mb0
	layer6_expert1_mlp2_dp0_mb0_gpu5 -> layer6_expert_agg_dp0_mb0
	layer6_expert1_mlp2_dp1_mb0_gpu5 -> layer6_expert_agg_dp1_mb0
	layer6_expert2_mlp2_dp0_mb0_gpu6 -> layer6_expert_agg_dp0_mb0
	layer6_expert2_mlp2_dp1_mb0_gpu6 -> layer6_expert_agg_dp1_mb0
	layer6_expert3_mlp2_dp0_mb0_gpu7 -> layer6_expert_agg_dp0_mb0
	layer6_expert3_mlp2_dp1_mb0_gpu7 -> layer6_expert_agg_dp1_mb0
	layer7_qkv_dp0_mb0_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb0_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb0 -> layer7_qkv_dp0_mb0_gpu4
	layer6_expert_agg_dp1_mb0 -> layer7_qkv_dp1_mb0_gpu4
	layer7_attn_scores_dp0_mb0_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb0_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb0_gpu4 -> layer7_attn_scores_dp0_mb0_gpu4
	layer7_qkv_dp1_mb0_gpu4 -> layer7_attn_scores_dp1_mb0_gpu4
	layer7_softmax_dp0_mb0_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb0_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb0_gpu4 -> layer7_softmax_dp0_mb0_gpu4
	layer7_attn_scores_dp1_mb0_gpu4 -> layer7_softmax_dp1_mb0_gpu4
	layer7_attn_out_dp0_mb0_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb0_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb0_gpu4 -> layer7_attn_out_dp0_mb0_gpu4
	layer7_softmax_dp1_mb0_gpu4 -> layer7_attn_out_dp1_mb0_gpu4
	layer7_qkv_dp0_mb0_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb0_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb0 -> layer7_qkv_dp0_mb0_gpu5
	layer6_expert_agg_dp1_mb0 -> layer7_qkv_dp1_mb0_gpu5
	layer7_attn_scores_dp0_mb0_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb0_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb0_gpu5 -> layer7_attn_scores_dp0_mb0_gpu5
	layer7_qkv_dp1_mb0_gpu5 -> layer7_attn_scores_dp1_mb0_gpu5
	layer7_softmax_dp0_mb0_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb0_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb0_gpu5 -> layer7_softmax_dp0_mb0_gpu5
	layer7_attn_scores_dp1_mb0_gpu5 -> layer7_softmax_dp1_mb0_gpu5
	layer7_attn_out_dp0_mb0_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb0_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb0_gpu5 -> layer7_attn_out_dp0_mb0_gpu5
	layer7_softmax_dp1_mb0_gpu5 -> layer7_attn_out_dp1_mb0_gpu5
	layer7_qkv_dp0_mb0_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb0_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb0 -> layer7_qkv_dp0_mb0_gpu6
	layer6_expert_agg_dp1_mb0 -> layer7_qkv_dp1_mb0_gpu6
	layer7_attn_scores_dp0_mb0_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb0_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb0_gpu6 -> layer7_attn_scores_dp0_mb0_gpu6
	layer7_qkv_dp1_mb0_gpu6 -> layer7_attn_scores_dp1_mb0_gpu6
	layer7_softmax_dp0_mb0_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb0_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb0_gpu6 -> layer7_softmax_dp0_mb0_gpu6
	layer7_attn_scores_dp1_mb0_gpu6 -> layer7_softmax_dp1_mb0_gpu6
	layer7_attn_out_dp0_mb0_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb0_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb0_gpu6 -> layer7_attn_out_dp0_mb0_gpu6
	layer7_softmax_dp1_mb0_gpu6 -> layer7_attn_out_dp1_mb0_gpu6
	layer7_qkv_dp0_mb0_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb0_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb0 -> layer7_qkv_dp0_mb0_gpu7
	layer6_expert_agg_dp1_mb0 -> layer7_qkv_dp1_mb0_gpu7
	layer7_attn_scores_dp0_mb0_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb0_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb0_gpu7 -> layer7_attn_scores_dp0_mb0_gpu7
	layer7_qkv_dp1_mb0_gpu7 -> layer7_attn_scores_dp1_mb0_gpu7
	layer7_softmax_dp0_mb0_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb0_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb0_gpu7 -> layer7_softmax_dp0_mb0_gpu7
	layer7_attn_scores_dp1_mb0_gpu7 -> layer7_softmax_dp1_mb0_gpu7
	layer7_attn_out_dp0_mb0_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb0_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb0_gpu7 -> layer7_attn_out_dp0_mb0_gpu7
	layer7_softmax_dp1_mb0_gpu7 -> layer7_attn_out_dp1_mb0_gpu7
	layer7_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb0_gpu4 -> layer7_attn_allreduce_dp0_mb0
	layer7_attn_out_dp1_mb0_gpu4 -> layer7_attn_allreduce_dp1_mb0
	layer7_attn_out_dp0_mb0_gpu5 -> layer7_attn_allreduce_dp0_mb0
	layer7_attn_out_dp1_mb0_gpu5 -> layer7_attn_allreduce_dp1_mb0
	layer7_attn_out_dp0_mb0_gpu6 -> layer7_attn_allreduce_dp0_mb0
	layer7_attn_out_dp1_mb0_gpu6 -> layer7_attn_allreduce_dp1_mb0
	layer7_attn_out_dp0_mb0_gpu7 -> layer7_attn_allreduce_dp0_mb0
	layer7_attn_out_dp1_mb0_gpu7 -> layer7_attn_allreduce_dp1_mb0
	layer7_router_dp0_mb0_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb0_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb0 -> layer7_router_dp0_mb0_gpu4
	layer7_attn_allreduce_dp1_mb0 -> layer7_router_dp1_mb0_gpu4
	layer7_expert0_mlp1_dp0_mb0_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb0_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb0_gpu4 -> layer7_expert0_mlp1_dp0_mb0_gpu4
	layer7_router_dp1_mb0_gpu4 -> layer7_expert0_mlp1_dp1_mb0_gpu4
	layer7_expert0_mlp2_dp0_mb0_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb0_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb0_gpu4 -> layer7_expert0_mlp2_dp0_mb0_gpu4
	layer7_expert0_mlp1_dp1_mb0_gpu4 -> layer7_expert0_mlp2_dp1_mb0_gpu4
	layer7_expert1_mlp1_dp0_mb0_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb0_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb0_gpu4 -> layer7_expert1_mlp1_dp0_mb0_gpu5
	layer7_router_dp1_mb0_gpu4 -> layer7_expert1_mlp1_dp1_mb0_gpu5
	layer7_expert1_mlp2_dp0_mb0_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb0_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb0_gpu5 -> layer7_expert1_mlp2_dp0_mb0_gpu5
	layer7_expert1_mlp1_dp1_mb0_gpu5 -> layer7_expert1_mlp2_dp1_mb0_gpu5
	layer7_expert2_mlp1_dp0_mb0_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb0_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb0_gpu4 -> layer7_expert2_mlp1_dp0_mb0_gpu6
	layer7_router_dp1_mb0_gpu4 -> layer7_expert2_mlp1_dp1_mb0_gpu6
	layer7_expert2_mlp2_dp0_mb0_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb0_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb0_gpu6 -> layer7_expert2_mlp2_dp0_mb0_gpu6
	layer7_expert2_mlp1_dp1_mb0_gpu6 -> layer7_expert2_mlp2_dp1_mb0_gpu6
	layer7_expert3_mlp1_dp0_mb0_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb0_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb0_gpu4 -> layer7_expert3_mlp1_dp0_mb0_gpu7
	layer7_router_dp1_mb0_gpu4 -> layer7_expert3_mlp1_dp1_mb0_gpu7
	layer7_expert3_mlp2_dp0_mb0_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb0_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb0_gpu7 -> layer7_expert3_mlp2_dp0_mb0_gpu7
	layer7_expert3_mlp1_dp1_mb0_gpu7 -> layer7_expert3_mlp2_dp1_mb0_gpu7
	layer7_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb0_gpu4 -> layer7_expert_agg_dp0_mb0
	layer7_expert0_mlp2_dp1_mb0_gpu4 -> layer7_expert_agg_dp1_mb0
	layer7_expert1_mlp2_dp0_mb0_gpu5 -> layer7_expert_agg_dp0_mb0
	layer7_expert1_mlp2_dp1_mb0_gpu5 -> layer7_expert_agg_dp1_mb0
	layer7_expert2_mlp2_dp0_mb0_gpu6 -> layer7_expert_agg_dp0_mb0
	layer7_expert2_mlp2_dp1_mb0_gpu6 -> layer7_expert_agg_dp1_mb0
	layer7_expert3_mlp2_dp0_mb0_gpu7 -> layer7_expert_agg_dp0_mb0
	layer7_expert3_mlp2_dp1_mb0_gpu7 -> layer7_expert_agg_dp1_mb0
	layer4_qkv_dp0_mb1_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb1_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb1 -> layer4_qkv_dp0_mb1_gpu4
	layer3_expert_agg_dp1_mb1 -> layer4_qkv_dp1_mb1_gpu4
	layer4_attn_scores_dp0_mb1_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb1_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb1_gpu4 -> layer4_attn_scores_dp0_mb1_gpu4
	layer4_qkv_dp1_mb1_gpu4 -> layer4_attn_scores_dp1_mb1_gpu4
	layer4_softmax_dp0_mb1_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb1_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb1_gpu4 -> layer4_softmax_dp0_mb1_gpu4
	layer4_attn_scores_dp1_mb1_gpu4 -> layer4_softmax_dp1_mb1_gpu4
	layer4_attn_out_dp0_mb1_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb1_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb1_gpu4 -> layer4_attn_out_dp0_mb1_gpu4
	layer4_softmax_dp1_mb1_gpu4 -> layer4_attn_out_dp1_mb1_gpu4
	layer4_qkv_dp0_mb1_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb1_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb1 -> layer4_qkv_dp0_mb1_gpu5
	layer3_expert_agg_dp1_mb1 -> layer4_qkv_dp1_mb1_gpu5
	layer4_attn_scores_dp0_mb1_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb1_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb1_gpu5 -> layer4_attn_scores_dp0_mb1_gpu5
	layer4_qkv_dp1_mb1_gpu5 -> layer4_attn_scores_dp1_mb1_gpu5
	layer4_softmax_dp0_mb1_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb1_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb1_gpu5 -> layer4_softmax_dp0_mb1_gpu5
	layer4_attn_scores_dp1_mb1_gpu5 -> layer4_softmax_dp1_mb1_gpu5
	layer4_attn_out_dp0_mb1_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb1_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb1_gpu5 -> layer4_attn_out_dp0_mb1_gpu5
	layer4_softmax_dp1_mb1_gpu5 -> layer4_attn_out_dp1_mb1_gpu5
	layer4_qkv_dp0_mb1_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb1_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb1 -> layer4_qkv_dp0_mb1_gpu6
	layer3_expert_agg_dp1_mb1 -> layer4_qkv_dp1_mb1_gpu6
	layer4_attn_scores_dp0_mb1_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb1_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb1_gpu6 -> layer4_attn_scores_dp0_mb1_gpu6
	layer4_qkv_dp1_mb1_gpu6 -> layer4_attn_scores_dp1_mb1_gpu6
	layer4_softmax_dp0_mb1_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb1_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb1_gpu6 -> layer4_softmax_dp0_mb1_gpu6
	layer4_attn_scores_dp1_mb1_gpu6 -> layer4_softmax_dp1_mb1_gpu6
	layer4_attn_out_dp0_mb1_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb1_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb1_gpu6 -> layer4_attn_out_dp0_mb1_gpu6
	layer4_softmax_dp1_mb1_gpu6 -> layer4_attn_out_dp1_mb1_gpu6
	layer4_qkv_dp0_mb1_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb1_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb1 -> layer4_qkv_dp0_mb1_gpu7
	layer3_expert_agg_dp1_mb1 -> layer4_qkv_dp1_mb1_gpu7
	layer4_attn_scores_dp0_mb1_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb1_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb1_gpu7 -> layer4_attn_scores_dp0_mb1_gpu7
	layer4_qkv_dp1_mb1_gpu7 -> layer4_attn_scores_dp1_mb1_gpu7
	layer4_softmax_dp0_mb1_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb1_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb1_gpu7 -> layer4_softmax_dp0_mb1_gpu7
	layer4_attn_scores_dp1_mb1_gpu7 -> layer4_softmax_dp1_mb1_gpu7
	layer4_attn_out_dp0_mb1_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb1_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb1_gpu7 -> layer4_attn_out_dp0_mb1_gpu7
	layer4_softmax_dp1_mb1_gpu7 -> layer4_attn_out_dp1_mb1_gpu7
	layer4_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb1_gpu4 -> layer4_attn_allreduce_dp0_mb1
	layer4_attn_out_dp1_mb1_gpu4 -> layer4_attn_allreduce_dp1_mb1
	layer4_attn_out_dp0_mb1_gpu5 -> layer4_attn_allreduce_dp0_mb1
	layer4_attn_out_dp1_mb1_gpu5 -> layer4_attn_allreduce_dp1_mb1
	layer4_attn_out_dp0_mb1_gpu6 -> layer4_attn_allreduce_dp0_mb1
	layer4_attn_out_dp1_mb1_gpu6 -> layer4_attn_allreduce_dp1_mb1
	layer4_attn_out_dp0_mb1_gpu7 -> layer4_attn_allreduce_dp0_mb1
	layer4_attn_out_dp1_mb1_gpu7 -> layer4_attn_allreduce_dp1_mb1
	layer4_router_dp0_mb1_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb1_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb1 -> layer4_router_dp0_mb1_gpu4
	layer4_attn_allreduce_dp1_mb1 -> layer4_router_dp1_mb1_gpu4
	layer4_expert0_mlp1_dp0_mb1_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb1_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb1_gpu4 -> layer4_expert0_mlp1_dp0_mb1_gpu4
	layer4_router_dp1_mb1_gpu4 -> layer4_expert0_mlp1_dp1_mb1_gpu4
	layer4_expert0_mlp2_dp0_mb1_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb1_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb1_gpu4 -> layer4_expert0_mlp2_dp0_mb1_gpu4
	layer4_expert0_mlp1_dp1_mb1_gpu4 -> layer4_expert0_mlp2_dp1_mb1_gpu4
	layer4_expert1_mlp1_dp0_mb1_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb1_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb1_gpu4 -> layer4_expert1_mlp1_dp0_mb1_gpu5
	layer4_router_dp1_mb1_gpu4 -> layer4_expert1_mlp1_dp1_mb1_gpu5
	layer4_expert1_mlp2_dp0_mb1_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb1_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb1_gpu5 -> layer4_expert1_mlp2_dp0_mb1_gpu5
	layer4_expert1_mlp1_dp1_mb1_gpu5 -> layer4_expert1_mlp2_dp1_mb1_gpu5
	layer4_expert2_mlp1_dp0_mb1_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb1_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb1_gpu4 -> layer4_expert2_mlp1_dp0_mb1_gpu6
	layer4_router_dp1_mb1_gpu4 -> layer4_expert2_mlp1_dp1_mb1_gpu6
	layer4_expert2_mlp2_dp0_mb1_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb1_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb1_gpu6 -> layer4_expert2_mlp2_dp0_mb1_gpu6
	layer4_expert2_mlp1_dp1_mb1_gpu6 -> layer4_expert2_mlp2_dp1_mb1_gpu6
	layer4_expert3_mlp1_dp0_mb1_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb1_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb1_gpu4 -> layer4_expert3_mlp1_dp0_mb1_gpu7
	layer4_router_dp1_mb1_gpu4 -> layer4_expert3_mlp1_dp1_mb1_gpu7
	layer4_expert3_mlp2_dp0_mb1_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb1_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb1_gpu7 -> layer4_expert3_mlp2_dp0_mb1_gpu7
	layer4_expert3_mlp1_dp1_mb1_gpu7 -> layer4_expert3_mlp2_dp1_mb1_gpu7
	layer4_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb1_gpu4 -> layer4_expert_agg_dp0_mb1
	layer4_expert0_mlp2_dp1_mb1_gpu4 -> layer4_expert_agg_dp1_mb1
	layer4_expert1_mlp2_dp0_mb1_gpu5 -> layer4_expert_agg_dp0_mb1
	layer4_expert1_mlp2_dp1_mb1_gpu5 -> layer4_expert_agg_dp1_mb1
	layer4_expert2_mlp2_dp0_mb1_gpu6 -> layer4_expert_agg_dp0_mb1
	layer4_expert2_mlp2_dp1_mb1_gpu6 -> layer4_expert_agg_dp1_mb1
	layer4_expert3_mlp2_dp0_mb1_gpu7 -> layer4_expert_agg_dp0_mb1
	layer4_expert3_mlp2_dp1_mb1_gpu7 -> layer4_expert_agg_dp1_mb1
	layer5_qkv_dp0_mb1_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb1_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb1 -> layer5_qkv_dp0_mb1_gpu4
	layer4_expert_agg_dp1_mb1 -> layer5_qkv_dp1_mb1_gpu4
	layer5_attn_scores_dp0_mb1_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb1_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb1_gpu4 -> layer5_attn_scores_dp0_mb1_gpu4
	layer5_qkv_dp1_mb1_gpu4 -> layer5_attn_scores_dp1_mb1_gpu4
	layer5_softmax_dp0_mb1_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb1_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb1_gpu4 -> layer5_softmax_dp0_mb1_gpu4
	layer5_attn_scores_dp1_mb1_gpu4 -> layer5_softmax_dp1_mb1_gpu4
	layer5_attn_out_dp0_mb1_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb1_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb1_gpu4 -> layer5_attn_out_dp0_mb1_gpu4
	layer5_softmax_dp1_mb1_gpu4 -> layer5_attn_out_dp1_mb1_gpu4
	layer5_qkv_dp0_mb1_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb1_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb1 -> layer5_qkv_dp0_mb1_gpu5
	layer4_expert_agg_dp1_mb1 -> layer5_qkv_dp1_mb1_gpu5
	layer5_attn_scores_dp0_mb1_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb1_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb1_gpu5 -> layer5_attn_scores_dp0_mb1_gpu5
	layer5_qkv_dp1_mb1_gpu5 -> layer5_attn_scores_dp1_mb1_gpu5
	layer5_softmax_dp0_mb1_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb1_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb1_gpu5 -> layer5_softmax_dp0_mb1_gpu5
	layer5_attn_scores_dp1_mb1_gpu5 -> layer5_softmax_dp1_mb1_gpu5
	layer5_attn_out_dp0_mb1_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb1_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb1_gpu5 -> layer5_attn_out_dp0_mb1_gpu5
	layer5_softmax_dp1_mb1_gpu5 -> layer5_attn_out_dp1_mb1_gpu5
	layer5_qkv_dp0_mb1_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb1_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb1 -> layer5_qkv_dp0_mb1_gpu6
	layer4_expert_agg_dp1_mb1 -> layer5_qkv_dp1_mb1_gpu6
	layer5_attn_scores_dp0_mb1_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb1_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb1_gpu6 -> layer5_attn_scores_dp0_mb1_gpu6
	layer5_qkv_dp1_mb1_gpu6 -> layer5_attn_scores_dp1_mb1_gpu6
	layer5_softmax_dp0_mb1_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb1_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb1_gpu6 -> layer5_softmax_dp0_mb1_gpu6
	layer5_attn_scores_dp1_mb1_gpu6 -> layer5_softmax_dp1_mb1_gpu6
	layer5_attn_out_dp0_mb1_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb1_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb1_gpu6 -> layer5_attn_out_dp0_mb1_gpu6
	layer5_softmax_dp1_mb1_gpu6 -> layer5_attn_out_dp1_mb1_gpu6
	layer5_qkv_dp0_mb1_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb1_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb1 -> layer5_qkv_dp0_mb1_gpu7
	layer4_expert_agg_dp1_mb1 -> layer5_qkv_dp1_mb1_gpu7
	layer5_attn_scores_dp0_mb1_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb1_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb1_gpu7 -> layer5_attn_scores_dp0_mb1_gpu7
	layer5_qkv_dp1_mb1_gpu7 -> layer5_attn_scores_dp1_mb1_gpu7
	layer5_softmax_dp0_mb1_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb1_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb1_gpu7 -> layer5_softmax_dp0_mb1_gpu7
	layer5_attn_scores_dp1_mb1_gpu7 -> layer5_softmax_dp1_mb1_gpu7
	layer5_attn_out_dp0_mb1_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb1_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb1_gpu7 -> layer5_attn_out_dp0_mb1_gpu7
	layer5_softmax_dp1_mb1_gpu7 -> layer5_attn_out_dp1_mb1_gpu7
	layer5_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb1_gpu4 -> layer5_attn_allreduce_dp0_mb1
	layer5_attn_out_dp1_mb1_gpu4 -> layer5_attn_allreduce_dp1_mb1
	layer5_attn_out_dp0_mb1_gpu5 -> layer5_attn_allreduce_dp0_mb1
	layer5_attn_out_dp1_mb1_gpu5 -> layer5_attn_allreduce_dp1_mb1
	layer5_attn_out_dp0_mb1_gpu6 -> layer5_attn_allreduce_dp0_mb1
	layer5_attn_out_dp1_mb1_gpu6 -> layer5_attn_allreduce_dp1_mb1
	layer5_attn_out_dp0_mb1_gpu7 -> layer5_attn_allreduce_dp0_mb1
	layer5_attn_out_dp1_mb1_gpu7 -> layer5_attn_allreduce_dp1_mb1
	layer5_router_dp0_mb1_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb1_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb1 -> layer5_router_dp0_mb1_gpu4
	layer5_attn_allreduce_dp1_mb1 -> layer5_router_dp1_mb1_gpu4
	layer5_expert0_mlp1_dp0_mb1_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb1_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb1_gpu4 -> layer5_expert0_mlp1_dp0_mb1_gpu4
	layer5_router_dp1_mb1_gpu4 -> layer5_expert0_mlp1_dp1_mb1_gpu4
	layer5_expert0_mlp2_dp0_mb1_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb1_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb1_gpu4 -> layer5_expert0_mlp2_dp0_mb1_gpu4
	layer5_expert0_mlp1_dp1_mb1_gpu4 -> layer5_expert0_mlp2_dp1_mb1_gpu4
	layer5_expert1_mlp1_dp0_mb1_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb1_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb1_gpu4 -> layer5_expert1_mlp1_dp0_mb1_gpu5
	layer5_router_dp1_mb1_gpu4 -> layer5_expert1_mlp1_dp1_mb1_gpu5
	layer5_expert1_mlp2_dp0_mb1_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb1_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb1_gpu5 -> layer5_expert1_mlp2_dp0_mb1_gpu5
	layer5_expert1_mlp1_dp1_mb1_gpu5 -> layer5_expert1_mlp2_dp1_mb1_gpu5
	layer5_expert2_mlp1_dp0_mb1_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb1_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb1_gpu4 -> layer5_expert2_mlp1_dp0_mb1_gpu6
	layer5_router_dp1_mb1_gpu4 -> layer5_expert2_mlp1_dp1_mb1_gpu6
	layer5_expert2_mlp2_dp0_mb1_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb1_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb1_gpu6 -> layer5_expert2_mlp2_dp0_mb1_gpu6
	layer5_expert2_mlp1_dp1_mb1_gpu6 -> layer5_expert2_mlp2_dp1_mb1_gpu6
	layer5_expert3_mlp1_dp0_mb1_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb1_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb1_gpu4 -> layer5_expert3_mlp1_dp0_mb1_gpu7
	layer5_router_dp1_mb1_gpu4 -> layer5_expert3_mlp1_dp1_mb1_gpu7
	layer5_expert3_mlp2_dp0_mb1_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb1_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb1_gpu7 -> layer5_expert3_mlp2_dp0_mb1_gpu7
	layer5_expert3_mlp1_dp1_mb1_gpu7 -> layer5_expert3_mlp2_dp1_mb1_gpu7
	layer5_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb1_gpu4 -> layer5_expert_agg_dp0_mb1
	layer5_expert0_mlp2_dp1_mb1_gpu4 -> layer5_expert_agg_dp1_mb1
	layer5_expert1_mlp2_dp0_mb1_gpu5 -> layer5_expert_agg_dp0_mb1
	layer5_expert1_mlp2_dp1_mb1_gpu5 -> layer5_expert_agg_dp1_mb1
	layer5_expert2_mlp2_dp0_mb1_gpu6 -> layer5_expert_agg_dp0_mb1
	layer5_expert2_mlp2_dp1_mb1_gpu6 -> layer5_expert_agg_dp1_mb1
	layer5_expert3_mlp2_dp0_mb1_gpu7 -> layer5_expert_agg_dp0_mb1
	layer5_expert3_mlp2_dp1_mb1_gpu7 -> layer5_expert_agg_dp1_mb1
	layer6_qkv_dp0_mb1_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb1_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb1 -> layer6_qkv_dp0_mb1_gpu4
	layer5_expert_agg_dp1_mb1 -> layer6_qkv_dp1_mb1_gpu4
	layer6_attn_scores_dp0_mb1_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb1_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb1_gpu4 -> layer6_attn_scores_dp0_mb1_gpu4
	layer6_qkv_dp1_mb1_gpu4 -> layer6_attn_scores_dp1_mb1_gpu4
	layer6_softmax_dp0_mb1_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb1_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb1_gpu4 -> layer6_softmax_dp0_mb1_gpu4
	layer6_attn_scores_dp1_mb1_gpu4 -> layer6_softmax_dp1_mb1_gpu4
	layer6_attn_out_dp0_mb1_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb1_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb1_gpu4 -> layer6_attn_out_dp0_mb1_gpu4
	layer6_softmax_dp1_mb1_gpu4 -> layer6_attn_out_dp1_mb1_gpu4
	layer6_qkv_dp0_mb1_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb1_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb1 -> layer6_qkv_dp0_mb1_gpu5
	layer5_expert_agg_dp1_mb1 -> layer6_qkv_dp1_mb1_gpu5
	layer6_attn_scores_dp0_mb1_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb1_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb1_gpu5 -> layer6_attn_scores_dp0_mb1_gpu5
	layer6_qkv_dp1_mb1_gpu5 -> layer6_attn_scores_dp1_mb1_gpu5
	layer6_softmax_dp0_mb1_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb1_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb1_gpu5 -> layer6_softmax_dp0_mb1_gpu5
	layer6_attn_scores_dp1_mb1_gpu5 -> layer6_softmax_dp1_mb1_gpu5
	layer6_attn_out_dp0_mb1_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb1_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb1_gpu5 -> layer6_attn_out_dp0_mb1_gpu5
	layer6_softmax_dp1_mb1_gpu5 -> layer6_attn_out_dp1_mb1_gpu5
	layer6_qkv_dp0_mb1_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb1_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb1 -> layer6_qkv_dp0_mb1_gpu6
	layer5_expert_agg_dp1_mb1 -> layer6_qkv_dp1_mb1_gpu6
	layer6_attn_scores_dp0_mb1_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb1_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb1_gpu6 -> layer6_attn_scores_dp0_mb1_gpu6
	layer6_qkv_dp1_mb1_gpu6 -> layer6_attn_scores_dp1_mb1_gpu6
	layer6_softmax_dp0_mb1_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb1_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb1_gpu6 -> layer6_softmax_dp0_mb1_gpu6
	layer6_attn_scores_dp1_mb1_gpu6 -> layer6_softmax_dp1_mb1_gpu6
	layer6_attn_out_dp0_mb1_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb1_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb1_gpu6 -> layer6_attn_out_dp0_mb1_gpu6
	layer6_softmax_dp1_mb1_gpu6 -> layer6_attn_out_dp1_mb1_gpu6
	layer6_qkv_dp0_mb1_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb1_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb1 -> layer6_qkv_dp0_mb1_gpu7
	layer5_expert_agg_dp1_mb1 -> layer6_qkv_dp1_mb1_gpu7
	layer6_attn_scores_dp0_mb1_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb1_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb1_gpu7 -> layer6_attn_scores_dp0_mb1_gpu7
	layer6_qkv_dp1_mb1_gpu7 -> layer6_attn_scores_dp1_mb1_gpu7
	layer6_softmax_dp0_mb1_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb1_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb1_gpu7 -> layer6_softmax_dp0_mb1_gpu7
	layer6_attn_scores_dp1_mb1_gpu7 -> layer6_softmax_dp1_mb1_gpu7
	layer6_attn_out_dp0_mb1_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb1_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb1_gpu7 -> layer6_attn_out_dp0_mb1_gpu7
	layer6_softmax_dp1_mb1_gpu7 -> layer6_attn_out_dp1_mb1_gpu7
	layer6_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb1_gpu4 -> layer6_attn_allreduce_dp0_mb1
	layer6_attn_out_dp1_mb1_gpu4 -> layer6_attn_allreduce_dp1_mb1
	layer6_attn_out_dp0_mb1_gpu5 -> layer6_attn_allreduce_dp0_mb1
	layer6_attn_out_dp1_mb1_gpu5 -> layer6_attn_allreduce_dp1_mb1
	layer6_attn_out_dp0_mb1_gpu6 -> layer6_attn_allreduce_dp0_mb1
	layer6_attn_out_dp1_mb1_gpu6 -> layer6_attn_allreduce_dp1_mb1
	layer6_attn_out_dp0_mb1_gpu7 -> layer6_attn_allreduce_dp0_mb1
	layer6_attn_out_dp1_mb1_gpu7 -> layer6_attn_allreduce_dp1_mb1
	layer6_router_dp0_mb1_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb1_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb1 -> layer6_router_dp0_mb1_gpu4
	layer6_attn_allreduce_dp1_mb1 -> layer6_router_dp1_mb1_gpu4
	layer6_expert0_mlp1_dp0_mb1_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb1_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb1_gpu4 -> layer6_expert0_mlp1_dp0_mb1_gpu4
	layer6_router_dp1_mb1_gpu4 -> layer6_expert0_mlp1_dp1_mb1_gpu4
	layer6_expert0_mlp2_dp0_mb1_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb1_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb1_gpu4 -> layer6_expert0_mlp2_dp0_mb1_gpu4
	layer6_expert0_mlp1_dp1_mb1_gpu4 -> layer6_expert0_mlp2_dp1_mb1_gpu4
	layer6_expert1_mlp1_dp0_mb1_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb1_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb1_gpu4 -> layer6_expert1_mlp1_dp0_mb1_gpu5
	layer6_router_dp1_mb1_gpu4 -> layer6_expert1_mlp1_dp1_mb1_gpu5
	layer6_expert1_mlp2_dp0_mb1_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb1_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb1_gpu5 -> layer6_expert1_mlp2_dp0_mb1_gpu5
	layer6_expert1_mlp1_dp1_mb1_gpu5 -> layer6_expert1_mlp2_dp1_mb1_gpu5
	layer6_expert2_mlp1_dp0_mb1_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb1_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb1_gpu4 -> layer6_expert2_mlp1_dp0_mb1_gpu6
	layer6_router_dp1_mb1_gpu4 -> layer6_expert2_mlp1_dp1_mb1_gpu6
	layer6_expert2_mlp2_dp0_mb1_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb1_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb1_gpu6 -> layer6_expert2_mlp2_dp0_mb1_gpu6
	layer6_expert2_mlp1_dp1_mb1_gpu6 -> layer6_expert2_mlp2_dp1_mb1_gpu6
	layer6_expert3_mlp1_dp0_mb1_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb1_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb1_gpu4 -> layer6_expert3_mlp1_dp0_mb1_gpu7
	layer6_router_dp1_mb1_gpu4 -> layer6_expert3_mlp1_dp1_mb1_gpu7
	layer6_expert3_mlp2_dp0_mb1_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb1_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb1_gpu7 -> layer6_expert3_mlp2_dp0_mb1_gpu7
	layer6_expert3_mlp1_dp1_mb1_gpu7 -> layer6_expert3_mlp2_dp1_mb1_gpu7
	layer6_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb1_gpu4 -> layer6_expert_agg_dp0_mb1
	layer6_expert0_mlp2_dp1_mb1_gpu4 -> layer6_expert_agg_dp1_mb1
	layer6_expert1_mlp2_dp0_mb1_gpu5 -> layer6_expert_agg_dp0_mb1
	layer6_expert1_mlp2_dp1_mb1_gpu5 -> layer6_expert_agg_dp1_mb1
	layer6_expert2_mlp2_dp0_mb1_gpu6 -> layer6_expert_agg_dp0_mb1
	layer6_expert2_mlp2_dp1_mb1_gpu6 -> layer6_expert_agg_dp1_mb1
	layer6_expert3_mlp2_dp0_mb1_gpu7 -> layer6_expert_agg_dp0_mb1
	layer6_expert3_mlp2_dp1_mb1_gpu7 -> layer6_expert_agg_dp1_mb1
	layer7_qkv_dp0_mb1_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb1_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb1 -> layer7_qkv_dp0_mb1_gpu4
	layer6_expert_agg_dp1_mb1 -> layer7_qkv_dp1_mb1_gpu4
	layer7_attn_scores_dp0_mb1_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb1_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb1_gpu4 -> layer7_attn_scores_dp0_mb1_gpu4
	layer7_qkv_dp1_mb1_gpu4 -> layer7_attn_scores_dp1_mb1_gpu4
	layer7_softmax_dp0_mb1_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb1_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb1_gpu4 -> layer7_softmax_dp0_mb1_gpu4
	layer7_attn_scores_dp1_mb1_gpu4 -> layer7_softmax_dp1_mb1_gpu4
	layer7_attn_out_dp0_mb1_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb1_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb1_gpu4 -> layer7_attn_out_dp0_mb1_gpu4
	layer7_softmax_dp1_mb1_gpu4 -> layer7_attn_out_dp1_mb1_gpu4
	layer7_qkv_dp0_mb1_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb1_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb1 -> layer7_qkv_dp0_mb1_gpu5
	layer6_expert_agg_dp1_mb1 -> layer7_qkv_dp1_mb1_gpu5
	layer7_attn_scores_dp0_mb1_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb1_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb1_gpu5 -> layer7_attn_scores_dp0_mb1_gpu5
	layer7_qkv_dp1_mb1_gpu5 -> layer7_attn_scores_dp1_mb1_gpu5
	layer7_softmax_dp0_mb1_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb1_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb1_gpu5 -> layer7_softmax_dp0_mb1_gpu5
	layer7_attn_scores_dp1_mb1_gpu5 -> layer7_softmax_dp1_mb1_gpu5
	layer7_attn_out_dp0_mb1_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb1_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb1_gpu5 -> layer7_attn_out_dp0_mb1_gpu5
	layer7_softmax_dp1_mb1_gpu5 -> layer7_attn_out_dp1_mb1_gpu5
	layer7_qkv_dp0_mb1_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb1_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb1 -> layer7_qkv_dp0_mb1_gpu6
	layer6_expert_agg_dp1_mb1 -> layer7_qkv_dp1_mb1_gpu6
	layer7_attn_scores_dp0_mb1_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb1_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb1_gpu6 -> layer7_attn_scores_dp0_mb1_gpu6
	layer7_qkv_dp1_mb1_gpu6 -> layer7_attn_scores_dp1_mb1_gpu6
	layer7_softmax_dp0_mb1_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb1_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb1_gpu6 -> layer7_softmax_dp0_mb1_gpu6
	layer7_attn_scores_dp1_mb1_gpu6 -> layer7_softmax_dp1_mb1_gpu6
	layer7_attn_out_dp0_mb1_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb1_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb1_gpu6 -> layer7_attn_out_dp0_mb1_gpu6
	layer7_softmax_dp1_mb1_gpu6 -> layer7_attn_out_dp1_mb1_gpu6
	layer7_qkv_dp0_mb1_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb1_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb1 -> layer7_qkv_dp0_mb1_gpu7
	layer6_expert_agg_dp1_mb1 -> layer7_qkv_dp1_mb1_gpu7
	layer7_attn_scores_dp0_mb1_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb1_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb1_gpu7 -> layer7_attn_scores_dp0_mb1_gpu7
	layer7_qkv_dp1_mb1_gpu7 -> layer7_attn_scores_dp1_mb1_gpu7
	layer7_softmax_dp0_mb1_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb1_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb1_gpu7 -> layer7_softmax_dp0_mb1_gpu7
	layer7_attn_scores_dp1_mb1_gpu7 -> layer7_softmax_dp1_mb1_gpu7
	layer7_attn_out_dp0_mb1_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb1_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb1_gpu7 -> layer7_attn_out_dp0_mb1_gpu7
	layer7_softmax_dp1_mb1_gpu7 -> layer7_attn_out_dp1_mb1_gpu7
	layer7_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb1_gpu4 -> layer7_attn_allreduce_dp0_mb1
	layer7_attn_out_dp1_mb1_gpu4 -> layer7_attn_allreduce_dp1_mb1
	layer7_attn_out_dp0_mb1_gpu5 -> layer7_attn_allreduce_dp0_mb1
	layer7_attn_out_dp1_mb1_gpu5 -> layer7_attn_allreduce_dp1_mb1
	layer7_attn_out_dp0_mb1_gpu6 -> layer7_attn_allreduce_dp0_mb1
	layer7_attn_out_dp1_mb1_gpu6 -> layer7_attn_allreduce_dp1_mb1
	layer7_attn_out_dp0_mb1_gpu7 -> layer7_attn_allreduce_dp0_mb1
	layer7_attn_out_dp1_mb1_gpu7 -> layer7_attn_allreduce_dp1_mb1
	layer7_router_dp0_mb1_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb1_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb1 -> layer7_router_dp0_mb1_gpu4
	layer7_attn_allreduce_dp1_mb1 -> layer7_router_dp1_mb1_gpu4
	layer7_expert0_mlp1_dp0_mb1_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb1_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb1_gpu4 -> layer7_expert0_mlp1_dp0_mb1_gpu4
	layer7_router_dp1_mb1_gpu4 -> layer7_expert0_mlp1_dp1_mb1_gpu4
	layer7_expert0_mlp2_dp0_mb1_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb1_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb1_gpu4 -> layer7_expert0_mlp2_dp0_mb1_gpu4
	layer7_expert0_mlp1_dp1_mb1_gpu4 -> layer7_expert0_mlp2_dp1_mb1_gpu4
	layer7_expert1_mlp1_dp0_mb1_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb1_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb1_gpu4 -> layer7_expert1_mlp1_dp0_mb1_gpu5
	layer7_router_dp1_mb1_gpu4 -> layer7_expert1_mlp1_dp1_mb1_gpu5
	layer7_expert1_mlp2_dp0_mb1_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb1_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb1_gpu5 -> layer7_expert1_mlp2_dp0_mb1_gpu5
	layer7_expert1_mlp1_dp1_mb1_gpu5 -> layer7_expert1_mlp2_dp1_mb1_gpu5
	layer7_expert2_mlp1_dp0_mb1_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb1_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb1_gpu4 -> layer7_expert2_mlp1_dp0_mb1_gpu6
	layer7_router_dp1_mb1_gpu4 -> layer7_expert2_mlp1_dp1_mb1_gpu6
	layer7_expert2_mlp2_dp0_mb1_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb1_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb1_gpu6 -> layer7_expert2_mlp2_dp0_mb1_gpu6
	layer7_expert2_mlp1_dp1_mb1_gpu6 -> layer7_expert2_mlp2_dp1_mb1_gpu6
	layer7_expert3_mlp1_dp0_mb1_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb1_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb1_gpu4 -> layer7_expert3_mlp1_dp0_mb1_gpu7
	layer7_router_dp1_mb1_gpu4 -> layer7_expert3_mlp1_dp1_mb1_gpu7
	layer7_expert3_mlp2_dp0_mb1_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb1_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb1_gpu7 -> layer7_expert3_mlp2_dp0_mb1_gpu7
	layer7_expert3_mlp1_dp1_mb1_gpu7 -> layer7_expert3_mlp2_dp1_mb1_gpu7
	layer7_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb1_gpu4 -> layer7_expert_agg_dp0_mb1
	layer7_expert0_mlp2_dp1_mb1_gpu4 -> layer7_expert_agg_dp1_mb1
	layer7_expert1_mlp2_dp0_mb1_gpu5 -> layer7_expert_agg_dp0_mb1
	layer7_expert1_mlp2_dp1_mb1_gpu5 -> layer7_expert_agg_dp1_mb1
	layer7_expert2_mlp2_dp0_mb1_gpu6 -> layer7_expert_agg_dp0_mb1
	layer7_expert2_mlp2_dp1_mb1_gpu6 -> layer7_expert_agg_dp1_mb1
	layer7_expert3_mlp2_dp0_mb1_gpu7 -> layer7_expert_agg_dp0_mb1
	layer7_expert3_mlp2_dp1_mb1_gpu7 -> layer7_expert_agg_dp1_mb1
	layer4_qkv_dp0_mb2_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb2_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb2 -> layer4_qkv_dp0_mb2_gpu4
	layer3_expert_agg_dp1_mb2 -> layer4_qkv_dp1_mb2_gpu4
	layer4_attn_scores_dp0_mb2_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb2_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb2_gpu4 -> layer4_attn_scores_dp0_mb2_gpu4
	layer4_qkv_dp1_mb2_gpu4 -> layer4_attn_scores_dp1_mb2_gpu4
	layer4_softmax_dp0_mb2_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb2_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb2_gpu4 -> layer4_softmax_dp0_mb2_gpu4
	layer4_attn_scores_dp1_mb2_gpu4 -> layer4_softmax_dp1_mb2_gpu4
	layer4_attn_out_dp0_mb2_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb2_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb2_gpu4 -> layer4_attn_out_dp0_mb2_gpu4
	layer4_softmax_dp1_mb2_gpu4 -> layer4_attn_out_dp1_mb2_gpu4
	layer4_qkv_dp0_mb2_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb2_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb2 -> layer4_qkv_dp0_mb2_gpu5
	layer3_expert_agg_dp1_mb2 -> layer4_qkv_dp1_mb2_gpu5
	layer4_attn_scores_dp0_mb2_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb2_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb2_gpu5 -> layer4_attn_scores_dp0_mb2_gpu5
	layer4_qkv_dp1_mb2_gpu5 -> layer4_attn_scores_dp1_mb2_gpu5
	layer4_softmax_dp0_mb2_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb2_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb2_gpu5 -> layer4_softmax_dp0_mb2_gpu5
	layer4_attn_scores_dp1_mb2_gpu5 -> layer4_softmax_dp1_mb2_gpu5
	layer4_attn_out_dp0_mb2_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb2_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb2_gpu5 -> layer4_attn_out_dp0_mb2_gpu5
	layer4_softmax_dp1_mb2_gpu5 -> layer4_attn_out_dp1_mb2_gpu5
	layer4_qkv_dp0_mb2_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb2_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb2 -> layer4_qkv_dp0_mb2_gpu6
	layer3_expert_agg_dp1_mb2 -> layer4_qkv_dp1_mb2_gpu6
	layer4_attn_scores_dp0_mb2_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb2_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb2_gpu6 -> layer4_attn_scores_dp0_mb2_gpu6
	layer4_qkv_dp1_mb2_gpu6 -> layer4_attn_scores_dp1_mb2_gpu6
	layer4_softmax_dp0_mb2_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb2_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb2_gpu6 -> layer4_softmax_dp0_mb2_gpu6
	layer4_attn_scores_dp1_mb2_gpu6 -> layer4_softmax_dp1_mb2_gpu6
	layer4_attn_out_dp0_mb2_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb2_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb2_gpu6 -> layer4_attn_out_dp0_mb2_gpu6
	layer4_softmax_dp1_mb2_gpu6 -> layer4_attn_out_dp1_mb2_gpu6
	layer4_qkv_dp0_mb2_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb2_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb2 -> layer4_qkv_dp0_mb2_gpu7
	layer3_expert_agg_dp1_mb2 -> layer4_qkv_dp1_mb2_gpu7
	layer4_attn_scores_dp0_mb2_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb2_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb2_gpu7 -> layer4_attn_scores_dp0_mb2_gpu7
	layer4_qkv_dp1_mb2_gpu7 -> layer4_attn_scores_dp1_mb2_gpu7
	layer4_softmax_dp0_mb2_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb2_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb2_gpu7 -> layer4_softmax_dp0_mb2_gpu7
	layer4_attn_scores_dp1_mb2_gpu7 -> layer4_softmax_dp1_mb2_gpu7
	layer4_attn_out_dp0_mb2_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb2_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb2_gpu7 -> layer4_attn_out_dp0_mb2_gpu7
	layer4_softmax_dp1_mb2_gpu7 -> layer4_attn_out_dp1_mb2_gpu7
	layer4_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb2_gpu4 -> layer4_attn_allreduce_dp0_mb2
	layer4_attn_out_dp1_mb2_gpu4 -> layer4_attn_allreduce_dp1_mb2
	layer4_attn_out_dp0_mb2_gpu5 -> layer4_attn_allreduce_dp0_mb2
	layer4_attn_out_dp1_mb2_gpu5 -> layer4_attn_allreduce_dp1_mb2
	layer4_attn_out_dp0_mb2_gpu6 -> layer4_attn_allreduce_dp0_mb2
	layer4_attn_out_dp1_mb2_gpu6 -> layer4_attn_allreduce_dp1_mb2
	layer4_attn_out_dp0_mb2_gpu7 -> layer4_attn_allreduce_dp0_mb2
	layer4_attn_out_dp1_mb2_gpu7 -> layer4_attn_allreduce_dp1_mb2
	layer4_router_dp0_mb2_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb2_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb2 -> layer4_router_dp0_mb2_gpu4
	layer4_attn_allreduce_dp1_mb2 -> layer4_router_dp1_mb2_gpu4
	layer4_expert0_mlp1_dp0_mb2_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb2_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb2_gpu4 -> layer4_expert0_mlp1_dp0_mb2_gpu4
	layer4_router_dp1_mb2_gpu4 -> layer4_expert0_mlp1_dp1_mb2_gpu4
	layer4_expert0_mlp2_dp0_mb2_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb2_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb2_gpu4 -> layer4_expert0_mlp2_dp0_mb2_gpu4
	layer4_expert0_mlp1_dp1_mb2_gpu4 -> layer4_expert0_mlp2_dp1_mb2_gpu4
	layer4_expert1_mlp1_dp0_mb2_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb2_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb2_gpu4 -> layer4_expert1_mlp1_dp0_mb2_gpu5
	layer4_router_dp1_mb2_gpu4 -> layer4_expert1_mlp1_dp1_mb2_gpu5
	layer4_expert1_mlp2_dp0_mb2_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb2_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb2_gpu5 -> layer4_expert1_mlp2_dp0_mb2_gpu5
	layer4_expert1_mlp1_dp1_mb2_gpu5 -> layer4_expert1_mlp2_dp1_mb2_gpu5
	layer4_expert2_mlp1_dp0_mb2_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb2_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb2_gpu4 -> layer4_expert2_mlp1_dp0_mb2_gpu6
	layer4_router_dp1_mb2_gpu4 -> layer4_expert2_mlp1_dp1_mb2_gpu6
	layer4_expert2_mlp2_dp0_mb2_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb2_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb2_gpu6 -> layer4_expert2_mlp2_dp0_mb2_gpu6
	layer4_expert2_mlp1_dp1_mb2_gpu6 -> layer4_expert2_mlp2_dp1_mb2_gpu6
	layer4_expert3_mlp1_dp0_mb2_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb2_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb2_gpu4 -> layer4_expert3_mlp1_dp0_mb2_gpu7
	layer4_router_dp1_mb2_gpu4 -> layer4_expert3_mlp1_dp1_mb2_gpu7
	layer4_expert3_mlp2_dp0_mb2_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb2_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb2_gpu7 -> layer4_expert3_mlp2_dp0_mb2_gpu7
	layer4_expert3_mlp1_dp1_mb2_gpu7 -> layer4_expert3_mlp2_dp1_mb2_gpu7
	layer4_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb2_gpu4 -> layer4_expert_agg_dp0_mb2
	layer4_expert0_mlp2_dp1_mb2_gpu4 -> layer4_expert_agg_dp1_mb2
	layer4_expert1_mlp2_dp0_mb2_gpu5 -> layer4_expert_agg_dp0_mb2
	layer4_expert1_mlp2_dp1_mb2_gpu5 -> layer4_expert_agg_dp1_mb2
	layer4_expert2_mlp2_dp0_mb2_gpu6 -> layer4_expert_agg_dp0_mb2
	layer4_expert2_mlp2_dp1_mb2_gpu6 -> layer4_expert_agg_dp1_mb2
	layer4_expert3_mlp2_dp0_mb2_gpu7 -> layer4_expert_agg_dp0_mb2
	layer4_expert3_mlp2_dp1_mb2_gpu7 -> layer4_expert_agg_dp1_mb2
	layer5_qkv_dp0_mb2_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb2_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb2 -> layer5_qkv_dp0_mb2_gpu4
	layer4_expert_agg_dp1_mb2 -> layer5_qkv_dp1_mb2_gpu4
	layer5_attn_scores_dp0_mb2_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb2_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb2_gpu4 -> layer5_attn_scores_dp0_mb2_gpu4
	layer5_qkv_dp1_mb2_gpu4 -> layer5_attn_scores_dp1_mb2_gpu4
	layer5_softmax_dp0_mb2_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb2_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb2_gpu4 -> layer5_softmax_dp0_mb2_gpu4
	layer5_attn_scores_dp1_mb2_gpu4 -> layer5_softmax_dp1_mb2_gpu4
	layer5_attn_out_dp0_mb2_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb2_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb2_gpu4 -> layer5_attn_out_dp0_mb2_gpu4
	layer5_softmax_dp1_mb2_gpu4 -> layer5_attn_out_dp1_mb2_gpu4
	layer5_qkv_dp0_mb2_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb2_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb2 -> layer5_qkv_dp0_mb2_gpu5
	layer4_expert_agg_dp1_mb2 -> layer5_qkv_dp1_mb2_gpu5
	layer5_attn_scores_dp0_mb2_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb2_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb2_gpu5 -> layer5_attn_scores_dp0_mb2_gpu5
	layer5_qkv_dp1_mb2_gpu5 -> layer5_attn_scores_dp1_mb2_gpu5
	layer5_softmax_dp0_mb2_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb2_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb2_gpu5 -> layer5_softmax_dp0_mb2_gpu5
	layer5_attn_scores_dp1_mb2_gpu5 -> layer5_softmax_dp1_mb2_gpu5
	layer5_attn_out_dp0_mb2_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb2_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb2_gpu5 -> layer5_attn_out_dp0_mb2_gpu5
	layer5_softmax_dp1_mb2_gpu5 -> layer5_attn_out_dp1_mb2_gpu5
	layer5_qkv_dp0_mb2_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb2_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb2 -> layer5_qkv_dp0_mb2_gpu6
	layer4_expert_agg_dp1_mb2 -> layer5_qkv_dp1_mb2_gpu6
	layer5_attn_scores_dp0_mb2_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb2_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb2_gpu6 -> layer5_attn_scores_dp0_mb2_gpu6
	layer5_qkv_dp1_mb2_gpu6 -> layer5_attn_scores_dp1_mb2_gpu6
	layer5_softmax_dp0_mb2_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb2_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb2_gpu6 -> layer5_softmax_dp0_mb2_gpu6
	layer5_attn_scores_dp1_mb2_gpu6 -> layer5_softmax_dp1_mb2_gpu6
	layer5_attn_out_dp0_mb2_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb2_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb2_gpu6 -> layer5_attn_out_dp0_mb2_gpu6
	layer5_softmax_dp1_mb2_gpu6 -> layer5_attn_out_dp1_mb2_gpu6
	layer5_qkv_dp0_mb2_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb2_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb2 -> layer5_qkv_dp0_mb2_gpu7
	layer4_expert_agg_dp1_mb2 -> layer5_qkv_dp1_mb2_gpu7
	layer5_attn_scores_dp0_mb2_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb2_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb2_gpu7 -> layer5_attn_scores_dp0_mb2_gpu7
	layer5_qkv_dp1_mb2_gpu7 -> layer5_attn_scores_dp1_mb2_gpu7
	layer5_softmax_dp0_mb2_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb2_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb2_gpu7 -> layer5_softmax_dp0_mb2_gpu7
	layer5_attn_scores_dp1_mb2_gpu7 -> layer5_softmax_dp1_mb2_gpu7
	layer5_attn_out_dp0_mb2_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb2_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb2_gpu7 -> layer5_attn_out_dp0_mb2_gpu7
	layer5_softmax_dp1_mb2_gpu7 -> layer5_attn_out_dp1_mb2_gpu7
	layer5_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb2_gpu4 -> layer5_attn_allreduce_dp0_mb2
	layer5_attn_out_dp1_mb2_gpu4 -> layer5_attn_allreduce_dp1_mb2
	layer5_attn_out_dp0_mb2_gpu5 -> layer5_attn_allreduce_dp0_mb2
	layer5_attn_out_dp1_mb2_gpu5 -> layer5_attn_allreduce_dp1_mb2
	layer5_attn_out_dp0_mb2_gpu6 -> layer5_attn_allreduce_dp0_mb2
	layer5_attn_out_dp1_mb2_gpu6 -> layer5_attn_allreduce_dp1_mb2
	layer5_attn_out_dp0_mb2_gpu7 -> layer5_attn_allreduce_dp0_mb2
	layer5_attn_out_dp1_mb2_gpu7 -> layer5_attn_allreduce_dp1_mb2
	layer5_router_dp0_mb2_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb2_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb2 -> layer5_router_dp0_mb2_gpu4
	layer5_attn_allreduce_dp1_mb2 -> layer5_router_dp1_mb2_gpu4
	layer5_expert0_mlp1_dp0_mb2_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb2_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb2_gpu4 -> layer5_expert0_mlp1_dp0_mb2_gpu4
	layer5_router_dp1_mb2_gpu4 -> layer5_expert0_mlp1_dp1_mb2_gpu4
	layer5_expert0_mlp2_dp0_mb2_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb2_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb2_gpu4 -> layer5_expert0_mlp2_dp0_mb2_gpu4
	layer5_expert0_mlp1_dp1_mb2_gpu4 -> layer5_expert0_mlp2_dp1_mb2_gpu4
	layer5_expert1_mlp1_dp0_mb2_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb2_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb2_gpu4 -> layer5_expert1_mlp1_dp0_mb2_gpu5
	layer5_router_dp1_mb2_gpu4 -> layer5_expert1_mlp1_dp1_mb2_gpu5
	layer5_expert1_mlp2_dp0_mb2_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb2_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb2_gpu5 -> layer5_expert1_mlp2_dp0_mb2_gpu5
	layer5_expert1_mlp1_dp1_mb2_gpu5 -> layer5_expert1_mlp2_dp1_mb2_gpu5
	layer5_expert2_mlp1_dp0_mb2_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb2_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb2_gpu4 -> layer5_expert2_mlp1_dp0_mb2_gpu6
	layer5_router_dp1_mb2_gpu4 -> layer5_expert2_mlp1_dp1_mb2_gpu6
	layer5_expert2_mlp2_dp0_mb2_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb2_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb2_gpu6 -> layer5_expert2_mlp2_dp0_mb2_gpu6
	layer5_expert2_mlp1_dp1_mb2_gpu6 -> layer5_expert2_mlp2_dp1_mb2_gpu6
	layer5_expert3_mlp1_dp0_mb2_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb2_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb2_gpu4 -> layer5_expert3_mlp1_dp0_mb2_gpu7
	layer5_router_dp1_mb2_gpu4 -> layer5_expert3_mlp1_dp1_mb2_gpu7
	layer5_expert3_mlp2_dp0_mb2_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb2_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb2_gpu7 -> layer5_expert3_mlp2_dp0_mb2_gpu7
	layer5_expert3_mlp1_dp1_mb2_gpu7 -> layer5_expert3_mlp2_dp1_mb2_gpu7
	layer5_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb2_gpu4 -> layer5_expert_agg_dp0_mb2
	layer5_expert0_mlp2_dp1_mb2_gpu4 -> layer5_expert_agg_dp1_mb2
	layer5_expert1_mlp2_dp0_mb2_gpu5 -> layer5_expert_agg_dp0_mb2
	layer5_expert1_mlp2_dp1_mb2_gpu5 -> layer5_expert_agg_dp1_mb2
	layer5_expert2_mlp2_dp0_mb2_gpu6 -> layer5_expert_agg_dp0_mb2
	layer5_expert2_mlp2_dp1_mb2_gpu6 -> layer5_expert_agg_dp1_mb2
	layer5_expert3_mlp2_dp0_mb2_gpu7 -> layer5_expert_agg_dp0_mb2
	layer5_expert3_mlp2_dp1_mb2_gpu7 -> layer5_expert_agg_dp1_mb2
	layer6_qkv_dp0_mb2_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb2_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb2 -> layer6_qkv_dp0_mb2_gpu4
	layer5_expert_agg_dp1_mb2 -> layer6_qkv_dp1_mb2_gpu4
	layer6_attn_scores_dp0_mb2_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb2_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb2_gpu4 -> layer6_attn_scores_dp0_mb2_gpu4
	layer6_qkv_dp1_mb2_gpu4 -> layer6_attn_scores_dp1_mb2_gpu4
	layer6_softmax_dp0_mb2_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb2_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb2_gpu4 -> layer6_softmax_dp0_mb2_gpu4
	layer6_attn_scores_dp1_mb2_gpu4 -> layer6_softmax_dp1_mb2_gpu4
	layer6_attn_out_dp0_mb2_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb2_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb2_gpu4 -> layer6_attn_out_dp0_mb2_gpu4
	layer6_softmax_dp1_mb2_gpu4 -> layer6_attn_out_dp1_mb2_gpu4
	layer6_qkv_dp0_mb2_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb2_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb2 -> layer6_qkv_dp0_mb2_gpu5
	layer5_expert_agg_dp1_mb2 -> layer6_qkv_dp1_mb2_gpu5
	layer6_attn_scores_dp0_mb2_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb2_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb2_gpu5 -> layer6_attn_scores_dp0_mb2_gpu5
	layer6_qkv_dp1_mb2_gpu5 -> layer6_attn_scores_dp1_mb2_gpu5
	layer6_softmax_dp0_mb2_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb2_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb2_gpu5 -> layer6_softmax_dp0_mb2_gpu5
	layer6_attn_scores_dp1_mb2_gpu5 -> layer6_softmax_dp1_mb2_gpu5
	layer6_attn_out_dp0_mb2_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb2_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb2_gpu5 -> layer6_attn_out_dp0_mb2_gpu5
	layer6_softmax_dp1_mb2_gpu5 -> layer6_attn_out_dp1_mb2_gpu5
	layer6_qkv_dp0_mb2_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb2_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb2 -> layer6_qkv_dp0_mb2_gpu6
	layer5_expert_agg_dp1_mb2 -> layer6_qkv_dp1_mb2_gpu6
	layer6_attn_scores_dp0_mb2_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb2_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb2_gpu6 -> layer6_attn_scores_dp0_mb2_gpu6
	layer6_qkv_dp1_mb2_gpu6 -> layer6_attn_scores_dp1_mb2_gpu6
	layer6_softmax_dp0_mb2_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb2_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb2_gpu6 -> layer6_softmax_dp0_mb2_gpu6
	layer6_attn_scores_dp1_mb2_gpu6 -> layer6_softmax_dp1_mb2_gpu6
	layer6_attn_out_dp0_mb2_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb2_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb2_gpu6 -> layer6_attn_out_dp0_mb2_gpu6
	layer6_softmax_dp1_mb2_gpu6 -> layer6_attn_out_dp1_mb2_gpu6
	layer6_qkv_dp0_mb2_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb2_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb2 -> layer6_qkv_dp0_mb2_gpu7
	layer5_expert_agg_dp1_mb2 -> layer6_qkv_dp1_mb2_gpu7
	layer6_attn_scores_dp0_mb2_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb2_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb2_gpu7 -> layer6_attn_scores_dp0_mb2_gpu7
	layer6_qkv_dp1_mb2_gpu7 -> layer6_attn_scores_dp1_mb2_gpu7
	layer6_softmax_dp0_mb2_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb2_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb2_gpu7 -> layer6_softmax_dp0_mb2_gpu7
	layer6_attn_scores_dp1_mb2_gpu7 -> layer6_softmax_dp1_mb2_gpu7
	layer6_attn_out_dp0_mb2_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb2_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb2_gpu7 -> layer6_attn_out_dp0_mb2_gpu7
	layer6_softmax_dp1_mb2_gpu7 -> layer6_attn_out_dp1_mb2_gpu7
	layer6_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb2_gpu4 -> layer6_attn_allreduce_dp0_mb2
	layer6_attn_out_dp1_mb2_gpu4 -> layer6_attn_allreduce_dp1_mb2
	layer6_attn_out_dp0_mb2_gpu5 -> layer6_attn_allreduce_dp0_mb2
	layer6_attn_out_dp1_mb2_gpu5 -> layer6_attn_allreduce_dp1_mb2
	layer6_attn_out_dp0_mb2_gpu6 -> layer6_attn_allreduce_dp0_mb2
	layer6_attn_out_dp1_mb2_gpu6 -> layer6_attn_allreduce_dp1_mb2
	layer6_attn_out_dp0_mb2_gpu7 -> layer6_attn_allreduce_dp0_mb2
	layer6_attn_out_dp1_mb2_gpu7 -> layer6_attn_allreduce_dp1_mb2
	layer6_router_dp0_mb2_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb2_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb2 -> layer6_router_dp0_mb2_gpu4
	layer6_attn_allreduce_dp1_mb2 -> layer6_router_dp1_mb2_gpu4
	layer6_expert0_mlp1_dp0_mb2_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb2_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb2_gpu4 -> layer6_expert0_mlp1_dp0_mb2_gpu4
	layer6_router_dp1_mb2_gpu4 -> layer6_expert0_mlp1_dp1_mb2_gpu4
	layer6_expert0_mlp2_dp0_mb2_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb2_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb2_gpu4 -> layer6_expert0_mlp2_dp0_mb2_gpu4
	layer6_expert0_mlp1_dp1_mb2_gpu4 -> layer6_expert0_mlp2_dp1_mb2_gpu4
	layer6_expert1_mlp1_dp0_mb2_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb2_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb2_gpu4 -> layer6_expert1_mlp1_dp0_mb2_gpu5
	layer6_router_dp1_mb2_gpu4 -> layer6_expert1_mlp1_dp1_mb2_gpu5
	layer6_expert1_mlp2_dp0_mb2_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb2_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb2_gpu5 -> layer6_expert1_mlp2_dp0_mb2_gpu5
	layer6_expert1_mlp1_dp1_mb2_gpu5 -> layer6_expert1_mlp2_dp1_mb2_gpu5
	layer6_expert2_mlp1_dp0_mb2_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb2_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb2_gpu4 -> layer6_expert2_mlp1_dp0_mb2_gpu6
	layer6_router_dp1_mb2_gpu4 -> layer6_expert2_mlp1_dp1_mb2_gpu6
	layer6_expert2_mlp2_dp0_mb2_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb2_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb2_gpu6 -> layer6_expert2_mlp2_dp0_mb2_gpu6
	layer6_expert2_mlp1_dp1_mb2_gpu6 -> layer6_expert2_mlp2_dp1_mb2_gpu6
	layer6_expert3_mlp1_dp0_mb2_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb2_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb2_gpu4 -> layer6_expert3_mlp1_dp0_mb2_gpu7
	layer6_router_dp1_mb2_gpu4 -> layer6_expert3_mlp1_dp1_mb2_gpu7
	layer6_expert3_mlp2_dp0_mb2_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb2_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb2_gpu7 -> layer6_expert3_mlp2_dp0_mb2_gpu7
	layer6_expert3_mlp1_dp1_mb2_gpu7 -> layer6_expert3_mlp2_dp1_mb2_gpu7
	layer6_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb2_gpu4 -> layer6_expert_agg_dp0_mb2
	layer6_expert0_mlp2_dp1_mb2_gpu4 -> layer6_expert_agg_dp1_mb2
	layer6_expert1_mlp2_dp0_mb2_gpu5 -> layer6_expert_agg_dp0_mb2
	layer6_expert1_mlp2_dp1_mb2_gpu5 -> layer6_expert_agg_dp1_mb2
	layer6_expert2_mlp2_dp0_mb2_gpu6 -> layer6_expert_agg_dp0_mb2
	layer6_expert2_mlp2_dp1_mb2_gpu6 -> layer6_expert_agg_dp1_mb2
	layer6_expert3_mlp2_dp0_mb2_gpu7 -> layer6_expert_agg_dp0_mb2
	layer6_expert3_mlp2_dp1_mb2_gpu7 -> layer6_expert_agg_dp1_mb2
	layer7_qkv_dp0_mb2_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb2_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb2 -> layer7_qkv_dp0_mb2_gpu4
	layer6_expert_agg_dp1_mb2 -> layer7_qkv_dp1_mb2_gpu4
	layer7_attn_scores_dp0_mb2_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb2_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb2_gpu4 -> layer7_attn_scores_dp0_mb2_gpu4
	layer7_qkv_dp1_mb2_gpu4 -> layer7_attn_scores_dp1_mb2_gpu4
	layer7_softmax_dp0_mb2_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb2_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb2_gpu4 -> layer7_softmax_dp0_mb2_gpu4
	layer7_attn_scores_dp1_mb2_gpu4 -> layer7_softmax_dp1_mb2_gpu4
	layer7_attn_out_dp0_mb2_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb2_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb2_gpu4 -> layer7_attn_out_dp0_mb2_gpu4
	layer7_softmax_dp1_mb2_gpu4 -> layer7_attn_out_dp1_mb2_gpu4
	layer7_qkv_dp0_mb2_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb2_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb2 -> layer7_qkv_dp0_mb2_gpu5
	layer6_expert_agg_dp1_mb2 -> layer7_qkv_dp1_mb2_gpu5
	layer7_attn_scores_dp0_mb2_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb2_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb2_gpu5 -> layer7_attn_scores_dp0_mb2_gpu5
	layer7_qkv_dp1_mb2_gpu5 -> layer7_attn_scores_dp1_mb2_gpu5
	layer7_softmax_dp0_mb2_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb2_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb2_gpu5 -> layer7_softmax_dp0_mb2_gpu5
	layer7_attn_scores_dp1_mb2_gpu5 -> layer7_softmax_dp1_mb2_gpu5
	layer7_attn_out_dp0_mb2_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb2_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb2_gpu5 -> layer7_attn_out_dp0_mb2_gpu5
	layer7_softmax_dp1_mb2_gpu5 -> layer7_attn_out_dp1_mb2_gpu5
	layer7_qkv_dp0_mb2_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb2_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb2 -> layer7_qkv_dp0_mb2_gpu6
	layer6_expert_agg_dp1_mb2 -> layer7_qkv_dp1_mb2_gpu6
	layer7_attn_scores_dp0_mb2_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb2_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb2_gpu6 -> layer7_attn_scores_dp0_mb2_gpu6
	layer7_qkv_dp1_mb2_gpu6 -> layer7_attn_scores_dp1_mb2_gpu6
	layer7_softmax_dp0_mb2_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb2_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb2_gpu6 -> layer7_softmax_dp0_mb2_gpu6
	layer7_attn_scores_dp1_mb2_gpu6 -> layer7_softmax_dp1_mb2_gpu6
	layer7_attn_out_dp0_mb2_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb2_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb2_gpu6 -> layer7_attn_out_dp0_mb2_gpu6
	layer7_softmax_dp1_mb2_gpu6 -> layer7_attn_out_dp1_mb2_gpu6
	layer7_qkv_dp0_mb2_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb2_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb2 -> layer7_qkv_dp0_mb2_gpu7
	layer6_expert_agg_dp1_mb2 -> layer7_qkv_dp1_mb2_gpu7
	layer7_attn_scores_dp0_mb2_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb2_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb2_gpu7 -> layer7_attn_scores_dp0_mb2_gpu7
	layer7_qkv_dp1_mb2_gpu7 -> layer7_attn_scores_dp1_mb2_gpu7
	layer7_softmax_dp0_mb2_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb2_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb2_gpu7 -> layer7_softmax_dp0_mb2_gpu7
	layer7_attn_scores_dp1_mb2_gpu7 -> layer7_softmax_dp1_mb2_gpu7
	layer7_attn_out_dp0_mb2_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb2_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb2_gpu7 -> layer7_attn_out_dp0_mb2_gpu7
	layer7_softmax_dp1_mb2_gpu7 -> layer7_attn_out_dp1_mb2_gpu7
	layer7_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb2_gpu4 -> layer7_attn_allreduce_dp0_mb2
	layer7_attn_out_dp1_mb2_gpu4 -> layer7_attn_allreduce_dp1_mb2
	layer7_attn_out_dp0_mb2_gpu5 -> layer7_attn_allreduce_dp0_mb2
	layer7_attn_out_dp1_mb2_gpu5 -> layer7_attn_allreduce_dp1_mb2
	layer7_attn_out_dp0_mb2_gpu6 -> layer7_attn_allreduce_dp0_mb2
	layer7_attn_out_dp1_mb2_gpu6 -> layer7_attn_allreduce_dp1_mb2
	layer7_attn_out_dp0_mb2_gpu7 -> layer7_attn_allreduce_dp0_mb2
	layer7_attn_out_dp1_mb2_gpu7 -> layer7_attn_allreduce_dp1_mb2
	layer7_router_dp0_mb2_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb2_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb2 -> layer7_router_dp0_mb2_gpu4
	layer7_attn_allreduce_dp1_mb2 -> layer7_router_dp1_mb2_gpu4
	layer7_expert0_mlp1_dp0_mb2_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb2_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb2_gpu4 -> layer7_expert0_mlp1_dp0_mb2_gpu4
	layer7_router_dp1_mb2_gpu4 -> layer7_expert0_mlp1_dp1_mb2_gpu4
	layer7_expert0_mlp2_dp0_mb2_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb2_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb2_gpu4 -> layer7_expert0_mlp2_dp0_mb2_gpu4
	layer7_expert0_mlp1_dp1_mb2_gpu4 -> layer7_expert0_mlp2_dp1_mb2_gpu4
	layer7_expert1_mlp1_dp0_mb2_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb2_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb2_gpu4 -> layer7_expert1_mlp1_dp0_mb2_gpu5
	layer7_router_dp1_mb2_gpu4 -> layer7_expert1_mlp1_dp1_mb2_gpu5
	layer7_expert1_mlp2_dp0_mb2_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb2_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb2_gpu5 -> layer7_expert1_mlp2_dp0_mb2_gpu5
	layer7_expert1_mlp1_dp1_mb2_gpu5 -> layer7_expert1_mlp2_dp1_mb2_gpu5
	layer7_expert2_mlp1_dp0_mb2_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb2_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb2_gpu4 -> layer7_expert2_mlp1_dp0_mb2_gpu6
	layer7_router_dp1_mb2_gpu4 -> layer7_expert2_mlp1_dp1_mb2_gpu6
	layer7_expert2_mlp2_dp0_mb2_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb2_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb2_gpu6 -> layer7_expert2_mlp2_dp0_mb2_gpu6
	layer7_expert2_mlp1_dp1_mb2_gpu6 -> layer7_expert2_mlp2_dp1_mb2_gpu6
	layer7_expert3_mlp1_dp0_mb2_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb2_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb2_gpu4 -> layer7_expert3_mlp1_dp0_mb2_gpu7
	layer7_router_dp1_mb2_gpu4 -> layer7_expert3_mlp1_dp1_mb2_gpu7
	layer7_expert3_mlp2_dp0_mb2_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb2_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb2_gpu7 -> layer7_expert3_mlp2_dp0_mb2_gpu7
	layer7_expert3_mlp1_dp1_mb2_gpu7 -> layer7_expert3_mlp2_dp1_mb2_gpu7
	layer7_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb2_gpu4 -> layer7_expert_agg_dp0_mb2
	layer7_expert0_mlp2_dp1_mb2_gpu4 -> layer7_expert_agg_dp1_mb2
	layer7_expert1_mlp2_dp0_mb2_gpu5 -> layer7_expert_agg_dp0_mb2
	layer7_expert1_mlp2_dp1_mb2_gpu5 -> layer7_expert_agg_dp1_mb2
	layer7_expert2_mlp2_dp0_mb2_gpu6 -> layer7_expert_agg_dp0_mb2
	layer7_expert2_mlp2_dp1_mb2_gpu6 -> layer7_expert_agg_dp1_mb2
	layer7_expert3_mlp2_dp0_mb2_gpu7 -> layer7_expert_agg_dp0_mb2
	layer7_expert3_mlp2_dp1_mb2_gpu7 -> layer7_expert_agg_dp1_mb2
	layer4_qkv_dp0_mb3_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb3_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb3 -> layer4_qkv_dp0_mb3_gpu4
	layer3_expert_agg_dp1_mb3 -> layer4_qkv_dp1_mb3_gpu4
	layer4_attn_scores_dp0_mb3_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb3_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb3_gpu4 -> layer4_attn_scores_dp0_mb3_gpu4
	layer4_qkv_dp1_mb3_gpu4 -> layer4_attn_scores_dp1_mb3_gpu4
	layer4_softmax_dp0_mb3_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb3_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb3_gpu4 -> layer4_softmax_dp0_mb3_gpu4
	layer4_attn_scores_dp1_mb3_gpu4 -> layer4_softmax_dp1_mb3_gpu4
	layer4_attn_out_dp0_mb3_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb3_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb3_gpu4 -> layer4_attn_out_dp0_mb3_gpu4
	layer4_softmax_dp1_mb3_gpu4 -> layer4_attn_out_dp1_mb3_gpu4
	layer4_qkv_dp0_mb3_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb3_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb3 -> layer4_qkv_dp0_mb3_gpu5
	layer3_expert_agg_dp1_mb3 -> layer4_qkv_dp1_mb3_gpu5
	layer4_attn_scores_dp0_mb3_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb3_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb3_gpu5 -> layer4_attn_scores_dp0_mb3_gpu5
	layer4_qkv_dp1_mb3_gpu5 -> layer4_attn_scores_dp1_mb3_gpu5
	layer4_softmax_dp0_mb3_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb3_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb3_gpu5 -> layer4_softmax_dp0_mb3_gpu5
	layer4_attn_scores_dp1_mb3_gpu5 -> layer4_softmax_dp1_mb3_gpu5
	layer4_attn_out_dp0_mb3_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb3_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb3_gpu5 -> layer4_attn_out_dp0_mb3_gpu5
	layer4_softmax_dp1_mb3_gpu5 -> layer4_attn_out_dp1_mb3_gpu5
	layer4_qkv_dp0_mb3_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb3_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb3 -> layer4_qkv_dp0_mb3_gpu6
	layer3_expert_agg_dp1_mb3 -> layer4_qkv_dp1_mb3_gpu6
	layer4_attn_scores_dp0_mb3_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb3_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb3_gpu6 -> layer4_attn_scores_dp0_mb3_gpu6
	layer4_qkv_dp1_mb3_gpu6 -> layer4_attn_scores_dp1_mb3_gpu6
	layer4_softmax_dp0_mb3_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb3_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb3_gpu6 -> layer4_softmax_dp0_mb3_gpu6
	layer4_attn_scores_dp1_mb3_gpu6 -> layer4_softmax_dp1_mb3_gpu6
	layer4_attn_out_dp0_mb3_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb3_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb3_gpu6 -> layer4_attn_out_dp0_mb3_gpu6
	layer4_softmax_dp1_mb3_gpu6 -> layer4_attn_out_dp1_mb3_gpu6
	layer4_qkv_dp0_mb3_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb3_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb3 -> layer4_qkv_dp0_mb3_gpu7
	layer3_expert_agg_dp1_mb3 -> layer4_qkv_dp1_mb3_gpu7
	layer4_attn_scores_dp0_mb3_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb3_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb3_gpu7 -> layer4_attn_scores_dp0_mb3_gpu7
	layer4_qkv_dp1_mb3_gpu7 -> layer4_attn_scores_dp1_mb3_gpu7
	layer4_softmax_dp0_mb3_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb3_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb3_gpu7 -> layer4_softmax_dp0_mb3_gpu7
	layer4_attn_scores_dp1_mb3_gpu7 -> layer4_softmax_dp1_mb3_gpu7
	layer4_attn_out_dp0_mb3_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb3_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb3_gpu7 -> layer4_attn_out_dp0_mb3_gpu7
	layer4_softmax_dp1_mb3_gpu7 -> layer4_attn_out_dp1_mb3_gpu7
	layer4_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb3_gpu4 -> layer4_attn_allreduce_dp0_mb3
	layer4_attn_out_dp1_mb3_gpu4 -> layer4_attn_allreduce_dp1_mb3
	layer4_attn_out_dp0_mb3_gpu5 -> layer4_attn_allreduce_dp0_mb3
	layer4_attn_out_dp1_mb3_gpu5 -> layer4_attn_allreduce_dp1_mb3
	layer4_attn_out_dp0_mb3_gpu6 -> layer4_attn_allreduce_dp0_mb3
	layer4_attn_out_dp1_mb3_gpu6 -> layer4_attn_allreduce_dp1_mb3
	layer4_attn_out_dp0_mb3_gpu7 -> layer4_attn_allreduce_dp0_mb3
	layer4_attn_out_dp1_mb3_gpu7 -> layer4_attn_allreduce_dp1_mb3
	layer4_router_dp0_mb3_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb3_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb3 -> layer4_router_dp0_mb3_gpu4
	layer4_attn_allreduce_dp1_mb3 -> layer4_router_dp1_mb3_gpu4
	layer4_expert0_mlp1_dp0_mb3_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb3_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb3_gpu4 -> layer4_expert0_mlp1_dp0_mb3_gpu4
	layer4_router_dp1_mb3_gpu4 -> layer4_expert0_mlp1_dp1_mb3_gpu4
	layer4_expert0_mlp2_dp0_mb3_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb3_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb3_gpu4 -> layer4_expert0_mlp2_dp0_mb3_gpu4
	layer4_expert0_mlp1_dp1_mb3_gpu4 -> layer4_expert0_mlp2_dp1_mb3_gpu4
	layer4_expert1_mlp1_dp0_mb3_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb3_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb3_gpu4 -> layer4_expert1_mlp1_dp0_mb3_gpu5
	layer4_router_dp1_mb3_gpu4 -> layer4_expert1_mlp1_dp1_mb3_gpu5
	layer4_expert1_mlp2_dp0_mb3_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb3_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb3_gpu5 -> layer4_expert1_mlp2_dp0_mb3_gpu5
	layer4_expert1_mlp1_dp1_mb3_gpu5 -> layer4_expert1_mlp2_dp1_mb3_gpu5
	layer4_expert2_mlp1_dp0_mb3_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb3_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb3_gpu4 -> layer4_expert2_mlp1_dp0_mb3_gpu6
	layer4_router_dp1_mb3_gpu4 -> layer4_expert2_mlp1_dp1_mb3_gpu6
	layer4_expert2_mlp2_dp0_mb3_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb3_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb3_gpu6 -> layer4_expert2_mlp2_dp0_mb3_gpu6
	layer4_expert2_mlp1_dp1_mb3_gpu6 -> layer4_expert2_mlp2_dp1_mb3_gpu6
	layer4_expert3_mlp1_dp0_mb3_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb3_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb3_gpu4 -> layer4_expert3_mlp1_dp0_mb3_gpu7
	layer4_router_dp1_mb3_gpu4 -> layer4_expert3_mlp1_dp1_mb3_gpu7
	layer4_expert3_mlp2_dp0_mb3_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb3_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb3_gpu7 -> layer4_expert3_mlp2_dp0_mb3_gpu7
	layer4_expert3_mlp1_dp1_mb3_gpu7 -> layer4_expert3_mlp2_dp1_mb3_gpu7
	layer4_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb3_gpu4 -> layer4_expert_agg_dp0_mb3
	layer4_expert0_mlp2_dp1_mb3_gpu4 -> layer4_expert_agg_dp1_mb3
	layer4_expert1_mlp2_dp0_mb3_gpu5 -> layer4_expert_agg_dp0_mb3
	layer4_expert1_mlp2_dp1_mb3_gpu5 -> layer4_expert_agg_dp1_mb3
	layer4_expert2_mlp2_dp0_mb3_gpu6 -> layer4_expert_agg_dp0_mb3
	layer4_expert2_mlp2_dp1_mb3_gpu6 -> layer4_expert_agg_dp1_mb3
	layer4_expert3_mlp2_dp0_mb3_gpu7 -> layer4_expert_agg_dp0_mb3
	layer4_expert3_mlp2_dp1_mb3_gpu7 -> layer4_expert_agg_dp1_mb3
	layer5_qkv_dp0_mb3_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb3_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb3 -> layer5_qkv_dp0_mb3_gpu4
	layer4_expert_agg_dp1_mb3 -> layer5_qkv_dp1_mb3_gpu4
	layer5_attn_scores_dp0_mb3_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb3_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb3_gpu4 -> layer5_attn_scores_dp0_mb3_gpu4
	layer5_qkv_dp1_mb3_gpu4 -> layer5_attn_scores_dp1_mb3_gpu4
	layer5_softmax_dp0_mb3_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb3_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb3_gpu4 -> layer5_softmax_dp0_mb3_gpu4
	layer5_attn_scores_dp1_mb3_gpu4 -> layer5_softmax_dp1_mb3_gpu4
	layer5_attn_out_dp0_mb3_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb3_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb3_gpu4 -> layer5_attn_out_dp0_mb3_gpu4
	layer5_softmax_dp1_mb3_gpu4 -> layer5_attn_out_dp1_mb3_gpu4
	layer5_qkv_dp0_mb3_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb3_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb3 -> layer5_qkv_dp0_mb3_gpu5
	layer4_expert_agg_dp1_mb3 -> layer5_qkv_dp1_mb3_gpu5
	layer5_attn_scores_dp0_mb3_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb3_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb3_gpu5 -> layer5_attn_scores_dp0_mb3_gpu5
	layer5_qkv_dp1_mb3_gpu5 -> layer5_attn_scores_dp1_mb3_gpu5
	layer5_softmax_dp0_mb3_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb3_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb3_gpu5 -> layer5_softmax_dp0_mb3_gpu5
	layer5_attn_scores_dp1_mb3_gpu5 -> layer5_softmax_dp1_mb3_gpu5
	layer5_attn_out_dp0_mb3_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb3_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb3_gpu5 -> layer5_attn_out_dp0_mb3_gpu5
	layer5_softmax_dp1_mb3_gpu5 -> layer5_attn_out_dp1_mb3_gpu5
	layer5_qkv_dp0_mb3_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb3_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb3 -> layer5_qkv_dp0_mb3_gpu6
	layer4_expert_agg_dp1_mb3 -> layer5_qkv_dp1_mb3_gpu6
	layer5_attn_scores_dp0_mb3_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb3_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb3_gpu6 -> layer5_attn_scores_dp0_mb3_gpu6
	layer5_qkv_dp1_mb3_gpu6 -> layer5_attn_scores_dp1_mb3_gpu6
	layer5_softmax_dp0_mb3_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb3_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb3_gpu6 -> layer5_softmax_dp0_mb3_gpu6
	layer5_attn_scores_dp1_mb3_gpu6 -> layer5_softmax_dp1_mb3_gpu6
	layer5_attn_out_dp0_mb3_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb3_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb3_gpu6 -> layer5_attn_out_dp0_mb3_gpu6
	layer5_softmax_dp1_mb3_gpu6 -> layer5_attn_out_dp1_mb3_gpu6
	layer5_qkv_dp0_mb3_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb3_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb3 -> layer5_qkv_dp0_mb3_gpu7
	layer4_expert_agg_dp1_mb3 -> layer5_qkv_dp1_mb3_gpu7
	layer5_attn_scores_dp0_mb3_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb3_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb3_gpu7 -> layer5_attn_scores_dp0_mb3_gpu7
	layer5_qkv_dp1_mb3_gpu7 -> layer5_attn_scores_dp1_mb3_gpu7
	layer5_softmax_dp0_mb3_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb3_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb3_gpu7 -> layer5_softmax_dp0_mb3_gpu7
	layer5_attn_scores_dp1_mb3_gpu7 -> layer5_softmax_dp1_mb3_gpu7
	layer5_attn_out_dp0_mb3_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb3_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb3_gpu7 -> layer5_attn_out_dp0_mb3_gpu7
	layer5_softmax_dp1_mb3_gpu7 -> layer5_attn_out_dp1_mb3_gpu7
	layer5_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb3_gpu4 -> layer5_attn_allreduce_dp0_mb3
	layer5_attn_out_dp1_mb3_gpu4 -> layer5_attn_allreduce_dp1_mb3
	layer5_attn_out_dp0_mb3_gpu5 -> layer5_attn_allreduce_dp0_mb3
	layer5_attn_out_dp1_mb3_gpu5 -> layer5_attn_allreduce_dp1_mb3
	layer5_attn_out_dp0_mb3_gpu6 -> layer5_attn_allreduce_dp0_mb3
	layer5_attn_out_dp1_mb3_gpu6 -> layer5_attn_allreduce_dp1_mb3
	layer5_attn_out_dp0_mb3_gpu7 -> layer5_attn_allreduce_dp0_mb3
	layer5_attn_out_dp1_mb3_gpu7 -> layer5_attn_allreduce_dp1_mb3
	layer5_router_dp0_mb3_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb3_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb3 -> layer5_router_dp0_mb3_gpu4
	layer5_attn_allreduce_dp1_mb3 -> layer5_router_dp1_mb3_gpu4
	layer5_expert0_mlp1_dp0_mb3_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb3_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb3_gpu4 -> layer5_expert0_mlp1_dp0_mb3_gpu4
	layer5_router_dp1_mb3_gpu4 -> layer5_expert0_mlp1_dp1_mb3_gpu4
	layer5_expert0_mlp2_dp0_mb3_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb3_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb3_gpu4 -> layer5_expert0_mlp2_dp0_mb3_gpu4
	layer5_expert0_mlp1_dp1_mb3_gpu4 -> layer5_expert0_mlp2_dp1_mb3_gpu4
	layer5_expert1_mlp1_dp0_mb3_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb3_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb3_gpu4 -> layer5_expert1_mlp1_dp0_mb3_gpu5
	layer5_router_dp1_mb3_gpu4 -> layer5_expert1_mlp1_dp1_mb3_gpu5
	layer5_expert1_mlp2_dp0_mb3_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb3_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb3_gpu5 -> layer5_expert1_mlp2_dp0_mb3_gpu5
	layer5_expert1_mlp1_dp1_mb3_gpu5 -> layer5_expert1_mlp2_dp1_mb3_gpu5
	layer5_expert2_mlp1_dp0_mb3_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb3_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb3_gpu4 -> layer5_expert2_mlp1_dp0_mb3_gpu6
	layer5_router_dp1_mb3_gpu4 -> layer5_expert2_mlp1_dp1_mb3_gpu6
	layer5_expert2_mlp2_dp0_mb3_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb3_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb3_gpu6 -> layer5_expert2_mlp2_dp0_mb3_gpu6
	layer5_expert2_mlp1_dp1_mb3_gpu6 -> layer5_expert2_mlp2_dp1_mb3_gpu6
	layer5_expert3_mlp1_dp0_mb3_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb3_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb3_gpu4 -> layer5_expert3_mlp1_dp0_mb3_gpu7
	layer5_router_dp1_mb3_gpu4 -> layer5_expert3_mlp1_dp1_mb3_gpu7
	layer5_expert3_mlp2_dp0_mb3_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb3_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb3_gpu7 -> layer5_expert3_mlp2_dp0_mb3_gpu7
	layer5_expert3_mlp1_dp1_mb3_gpu7 -> layer5_expert3_mlp2_dp1_mb3_gpu7
	layer5_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb3_gpu4 -> layer5_expert_agg_dp0_mb3
	layer5_expert0_mlp2_dp1_mb3_gpu4 -> layer5_expert_agg_dp1_mb3
	layer5_expert1_mlp2_dp0_mb3_gpu5 -> layer5_expert_agg_dp0_mb3
	layer5_expert1_mlp2_dp1_mb3_gpu5 -> layer5_expert_agg_dp1_mb3
	layer5_expert2_mlp2_dp0_mb3_gpu6 -> layer5_expert_agg_dp0_mb3
	layer5_expert2_mlp2_dp1_mb3_gpu6 -> layer5_expert_agg_dp1_mb3
	layer5_expert3_mlp2_dp0_mb3_gpu7 -> layer5_expert_agg_dp0_mb3
	layer5_expert3_mlp2_dp1_mb3_gpu7 -> layer5_expert_agg_dp1_mb3
	layer6_qkv_dp0_mb3_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb3_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb3 -> layer6_qkv_dp0_mb3_gpu4
	layer5_expert_agg_dp1_mb3 -> layer6_qkv_dp1_mb3_gpu4
	layer6_attn_scores_dp0_mb3_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb3_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb3_gpu4 -> layer6_attn_scores_dp0_mb3_gpu4
	layer6_qkv_dp1_mb3_gpu4 -> layer6_attn_scores_dp1_mb3_gpu4
	layer6_softmax_dp0_mb3_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb3_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb3_gpu4 -> layer6_softmax_dp0_mb3_gpu4
	layer6_attn_scores_dp1_mb3_gpu4 -> layer6_softmax_dp1_mb3_gpu4
	layer6_attn_out_dp0_mb3_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb3_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb3_gpu4 -> layer6_attn_out_dp0_mb3_gpu4
	layer6_softmax_dp1_mb3_gpu4 -> layer6_attn_out_dp1_mb3_gpu4
	layer6_qkv_dp0_mb3_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb3_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb3 -> layer6_qkv_dp0_mb3_gpu5
	layer5_expert_agg_dp1_mb3 -> layer6_qkv_dp1_mb3_gpu5
	layer6_attn_scores_dp0_mb3_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb3_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb3_gpu5 -> layer6_attn_scores_dp0_mb3_gpu5
	layer6_qkv_dp1_mb3_gpu5 -> layer6_attn_scores_dp1_mb3_gpu5
	layer6_softmax_dp0_mb3_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb3_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb3_gpu5 -> layer6_softmax_dp0_mb3_gpu5
	layer6_attn_scores_dp1_mb3_gpu5 -> layer6_softmax_dp1_mb3_gpu5
	layer6_attn_out_dp0_mb3_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb3_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb3_gpu5 -> layer6_attn_out_dp0_mb3_gpu5
	layer6_softmax_dp1_mb3_gpu5 -> layer6_attn_out_dp1_mb3_gpu5
	layer6_qkv_dp0_mb3_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb3_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb3 -> layer6_qkv_dp0_mb3_gpu6
	layer5_expert_agg_dp1_mb3 -> layer6_qkv_dp1_mb3_gpu6
	layer6_attn_scores_dp0_mb3_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb3_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb3_gpu6 -> layer6_attn_scores_dp0_mb3_gpu6
	layer6_qkv_dp1_mb3_gpu6 -> layer6_attn_scores_dp1_mb3_gpu6
	layer6_softmax_dp0_mb3_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb3_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb3_gpu6 -> layer6_softmax_dp0_mb3_gpu6
	layer6_attn_scores_dp1_mb3_gpu6 -> layer6_softmax_dp1_mb3_gpu6
	layer6_attn_out_dp0_mb3_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb3_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb3_gpu6 -> layer6_attn_out_dp0_mb3_gpu6
	layer6_softmax_dp1_mb3_gpu6 -> layer6_attn_out_dp1_mb3_gpu6
	layer6_qkv_dp0_mb3_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb3_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb3 -> layer6_qkv_dp0_mb3_gpu7
	layer5_expert_agg_dp1_mb3 -> layer6_qkv_dp1_mb3_gpu7
	layer6_attn_scores_dp0_mb3_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb3_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb3_gpu7 -> layer6_attn_scores_dp0_mb3_gpu7
	layer6_qkv_dp1_mb3_gpu7 -> layer6_attn_scores_dp1_mb3_gpu7
	layer6_softmax_dp0_mb3_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb3_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb3_gpu7 -> layer6_softmax_dp0_mb3_gpu7
	layer6_attn_scores_dp1_mb3_gpu7 -> layer6_softmax_dp1_mb3_gpu7
	layer6_attn_out_dp0_mb3_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb3_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb3_gpu7 -> layer6_attn_out_dp0_mb3_gpu7
	layer6_softmax_dp1_mb3_gpu7 -> layer6_attn_out_dp1_mb3_gpu7
	layer6_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb3_gpu4 -> layer6_attn_allreduce_dp0_mb3
	layer6_attn_out_dp1_mb3_gpu4 -> layer6_attn_allreduce_dp1_mb3
	layer6_attn_out_dp0_mb3_gpu5 -> layer6_attn_allreduce_dp0_mb3
	layer6_attn_out_dp1_mb3_gpu5 -> layer6_attn_allreduce_dp1_mb3
	layer6_attn_out_dp0_mb3_gpu6 -> layer6_attn_allreduce_dp0_mb3
	layer6_attn_out_dp1_mb3_gpu6 -> layer6_attn_allreduce_dp1_mb3
	layer6_attn_out_dp0_mb3_gpu7 -> layer6_attn_allreduce_dp0_mb3
	layer6_attn_out_dp1_mb3_gpu7 -> layer6_attn_allreduce_dp1_mb3
	layer6_router_dp0_mb3_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb3_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb3 -> layer6_router_dp0_mb3_gpu4
	layer6_attn_allreduce_dp1_mb3 -> layer6_router_dp1_mb3_gpu4
	layer6_expert0_mlp1_dp0_mb3_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb3_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb3_gpu4 -> layer6_expert0_mlp1_dp0_mb3_gpu4
	layer6_router_dp1_mb3_gpu4 -> layer6_expert0_mlp1_dp1_mb3_gpu4
	layer6_expert0_mlp2_dp0_mb3_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb3_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb3_gpu4 -> layer6_expert0_mlp2_dp0_mb3_gpu4
	layer6_expert0_mlp1_dp1_mb3_gpu4 -> layer6_expert0_mlp2_dp1_mb3_gpu4
	layer6_expert1_mlp1_dp0_mb3_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb3_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb3_gpu4 -> layer6_expert1_mlp1_dp0_mb3_gpu5
	layer6_router_dp1_mb3_gpu4 -> layer6_expert1_mlp1_dp1_mb3_gpu5
	layer6_expert1_mlp2_dp0_mb3_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb3_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb3_gpu5 -> layer6_expert1_mlp2_dp0_mb3_gpu5
	layer6_expert1_mlp1_dp1_mb3_gpu5 -> layer6_expert1_mlp2_dp1_mb3_gpu5
	layer6_expert2_mlp1_dp0_mb3_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb3_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb3_gpu4 -> layer6_expert2_mlp1_dp0_mb3_gpu6
	layer6_router_dp1_mb3_gpu4 -> layer6_expert2_mlp1_dp1_mb3_gpu6
	layer6_expert2_mlp2_dp0_mb3_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb3_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb3_gpu6 -> layer6_expert2_mlp2_dp0_mb3_gpu6
	layer6_expert2_mlp1_dp1_mb3_gpu6 -> layer6_expert2_mlp2_dp1_mb3_gpu6
	layer6_expert3_mlp1_dp0_mb3_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb3_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb3_gpu4 -> layer6_expert3_mlp1_dp0_mb3_gpu7
	layer6_router_dp1_mb3_gpu4 -> layer6_expert3_mlp1_dp1_mb3_gpu7
	layer6_expert3_mlp2_dp0_mb3_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb3_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb3_gpu7 -> layer6_expert3_mlp2_dp0_mb3_gpu7
	layer6_expert3_mlp1_dp1_mb3_gpu7 -> layer6_expert3_mlp2_dp1_mb3_gpu7
	layer6_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb3_gpu4 -> layer6_expert_agg_dp0_mb3
	layer6_expert0_mlp2_dp1_mb3_gpu4 -> layer6_expert_agg_dp1_mb3
	layer6_expert1_mlp2_dp0_mb3_gpu5 -> layer6_expert_agg_dp0_mb3
	layer6_expert1_mlp2_dp1_mb3_gpu5 -> layer6_expert_agg_dp1_mb3
	layer6_expert2_mlp2_dp0_mb3_gpu6 -> layer6_expert_agg_dp0_mb3
	layer6_expert2_mlp2_dp1_mb3_gpu6 -> layer6_expert_agg_dp1_mb3
	layer6_expert3_mlp2_dp0_mb3_gpu7 -> layer6_expert_agg_dp0_mb3
	layer6_expert3_mlp2_dp1_mb3_gpu7 -> layer6_expert_agg_dp1_mb3
	layer7_qkv_dp0_mb3_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb3_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb3 -> layer7_qkv_dp0_mb3_gpu4
	layer6_expert_agg_dp1_mb3 -> layer7_qkv_dp1_mb3_gpu4
	layer7_attn_scores_dp0_mb3_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb3_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb3_gpu4 -> layer7_attn_scores_dp0_mb3_gpu4
	layer7_qkv_dp1_mb3_gpu4 -> layer7_attn_scores_dp1_mb3_gpu4
	layer7_softmax_dp0_mb3_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb3_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb3_gpu4 -> layer7_softmax_dp0_mb3_gpu4
	layer7_attn_scores_dp1_mb3_gpu4 -> layer7_softmax_dp1_mb3_gpu4
	layer7_attn_out_dp0_mb3_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb3_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb3_gpu4 -> layer7_attn_out_dp0_mb3_gpu4
	layer7_softmax_dp1_mb3_gpu4 -> layer7_attn_out_dp1_mb3_gpu4
	layer7_qkv_dp0_mb3_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb3_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb3 -> layer7_qkv_dp0_mb3_gpu5
	layer6_expert_agg_dp1_mb3 -> layer7_qkv_dp1_mb3_gpu5
	layer7_attn_scores_dp0_mb3_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb3_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb3_gpu5 -> layer7_attn_scores_dp0_mb3_gpu5
	layer7_qkv_dp1_mb3_gpu5 -> layer7_attn_scores_dp1_mb3_gpu5
	layer7_softmax_dp0_mb3_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb3_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb3_gpu5 -> layer7_softmax_dp0_mb3_gpu5
	layer7_attn_scores_dp1_mb3_gpu5 -> layer7_softmax_dp1_mb3_gpu5
	layer7_attn_out_dp0_mb3_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb3_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb3_gpu5 -> layer7_attn_out_dp0_mb3_gpu5
	layer7_softmax_dp1_mb3_gpu5 -> layer7_attn_out_dp1_mb3_gpu5
	layer7_qkv_dp0_mb3_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb3_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb3 -> layer7_qkv_dp0_mb3_gpu6
	layer6_expert_agg_dp1_mb3 -> layer7_qkv_dp1_mb3_gpu6
	layer7_attn_scores_dp0_mb3_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb3_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb3_gpu6 -> layer7_attn_scores_dp0_mb3_gpu6
	layer7_qkv_dp1_mb3_gpu6 -> layer7_attn_scores_dp1_mb3_gpu6
	layer7_softmax_dp0_mb3_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb3_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb3_gpu6 -> layer7_softmax_dp0_mb3_gpu6
	layer7_attn_scores_dp1_mb3_gpu6 -> layer7_softmax_dp1_mb3_gpu6
	layer7_attn_out_dp0_mb3_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb3_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb3_gpu6 -> layer7_attn_out_dp0_mb3_gpu6
	layer7_softmax_dp1_mb3_gpu6 -> layer7_attn_out_dp1_mb3_gpu6
	layer7_qkv_dp0_mb3_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb3_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb3 -> layer7_qkv_dp0_mb3_gpu7
	layer6_expert_agg_dp1_mb3 -> layer7_qkv_dp1_mb3_gpu7
	layer7_attn_scores_dp0_mb3_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb3_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb3_gpu7 -> layer7_attn_scores_dp0_mb3_gpu7
	layer7_qkv_dp1_mb3_gpu7 -> layer7_attn_scores_dp1_mb3_gpu7
	layer7_softmax_dp0_mb3_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb3_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb3_gpu7 -> layer7_softmax_dp0_mb3_gpu7
	layer7_attn_scores_dp1_mb3_gpu7 -> layer7_softmax_dp1_mb3_gpu7
	layer7_attn_out_dp0_mb3_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb3_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb3_gpu7 -> layer7_attn_out_dp0_mb3_gpu7
	layer7_softmax_dp1_mb3_gpu7 -> layer7_attn_out_dp1_mb3_gpu7
	layer7_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb3_gpu4 -> layer7_attn_allreduce_dp0_mb3
	layer7_attn_out_dp1_mb3_gpu4 -> layer7_attn_allreduce_dp1_mb3
	layer7_attn_out_dp0_mb3_gpu5 -> layer7_attn_allreduce_dp0_mb3
	layer7_attn_out_dp1_mb3_gpu5 -> layer7_attn_allreduce_dp1_mb3
	layer7_attn_out_dp0_mb3_gpu6 -> layer7_attn_allreduce_dp0_mb3
	layer7_attn_out_dp1_mb3_gpu6 -> layer7_attn_allreduce_dp1_mb3
	layer7_attn_out_dp0_mb3_gpu7 -> layer7_attn_allreduce_dp0_mb3
	layer7_attn_out_dp1_mb3_gpu7 -> layer7_attn_allreduce_dp1_mb3
	layer7_router_dp0_mb3_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb3_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb3 -> layer7_router_dp0_mb3_gpu4
	layer7_attn_allreduce_dp1_mb3 -> layer7_router_dp1_mb3_gpu4
	layer7_expert0_mlp1_dp0_mb3_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb3_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb3_gpu4 -> layer7_expert0_mlp1_dp0_mb3_gpu4
	layer7_router_dp1_mb3_gpu4 -> layer7_expert0_mlp1_dp1_mb3_gpu4
	layer7_expert0_mlp2_dp0_mb3_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb3_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb3_gpu4 -> layer7_expert0_mlp2_dp0_mb3_gpu4
	layer7_expert0_mlp1_dp1_mb3_gpu4 -> layer7_expert0_mlp2_dp1_mb3_gpu4
	layer7_expert1_mlp1_dp0_mb3_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb3_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb3_gpu4 -> layer7_expert1_mlp1_dp0_mb3_gpu5
	layer7_router_dp1_mb3_gpu4 -> layer7_expert1_mlp1_dp1_mb3_gpu5
	layer7_expert1_mlp2_dp0_mb3_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb3_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb3_gpu5 -> layer7_expert1_mlp2_dp0_mb3_gpu5
	layer7_expert1_mlp1_dp1_mb3_gpu5 -> layer7_expert1_mlp2_dp1_mb3_gpu5
	layer7_expert2_mlp1_dp0_mb3_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb3_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb3_gpu4 -> layer7_expert2_mlp1_dp0_mb3_gpu6
	layer7_router_dp1_mb3_gpu4 -> layer7_expert2_mlp1_dp1_mb3_gpu6
	layer7_expert2_mlp2_dp0_mb3_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb3_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb3_gpu6 -> layer7_expert2_mlp2_dp0_mb3_gpu6
	layer7_expert2_mlp1_dp1_mb3_gpu6 -> layer7_expert2_mlp2_dp1_mb3_gpu6
	layer7_expert3_mlp1_dp0_mb3_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb3_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb3_gpu4 -> layer7_expert3_mlp1_dp0_mb3_gpu7
	layer7_router_dp1_mb3_gpu4 -> layer7_expert3_mlp1_dp1_mb3_gpu7
	layer7_expert3_mlp2_dp0_mb3_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb3_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb3_gpu7 -> layer7_expert3_mlp2_dp0_mb3_gpu7
	layer7_expert3_mlp1_dp1_mb3_gpu7 -> layer7_expert3_mlp2_dp1_mb3_gpu7
	layer7_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb3_gpu4 -> layer7_expert_agg_dp0_mb3
	layer7_expert0_mlp2_dp1_mb3_gpu4 -> layer7_expert_agg_dp1_mb3
	layer7_expert1_mlp2_dp0_mb3_gpu5 -> layer7_expert_agg_dp0_mb3
	layer7_expert1_mlp2_dp1_mb3_gpu5 -> layer7_expert_agg_dp1_mb3
	layer7_expert2_mlp2_dp0_mb3_gpu6 -> layer7_expert_agg_dp0_mb3
	layer7_expert2_mlp2_dp1_mb3_gpu6 -> layer7_expert_agg_dp1_mb3
	layer7_expert3_mlp2_dp0_mb3_gpu7 -> layer7_expert_agg_dp0_mb3
	layer7_expert3_mlp2_dp1_mb3_gpu7 -> layer7_expert_agg_dp1_mb3
	layer4_qkv_dp0_mb4_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb4_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb4 -> layer4_qkv_dp0_mb4_gpu4
	layer3_expert_agg_dp1_mb4 -> layer4_qkv_dp1_mb4_gpu4
	layer4_attn_scores_dp0_mb4_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb4_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb4_gpu4 -> layer4_attn_scores_dp0_mb4_gpu4
	layer4_qkv_dp1_mb4_gpu4 -> layer4_attn_scores_dp1_mb4_gpu4
	layer4_softmax_dp0_mb4_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb4_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb4_gpu4 -> layer4_softmax_dp0_mb4_gpu4
	layer4_attn_scores_dp1_mb4_gpu4 -> layer4_softmax_dp1_mb4_gpu4
	layer4_attn_out_dp0_mb4_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb4_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb4_gpu4 -> layer4_attn_out_dp0_mb4_gpu4
	layer4_softmax_dp1_mb4_gpu4 -> layer4_attn_out_dp1_mb4_gpu4
	layer4_qkv_dp0_mb4_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb4_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb4 -> layer4_qkv_dp0_mb4_gpu5
	layer3_expert_agg_dp1_mb4 -> layer4_qkv_dp1_mb4_gpu5
	layer4_attn_scores_dp0_mb4_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb4_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb4_gpu5 -> layer4_attn_scores_dp0_mb4_gpu5
	layer4_qkv_dp1_mb4_gpu5 -> layer4_attn_scores_dp1_mb4_gpu5
	layer4_softmax_dp0_mb4_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb4_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb4_gpu5 -> layer4_softmax_dp0_mb4_gpu5
	layer4_attn_scores_dp1_mb4_gpu5 -> layer4_softmax_dp1_mb4_gpu5
	layer4_attn_out_dp0_mb4_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb4_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb4_gpu5 -> layer4_attn_out_dp0_mb4_gpu5
	layer4_softmax_dp1_mb4_gpu5 -> layer4_attn_out_dp1_mb4_gpu5
	layer4_qkv_dp0_mb4_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb4_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb4 -> layer4_qkv_dp0_mb4_gpu6
	layer3_expert_agg_dp1_mb4 -> layer4_qkv_dp1_mb4_gpu6
	layer4_attn_scores_dp0_mb4_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb4_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb4_gpu6 -> layer4_attn_scores_dp0_mb4_gpu6
	layer4_qkv_dp1_mb4_gpu6 -> layer4_attn_scores_dp1_mb4_gpu6
	layer4_softmax_dp0_mb4_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb4_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb4_gpu6 -> layer4_softmax_dp0_mb4_gpu6
	layer4_attn_scores_dp1_mb4_gpu6 -> layer4_softmax_dp1_mb4_gpu6
	layer4_attn_out_dp0_mb4_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb4_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb4_gpu6 -> layer4_attn_out_dp0_mb4_gpu6
	layer4_softmax_dp1_mb4_gpu6 -> layer4_attn_out_dp1_mb4_gpu6
	layer4_qkv_dp0_mb4_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb4_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb4 -> layer4_qkv_dp0_mb4_gpu7
	layer3_expert_agg_dp1_mb4 -> layer4_qkv_dp1_mb4_gpu7
	layer4_attn_scores_dp0_mb4_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb4_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb4_gpu7 -> layer4_attn_scores_dp0_mb4_gpu7
	layer4_qkv_dp1_mb4_gpu7 -> layer4_attn_scores_dp1_mb4_gpu7
	layer4_softmax_dp0_mb4_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb4_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb4_gpu7 -> layer4_softmax_dp0_mb4_gpu7
	layer4_attn_scores_dp1_mb4_gpu7 -> layer4_softmax_dp1_mb4_gpu7
	layer4_attn_out_dp0_mb4_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb4_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb4_gpu7 -> layer4_attn_out_dp0_mb4_gpu7
	layer4_softmax_dp1_mb4_gpu7 -> layer4_attn_out_dp1_mb4_gpu7
	layer4_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb4_gpu4 -> layer4_attn_allreduce_dp0_mb4
	layer4_attn_out_dp1_mb4_gpu4 -> layer4_attn_allreduce_dp1_mb4
	layer4_attn_out_dp0_mb4_gpu5 -> layer4_attn_allreduce_dp0_mb4
	layer4_attn_out_dp1_mb4_gpu5 -> layer4_attn_allreduce_dp1_mb4
	layer4_attn_out_dp0_mb4_gpu6 -> layer4_attn_allreduce_dp0_mb4
	layer4_attn_out_dp1_mb4_gpu6 -> layer4_attn_allreduce_dp1_mb4
	layer4_attn_out_dp0_mb4_gpu7 -> layer4_attn_allreduce_dp0_mb4
	layer4_attn_out_dp1_mb4_gpu7 -> layer4_attn_allreduce_dp1_mb4
	layer4_router_dp0_mb4_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb4_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb4 -> layer4_router_dp0_mb4_gpu4
	layer4_attn_allreduce_dp1_mb4 -> layer4_router_dp1_mb4_gpu4
	layer4_expert0_mlp1_dp0_mb4_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb4_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb4_gpu4 -> layer4_expert0_mlp1_dp0_mb4_gpu4
	layer4_router_dp1_mb4_gpu4 -> layer4_expert0_mlp1_dp1_mb4_gpu4
	layer4_expert0_mlp2_dp0_mb4_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb4_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb4_gpu4 -> layer4_expert0_mlp2_dp0_mb4_gpu4
	layer4_expert0_mlp1_dp1_mb4_gpu4 -> layer4_expert0_mlp2_dp1_mb4_gpu4
	layer4_expert1_mlp1_dp0_mb4_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb4_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb4_gpu4 -> layer4_expert1_mlp1_dp0_mb4_gpu5
	layer4_router_dp1_mb4_gpu4 -> layer4_expert1_mlp1_dp1_mb4_gpu5
	layer4_expert1_mlp2_dp0_mb4_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb4_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb4_gpu5 -> layer4_expert1_mlp2_dp0_mb4_gpu5
	layer4_expert1_mlp1_dp1_mb4_gpu5 -> layer4_expert1_mlp2_dp1_mb4_gpu5
	layer4_expert2_mlp1_dp0_mb4_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb4_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb4_gpu4 -> layer4_expert2_mlp1_dp0_mb4_gpu6
	layer4_router_dp1_mb4_gpu4 -> layer4_expert2_mlp1_dp1_mb4_gpu6
	layer4_expert2_mlp2_dp0_mb4_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb4_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb4_gpu6 -> layer4_expert2_mlp2_dp0_mb4_gpu6
	layer4_expert2_mlp1_dp1_mb4_gpu6 -> layer4_expert2_mlp2_dp1_mb4_gpu6
	layer4_expert3_mlp1_dp0_mb4_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb4_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb4_gpu4 -> layer4_expert3_mlp1_dp0_mb4_gpu7
	layer4_router_dp1_mb4_gpu4 -> layer4_expert3_mlp1_dp1_mb4_gpu7
	layer4_expert3_mlp2_dp0_mb4_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb4_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb4_gpu7 -> layer4_expert3_mlp2_dp0_mb4_gpu7
	layer4_expert3_mlp1_dp1_mb4_gpu7 -> layer4_expert3_mlp2_dp1_mb4_gpu7
	layer4_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb4_gpu4 -> layer4_expert_agg_dp0_mb4
	layer4_expert0_mlp2_dp1_mb4_gpu4 -> layer4_expert_agg_dp1_mb4
	layer4_expert1_mlp2_dp0_mb4_gpu5 -> layer4_expert_agg_dp0_mb4
	layer4_expert1_mlp2_dp1_mb4_gpu5 -> layer4_expert_agg_dp1_mb4
	layer4_expert2_mlp2_dp0_mb4_gpu6 -> layer4_expert_agg_dp0_mb4
	layer4_expert2_mlp2_dp1_mb4_gpu6 -> layer4_expert_agg_dp1_mb4
	layer4_expert3_mlp2_dp0_mb4_gpu7 -> layer4_expert_agg_dp0_mb4
	layer4_expert3_mlp2_dp1_mb4_gpu7 -> layer4_expert_agg_dp1_mb4
	layer5_qkv_dp0_mb4_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb4_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb4 -> layer5_qkv_dp0_mb4_gpu4
	layer4_expert_agg_dp1_mb4 -> layer5_qkv_dp1_mb4_gpu4
	layer5_attn_scores_dp0_mb4_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb4_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb4_gpu4 -> layer5_attn_scores_dp0_mb4_gpu4
	layer5_qkv_dp1_mb4_gpu4 -> layer5_attn_scores_dp1_mb4_gpu4
	layer5_softmax_dp0_mb4_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb4_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb4_gpu4 -> layer5_softmax_dp0_mb4_gpu4
	layer5_attn_scores_dp1_mb4_gpu4 -> layer5_softmax_dp1_mb4_gpu4
	layer5_attn_out_dp0_mb4_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb4_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb4_gpu4 -> layer5_attn_out_dp0_mb4_gpu4
	layer5_softmax_dp1_mb4_gpu4 -> layer5_attn_out_dp1_mb4_gpu4
	layer5_qkv_dp0_mb4_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb4_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb4 -> layer5_qkv_dp0_mb4_gpu5
	layer4_expert_agg_dp1_mb4 -> layer5_qkv_dp1_mb4_gpu5
	layer5_attn_scores_dp0_mb4_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb4_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb4_gpu5 -> layer5_attn_scores_dp0_mb4_gpu5
	layer5_qkv_dp1_mb4_gpu5 -> layer5_attn_scores_dp1_mb4_gpu5
	layer5_softmax_dp0_mb4_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb4_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb4_gpu5 -> layer5_softmax_dp0_mb4_gpu5
	layer5_attn_scores_dp1_mb4_gpu5 -> layer5_softmax_dp1_mb4_gpu5
	layer5_attn_out_dp0_mb4_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb4_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb4_gpu5 -> layer5_attn_out_dp0_mb4_gpu5
	layer5_softmax_dp1_mb4_gpu5 -> layer5_attn_out_dp1_mb4_gpu5
	layer5_qkv_dp0_mb4_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb4_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb4 -> layer5_qkv_dp0_mb4_gpu6
	layer4_expert_agg_dp1_mb4 -> layer5_qkv_dp1_mb4_gpu6
	layer5_attn_scores_dp0_mb4_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb4_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb4_gpu6 -> layer5_attn_scores_dp0_mb4_gpu6
	layer5_qkv_dp1_mb4_gpu6 -> layer5_attn_scores_dp1_mb4_gpu6
	layer5_softmax_dp0_mb4_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb4_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb4_gpu6 -> layer5_softmax_dp0_mb4_gpu6
	layer5_attn_scores_dp1_mb4_gpu6 -> layer5_softmax_dp1_mb4_gpu6
	layer5_attn_out_dp0_mb4_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb4_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb4_gpu6 -> layer5_attn_out_dp0_mb4_gpu6
	layer5_softmax_dp1_mb4_gpu6 -> layer5_attn_out_dp1_mb4_gpu6
	layer5_qkv_dp0_mb4_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb4_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb4 -> layer5_qkv_dp0_mb4_gpu7
	layer4_expert_agg_dp1_mb4 -> layer5_qkv_dp1_mb4_gpu7
	layer5_attn_scores_dp0_mb4_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb4_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb4_gpu7 -> layer5_attn_scores_dp0_mb4_gpu7
	layer5_qkv_dp1_mb4_gpu7 -> layer5_attn_scores_dp1_mb4_gpu7
	layer5_softmax_dp0_mb4_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb4_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb4_gpu7 -> layer5_softmax_dp0_mb4_gpu7
	layer5_attn_scores_dp1_mb4_gpu7 -> layer5_softmax_dp1_mb4_gpu7
	layer5_attn_out_dp0_mb4_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb4_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb4_gpu7 -> layer5_attn_out_dp0_mb4_gpu7
	layer5_softmax_dp1_mb4_gpu7 -> layer5_attn_out_dp1_mb4_gpu7
	layer5_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb4_gpu4 -> layer5_attn_allreduce_dp0_mb4
	layer5_attn_out_dp1_mb4_gpu4 -> layer5_attn_allreduce_dp1_mb4
	layer5_attn_out_dp0_mb4_gpu5 -> layer5_attn_allreduce_dp0_mb4
	layer5_attn_out_dp1_mb4_gpu5 -> layer5_attn_allreduce_dp1_mb4
	layer5_attn_out_dp0_mb4_gpu6 -> layer5_attn_allreduce_dp0_mb4
	layer5_attn_out_dp1_mb4_gpu6 -> layer5_attn_allreduce_dp1_mb4
	layer5_attn_out_dp0_mb4_gpu7 -> layer5_attn_allreduce_dp0_mb4
	layer5_attn_out_dp1_mb4_gpu7 -> layer5_attn_allreduce_dp1_mb4
	layer5_router_dp0_mb4_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb4_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb4 -> layer5_router_dp0_mb4_gpu4
	layer5_attn_allreduce_dp1_mb4 -> layer5_router_dp1_mb4_gpu4
	layer5_expert0_mlp1_dp0_mb4_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb4_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb4_gpu4 -> layer5_expert0_mlp1_dp0_mb4_gpu4
	layer5_router_dp1_mb4_gpu4 -> layer5_expert0_mlp1_dp1_mb4_gpu4
	layer5_expert0_mlp2_dp0_mb4_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb4_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb4_gpu4 -> layer5_expert0_mlp2_dp0_mb4_gpu4
	layer5_expert0_mlp1_dp1_mb4_gpu4 -> layer5_expert0_mlp2_dp1_mb4_gpu4
	layer5_expert1_mlp1_dp0_mb4_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb4_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb4_gpu4 -> layer5_expert1_mlp1_dp0_mb4_gpu5
	layer5_router_dp1_mb4_gpu4 -> layer5_expert1_mlp1_dp1_mb4_gpu5
	layer5_expert1_mlp2_dp0_mb4_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb4_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb4_gpu5 -> layer5_expert1_mlp2_dp0_mb4_gpu5
	layer5_expert1_mlp1_dp1_mb4_gpu5 -> layer5_expert1_mlp2_dp1_mb4_gpu5
	layer5_expert2_mlp1_dp0_mb4_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb4_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb4_gpu4 -> layer5_expert2_mlp1_dp0_mb4_gpu6
	layer5_router_dp1_mb4_gpu4 -> layer5_expert2_mlp1_dp1_mb4_gpu6
	layer5_expert2_mlp2_dp0_mb4_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb4_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb4_gpu6 -> layer5_expert2_mlp2_dp0_mb4_gpu6
	layer5_expert2_mlp1_dp1_mb4_gpu6 -> layer5_expert2_mlp2_dp1_mb4_gpu6
	layer5_expert3_mlp1_dp0_mb4_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb4_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb4_gpu4 -> layer5_expert3_mlp1_dp0_mb4_gpu7
	layer5_router_dp1_mb4_gpu4 -> layer5_expert3_mlp1_dp1_mb4_gpu7
	layer5_expert3_mlp2_dp0_mb4_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb4_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb4_gpu7 -> layer5_expert3_mlp2_dp0_mb4_gpu7
	layer5_expert3_mlp1_dp1_mb4_gpu7 -> layer5_expert3_mlp2_dp1_mb4_gpu7
	layer5_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb4_gpu4 -> layer5_expert_agg_dp0_mb4
	layer5_expert0_mlp2_dp1_mb4_gpu4 -> layer5_expert_agg_dp1_mb4
	layer5_expert1_mlp2_dp0_mb4_gpu5 -> layer5_expert_agg_dp0_mb4
	layer5_expert1_mlp2_dp1_mb4_gpu5 -> layer5_expert_agg_dp1_mb4
	layer5_expert2_mlp2_dp0_mb4_gpu6 -> layer5_expert_agg_dp0_mb4
	layer5_expert2_mlp2_dp1_mb4_gpu6 -> layer5_expert_agg_dp1_mb4
	layer5_expert3_mlp2_dp0_mb4_gpu7 -> layer5_expert_agg_dp0_mb4
	layer5_expert3_mlp2_dp1_mb4_gpu7 -> layer5_expert_agg_dp1_mb4
	layer6_qkv_dp0_mb4_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb4_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb4 -> layer6_qkv_dp0_mb4_gpu4
	layer5_expert_agg_dp1_mb4 -> layer6_qkv_dp1_mb4_gpu4
	layer6_attn_scores_dp0_mb4_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb4_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb4_gpu4 -> layer6_attn_scores_dp0_mb4_gpu4
	layer6_qkv_dp1_mb4_gpu4 -> layer6_attn_scores_dp1_mb4_gpu4
	layer6_softmax_dp0_mb4_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb4_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb4_gpu4 -> layer6_softmax_dp0_mb4_gpu4
	layer6_attn_scores_dp1_mb4_gpu4 -> layer6_softmax_dp1_mb4_gpu4
	layer6_attn_out_dp0_mb4_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb4_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb4_gpu4 -> layer6_attn_out_dp0_mb4_gpu4
	layer6_softmax_dp1_mb4_gpu4 -> layer6_attn_out_dp1_mb4_gpu4
	layer6_qkv_dp0_mb4_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb4_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb4 -> layer6_qkv_dp0_mb4_gpu5
	layer5_expert_agg_dp1_mb4 -> layer6_qkv_dp1_mb4_gpu5
	layer6_attn_scores_dp0_mb4_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb4_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb4_gpu5 -> layer6_attn_scores_dp0_mb4_gpu5
	layer6_qkv_dp1_mb4_gpu5 -> layer6_attn_scores_dp1_mb4_gpu5
	layer6_softmax_dp0_mb4_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb4_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb4_gpu5 -> layer6_softmax_dp0_mb4_gpu5
	layer6_attn_scores_dp1_mb4_gpu5 -> layer6_softmax_dp1_mb4_gpu5
	layer6_attn_out_dp0_mb4_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb4_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb4_gpu5 -> layer6_attn_out_dp0_mb4_gpu5
	layer6_softmax_dp1_mb4_gpu5 -> layer6_attn_out_dp1_mb4_gpu5
	layer6_qkv_dp0_mb4_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb4_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb4 -> layer6_qkv_dp0_mb4_gpu6
	layer5_expert_agg_dp1_mb4 -> layer6_qkv_dp1_mb4_gpu6
	layer6_attn_scores_dp0_mb4_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb4_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb4_gpu6 -> layer6_attn_scores_dp0_mb4_gpu6
	layer6_qkv_dp1_mb4_gpu6 -> layer6_attn_scores_dp1_mb4_gpu6
	layer6_softmax_dp0_mb4_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb4_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb4_gpu6 -> layer6_softmax_dp0_mb4_gpu6
	layer6_attn_scores_dp1_mb4_gpu6 -> layer6_softmax_dp1_mb4_gpu6
	layer6_attn_out_dp0_mb4_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb4_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb4_gpu6 -> layer6_attn_out_dp0_mb4_gpu6
	layer6_softmax_dp1_mb4_gpu6 -> layer6_attn_out_dp1_mb4_gpu6
	layer6_qkv_dp0_mb4_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb4_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb4 -> layer6_qkv_dp0_mb4_gpu7
	layer5_expert_agg_dp1_mb4 -> layer6_qkv_dp1_mb4_gpu7
	layer6_attn_scores_dp0_mb4_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb4_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb4_gpu7 -> layer6_attn_scores_dp0_mb4_gpu7
	layer6_qkv_dp1_mb4_gpu7 -> layer6_attn_scores_dp1_mb4_gpu7
	layer6_softmax_dp0_mb4_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb4_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb4_gpu7 -> layer6_softmax_dp0_mb4_gpu7
	layer6_attn_scores_dp1_mb4_gpu7 -> layer6_softmax_dp1_mb4_gpu7
	layer6_attn_out_dp0_mb4_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb4_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb4_gpu7 -> layer6_attn_out_dp0_mb4_gpu7
	layer6_softmax_dp1_mb4_gpu7 -> layer6_attn_out_dp1_mb4_gpu7
	layer6_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb4_gpu4 -> layer6_attn_allreduce_dp0_mb4
	layer6_attn_out_dp1_mb4_gpu4 -> layer6_attn_allreduce_dp1_mb4
	layer6_attn_out_dp0_mb4_gpu5 -> layer6_attn_allreduce_dp0_mb4
	layer6_attn_out_dp1_mb4_gpu5 -> layer6_attn_allreduce_dp1_mb4
	layer6_attn_out_dp0_mb4_gpu6 -> layer6_attn_allreduce_dp0_mb4
	layer6_attn_out_dp1_mb4_gpu6 -> layer6_attn_allreduce_dp1_mb4
	layer6_attn_out_dp0_mb4_gpu7 -> layer6_attn_allreduce_dp0_mb4
	layer6_attn_out_dp1_mb4_gpu7 -> layer6_attn_allreduce_dp1_mb4
	layer6_router_dp0_mb4_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb4_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb4 -> layer6_router_dp0_mb4_gpu4
	layer6_attn_allreduce_dp1_mb4 -> layer6_router_dp1_mb4_gpu4
	layer6_expert0_mlp1_dp0_mb4_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb4_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb4_gpu4 -> layer6_expert0_mlp1_dp0_mb4_gpu4
	layer6_router_dp1_mb4_gpu4 -> layer6_expert0_mlp1_dp1_mb4_gpu4
	layer6_expert0_mlp2_dp0_mb4_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb4_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb4_gpu4 -> layer6_expert0_mlp2_dp0_mb4_gpu4
	layer6_expert0_mlp1_dp1_mb4_gpu4 -> layer6_expert0_mlp2_dp1_mb4_gpu4
	layer6_expert1_mlp1_dp0_mb4_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb4_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb4_gpu4 -> layer6_expert1_mlp1_dp0_mb4_gpu5
	layer6_router_dp1_mb4_gpu4 -> layer6_expert1_mlp1_dp1_mb4_gpu5
	layer6_expert1_mlp2_dp0_mb4_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb4_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb4_gpu5 -> layer6_expert1_mlp2_dp0_mb4_gpu5
	layer6_expert1_mlp1_dp1_mb4_gpu5 -> layer6_expert1_mlp2_dp1_mb4_gpu5
	layer6_expert2_mlp1_dp0_mb4_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb4_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb4_gpu4 -> layer6_expert2_mlp1_dp0_mb4_gpu6
	layer6_router_dp1_mb4_gpu4 -> layer6_expert2_mlp1_dp1_mb4_gpu6
	layer6_expert2_mlp2_dp0_mb4_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb4_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb4_gpu6 -> layer6_expert2_mlp2_dp0_mb4_gpu6
	layer6_expert2_mlp1_dp1_mb4_gpu6 -> layer6_expert2_mlp2_dp1_mb4_gpu6
	layer6_expert3_mlp1_dp0_mb4_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb4_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb4_gpu4 -> layer6_expert3_mlp1_dp0_mb4_gpu7
	layer6_router_dp1_mb4_gpu4 -> layer6_expert3_mlp1_dp1_mb4_gpu7
	layer6_expert3_mlp2_dp0_mb4_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb4_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb4_gpu7 -> layer6_expert3_mlp2_dp0_mb4_gpu7
	layer6_expert3_mlp1_dp1_mb4_gpu7 -> layer6_expert3_mlp2_dp1_mb4_gpu7
	layer6_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb4_gpu4 -> layer6_expert_agg_dp0_mb4
	layer6_expert0_mlp2_dp1_mb4_gpu4 -> layer6_expert_agg_dp1_mb4
	layer6_expert1_mlp2_dp0_mb4_gpu5 -> layer6_expert_agg_dp0_mb4
	layer6_expert1_mlp2_dp1_mb4_gpu5 -> layer6_expert_agg_dp1_mb4
	layer6_expert2_mlp2_dp0_mb4_gpu6 -> layer6_expert_agg_dp0_mb4
	layer6_expert2_mlp2_dp1_mb4_gpu6 -> layer6_expert_agg_dp1_mb4
	layer6_expert3_mlp2_dp0_mb4_gpu7 -> layer6_expert_agg_dp0_mb4
	layer6_expert3_mlp2_dp1_mb4_gpu7 -> layer6_expert_agg_dp1_mb4
	layer7_qkv_dp0_mb4_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb4_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb4 -> layer7_qkv_dp0_mb4_gpu4
	layer6_expert_agg_dp1_mb4 -> layer7_qkv_dp1_mb4_gpu4
	layer7_attn_scores_dp0_mb4_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb4_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb4_gpu4 -> layer7_attn_scores_dp0_mb4_gpu4
	layer7_qkv_dp1_mb4_gpu4 -> layer7_attn_scores_dp1_mb4_gpu4
	layer7_softmax_dp0_mb4_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb4_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb4_gpu4 -> layer7_softmax_dp0_mb4_gpu4
	layer7_attn_scores_dp1_mb4_gpu4 -> layer7_softmax_dp1_mb4_gpu4
	layer7_attn_out_dp0_mb4_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb4_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb4_gpu4 -> layer7_attn_out_dp0_mb4_gpu4
	layer7_softmax_dp1_mb4_gpu4 -> layer7_attn_out_dp1_mb4_gpu4
	layer7_qkv_dp0_mb4_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb4_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb4 -> layer7_qkv_dp0_mb4_gpu5
	layer6_expert_agg_dp1_mb4 -> layer7_qkv_dp1_mb4_gpu5
	layer7_attn_scores_dp0_mb4_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb4_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb4_gpu5 -> layer7_attn_scores_dp0_mb4_gpu5
	layer7_qkv_dp1_mb4_gpu5 -> layer7_attn_scores_dp1_mb4_gpu5
	layer7_softmax_dp0_mb4_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb4_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb4_gpu5 -> layer7_softmax_dp0_mb4_gpu5
	layer7_attn_scores_dp1_mb4_gpu5 -> layer7_softmax_dp1_mb4_gpu5
	layer7_attn_out_dp0_mb4_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb4_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb4_gpu5 -> layer7_attn_out_dp0_mb4_gpu5
	layer7_softmax_dp1_mb4_gpu5 -> layer7_attn_out_dp1_mb4_gpu5
	layer7_qkv_dp0_mb4_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb4_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb4 -> layer7_qkv_dp0_mb4_gpu6
	layer6_expert_agg_dp1_mb4 -> layer7_qkv_dp1_mb4_gpu6
	layer7_attn_scores_dp0_mb4_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb4_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb4_gpu6 -> layer7_attn_scores_dp0_mb4_gpu6
	layer7_qkv_dp1_mb4_gpu6 -> layer7_attn_scores_dp1_mb4_gpu6
	layer7_softmax_dp0_mb4_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb4_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb4_gpu6 -> layer7_softmax_dp0_mb4_gpu6
	layer7_attn_scores_dp1_mb4_gpu6 -> layer7_softmax_dp1_mb4_gpu6
	layer7_attn_out_dp0_mb4_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb4_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb4_gpu6 -> layer7_attn_out_dp0_mb4_gpu6
	layer7_softmax_dp1_mb4_gpu6 -> layer7_attn_out_dp1_mb4_gpu6
	layer7_qkv_dp0_mb4_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb4_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb4 -> layer7_qkv_dp0_mb4_gpu7
	layer6_expert_agg_dp1_mb4 -> layer7_qkv_dp1_mb4_gpu7
	layer7_attn_scores_dp0_mb4_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb4_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb4_gpu7 -> layer7_attn_scores_dp0_mb4_gpu7
	layer7_qkv_dp1_mb4_gpu7 -> layer7_attn_scores_dp1_mb4_gpu7
	layer7_softmax_dp0_mb4_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb4_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb4_gpu7 -> layer7_softmax_dp0_mb4_gpu7
	layer7_attn_scores_dp1_mb4_gpu7 -> layer7_softmax_dp1_mb4_gpu7
	layer7_attn_out_dp0_mb4_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb4_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb4_gpu7 -> layer7_attn_out_dp0_mb4_gpu7
	layer7_softmax_dp1_mb4_gpu7 -> layer7_attn_out_dp1_mb4_gpu7
	layer7_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb4_gpu4 -> layer7_attn_allreduce_dp0_mb4
	layer7_attn_out_dp1_mb4_gpu4 -> layer7_attn_allreduce_dp1_mb4
	layer7_attn_out_dp0_mb4_gpu5 -> layer7_attn_allreduce_dp0_mb4
	layer7_attn_out_dp1_mb4_gpu5 -> layer7_attn_allreduce_dp1_mb4
	layer7_attn_out_dp0_mb4_gpu6 -> layer7_attn_allreduce_dp0_mb4
	layer7_attn_out_dp1_mb4_gpu6 -> layer7_attn_allreduce_dp1_mb4
	layer7_attn_out_dp0_mb4_gpu7 -> layer7_attn_allreduce_dp0_mb4
	layer7_attn_out_dp1_mb4_gpu7 -> layer7_attn_allreduce_dp1_mb4
	layer7_router_dp0_mb4_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb4_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb4 -> layer7_router_dp0_mb4_gpu4
	layer7_attn_allreduce_dp1_mb4 -> layer7_router_dp1_mb4_gpu4
	layer7_expert0_mlp1_dp0_mb4_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb4_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb4_gpu4 -> layer7_expert0_mlp1_dp0_mb4_gpu4
	layer7_router_dp1_mb4_gpu4 -> layer7_expert0_mlp1_dp1_mb4_gpu4
	layer7_expert0_mlp2_dp0_mb4_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb4_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb4_gpu4 -> layer7_expert0_mlp2_dp0_mb4_gpu4
	layer7_expert0_mlp1_dp1_mb4_gpu4 -> layer7_expert0_mlp2_dp1_mb4_gpu4
	layer7_expert1_mlp1_dp0_mb4_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb4_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb4_gpu4 -> layer7_expert1_mlp1_dp0_mb4_gpu5
	layer7_router_dp1_mb4_gpu4 -> layer7_expert1_mlp1_dp1_mb4_gpu5
	layer7_expert1_mlp2_dp0_mb4_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb4_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb4_gpu5 -> layer7_expert1_mlp2_dp0_mb4_gpu5
	layer7_expert1_mlp1_dp1_mb4_gpu5 -> layer7_expert1_mlp2_dp1_mb4_gpu5
	layer7_expert2_mlp1_dp0_mb4_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb4_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb4_gpu4 -> layer7_expert2_mlp1_dp0_mb4_gpu6
	layer7_router_dp1_mb4_gpu4 -> layer7_expert2_mlp1_dp1_mb4_gpu6
	layer7_expert2_mlp2_dp0_mb4_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb4_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb4_gpu6 -> layer7_expert2_mlp2_dp0_mb4_gpu6
	layer7_expert2_mlp1_dp1_mb4_gpu6 -> layer7_expert2_mlp2_dp1_mb4_gpu6
	layer7_expert3_mlp1_dp0_mb4_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb4_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb4_gpu4 -> layer7_expert3_mlp1_dp0_mb4_gpu7
	layer7_router_dp1_mb4_gpu4 -> layer7_expert3_mlp1_dp1_mb4_gpu7
	layer7_expert3_mlp2_dp0_mb4_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb4_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb4_gpu7 -> layer7_expert3_mlp2_dp0_mb4_gpu7
	layer7_expert3_mlp1_dp1_mb4_gpu7 -> layer7_expert3_mlp2_dp1_mb4_gpu7
	layer7_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb4_gpu4 -> layer7_expert_agg_dp0_mb4
	layer7_expert0_mlp2_dp1_mb4_gpu4 -> layer7_expert_agg_dp1_mb4
	layer7_expert1_mlp2_dp0_mb4_gpu5 -> layer7_expert_agg_dp0_mb4
	layer7_expert1_mlp2_dp1_mb4_gpu5 -> layer7_expert_agg_dp1_mb4
	layer7_expert2_mlp2_dp0_mb4_gpu6 -> layer7_expert_agg_dp0_mb4
	layer7_expert2_mlp2_dp1_mb4_gpu6 -> layer7_expert_agg_dp1_mb4
	layer7_expert3_mlp2_dp0_mb4_gpu7 -> layer7_expert_agg_dp0_mb4
	layer7_expert3_mlp2_dp1_mb4_gpu7 -> layer7_expert_agg_dp1_mb4
	layer4_qkv_dp0_mb5_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb5_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb5 -> layer4_qkv_dp0_mb5_gpu4
	layer3_expert_agg_dp1_mb5 -> layer4_qkv_dp1_mb5_gpu4
	layer4_attn_scores_dp0_mb5_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb5_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb5_gpu4 -> layer4_attn_scores_dp0_mb5_gpu4
	layer4_qkv_dp1_mb5_gpu4 -> layer4_attn_scores_dp1_mb5_gpu4
	layer4_softmax_dp0_mb5_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb5_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb5_gpu4 -> layer4_softmax_dp0_mb5_gpu4
	layer4_attn_scores_dp1_mb5_gpu4 -> layer4_softmax_dp1_mb5_gpu4
	layer4_attn_out_dp0_mb5_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb5_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb5_gpu4 -> layer4_attn_out_dp0_mb5_gpu4
	layer4_softmax_dp1_mb5_gpu4 -> layer4_attn_out_dp1_mb5_gpu4
	layer4_qkv_dp0_mb5_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb5_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb5 -> layer4_qkv_dp0_mb5_gpu5
	layer3_expert_agg_dp1_mb5 -> layer4_qkv_dp1_mb5_gpu5
	layer4_attn_scores_dp0_mb5_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb5_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb5_gpu5 -> layer4_attn_scores_dp0_mb5_gpu5
	layer4_qkv_dp1_mb5_gpu5 -> layer4_attn_scores_dp1_mb5_gpu5
	layer4_softmax_dp0_mb5_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb5_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb5_gpu5 -> layer4_softmax_dp0_mb5_gpu5
	layer4_attn_scores_dp1_mb5_gpu5 -> layer4_softmax_dp1_mb5_gpu5
	layer4_attn_out_dp0_mb5_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb5_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb5_gpu5 -> layer4_attn_out_dp0_mb5_gpu5
	layer4_softmax_dp1_mb5_gpu5 -> layer4_attn_out_dp1_mb5_gpu5
	layer4_qkv_dp0_mb5_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb5_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb5 -> layer4_qkv_dp0_mb5_gpu6
	layer3_expert_agg_dp1_mb5 -> layer4_qkv_dp1_mb5_gpu6
	layer4_attn_scores_dp0_mb5_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb5_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb5_gpu6 -> layer4_attn_scores_dp0_mb5_gpu6
	layer4_qkv_dp1_mb5_gpu6 -> layer4_attn_scores_dp1_mb5_gpu6
	layer4_softmax_dp0_mb5_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb5_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb5_gpu6 -> layer4_softmax_dp0_mb5_gpu6
	layer4_attn_scores_dp1_mb5_gpu6 -> layer4_softmax_dp1_mb5_gpu6
	layer4_attn_out_dp0_mb5_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb5_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb5_gpu6 -> layer4_attn_out_dp0_mb5_gpu6
	layer4_softmax_dp1_mb5_gpu6 -> layer4_attn_out_dp1_mb5_gpu6
	layer4_qkv_dp0_mb5_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb5_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb5 -> layer4_qkv_dp0_mb5_gpu7
	layer3_expert_agg_dp1_mb5 -> layer4_qkv_dp1_mb5_gpu7
	layer4_attn_scores_dp0_mb5_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb5_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb5_gpu7 -> layer4_attn_scores_dp0_mb5_gpu7
	layer4_qkv_dp1_mb5_gpu7 -> layer4_attn_scores_dp1_mb5_gpu7
	layer4_softmax_dp0_mb5_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb5_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb5_gpu7 -> layer4_softmax_dp0_mb5_gpu7
	layer4_attn_scores_dp1_mb5_gpu7 -> layer4_softmax_dp1_mb5_gpu7
	layer4_attn_out_dp0_mb5_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb5_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb5_gpu7 -> layer4_attn_out_dp0_mb5_gpu7
	layer4_softmax_dp1_mb5_gpu7 -> layer4_attn_out_dp1_mb5_gpu7
	layer4_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb5_gpu4 -> layer4_attn_allreduce_dp0_mb5
	layer4_attn_out_dp1_mb5_gpu4 -> layer4_attn_allreduce_dp1_mb5
	layer4_attn_out_dp0_mb5_gpu5 -> layer4_attn_allreduce_dp0_mb5
	layer4_attn_out_dp1_mb5_gpu5 -> layer4_attn_allreduce_dp1_mb5
	layer4_attn_out_dp0_mb5_gpu6 -> layer4_attn_allreduce_dp0_mb5
	layer4_attn_out_dp1_mb5_gpu6 -> layer4_attn_allreduce_dp1_mb5
	layer4_attn_out_dp0_mb5_gpu7 -> layer4_attn_allreduce_dp0_mb5
	layer4_attn_out_dp1_mb5_gpu7 -> layer4_attn_allreduce_dp1_mb5
	layer4_router_dp0_mb5_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb5_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb5 -> layer4_router_dp0_mb5_gpu4
	layer4_attn_allreduce_dp1_mb5 -> layer4_router_dp1_mb5_gpu4
	layer4_expert0_mlp1_dp0_mb5_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb5_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb5_gpu4 -> layer4_expert0_mlp1_dp0_mb5_gpu4
	layer4_router_dp1_mb5_gpu4 -> layer4_expert0_mlp1_dp1_mb5_gpu4
	layer4_expert0_mlp2_dp0_mb5_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb5_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb5_gpu4 -> layer4_expert0_mlp2_dp0_mb5_gpu4
	layer4_expert0_mlp1_dp1_mb5_gpu4 -> layer4_expert0_mlp2_dp1_mb5_gpu4
	layer4_expert1_mlp1_dp0_mb5_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb5_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb5_gpu4 -> layer4_expert1_mlp1_dp0_mb5_gpu5
	layer4_router_dp1_mb5_gpu4 -> layer4_expert1_mlp1_dp1_mb5_gpu5
	layer4_expert1_mlp2_dp0_mb5_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb5_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb5_gpu5 -> layer4_expert1_mlp2_dp0_mb5_gpu5
	layer4_expert1_mlp1_dp1_mb5_gpu5 -> layer4_expert1_mlp2_dp1_mb5_gpu5
	layer4_expert2_mlp1_dp0_mb5_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb5_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb5_gpu4 -> layer4_expert2_mlp1_dp0_mb5_gpu6
	layer4_router_dp1_mb5_gpu4 -> layer4_expert2_mlp1_dp1_mb5_gpu6
	layer4_expert2_mlp2_dp0_mb5_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb5_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb5_gpu6 -> layer4_expert2_mlp2_dp0_mb5_gpu6
	layer4_expert2_mlp1_dp1_mb5_gpu6 -> layer4_expert2_mlp2_dp1_mb5_gpu6
	layer4_expert3_mlp1_dp0_mb5_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb5_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb5_gpu4 -> layer4_expert3_mlp1_dp0_mb5_gpu7
	layer4_router_dp1_mb5_gpu4 -> layer4_expert3_mlp1_dp1_mb5_gpu7
	layer4_expert3_mlp2_dp0_mb5_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb5_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb5_gpu7 -> layer4_expert3_mlp2_dp0_mb5_gpu7
	layer4_expert3_mlp1_dp1_mb5_gpu7 -> layer4_expert3_mlp2_dp1_mb5_gpu7
	layer4_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb5_gpu4 -> layer4_expert_agg_dp0_mb5
	layer4_expert0_mlp2_dp1_mb5_gpu4 -> layer4_expert_agg_dp1_mb5
	layer4_expert1_mlp2_dp0_mb5_gpu5 -> layer4_expert_agg_dp0_mb5
	layer4_expert1_mlp2_dp1_mb5_gpu5 -> layer4_expert_agg_dp1_mb5
	layer4_expert2_mlp2_dp0_mb5_gpu6 -> layer4_expert_agg_dp0_mb5
	layer4_expert2_mlp2_dp1_mb5_gpu6 -> layer4_expert_agg_dp1_mb5
	layer4_expert3_mlp2_dp0_mb5_gpu7 -> layer4_expert_agg_dp0_mb5
	layer4_expert3_mlp2_dp1_mb5_gpu7 -> layer4_expert_agg_dp1_mb5
	layer5_qkv_dp0_mb5_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb5_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb5 -> layer5_qkv_dp0_mb5_gpu4
	layer4_expert_agg_dp1_mb5 -> layer5_qkv_dp1_mb5_gpu4
	layer5_attn_scores_dp0_mb5_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb5_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb5_gpu4 -> layer5_attn_scores_dp0_mb5_gpu4
	layer5_qkv_dp1_mb5_gpu4 -> layer5_attn_scores_dp1_mb5_gpu4
	layer5_softmax_dp0_mb5_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb5_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb5_gpu4 -> layer5_softmax_dp0_mb5_gpu4
	layer5_attn_scores_dp1_mb5_gpu4 -> layer5_softmax_dp1_mb5_gpu4
	layer5_attn_out_dp0_mb5_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb5_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb5_gpu4 -> layer5_attn_out_dp0_mb5_gpu4
	layer5_softmax_dp1_mb5_gpu4 -> layer5_attn_out_dp1_mb5_gpu4
	layer5_qkv_dp0_mb5_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb5_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb5 -> layer5_qkv_dp0_mb5_gpu5
	layer4_expert_agg_dp1_mb5 -> layer5_qkv_dp1_mb5_gpu5
	layer5_attn_scores_dp0_mb5_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb5_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb5_gpu5 -> layer5_attn_scores_dp0_mb5_gpu5
	layer5_qkv_dp1_mb5_gpu5 -> layer5_attn_scores_dp1_mb5_gpu5
	layer5_softmax_dp0_mb5_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb5_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb5_gpu5 -> layer5_softmax_dp0_mb5_gpu5
	layer5_attn_scores_dp1_mb5_gpu5 -> layer5_softmax_dp1_mb5_gpu5
	layer5_attn_out_dp0_mb5_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb5_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb5_gpu5 -> layer5_attn_out_dp0_mb5_gpu5
	layer5_softmax_dp1_mb5_gpu5 -> layer5_attn_out_dp1_mb5_gpu5
	layer5_qkv_dp0_mb5_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb5_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb5 -> layer5_qkv_dp0_mb5_gpu6
	layer4_expert_agg_dp1_mb5 -> layer5_qkv_dp1_mb5_gpu6
	layer5_attn_scores_dp0_mb5_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb5_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb5_gpu6 -> layer5_attn_scores_dp0_mb5_gpu6
	layer5_qkv_dp1_mb5_gpu6 -> layer5_attn_scores_dp1_mb5_gpu6
	layer5_softmax_dp0_mb5_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb5_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb5_gpu6 -> layer5_softmax_dp0_mb5_gpu6
	layer5_attn_scores_dp1_mb5_gpu6 -> layer5_softmax_dp1_mb5_gpu6
	layer5_attn_out_dp0_mb5_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb5_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb5_gpu6 -> layer5_attn_out_dp0_mb5_gpu6
	layer5_softmax_dp1_mb5_gpu6 -> layer5_attn_out_dp1_mb5_gpu6
	layer5_qkv_dp0_mb5_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb5_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb5 -> layer5_qkv_dp0_mb5_gpu7
	layer4_expert_agg_dp1_mb5 -> layer5_qkv_dp1_mb5_gpu7
	layer5_attn_scores_dp0_mb5_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb5_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb5_gpu7 -> layer5_attn_scores_dp0_mb5_gpu7
	layer5_qkv_dp1_mb5_gpu7 -> layer5_attn_scores_dp1_mb5_gpu7
	layer5_softmax_dp0_mb5_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb5_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb5_gpu7 -> layer5_softmax_dp0_mb5_gpu7
	layer5_attn_scores_dp1_mb5_gpu7 -> layer5_softmax_dp1_mb5_gpu7
	layer5_attn_out_dp0_mb5_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb5_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb5_gpu7 -> layer5_attn_out_dp0_mb5_gpu7
	layer5_softmax_dp1_mb5_gpu7 -> layer5_attn_out_dp1_mb5_gpu7
	layer5_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb5_gpu4 -> layer5_attn_allreduce_dp0_mb5
	layer5_attn_out_dp1_mb5_gpu4 -> layer5_attn_allreduce_dp1_mb5
	layer5_attn_out_dp0_mb5_gpu5 -> layer5_attn_allreduce_dp0_mb5
	layer5_attn_out_dp1_mb5_gpu5 -> layer5_attn_allreduce_dp1_mb5
	layer5_attn_out_dp0_mb5_gpu6 -> layer5_attn_allreduce_dp0_mb5
	layer5_attn_out_dp1_mb5_gpu6 -> layer5_attn_allreduce_dp1_mb5
	layer5_attn_out_dp0_mb5_gpu7 -> layer5_attn_allreduce_dp0_mb5
	layer5_attn_out_dp1_mb5_gpu7 -> layer5_attn_allreduce_dp1_mb5
	layer5_router_dp0_mb5_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb5_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb5 -> layer5_router_dp0_mb5_gpu4
	layer5_attn_allreduce_dp1_mb5 -> layer5_router_dp1_mb5_gpu4
	layer5_expert0_mlp1_dp0_mb5_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb5_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb5_gpu4 -> layer5_expert0_mlp1_dp0_mb5_gpu4
	layer5_router_dp1_mb5_gpu4 -> layer5_expert0_mlp1_dp1_mb5_gpu4
	layer5_expert0_mlp2_dp0_mb5_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb5_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb5_gpu4 -> layer5_expert0_mlp2_dp0_mb5_gpu4
	layer5_expert0_mlp1_dp1_mb5_gpu4 -> layer5_expert0_mlp2_dp1_mb5_gpu4
	layer5_expert1_mlp1_dp0_mb5_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb5_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb5_gpu4 -> layer5_expert1_mlp1_dp0_mb5_gpu5
	layer5_router_dp1_mb5_gpu4 -> layer5_expert1_mlp1_dp1_mb5_gpu5
	layer5_expert1_mlp2_dp0_mb5_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb5_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb5_gpu5 -> layer5_expert1_mlp2_dp0_mb5_gpu5
	layer5_expert1_mlp1_dp1_mb5_gpu5 -> layer5_expert1_mlp2_dp1_mb5_gpu5
	layer5_expert2_mlp1_dp0_mb5_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb5_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb5_gpu4 -> layer5_expert2_mlp1_dp0_mb5_gpu6
	layer5_router_dp1_mb5_gpu4 -> layer5_expert2_mlp1_dp1_mb5_gpu6
	layer5_expert2_mlp2_dp0_mb5_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb5_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb5_gpu6 -> layer5_expert2_mlp2_dp0_mb5_gpu6
	layer5_expert2_mlp1_dp1_mb5_gpu6 -> layer5_expert2_mlp2_dp1_mb5_gpu6
	layer5_expert3_mlp1_dp0_mb5_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb5_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb5_gpu4 -> layer5_expert3_mlp1_dp0_mb5_gpu7
	layer5_router_dp1_mb5_gpu4 -> layer5_expert3_mlp1_dp1_mb5_gpu7
	layer5_expert3_mlp2_dp0_mb5_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb5_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb5_gpu7 -> layer5_expert3_mlp2_dp0_mb5_gpu7
	layer5_expert3_mlp1_dp1_mb5_gpu7 -> layer5_expert3_mlp2_dp1_mb5_gpu7
	layer5_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb5_gpu4 -> layer5_expert_agg_dp0_mb5
	layer5_expert0_mlp2_dp1_mb5_gpu4 -> layer5_expert_agg_dp1_mb5
	layer5_expert1_mlp2_dp0_mb5_gpu5 -> layer5_expert_agg_dp0_mb5
	layer5_expert1_mlp2_dp1_mb5_gpu5 -> layer5_expert_agg_dp1_mb5
	layer5_expert2_mlp2_dp0_mb5_gpu6 -> layer5_expert_agg_dp0_mb5
	layer5_expert2_mlp2_dp1_mb5_gpu6 -> layer5_expert_agg_dp1_mb5
	layer5_expert3_mlp2_dp0_mb5_gpu7 -> layer5_expert_agg_dp0_mb5
	layer5_expert3_mlp2_dp1_mb5_gpu7 -> layer5_expert_agg_dp1_mb5
	layer6_qkv_dp0_mb5_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb5_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb5 -> layer6_qkv_dp0_mb5_gpu4
	layer5_expert_agg_dp1_mb5 -> layer6_qkv_dp1_mb5_gpu4
	layer6_attn_scores_dp0_mb5_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb5_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb5_gpu4 -> layer6_attn_scores_dp0_mb5_gpu4
	layer6_qkv_dp1_mb5_gpu4 -> layer6_attn_scores_dp1_mb5_gpu4
	layer6_softmax_dp0_mb5_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb5_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb5_gpu4 -> layer6_softmax_dp0_mb5_gpu4
	layer6_attn_scores_dp1_mb5_gpu4 -> layer6_softmax_dp1_mb5_gpu4
	layer6_attn_out_dp0_mb5_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb5_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb5_gpu4 -> layer6_attn_out_dp0_mb5_gpu4
	layer6_softmax_dp1_mb5_gpu4 -> layer6_attn_out_dp1_mb5_gpu4
	layer6_qkv_dp0_mb5_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb5_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb5 -> layer6_qkv_dp0_mb5_gpu5
	layer5_expert_agg_dp1_mb5 -> layer6_qkv_dp1_mb5_gpu5
	layer6_attn_scores_dp0_mb5_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb5_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb5_gpu5 -> layer6_attn_scores_dp0_mb5_gpu5
	layer6_qkv_dp1_mb5_gpu5 -> layer6_attn_scores_dp1_mb5_gpu5
	layer6_softmax_dp0_mb5_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb5_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb5_gpu5 -> layer6_softmax_dp0_mb5_gpu5
	layer6_attn_scores_dp1_mb5_gpu5 -> layer6_softmax_dp1_mb5_gpu5
	layer6_attn_out_dp0_mb5_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb5_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb5_gpu5 -> layer6_attn_out_dp0_mb5_gpu5
	layer6_softmax_dp1_mb5_gpu5 -> layer6_attn_out_dp1_mb5_gpu5
	layer6_qkv_dp0_mb5_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb5_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb5 -> layer6_qkv_dp0_mb5_gpu6
	layer5_expert_agg_dp1_mb5 -> layer6_qkv_dp1_mb5_gpu6
	layer6_attn_scores_dp0_mb5_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb5_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb5_gpu6 -> layer6_attn_scores_dp0_mb5_gpu6
	layer6_qkv_dp1_mb5_gpu6 -> layer6_attn_scores_dp1_mb5_gpu6
	layer6_softmax_dp0_mb5_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb5_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb5_gpu6 -> layer6_softmax_dp0_mb5_gpu6
	layer6_attn_scores_dp1_mb5_gpu6 -> layer6_softmax_dp1_mb5_gpu6
	layer6_attn_out_dp0_mb5_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb5_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb5_gpu6 -> layer6_attn_out_dp0_mb5_gpu6
	layer6_softmax_dp1_mb5_gpu6 -> layer6_attn_out_dp1_mb5_gpu6
	layer6_qkv_dp0_mb5_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb5_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb5 -> layer6_qkv_dp0_mb5_gpu7
	layer5_expert_agg_dp1_mb5 -> layer6_qkv_dp1_mb5_gpu7
	layer6_attn_scores_dp0_mb5_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb5_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb5_gpu7 -> layer6_attn_scores_dp0_mb5_gpu7
	layer6_qkv_dp1_mb5_gpu7 -> layer6_attn_scores_dp1_mb5_gpu7
	layer6_softmax_dp0_mb5_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb5_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb5_gpu7 -> layer6_softmax_dp0_mb5_gpu7
	layer6_attn_scores_dp1_mb5_gpu7 -> layer6_softmax_dp1_mb5_gpu7
	layer6_attn_out_dp0_mb5_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb5_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb5_gpu7 -> layer6_attn_out_dp0_mb5_gpu7
	layer6_softmax_dp1_mb5_gpu7 -> layer6_attn_out_dp1_mb5_gpu7
	layer6_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb5_gpu4 -> layer6_attn_allreduce_dp0_mb5
	layer6_attn_out_dp1_mb5_gpu4 -> layer6_attn_allreduce_dp1_mb5
	layer6_attn_out_dp0_mb5_gpu5 -> layer6_attn_allreduce_dp0_mb5
	layer6_attn_out_dp1_mb5_gpu5 -> layer6_attn_allreduce_dp1_mb5
	layer6_attn_out_dp0_mb5_gpu6 -> layer6_attn_allreduce_dp0_mb5
	layer6_attn_out_dp1_mb5_gpu6 -> layer6_attn_allreduce_dp1_mb5
	layer6_attn_out_dp0_mb5_gpu7 -> layer6_attn_allreduce_dp0_mb5
	layer6_attn_out_dp1_mb5_gpu7 -> layer6_attn_allreduce_dp1_mb5
	layer6_router_dp0_mb5_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb5_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb5 -> layer6_router_dp0_mb5_gpu4
	layer6_attn_allreduce_dp1_mb5 -> layer6_router_dp1_mb5_gpu4
	layer6_expert0_mlp1_dp0_mb5_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb5_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb5_gpu4 -> layer6_expert0_mlp1_dp0_mb5_gpu4
	layer6_router_dp1_mb5_gpu4 -> layer6_expert0_mlp1_dp1_mb5_gpu4
	layer6_expert0_mlp2_dp0_mb5_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb5_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb5_gpu4 -> layer6_expert0_mlp2_dp0_mb5_gpu4
	layer6_expert0_mlp1_dp1_mb5_gpu4 -> layer6_expert0_mlp2_dp1_mb5_gpu4
	layer6_expert1_mlp1_dp0_mb5_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb5_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb5_gpu4 -> layer6_expert1_mlp1_dp0_mb5_gpu5
	layer6_router_dp1_mb5_gpu4 -> layer6_expert1_mlp1_dp1_mb5_gpu5
	layer6_expert1_mlp2_dp0_mb5_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb5_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb5_gpu5 -> layer6_expert1_mlp2_dp0_mb5_gpu5
	layer6_expert1_mlp1_dp1_mb5_gpu5 -> layer6_expert1_mlp2_dp1_mb5_gpu5
	layer6_expert2_mlp1_dp0_mb5_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb5_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb5_gpu4 -> layer6_expert2_mlp1_dp0_mb5_gpu6
	layer6_router_dp1_mb5_gpu4 -> layer6_expert2_mlp1_dp1_mb5_gpu6
	layer6_expert2_mlp2_dp0_mb5_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb5_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb5_gpu6 -> layer6_expert2_mlp2_dp0_mb5_gpu6
	layer6_expert2_mlp1_dp1_mb5_gpu6 -> layer6_expert2_mlp2_dp1_mb5_gpu6
	layer6_expert3_mlp1_dp0_mb5_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb5_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb5_gpu4 -> layer6_expert3_mlp1_dp0_mb5_gpu7
	layer6_router_dp1_mb5_gpu4 -> layer6_expert3_mlp1_dp1_mb5_gpu7
	layer6_expert3_mlp2_dp0_mb5_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb5_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb5_gpu7 -> layer6_expert3_mlp2_dp0_mb5_gpu7
	layer6_expert3_mlp1_dp1_mb5_gpu7 -> layer6_expert3_mlp2_dp1_mb5_gpu7
	layer6_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb5_gpu4 -> layer6_expert_agg_dp0_mb5
	layer6_expert0_mlp2_dp1_mb5_gpu4 -> layer6_expert_agg_dp1_mb5
	layer6_expert1_mlp2_dp0_mb5_gpu5 -> layer6_expert_agg_dp0_mb5
	layer6_expert1_mlp2_dp1_mb5_gpu5 -> layer6_expert_agg_dp1_mb5
	layer6_expert2_mlp2_dp0_mb5_gpu6 -> layer6_expert_agg_dp0_mb5
	layer6_expert2_mlp2_dp1_mb5_gpu6 -> layer6_expert_agg_dp1_mb5
	layer6_expert3_mlp2_dp0_mb5_gpu7 -> layer6_expert_agg_dp0_mb5
	layer6_expert3_mlp2_dp1_mb5_gpu7 -> layer6_expert_agg_dp1_mb5
	layer7_qkv_dp0_mb5_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb5_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb5 -> layer7_qkv_dp0_mb5_gpu4
	layer6_expert_agg_dp1_mb5 -> layer7_qkv_dp1_mb5_gpu4
	layer7_attn_scores_dp0_mb5_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb5_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb5_gpu4 -> layer7_attn_scores_dp0_mb5_gpu4
	layer7_qkv_dp1_mb5_gpu4 -> layer7_attn_scores_dp1_mb5_gpu4
	layer7_softmax_dp0_mb5_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb5_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb5_gpu4 -> layer7_softmax_dp0_mb5_gpu4
	layer7_attn_scores_dp1_mb5_gpu4 -> layer7_softmax_dp1_mb5_gpu4
	layer7_attn_out_dp0_mb5_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb5_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb5_gpu4 -> layer7_attn_out_dp0_mb5_gpu4
	layer7_softmax_dp1_mb5_gpu4 -> layer7_attn_out_dp1_mb5_gpu4
	layer7_qkv_dp0_mb5_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb5_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb5 -> layer7_qkv_dp0_mb5_gpu5
	layer6_expert_agg_dp1_mb5 -> layer7_qkv_dp1_mb5_gpu5
	layer7_attn_scores_dp0_mb5_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb5_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb5_gpu5 -> layer7_attn_scores_dp0_mb5_gpu5
	layer7_qkv_dp1_mb5_gpu5 -> layer7_attn_scores_dp1_mb5_gpu5
	layer7_softmax_dp0_mb5_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb5_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb5_gpu5 -> layer7_softmax_dp0_mb5_gpu5
	layer7_attn_scores_dp1_mb5_gpu5 -> layer7_softmax_dp1_mb5_gpu5
	layer7_attn_out_dp0_mb5_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb5_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb5_gpu5 -> layer7_attn_out_dp0_mb5_gpu5
	layer7_softmax_dp1_mb5_gpu5 -> layer7_attn_out_dp1_mb5_gpu5
	layer7_qkv_dp0_mb5_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb5_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb5 -> layer7_qkv_dp0_mb5_gpu6
	layer6_expert_agg_dp1_mb5 -> layer7_qkv_dp1_mb5_gpu6
	layer7_attn_scores_dp0_mb5_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb5_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb5_gpu6 -> layer7_attn_scores_dp0_mb5_gpu6
	layer7_qkv_dp1_mb5_gpu6 -> layer7_attn_scores_dp1_mb5_gpu6
	layer7_softmax_dp0_mb5_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb5_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb5_gpu6 -> layer7_softmax_dp0_mb5_gpu6
	layer7_attn_scores_dp1_mb5_gpu6 -> layer7_softmax_dp1_mb5_gpu6
	layer7_attn_out_dp0_mb5_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb5_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb5_gpu6 -> layer7_attn_out_dp0_mb5_gpu6
	layer7_softmax_dp1_mb5_gpu6 -> layer7_attn_out_dp1_mb5_gpu6
	layer7_qkv_dp0_mb5_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb5_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb5 -> layer7_qkv_dp0_mb5_gpu7
	layer6_expert_agg_dp1_mb5 -> layer7_qkv_dp1_mb5_gpu7
	layer7_attn_scores_dp0_mb5_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb5_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb5_gpu7 -> layer7_attn_scores_dp0_mb5_gpu7
	layer7_qkv_dp1_mb5_gpu7 -> layer7_attn_scores_dp1_mb5_gpu7
	layer7_softmax_dp0_mb5_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb5_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb5_gpu7 -> layer7_softmax_dp0_mb5_gpu7
	layer7_attn_scores_dp1_mb5_gpu7 -> layer7_softmax_dp1_mb5_gpu7
	layer7_attn_out_dp0_mb5_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb5_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb5_gpu7 -> layer7_attn_out_dp0_mb5_gpu7
	layer7_softmax_dp1_mb5_gpu7 -> layer7_attn_out_dp1_mb5_gpu7
	layer7_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb5_gpu4 -> layer7_attn_allreduce_dp0_mb5
	layer7_attn_out_dp1_mb5_gpu4 -> layer7_attn_allreduce_dp1_mb5
	layer7_attn_out_dp0_mb5_gpu5 -> layer7_attn_allreduce_dp0_mb5
	layer7_attn_out_dp1_mb5_gpu5 -> layer7_attn_allreduce_dp1_mb5
	layer7_attn_out_dp0_mb5_gpu6 -> layer7_attn_allreduce_dp0_mb5
	layer7_attn_out_dp1_mb5_gpu6 -> layer7_attn_allreduce_dp1_mb5
	layer7_attn_out_dp0_mb5_gpu7 -> layer7_attn_allreduce_dp0_mb5
	layer7_attn_out_dp1_mb5_gpu7 -> layer7_attn_allreduce_dp1_mb5
	layer7_router_dp0_mb5_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb5_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb5 -> layer7_router_dp0_mb5_gpu4
	layer7_attn_allreduce_dp1_mb5 -> layer7_router_dp1_mb5_gpu4
	layer7_expert0_mlp1_dp0_mb5_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb5_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb5_gpu4 -> layer7_expert0_mlp1_dp0_mb5_gpu4
	layer7_router_dp1_mb5_gpu4 -> layer7_expert0_mlp1_dp1_mb5_gpu4
	layer7_expert0_mlp2_dp0_mb5_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb5_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb5_gpu4 -> layer7_expert0_mlp2_dp0_mb5_gpu4
	layer7_expert0_mlp1_dp1_mb5_gpu4 -> layer7_expert0_mlp2_dp1_mb5_gpu4
	layer7_expert1_mlp1_dp0_mb5_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb5_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb5_gpu4 -> layer7_expert1_mlp1_dp0_mb5_gpu5
	layer7_router_dp1_mb5_gpu4 -> layer7_expert1_mlp1_dp1_mb5_gpu5
	layer7_expert1_mlp2_dp0_mb5_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb5_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb5_gpu5 -> layer7_expert1_mlp2_dp0_mb5_gpu5
	layer7_expert1_mlp1_dp1_mb5_gpu5 -> layer7_expert1_mlp2_dp1_mb5_gpu5
	layer7_expert2_mlp1_dp0_mb5_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb5_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb5_gpu4 -> layer7_expert2_mlp1_dp0_mb5_gpu6
	layer7_router_dp1_mb5_gpu4 -> layer7_expert2_mlp1_dp1_mb5_gpu6
	layer7_expert2_mlp2_dp0_mb5_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb5_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb5_gpu6 -> layer7_expert2_mlp2_dp0_mb5_gpu6
	layer7_expert2_mlp1_dp1_mb5_gpu6 -> layer7_expert2_mlp2_dp1_mb5_gpu6
	layer7_expert3_mlp1_dp0_mb5_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb5_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb5_gpu4 -> layer7_expert3_mlp1_dp0_mb5_gpu7
	layer7_router_dp1_mb5_gpu4 -> layer7_expert3_mlp1_dp1_mb5_gpu7
	layer7_expert3_mlp2_dp0_mb5_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb5_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb5_gpu7 -> layer7_expert3_mlp2_dp0_mb5_gpu7
	layer7_expert3_mlp1_dp1_mb5_gpu7 -> layer7_expert3_mlp2_dp1_mb5_gpu7
	layer7_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb5_gpu4 -> layer7_expert_agg_dp0_mb5
	layer7_expert0_mlp2_dp1_mb5_gpu4 -> layer7_expert_agg_dp1_mb5
	layer7_expert1_mlp2_dp0_mb5_gpu5 -> layer7_expert_agg_dp0_mb5
	layer7_expert1_mlp2_dp1_mb5_gpu5 -> layer7_expert_agg_dp1_mb5
	layer7_expert2_mlp2_dp0_mb5_gpu6 -> layer7_expert_agg_dp0_mb5
	layer7_expert2_mlp2_dp1_mb5_gpu6 -> layer7_expert_agg_dp1_mb5
	layer7_expert3_mlp2_dp0_mb5_gpu7 -> layer7_expert_agg_dp0_mb5
	layer7_expert3_mlp2_dp1_mb5_gpu7 -> layer7_expert_agg_dp1_mb5
	layer4_qkv_dp0_mb6_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb6_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb6 -> layer4_qkv_dp0_mb6_gpu4
	layer3_expert_agg_dp1_mb6 -> layer4_qkv_dp1_mb6_gpu4
	layer4_attn_scores_dp0_mb6_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb6_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb6_gpu4 -> layer4_attn_scores_dp0_mb6_gpu4
	layer4_qkv_dp1_mb6_gpu4 -> layer4_attn_scores_dp1_mb6_gpu4
	layer4_softmax_dp0_mb6_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb6_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb6_gpu4 -> layer4_softmax_dp0_mb6_gpu4
	layer4_attn_scores_dp1_mb6_gpu4 -> layer4_softmax_dp1_mb6_gpu4
	layer4_attn_out_dp0_mb6_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb6_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb6_gpu4 -> layer4_attn_out_dp0_mb6_gpu4
	layer4_softmax_dp1_mb6_gpu4 -> layer4_attn_out_dp1_mb6_gpu4
	layer4_qkv_dp0_mb6_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb6_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb6 -> layer4_qkv_dp0_mb6_gpu5
	layer3_expert_agg_dp1_mb6 -> layer4_qkv_dp1_mb6_gpu5
	layer4_attn_scores_dp0_mb6_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb6_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb6_gpu5 -> layer4_attn_scores_dp0_mb6_gpu5
	layer4_qkv_dp1_mb6_gpu5 -> layer4_attn_scores_dp1_mb6_gpu5
	layer4_softmax_dp0_mb6_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb6_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb6_gpu5 -> layer4_softmax_dp0_mb6_gpu5
	layer4_attn_scores_dp1_mb6_gpu5 -> layer4_softmax_dp1_mb6_gpu5
	layer4_attn_out_dp0_mb6_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb6_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb6_gpu5 -> layer4_attn_out_dp0_mb6_gpu5
	layer4_softmax_dp1_mb6_gpu5 -> layer4_attn_out_dp1_mb6_gpu5
	layer4_qkv_dp0_mb6_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb6_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb6 -> layer4_qkv_dp0_mb6_gpu6
	layer3_expert_agg_dp1_mb6 -> layer4_qkv_dp1_mb6_gpu6
	layer4_attn_scores_dp0_mb6_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb6_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb6_gpu6 -> layer4_attn_scores_dp0_mb6_gpu6
	layer4_qkv_dp1_mb6_gpu6 -> layer4_attn_scores_dp1_mb6_gpu6
	layer4_softmax_dp0_mb6_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb6_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb6_gpu6 -> layer4_softmax_dp0_mb6_gpu6
	layer4_attn_scores_dp1_mb6_gpu6 -> layer4_softmax_dp1_mb6_gpu6
	layer4_attn_out_dp0_mb6_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb6_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb6_gpu6 -> layer4_attn_out_dp0_mb6_gpu6
	layer4_softmax_dp1_mb6_gpu6 -> layer4_attn_out_dp1_mb6_gpu6
	layer4_qkv_dp0_mb6_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb6_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb6 -> layer4_qkv_dp0_mb6_gpu7
	layer3_expert_agg_dp1_mb6 -> layer4_qkv_dp1_mb6_gpu7
	layer4_attn_scores_dp0_mb6_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb6_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb6_gpu7 -> layer4_attn_scores_dp0_mb6_gpu7
	layer4_qkv_dp1_mb6_gpu7 -> layer4_attn_scores_dp1_mb6_gpu7
	layer4_softmax_dp0_mb6_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb6_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb6_gpu7 -> layer4_softmax_dp0_mb6_gpu7
	layer4_attn_scores_dp1_mb6_gpu7 -> layer4_softmax_dp1_mb6_gpu7
	layer4_attn_out_dp0_mb6_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb6_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb6_gpu7 -> layer4_attn_out_dp0_mb6_gpu7
	layer4_softmax_dp1_mb6_gpu7 -> layer4_attn_out_dp1_mb6_gpu7
	layer4_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb6_gpu4 -> layer4_attn_allreduce_dp0_mb6
	layer4_attn_out_dp1_mb6_gpu4 -> layer4_attn_allreduce_dp1_mb6
	layer4_attn_out_dp0_mb6_gpu5 -> layer4_attn_allreduce_dp0_mb6
	layer4_attn_out_dp1_mb6_gpu5 -> layer4_attn_allreduce_dp1_mb6
	layer4_attn_out_dp0_mb6_gpu6 -> layer4_attn_allreduce_dp0_mb6
	layer4_attn_out_dp1_mb6_gpu6 -> layer4_attn_allreduce_dp1_mb6
	layer4_attn_out_dp0_mb6_gpu7 -> layer4_attn_allreduce_dp0_mb6
	layer4_attn_out_dp1_mb6_gpu7 -> layer4_attn_allreduce_dp1_mb6
	layer4_router_dp0_mb6_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb6_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb6 -> layer4_router_dp0_mb6_gpu4
	layer4_attn_allreduce_dp1_mb6 -> layer4_router_dp1_mb6_gpu4
	layer4_expert0_mlp1_dp0_mb6_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb6_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb6_gpu4 -> layer4_expert0_mlp1_dp0_mb6_gpu4
	layer4_router_dp1_mb6_gpu4 -> layer4_expert0_mlp1_dp1_mb6_gpu4
	layer4_expert0_mlp2_dp0_mb6_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb6_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb6_gpu4 -> layer4_expert0_mlp2_dp0_mb6_gpu4
	layer4_expert0_mlp1_dp1_mb6_gpu4 -> layer4_expert0_mlp2_dp1_mb6_gpu4
	layer4_expert1_mlp1_dp0_mb6_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb6_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb6_gpu4 -> layer4_expert1_mlp1_dp0_mb6_gpu5
	layer4_router_dp1_mb6_gpu4 -> layer4_expert1_mlp1_dp1_mb6_gpu5
	layer4_expert1_mlp2_dp0_mb6_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb6_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb6_gpu5 -> layer4_expert1_mlp2_dp0_mb6_gpu5
	layer4_expert1_mlp1_dp1_mb6_gpu5 -> layer4_expert1_mlp2_dp1_mb6_gpu5
	layer4_expert2_mlp1_dp0_mb6_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb6_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb6_gpu4 -> layer4_expert2_mlp1_dp0_mb6_gpu6
	layer4_router_dp1_mb6_gpu4 -> layer4_expert2_mlp1_dp1_mb6_gpu6
	layer4_expert2_mlp2_dp0_mb6_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb6_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb6_gpu6 -> layer4_expert2_mlp2_dp0_mb6_gpu6
	layer4_expert2_mlp1_dp1_mb6_gpu6 -> layer4_expert2_mlp2_dp1_mb6_gpu6
	layer4_expert3_mlp1_dp0_mb6_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb6_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb6_gpu4 -> layer4_expert3_mlp1_dp0_mb6_gpu7
	layer4_router_dp1_mb6_gpu4 -> layer4_expert3_mlp1_dp1_mb6_gpu7
	layer4_expert3_mlp2_dp0_mb6_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb6_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb6_gpu7 -> layer4_expert3_mlp2_dp0_mb6_gpu7
	layer4_expert3_mlp1_dp1_mb6_gpu7 -> layer4_expert3_mlp2_dp1_mb6_gpu7
	layer4_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb6_gpu4 -> layer4_expert_agg_dp0_mb6
	layer4_expert0_mlp2_dp1_mb6_gpu4 -> layer4_expert_agg_dp1_mb6
	layer4_expert1_mlp2_dp0_mb6_gpu5 -> layer4_expert_agg_dp0_mb6
	layer4_expert1_mlp2_dp1_mb6_gpu5 -> layer4_expert_agg_dp1_mb6
	layer4_expert2_mlp2_dp0_mb6_gpu6 -> layer4_expert_agg_dp0_mb6
	layer4_expert2_mlp2_dp1_mb6_gpu6 -> layer4_expert_agg_dp1_mb6
	layer4_expert3_mlp2_dp0_mb6_gpu7 -> layer4_expert_agg_dp0_mb6
	layer4_expert3_mlp2_dp1_mb6_gpu7 -> layer4_expert_agg_dp1_mb6
	layer5_qkv_dp0_mb6_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb6_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb6 -> layer5_qkv_dp0_mb6_gpu4
	layer4_expert_agg_dp1_mb6 -> layer5_qkv_dp1_mb6_gpu4
	layer5_attn_scores_dp0_mb6_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb6_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb6_gpu4 -> layer5_attn_scores_dp0_mb6_gpu4
	layer5_qkv_dp1_mb6_gpu4 -> layer5_attn_scores_dp1_mb6_gpu4
	layer5_softmax_dp0_mb6_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb6_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb6_gpu4 -> layer5_softmax_dp0_mb6_gpu4
	layer5_attn_scores_dp1_mb6_gpu4 -> layer5_softmax_dp1_mb6_gpu4
	layer5_attn_out_dp0_mb6_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb6_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb6_gpu4 -> layer5_attn_out_dp0_mb6_gpu4
	layer5_softmax_dp1_mb6_gpu4 -> layer5_attn_out_dp1_mb6_gpu4
	layer5_qkv_dp0_mb6_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb6_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb6 -> layer5_qkv_dp0_mb6_gpu5
	layer4_expert_agg_dp1_mb6 -> layer5_qkv_dp1_mb6_gpu5
	layer5_attn_scores_dp0_mb6_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb6_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb6_gpu5 -> layer5_attn_scores_dp0_mb6_gpu5
	layer5_qkv_dp1_mb6_gpu5 -> layer5_attn_scores_dp1_mb6_gpu5
	layer5_softmax_dp0_mb6_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb6_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb6_gpu5 -> layer5_softmax_dp0_mb6_gpu5
	layer5_attn_scores_dp1_mb6_gpu5 -> layer5_softmax_dp1_mb6_gpu5
	layer5_attn_out_dp0_mb6_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb6_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb6_gpu5 -> layer5_attn_out_dp0_mb6_gpu5
	layer5_softmax_dp1_mb6_gpu5 -> layer5_attn_out_dp1_mb6_gpu5
	layer5_qkv_dp0_mb6_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb6_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb6 -> layer5_qkv_dp0_mb6_gpu6
	layer4_expert_agg_dp1_mb6 -> layer5_qkv_dp1_mb6_gpu6
	layer5_attn_scores_dp0_mb6_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb6_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb6_gpu6 -> layer5_attn_scores_dp0_mb6_gpu6
	layer5_qkv_dp1_mb6_gpu6 -> layer5_attn_scores_dp1_mb6_gpu6
	layer5_softmax_dp0_mb6_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb6_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb6_gpu6 -> layer5_softmax_dp0_mb6_gpu6
	layer5_attn_scores_dp1_mb6_gpu6 -> layer5_softmax_dp1_mb6_gpu6
	layer5_attn_out_dp0_mb6_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb6_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb6_gpu6 -> layer5_attn_out_dp0_mb6_gpu6
	layer5_softmax_dp1_mb6_gpu6 -> layer5_attn_out_dp1_mb6_gpu6
	layer5_qkv_dp0_mb6_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb6_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb6 -> layer5_qkv_dp0_mb6_gpu7
	layer4_expert_agg_dp1_mb6 -> layer5_qkv_dp1_mb6_gpu7
	layer5_attn_scores_dp0_mb6_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb6_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb6_gpu7 -> layer5_attn_scores_dp0_mb6_gpu7
	layer5_qkv_dp1_mb6_gpu7 -> layer5_attn_scores_dp1_mb6_gpu7
	layer5_softmax_dp0_mb6_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb6_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb6_gpu7 -> layer5_softmax_dp0_mb6_gpu7
	layer5_attn_scores_dp1_mb6_gpu7 -> layer5_softmax_dp1_mb6_gpu7
	layer5_attn_out_dp0_mb6_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb6_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb6_gpu7 -> layer5_attn_out_dp0_mb6_gpu7
	layer5_softmax_dp1_mb6_gpu7 -> layer5_attn_out_dp1_mb6_gpu7
	layer5_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb6_gpu4 -> layer5_attn_allreduce_dp0_mb6
	layer5_attn_out_dp1_mb6_gpu4 -> layer5_attn_allreduce_dp1_mb6
	layer5_attn_out_dp0_mb6_gpu5 -> layer5_attn_allreduce_dp0_mb6
	layer5_attn_out_dp1_mb6_gpu5 -> layer5_attn_allreduce_dp1_mb6
	layer5_attn_out_dp0_mb6_gpu6 -> layer5_attn_allreduce_dp0_mb6
	layer5_attn_out_dp1_mb6_gpu6 -> layer5_attn_allreduce_dp1_mb6
	layer5_attn_out_dp0_mb6_gpu7 -> layer5_attn_allreduce_dp0_mb6
	layer5_attn_out_dp1_mb6_gpu7 -> layer5_attn_allreduce_dp1_mb6
	layer5_router_dp0_mb6_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb6_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb6 -> layer5_router_dp0_mb6_gpu4
	layer5_attn_allreduce_dp1_mb6 -> layer5_router_dp1_mb6_gpu4
	layer5_expert0_mlp1_dp0_mb6_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb6_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb6_gpu4 -> layer5_expert0_mlp1_dp0_mb6_gpu4
	layer5_router_dp1_mb6_gpu4 -> layer5_expert0_mlp1_dp1_mb6_gpu4
	layer5_expert0_mlp2_dp0_mb6_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb6_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb6_gpu4 -> layer5_expert0_mlp2_dp0_mb6_gpu4
	layer5_expert0_mlp1_dp1_mb6_gpu4 -> layer5_expert0_mlp2_dp1_mb6_gpu4
	layer5_expert1_mlp1_dp0_mb6_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb6_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb6_gpu4 -> layer5_expert1_mlp1_dp0_mb6_gpu5
	layer5_router_dp1_mb6_gpu4 -> layer5_expert1_mlp1_dp1_mb6_gpu5
	layer5_expert1_mlp2_dp0_mb6_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb6_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb6_gpu5 -> layer5_expert1_mlp2_dp0_mb6_gpu5
	layer5_expert1_mlp1_dp1_mb6_gpu5 -> layer5_expert1_mlp2_dp1_mb6_gpu5
	layer5_expert2_mlp1_dp0_mb6_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb6_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb6_gpu4 -> layer5_expert2_mlp1_dp0_mb6_gpu6
	layer5_router_dp1_mb6_gpu4 -> layer5_expert2_mlp1_dp1_mb6_gpu6
	layer5_expert2_mlp2_dp0_mb6_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb6_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb6_gpu6 -> layer5_expert2_mlp2_dp0_mb6_gpu6
	layer5_expert2_mlp1_dp1_mb6_gpu6 -> layer5_expert2_mlp2_dp1_mb6_gpu6
	layer5_expert3_mlp1_dp0_mb6_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb6_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb6_gpu4 -> layer5_expert3_mlp1_dp0_mb6_gpu7
	layer5_router_dp1_mb6_gpu4 -> layer5_expert3_mlp1_dp1_mb6_gpu7
	layer5_expert3_mlp2_dp0_mb6_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb6_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb6_gpu7 -> layer5_expert3_mlp2_dp0_mb6_gpu7
	layer5_expert3_mlp1_dp1_mb6_gpu7 -> layer5_expert3_mlp2_dp1_mb6_gpu7
	layer5_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb6_gpu4 -> layer5_expert_agg_dp0_mb6
	layer5_expert0_mlp2_dp1_mb6_gpu4 -> layer5_expert_agg_dp1_mb6
	layer5_expert1_mlp2_dp0_mb6_gpu5 -> layer5_expert_agg_dp0_mb6
	layer5_expert1_mlp2_dp1_mb6_gpu5 -> layer5_expert_agg_dp1_mb6
	layer5_expert2_mlp2_dp0_mb6_gpu6 -> layer5_expert_agg_dp0_mb6
	layer5_expert2_mlp2_dp1_mb6_gpu6 -> layer5_expert_agg_dp1_mb6
	layer5_expert3_mlp2_dp0_mb6_gpu7 -> layer5_expert_agg_dp0_mb6
	layer5_expert3_mlp2_dp1_mb6_gpu7 -> layer5_expert_agg_dp1_mb6
	layer6_qkv_dp0_mb6_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb6_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb6 -> layer6_qkv_dp0_mb6_gpu4
	layer5_expert_agg_dp1_mb6 -> layer6_qkv_dp1_mb6_gpu4
	layer6_attn_scores_dp0_mb6_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb6_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb6_gpu4 -> layer6_attn_scores_dp0_mb6_gpu4
	layer6_qkv_dp1_mb6_gpu4 -> layer6_attn_scores_dp1_mb6_gpu4
	layer6_softmax_dp0_mb6_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb6_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb6_gpu4 -> layer6_softmax_dp0_mb6_gpu4
	layer6_attn_scores_dp1_mb6_gpu4 -> layer6_softmax_dp1_mb6_gpu4
	layer6_attn_out_dp0_mb6_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb6_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb6_gpu4 -> layer6_attn_out_dp0_mb6_gpu4
	layer6_softmax_dp1_mb6_gpu4 -> layer6_attn_out_dp1_mb6_gpu4
	layer6_qkv_dp0_mb6_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb6_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb6 -> layer6_qkv_dp0_mb6_gpu5
	layer5_expert_agg_dp1_mb6 -> layer6_qkv_dp1_mb6_gpu5
	layer6_attn_scores_dp0_mb6_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb6_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb6_gpu5 -> layer6_attn_scores_dp0_mb6_gpu5
	layer6_qkv_dp1_mb6_gpu5 -> layer6_attn_scores_dp1_mb6_gpu5
	layer6_softmax_dp0_mb6_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb6_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb6_gpu5 -> layer6_softmax_dp0_mb6_gpu5
	layer6_attn_scores_dp1_mb6_gpu5 -> layer6_softmax_dp1_mb6_gpu5
	layer6_attn_out_dp0_mb6_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb6_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb6_gpu5 -> layer6_attn_out_dp0_mb6_gpu5
	layer6_softmax_dp1_mb6_gpu5 -> layer6_attn_out_dp1_mb6_gpu5
	layer6_qkv_dp0_mb6_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb6_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb6 -> layer6_qkv_dp0_mb6_gpu6
	layer5_expert_agg_dp1_mb6 -> layer6_qkv_dp1_mb6_gpu6
	layer6_attn_scores_dp0_mb6_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb6_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb6_gpu6 -> layer6_attn_scores_dp0_mb6_gpu6
	layer6_qkv_dp1_mb6_gpu6 -> layer6_attn_scores_dp1_mb6_gpu6
	layer6_softmax_dp0_mb6_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb6_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb6_gpu6 -> layer6_softmax_dp0_mb6_gpu6
	layer6_attn_scores_dp1_mb6_gpu6 -> layer6_softmax_dp1_mb6_gpu6
	layer6_attn_out_dp0_mb6_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb6_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb6_gpu6 -> layer6_attn_out_dp0_mb6_gpu6
	layer6_softmax_dp1_mb6_gpu6 -> layer6_attn_out_dp1_mb6_gpu6
	layer6_qkv_dp0_mb6_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb6_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb6 -> layer6_qkv_dp0_mb6_gpu7
	layer5_expert_agg_dp1_mb6 -> layer6_qkv_dp1_mb6_gpu7
	layer6_attn_scores_dp0_mb6_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb6_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb6_gpu7 -> layer6_attn_scores_dp0_mb6_gpu7
	layer6_qkv_dp1_mb6_gpu7 -> layer6_attn_scores_dp1_mb6_gpu7
	layer6_softmax_dp0_mb6_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb6_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb6_gpu7 -> layer6_softmax_dp0_mb6_gpu7
	layer6_attn_scores_dp1_mb6_gpu7 -> layer6_softmax_dp1_mb6_gpu7
	layer6_attn_out_dp0_mb6_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb6_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb6_gpu7 -> layer6_attn_out_dp0_mb6_gpu7
	layer6_softmax_dp1_mb6_gpu7 -> layer6_attn_out_dp1_mb6_gpu7
	layer6_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb6_gpu4 -> layer6_attn_allreduce_dp0_mb6
	layer6_attn_out_dp1_mb6_gpu4 -> layer6_attn_allreduce_dp1_mb6
	layer6_attn_out_dp0_mb6_gpu5 -> layer6_attn_allreduce_dp0_mb6
	layer6_attn_out_dp1_mb6_gpu5 -> layer6_attn_allreduce_dp1_mb6
	layer6_attn_out_dp0_mb6_gpu6 -> layer6_attn_allreduce_dp0_mb6
	layer6_attn_out_dp1_mb6_gpu6 -> layer6_attn_allreduce_dp1_mb6
	layer6_attn_out_dp0_mb6_gpu7 -> layer6_attn_allreduce_dp0_mb6
	layer6_attn_out_dp1_mb6_gpu7 -> layer6_attn_allreduce_dp1_mb6
	layer6_router_dp0_mb6_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb6_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb6 -> layer6_router_dp0_mb6_gpu4
	layer6_attn_allreduce_dp1_mb6 -> layer6_router_dp1_mb6_gpu4
	layer6_expert0_mlp1_dp0_mb6_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb6_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb6_gpu4 -> layer6_expert0_mlp1_dp0_mb6_gpu4
	layer6_router_dp1_mb6_gpu4 -> layer6_expert0_mlp1_dp1_mb6_gpu4
	layer6_expert0_mlp2_dp0_mb6_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb6_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb6_gpu4 -> layer6_expert0_mlp2_dp0_mb6_gpu4
	layer6_expert0_mlp1_dp1_mb6_gpu4 -> layer6_expert0_mlp2_dp1_mb6_gpu4
	layer6_expert1_mlp1_dp0_mb6_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb6_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb6_gpu4 -> layer6_expert1_mlp1_dp0_mb6_gpu5
	layer6_router_dp1_mb6_gpu4 -> layer6_expert1_mlp1_dp1_mb6_gpu5
	layer6_expert1_mlp2_dp0_mb6_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb6_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb6_gpu5 -> layer6_expert1_mlp2_dp0_mb6_gpu5
	layer6_expert1_mlp1_dp1_mb6_gpu5 -> layer6_expert1_mlp2_dp1_mb6_gpu5
	layer6_expert2_mlp1_dp0_mb6_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb6_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb6_gpu4 -> layer6_expert2_mlp1_dp0_mb6_gpu6
	layer6_router_dp1_mb6_gpu4 -> layer6_expert2_mlp1_dp1_mb6_gpu6
	layer6_expert2_mlp2_dp0_mb6_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb6_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb6_gpu6 -> layer6_expert2_mlp2_dp0_mb6_gpu6
	layer6_expert2_mlp1_dp1_mb6_gpu6 -> layer6_expert2_mlp2_dp1_mb6_gpu6
	layer6_expert3_mlp1_dp0_mb6_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb6_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb6_gpu4 -> layer6_expert3_mlp1_dp0_mb6_gpu7
	layer6_router_dp1_mb6_gpu4 -> layer6_expert3_mlp1_dp1_mb6_gpu7
	layer6_expert3_mlp2_dp0_mb6_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb6_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb6_gpu7 -> layer6_expert3_mlp2_dp0_mb6_gpu7
	layer6_expert3_mlp1_dp1_mb6_gpu7 -> layer6_expert3_mlp2_dp1_mb6_gpu7
	layer6_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb6_gpu4 -> layer6_expert_agg_dp0_mb6
	layer6_expert0_mlp2_dp1_mb6_gpu4 -> layer6_expert_agg_dp1_mb6
	layer6_expert1_mlp2_dp0_mb6_gpu5 -> layer6_expert_agg_dp0_mb6
	layer6_expert1_mlp2_dp1_mb6_gpu5 -> layer6_expert_agg_dp1_mb6
	layer6_expert2_mlp2_dp0_mb6_gpu6 -> layer6_expert_agg_dp0_mb6
	layer6_expert2_mlp2_dp1_mb6_gpu6 -> layer6_expert_agg_dp1_mb6
	layer6_expert3_mlp2_dp0_mb6_gpu7 -> layer6_expert_agg_dp0_mb6
	layer6_expert3_mlp2_dp1_mb6_gpu7 -> layer6_expert_agg_dp1_mb6
	layer7_qkv_dp0_mb6_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb6_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb6 -> layer7_qkv_dp0_mb6_gpu4
	layer6_expert_agg_dp1_mb6 -> layer7_qkv_dp1_mb6_gpu4
	layer7_attn_scores_dp0_mb6_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb6_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb6_gpu4 -> layer7_attn_scores_dp0_mb6_gpu4
	layer7_qkv_dp1_mb6_gpu4 -> layer7_attn_scores_dp1_mb6_gpu4
	layer7_softmax_dp0_mb6_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb6_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb6_gpu4 -> layer7_softmax_dp0_mb6_gpu4
	layer7_attn_scores_dp1_mb6_gpu4 -> layer7_softmax_dp1_mb6_gpu4
	layer7_attn_out_dp0_mb6_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb6_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb6_gpu4 -> layer7_attn_out_dp0_mb6_gpu4
	layer7_softmax_dp1_mb6_gpu4 -> layer7_attn_out_dp1_mb6_gpu4
	layer7_qkv_dp0_mb6_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb6_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb6 -> layer7_qkv_dp0_mb6_gpu5
	layer6_expert_agg_dp1_mb6 -> layer7_qkv_dp1_mb6_gpu5
	layer7_attn_scores_dp0_mb6_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb6_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb6_gpu5 -> layer7_attn_scores_dp0_mb6_gpu5
	layer7_qkv_dp1_mb6_gpu5 -> layer7_attn_scores_dp1_mb6_gpu5
	layer7_softmax_dp0_mb6_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb6_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb6_gpu5 -> layer7_softmax_dp0_mb6_gpu5
	layer7_attn_scores_dp1_mb6_gpu5 -> layer7_softmax_dp1_mb6_gpu5
	layer7_attn_out_dp0_mb6_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb6_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb6_gpu5 -> layer7_attn_out_dp0_mb6_gpu5
	layer7_softmax_dp1_mb6_gpu5 -> layer7_attn_out_dp1_mb6_gpu5
	layer7_qkv_dp0_mb6_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb6_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb6 -> layer7_qkv_dp0_mb6_gpu6
	layer6_expert_agg_dp1_mb6 -> layer7_qkv_dp1_mb6_gpu6
	layer7_attn_scores_dp0_mb6_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb6_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb6_gpu6 -> layer7_attn_scores_dp0_mb6_gpu6
	layer7_qkv_dp1_mb6_gpu6 -> layer7_attn_scores_dp1_mb6_gpu6
	layer7_softmax_dp0_mb6_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb6_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb6_gpu6 -> layer7_softmax_dp0_mb6_gpu6
	layer7_attn_scores_dp1_mb6_gpu6 -> layer7_softmax_dp1_mb6_gpu6
	layer7_attn_out_dp0_mb6_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb6_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb6_gpu6 -> layer7_attn_out_dp0_mb6_gpu6
	layer7_softmax_dp1_mb6_gpu6 -> layer7_attn_out_dp1_mb6_gpu6
	layer7_qkv_dp0_mb6_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb6_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb6 -> layer7_qkv_dp0_mb6_gpu7
	layer6_expert_agg_dp1_mb6 -> layer7_qkv_dp1_mb6_gpu7
	layer7_attn_scores_dp0_mb6_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb6_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb6_gpu7 -> layer7_attn_scores_dp0_mb6_gpu7
	layer7_qkv_dp1_mb6_gpu7 -> layer7_attn_scores_dp1_mb6_gpu7
	layer7_softmax_dp0_mb6_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb6_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb6_gpu7 -> layer7_softmax_dp0_mb6_gpu7
	layer7_attn_scores_dp1_mb6_gpu7 -> layer7_softmax_dp1_mb6_gpu7
	layer7_attn_out_dp0_mb6_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb6_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb6_gpu7 -> layer7_attn_out_dp0_mb6_gpu7
	layer7_softmax_dp1_mb6_gpu7 -> layer7_attn_out_dp1_mb6_gpu7
	layer7_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb6_gpu4 -> layer7_attn_allreduce_dp0_mb6
	layer7_attn_out_dp1_mb6_gpu4 -> layer7_attn_allreduce_dp1_mb6
	layer7_attn_out_dp0_mb6_gpu5 -> layer7_attn_allreduce_dp0_mb6
	layer7_attn_out_dp1_mb6_gpu5 -> layer7_attn_allreduce_dp1_mb6
	layer7_attn_out_dp0_mb6_gpu6 -> layer7_attn_allreduce_dp0_mb6
	layer7_attn_out_dp1_mb6_gpu6 -> layer7_attn_allreduce_dp1_mb6
	layer7_attn_out_dp0_mb6_gpu7 -> layer7_attn_allreduce_dp0_mb6
	layer7_attn_out_dp1_mb6_gpu7 -> layer7_attn_allreduce_dp1_mb6
	layer7_router_dp0_mb6_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb6_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb6 -> layer7_router_dp0_mb6_gpu4
	layer7_attn_allreduce_dp1_mb6 -> layer7_router_dp1_mb6_gpu4
	layer7_expert0_mlp1_dp0_mb6_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb6_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb6_gpu4 -> layer7_expert0_mlp1_dp0_mb6_gpu4
	layer7_router_dp1_mb6_gpu4 -> layer7_expert0_mlp1_dp1_mb6_gpu4
	layer7_expert0_mlp2_dp0_mb6_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb6_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb6_gpu4 -> layer7_expert0_mlp2_dp0_mb6_gpu4
	layer7_expert0_mlp1_dp1_mb6_gpu4 -> layer7_expert0_mlp2_dp1_mb6_gpu4
	layer7_expert1_mlp1_dp0_mb6_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb6_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb6_gpu4 -> layer7_expert1_mlp1_dp0_mb6_gpu5
	layer7_router_dp1_mb6_gpu4 -> layer7_expert1_mlp1_dp1_mb6_gpu5
	layer7_expert1_mlp2_dp0_mb6_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb6_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb6_gpu5 -> layer7_expert1_mlp2_dp0_mb6_gpu5
	layer7_expert1_mlp1_dp1_mb6_gpu5 -> layer7_expert1_mlp2_dp1_mb6_gpu5
	layer7_expert2_mlp1_dp0_mb6_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb6_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb6_gpu4 -> layer7_expert2_mlp1_dp0_mb6_gpu6
	layer7_router_dp1_mb6_gpu4 -> layer7_expert2_mlp1_dp1_mb6_gpu6
	layer7_expert2_mlp2_dp0_mb6_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb6_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb6_gpu6 -> layer7_expert2_mlp2_dp0_mb6_gpu6
	layer7_expert2_mlp1_dp1_mb6_gpu6 -> layer7_expert2_mlp2_dp1_mb6_gpu6
	layer7_expert3_mlp1_dp0_mb6_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb6_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb6_gpu4 -> layer7_expert3_mlp1_dp0_mb6_gpu7
	layer7_router_dp1_mb6_gpu4 -> layer7_expert3_mlp1_dp1_mb6_gpu7
	layer7_expert3_mlp2_dp0_mb6_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb6_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb6_gpu7 -> layer7_expert3_mlp2_dp0_mb6_gpu7
	layer7_expert3_mlp1_dp1_mb6_gpu7 -> layer7_expert3_mlp2_dp1_mb6_gpu7
	layer7_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb6_gpu4 -> layer7_expert_agg_dp0_mb6
	layer7_expert0_mlp2_dp1_mb6_gpu4 -> layer7_expert_agg_dp1_mb6
	layer7_expert1_mlp2_dp0_mb6_gpu5 -> layer7_expert_agg_dp0_mb6
	layer7_expert1_mlp2_dp1_mb6_gpu5 -> layer7_expert_agg_dp1_mb6
	layer7_expert2_mlp2_dp0_mb6_gpu6 -> layer7_expert_agg_dp0_mb6
	layer7_expert2_mlp2_dp1_mb6_gpu6 -> layer7_expert_agg_dp1_mb6
	layer7_expert3_mlp2_dp0_mb6_gpu7 -> layer7_expert_agg_dp0_mb6
	layer7_expert3_mlp2_dp1_mb6_gpu7 -> layer7_expert_agg_dp1_mb6
	layer4_qkv_dp0_mb7_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb7_gpu4 [label="Layer 4 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb7 -> layer4_qkv_dp0_mb7_gpu4
	layer3_expert_agg_dp1_mb7 -> layer4_qkv_dp1_mb7_gpu4
	layer4_attn_scores_dp0_mb7_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb7_gpu4 [label="Layer 4 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb7_gpu4 -> layer4_attn_scores_dp0_mb7_gpu4
	layer4_qkv_dp1_mb7_gpu4 -> layer4_attn_scores_dp1_mb7_gpu4
	layer4_softmax_dp0_mb7_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb7_gpu4 [label="Layer 4 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb7_gpu4 -> layer4_softmax_dp0_mb7_gpu4
	layer4_attn_scores_dp1_mb7_gpu4 -> layer4_softmax_dp1_mb7_gpu4
	layer4_attn_out_dp0_mb7_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb7_gpu4 [label="Layer 4 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb7_gpu4 -> layer4_attn_out_dp0_mb7_gpu4
	layer4_softmax_dp1_mb7_gpu4 -> layer4_attn_out_dp1_mb7_gpu4
	layer4_qkv_dp0_mb7_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb7_gpu5 [label="Layer 4 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb7 -> layer4_qkv_dp0_mb7_gpu5
	layer3_expert_agg_dp1_mb7 -> layer4_qkv_dp1_mb7_gpu5
	layer4_attn_scores_dp0_mb7_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb7_gpu5 [label="Layer 4 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb7_gpu5 -> layer4_attn_scores_dp0_mb7_gpu5
	layer4_qkv_dp1_mb7_gpu5 -> layer4_attn_scores_dp1_mb7_gpu5
	layer4_softmax_dp0_mb7_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb7_gpu5 [label="Layer 4 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb7_gpu5 -> layer4_softmax_dp0_mb7_gpu5
	layer4_attn_scores_dp1_mb7_gpu5 -> layer4_softmax_dp1_mb7_gpu5
	layer4_attn_out_dp0_mb7_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb7_gpu5 [label="Layer 4 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb7_gpu5 -> layer4_attn_out_dp0_mb7_gpu5
	layer4_softmax_dp1_mb7_gpu5 -> layer4_attn_out_dp1_mb7_gpu5
	layer4_qkv_dp0_mb7_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb7_gpu6 [label="Layer 4 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb7 -> layer4_qkv_dp0_mb7_gpu6
	layer3_expert_agg_dp1_mb7 -> layer4_qkv_dp1_mb7_gpu6
	layer4_attn_scores_dp0_mb7_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb7_gpu6 [label="Layer 4 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb7_gpu6 -> layer4_attn_scores_dp0_mb7_gpu6
	layer4_qkv_dp1_mb7_gpu6 -> layer4_attn_scores_dp1_mb7_gpu6
	layer4_softmax_dp0_mb7_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb7_gpu6 [label="Layer 4 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb7_gpu6 -> layer4_softmax_dp0_mb7_gpu6
	layer4_attn_scores_dp1_mb7_gpu6 -> layer4_softmax_dp1_mb7_gpu6
	layer4_attn_out_dp0_mb7_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb7_gpu6 [label="Layer 4 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb7_gpu6 -> layer4_attn_out_dp0_mb7_gpu6
	layer4_softmax_dp1_mb7_gpu6 -> layer4_attn_out_dp1_mb7_gpu6
	layer4_qkv_dp0_mb7_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp1_mb7_gpu7 [label="Layer 4 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer3_expert_agg_dp0_mb7 -> layer4_qkv_dp0_mb7_gpu7
	layer3_expert_agg_dp1_mb7 -> layer4_qkv_dp1_mb7_gpu7
	layer4_attn_scores_dp0_mb7_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp1_mb7_gpu7 [label="Layer 4 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_qkv_dp0_mb7_gpu7 -> layer4_attn_scores_dp0_mb7_gpu7
	layer4_qkv_dp1_mb7_gpu7 -> layer4_attn_scores_dp1_mb7_gpu7
	layer4_softmax_dp0_mb7_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp1_mb7_gpu7 [label="Layer 4 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_scores_dp0_mb7_gpu7 -> layer4_softmax_dp0_mb7_gpu7
	layer4_attn_scores_dp1_mb7_gpu7 -> layer4_softmax_dp1_mb7_gpu7
	layer4_attn_out_dp0_mb7_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_attn_out_dp1_mb7_gpu7 [label="Layer 4 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_softmax_dp0_mb7_gpu7 -> layer4_attn_out_dp0_mb7_gpu7
	layer4_softmax_dp1_mb7_gpu7 -> layer4_attn_out_dp1_mb7_gpu7
	layer4_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_attn_out_dp0_mb7_gpu4 -> layer4_attn_allreduce_dp0_mb7
	layer4_attn_out_dp1_mb7_gpu4 -> layer4_attn_allreduce_dp1_mb7
	layer4_attn_out_dp0_mb7_gpu5 -> layer4_attn_allreduce_dp0_mb7
	layer4_attn_out_dp1_mb7_gpu5 -> layer4_attn_allreduce_dp1_mb7
	layer4_attn_out_dp0_mb7_gpu6 -> layer4_attn_allreduce_dp0_mb7
	layer4_attn_out_dp1_mb7_gpu6 -> layer4_attn_allreduce_dp1_mb7
	layer4_attn_out_dp0_mb7_gpu7 -> layer4_attn_allreduce_dp0_mb7
	layer4_attn_out_dp1_mb7_gpu7 -> layer4_attn_allreduce_dp1_mb7
	layer4_router_dp0_mb7_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_router_dp1_mb7_gpu4 [label="Layer 4 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer4_attn_allreduce_dp0_mb7 -> layer4_router_dp0_mb7_gpu4
	layer4_attn_allreduce_dp1_mb7 -> layer4_router_dp1_mb7_gpu4
	layer4_expert0_mlp1_dp0_mb7_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp1_mb7_gpu4 [label="Layer 4 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb7_gpu4 -> layer4_expert0_mlp1_dp0_mb7_gpu4
	layer4_router_dp1_mb7_gpu4 -> layer4_expert0_mlp1_dp1_mb7_gpu4
	layer4_expert0_mlp2_dp0_mb7_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp2_dp1_mb7_gpu4 [label="Layer 4 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert0_mlp1_dp0_mb7_gpu4 -> layer4_expert0_mlp2_dp0_mb7_gpu4
	layer4_expert0_mlp1_dp1_mb7_gpu4 -> layer4_expert0_mlp2_dp1_mb7_gpu4
	layer4_expert1_mlp1_dp0_mb7_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp1_mb7_gpu5 [label="Layer 4 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb7_gpu4 -> layer4_expert1_mlp1_dp0_mb7_gpu5
	layer4_router_dp1_mb7_gpu4 -> layer4_expert1_mlp1_dp1_mb7_gpu5
	layer4_expert1_mlp2_dp0_mb7_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp2_dp1_mb7_gpu5 [label="Layer 4 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert1_mlp1_dp0_mb7_gpu5 -> layer4_expert1_mlp2_dp0_mb7_gpu5
	layer4_expert1_mlp1_dp1_mb7_gpu5 -> layer4_expert1_mlp2_dp1_mb7_gpu5
	layer4_expert2_mlp1_dp0_mb7_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp1_mb7_gpu6 [label="Layer 4 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb7_gpu4 -> layer4_expert2_mlp1_dp0_mb7_gpu6
	layer4_router_dp1_mb7_gpu4 -> layer4_expert2_mlp1_dp1_mb7_gpu6
	layer4_expert2_mlp2_dp0_mb7_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp2_dp1_mb7_gpu6 [label="Layer 4 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert2_mlp1_dp0_mb7_gpu6 -> layer4_expert2_mlp2_dp0_mb7_gpu6
	layer4_expert2_mlp1_dp1_mb7_gpu6 -> layer4_expert2_mlp2_dp1_mb7_gpu6
	layer4_expert3_mlp1_dp0_mb7_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp1_mb7_gpu7 [label="Layer 4 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_router_dp0_mb7_gpu4 -> layer4_expert3_mlp1_dp0_mb7_gpu7
	layer4_router_dp1_mb7_gpu4 -> layer4_expert3_mlp1_dp1_mb7_gpu7
	layer4_expert3_mlp2_dp0_mb7_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp2_dp1_mb7_gpu7 [label="Layer 4 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert3_mlp1_dp0_mb7_gpu7 -> layer4_expert3_mlp2_dp0_mb7_gpu7
	layer4_expert3_mlp1_dp1_mb7_gpu7 -> layer4_expert3_mlp2_dp1_mb7_gpu7
	layer4_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer4_expert0_mlp2_dp0_mb7_gpu4 -> layer4_expert_agg_dp0_mb7
	layer4_expert0_mlp2_dp1_mb7_gpu4 -> layer4_expert_agg_dp1_mb7
	layer4_expert1_mlp2_dp0_mb7_gpu5 -> layer4_expert_agg_dp0_mb7
	layer4_expert1_mlp2_dp1_mb7_gpu5 -> layer4_expert_agg_dp1_mb7
	layer4_expert2_mlp2_dp0_mb7_gpu6 -> layer4_expert_agg_dp0_mb7
	layer4_expert2_mlp2_dp1_mb7_gpu6 -> layer4_expert_agg_dp1_mb7
	layer4_expert3_mlp2_dp0_mb7_gpu7 -> layer4_expert_agg_dp0_mb7
	layer4_expert3_mlp2_dp1_mb7_gpu7 -> layer4_expert_agg_dp1_mb7
	layer5_qkv_dp0_mb7_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb7_gpu4 [label="Layer 5 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb7 -> layer5_qkv_dp0_mb7_gpu4
	layer4_expert_agg_dp1_mb7 -> layer5_qkv_dp1_mb7_gpu4
	layer5_attn_scores_dp0_mb7_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb7_gpu4 [label="Layer 5 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb7_gpu4 -> layer5_attn_scores_dp0_mb7_gpu4
	layer5_qkv_dp1_mb7_gpu4 -> layer5_attn_scores_dp1_mb7_gpu4
	layer5_softmax_dp0_mb7_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb7_gpu4 [label="Layer 5 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb7_gpu4 -> layer5_softmax_dp0_mb7_gpu4
	layer5_attn_scores_dp1_mb7_gpu4 -> layer5_softmax_dp1_mb7_gpu4
	layer5_attn_out_dp0_mb7_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb7_gpu4 [label="Layer 5 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb7_gpu4 -> layer5_attn_out_dp0_mb7_gpu4
	layer5_softmax_dp1_mb7_gpu4 -> layer5_attn_out_dp1_mb7_gpu4
	layer5_qkv_dp0_mb7_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb7_gpu5 [label="Layer 5 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb7 -> layer5_qkv_dp0_mb7_gpu5
	layer4_expert_agg_dp1_mb7 -> layer5_qkv_dp1_mb7_gpu5
	layer5_attn_scores_dp0_mb7_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb7_gpu5 [label="Layer 5 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb7_gpu5 -> layer5_attn_scores_dp0_mb7_gpu5
	layer5_qkv_dp1_mb7_gpu5 -> layer5_attn_scores_dp1_mb7_gpu5
	layer5_softmax_dp0_mb7_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb7_gpu5 [label="Layer 5 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb7_gpu5 -> layer5_softmax_dp0_mb7_gpu5
	layer5_attn_scores_dp1_mb7_gpu5 -> layer5_softmax_dp1_mb7_gpu5
	layer5_attn_out_dp0_mb7_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb7_gpu5 [label="Layer 5 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb7_gpu5 -> layer5_attn_out_dp0_mb7_gpu5
	layer5_softmax_dp1_mb7_gpu5 -> layer5_attn_out_dp1_mb7_gpu5
	layer5_qkv_dp0_mb7_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb7_gpu6 [label="Layer 5 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb7 -> layer5_qkv_dp0_mb7_gpu6
	layer4_expert_agg_dp1_mb7 -> layer5_qkv_dp1_mb7_gpu6
	layer5_attn_scores_dp0_mb7_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb7_gpu6 [label="Layer 5 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb7_gpu6 -> layer5_attn_scores_dp0_mb7_gpu6
	layer5_qkv_dp1_mb7_gpu6 -> layer5_attn_scores_dp1_mb7_gpu6
	layer5_softmax_dp0_mb7_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb7_gpu6 [label="Layer 5 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb7_gpu6 -> layer5_softmax_dp0_mb7_gpu6
	layer5_attn_scores_dp1_mb7_gpu6 -> layer5_softmax_dp1_mb7_gpu6
	layer5_attn_out_dp0_mb7_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb7_gpu6 [label="Layer 5 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb7_gpu6 -> layer5_attn_out_dp0_mb7_gpu6
	layer5_softmax_dp1_mb7_gpu6 -> layer5_attn_out_dp1_mb7_gpu6
	layer5_qkv_dp0_mb7_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp1_mb7_gpu7 [label="Layer 5 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer4_expert_agg_dp0_mb7 -> layer5_qkv_dp0_mb7_gpu7
	layer4_expert_agg_dp1_mb7 -> layer5_qkv_dp1_mb7_gpu7
	layer5_attn_scores_dp0_mb7_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp1_mb7_gpu7 [label="Layer 5 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_qkv_dp0_mb7_gpu7 -> layer5_attn_scores_dp0_mb7_gpu7
	layer5_qkv_dp1_mb7_gpu7 -> layer5_attn_scores_dp1_mb7_gpu7
	layer5_softmax_dp0_mb7_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp1_mb7_gpu7 [label="Layer 5 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_scores_dp0_mb7_gpu7 -> layer5_softmax_dp0_mb7_gpu7
	layer5_attn_scores_dp1_mb7_gpu7 -> layer5_softmax_dp1_mb7_gpu7
	layer5_attn_out_dp0_mb7_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_attn_out_dp1_mb7_gpu7 [label="Layer 5 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_softmax_dp0_mb7_gpu7 -> layer5_attn_out_dp0_mb7_gpu7
	layer5_softmax_dp1_mb7_gpu7 -> layer5_attn_out_dp1_mb7_gpu7
	layer5_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_attn_out_dp0_mb7_gpu4 -> layer5_attn_allreduce_dp0_mb7
	layer5_attn_out_dp1_mb7_gpu4 -> layer5_attn_allreduce_dp1_mb7
	layer5_attn_out_dp0_mb7_gpu5 -> layer5_attn_allreduce_dp0_mb7
	layer5_attn_out_dp1_mb7_gpu5 -> layer5_attn_allreduce_dp1_mb7
	layer5_attn_out_dp0_mb7_gpu6 -> layer5_attn_allreduce_dp0_mb7
	layer5_attn_out_dp1_mb7_gpu6 -> layer5_attn_allreduce_dp1_mb7
	layer5_attn_out_dp0_mb7_gpu7 -> layer5_attn_allreduce_dp0_mb7
	layer5_attn_out_dp1_mb7_gpu7 -> layer5_attn_allreduce_dp1_mb7
	layer5_router_dp0_mb7_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_router_dp1_mb7_gpu4 [label="Layer 5 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer5_attn_allreduce_dp0_mb7 -> layer5_router_dp0_mb7_gpu4
	layer5_attn_allreduce_dp1_mb7 -> layer5_router_dp1_mb7_gpu4
	layer5_expert0_mlp1_dp0_mb7_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp1_mb7_gpu4 [label="Layer 5 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb7_gpu4 -> layer5_expert0_mlp1_dp0_mb7_gpu4
	layer5_router_dp1_mb7_gpu4 -> layer5_expert0_mlp1_dp1_mb7_gpu4
	layer5_expert0_mlp2_dp0_mb7_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp2_dp1_mb7_gpu4 [label="Layer 5 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert0_mlp1_dp0_mb7_gpu4 -> layer5_expert0_mlp2_dp0_mb7_gpu4
	layer5_expert0_mlp1_dp1_mb7_gpu4 -> layer5_expert0_mlp2_dp1_mb7_gpu4
	layer5_expert1_mlp1_dp0_mb7_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp1_mb7_gpu5 [label="Layer 5 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb7_gpu4 -> layer5_expert1_mlp1_dp0_mb7_gpu5
	layer5_router_dp1_mb7_gpu4 -> layer5_expert1_mlp1_dp1_mb7_gpu5
	layer5_expert1_mlp2_dp0_mb7_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp2_dp1_mb7_gpu5 [label="Layer 5 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert1_mlp1_dp0_mb7_gpu5 -> layer5_expert1_mlp2_dp0_mb7_gpu5
	layer5_expert1_mlp1_dp1_mb7_gpu5 -> layer5_expert1_mlp2_dp1_mb7_gpu5
	layer5_expert2_mlp1_dp0_mb7_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp1_mb7_gpu6 [label="Layer 5 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb7_gpu4 -> layer5_expert2_mlp1_dp0_mb7_gpu6
	layer5_router_dp1_mb7_gpu4 -> layer5_expert2_mlp1_dp1_mb7_gpu6
	layer5_expert2_mlp2_dp0_mb7_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp2_dp1_mb7_gpu6 [label="Layer 5 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert2_mlp1_dp0_mb7_gpu6 -> layer5_expert2_mlp2_dp0_mb7_gpu6
	layer5_expert2_mlp1_dp1_mb7_gpu6 -> layer5_expert2_mlp2_dp1_mb7_gpu6
	layer5_expert3_mlp1_dp0_mb7_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp1_mb7_gpu7 [label="Layer 5 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_router_dp0_mb7_gpu4 -> layer5_expert3_mlp1_dp0_mb7_gpu7
	layer5_router_dp1_mb7_gpu4 -> layer5_expert3_mlp1_dp1_mb7_gpu7
	layer5_expert3_mlp2_dp0_mb7_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp2_dp1_mb7_gpu7 [label="Layer 5 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert3_mlp1_dp0_mb7_gpu7 -> layer5_expert3_mlp2_dp0_mb7_gpu7
	layer5_expert3_mlp1_dp1_mb7_gpu7 -> layer5_expert3_mlp2_dp1_mb7_gpu7
	layer5_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer5_expert0_mlp2_dp0_mb7_gpu4 -> layer5_expert_agg_dp0_mb7
	layer5_expert0_mlp2_dp1_mb7_gpu4 -> layer5_expert_agg_dp1_mb7
	layer5_expert1_mlp2_dp0_mb7_gpu5 -> layer5_expert_agg_dp0_mb7
	layer5_expert1_mlp2_dp1_mb7_gpu5 -> layer5_expert_agg_dp1_mb7
	layer5_expert2_mlp2_dp0_mb7_gpu6 -> layer5_expert_agg_dp0_mb7
	layer5_expert2_mlp2_dp1_mb7_gpu6 -> layer5_expert_agg_dp1_mb7
	layer5_expert3_mlp2_dp0_mb7_gpu7 -> layer5_expert_agg_dp0_mb7
	layer5_expert3_mlp2_dp1_mb7_gpu7 -> layer5_expert_agg_dp1_mb7
	layer6_qkv_dp0_mb7_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb7_gpu4 [label="Layer 6 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb7 -> layer6_qkv_dp0_mb7_gpu4
	layer5_expert_agg_dp1_mb7 -> layer6_qkv_dp1_mb7_gpu4
	layer6_attn_scores_dp0_mb7_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb7_gpu4 [label="Layer 6 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb7_gpu4 -> layer6_attn_scores_dp0_mb7_gpu4
	layer6_qkv_dp1_mb7_gpu4 -> layer6_attn_scores_dp1_mb7_gpu4
	layer6_softmax_dp0_mb7_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb7_gpu4 [label="Layer 6 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb7_gpu4 -> layer6_softmax_dp0_mb7_gpu4
	layer6_attn_scores_dp1_mb7_gpu4 -> layer6_softmax_dp1_mb7_gpu4
	layer6_attn_out_dp0_mb7_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb7_gpu4 [label="Layer 6 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb7_gpu4 -> layer6_attn_out_dp0_mb7_gpu4
	layer6_softmax_dp1_mb7_gpu4 -> layer6_attn_out_dp1_mb7_gpu4
	layer6_qkv_dp0_mb7_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb7_gpu5 [label="Layer 6 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb7 -> layer6_qkv_dp0_mb7_gpu5
	layer5_expert_agg_dp1_mb7 -> layer6_qkv_dp1_mb7_gpu5
	layer6_attn_scores_dp0_mb7_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb7_gpu5 [label="Layer 6 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb7_gpu5 -> layer6_attn_scores_dp0_mb7_gpu5
	layer6_qkv_dp1_mb7_gpu5 -> layer6_attn_scores_dp1_mb7_gpu5
	layer6_softmax_dp0_mb7_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb7_gpu5 [label="Layer 6 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb7_gpu5 -> layer6_softmax_dp0_mb7_gpu5
	layer6_attn_scores_dp1_mb7_gpu5 -> layer6_softmax_dp1_mb7_gpu5
	layer6_attn_out_dp0_mb7_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb7_gpu5 [label="Layer 6 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb7_gpu5 -> layer6_attn_out_dp0_mb7_gpu5
	layer6_softmax_dp1_mb7_gpu5 -> layer6_attn_out_dp1_mb7_gpu5
	layer6_qkv_dp0_mb7_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb7_gpu6 [label="Layer 6 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb7 -> layer6_qkv_dp0_mb7_gpu6
	layer5_expert_agg_dp1_mb7 -> layer6_qkv_dp1_mb7_gpu6
	layer6_attn_scores_dp0_mb7_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb7_gpu6 [label="Layer 6 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb7_gpu6 -> layer6_attn_scores_dp0_mb7_gpu6
	layer6_qkv_dp1_mb7_gpu6 -> layer6_attn_scores_dp1_mb7_gpu6
	layer6_softmax_dp0_mb7_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb7_gpu6 [label="Layer 6 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb7_gpu6 -> layer6_softmax_dp0_mb7_gpu6
	layer6_attn_scores_dp1_mb7_gpu6 -> layer6_softmax_dp1_mb7_gpu6
	layer6_attn_out_dp0_mb7_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb7_gpu6 [label="Layer 6 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb7_gpu6 -> layer6_attn_out_dp0_mb7_gpu6
	layer6_softmax_dp1_mb7_gpu6 -> layer6_attn_out_dp1_mb7_gpu6
	layer6_qkv_dp0_mb7_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp1_mb7_gpu7 [label="Layer 6 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer5_expert_agg_dp0_mb7 -> layer6_qkv_dp0_mb7_gpu7
	layer5_expert_agg_dp1_mb7 -> layer6_qkv_dp1_mb7_gpu7
	layer6_attn_scores_dp0_mb7_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp1_mb7_gpu7 [label="Layer 6 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_qkv_dp0_mb7_gpu7 -> layer6_attn_scores_dp0_mb7_gpu7
	layer6_qkv_dp1_mb7_gpu7 -> layer6_attn_scores_dp1_mb7_gpu7
	layer6_softmax_dp0_mb7_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp1_mb7_gpu7 [label="Layer 6 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_scores_dp0_mb7_gpu7 -> layer6_softmax_dp0_mb7_gpu7
	layer6_attn_scores_dp1_mb7_gpu7 -> layer6_softmax_dp1_mb7_gpu7
	layer6_attn_out_dp0_mb7_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_attn_out_dp1_mb7_gpu7 [label="Layer 6 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_softmax_dp0_mb7_gpu7 -> layer6_attn_out_dp0_mb7_gpu7
	layer6_softmax_dp1_mb7_gpu7 -> layer6_attn_out_dp1_mb7_gpu7
	layer6_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_attn_out_dp0_mb7_gpu4 -> layer6_attn_allreduce_dp0_mb7
	layer6_attn_out_dp1_mb7_gpu4 -> layer6_attn_allreduce_dp1_mb7
	layer6_attn_out_dp0_mb7_gpu5 -> layer6_attn_allreduce_dp0_mb7
	layer6_attn_out_dp1_mb7_gpu5 -> layer6_attn_allreduce_dp1_mb7
	layer6_attn_out_dp0_mb7_gpu6 -> layer6_attn_allreduce_dp0_mb7
	layer6_attn_out_dp1_mb7_gpu6 -> layer6_attn_allreduce_dp1_mb7
	layer6_attn_out_dp0_mb7_gpu7 -> layer6_attn_allreduce_dp0_mb7
	layer6_attn_out_dp1_mb7_gpu7 -> layer6_attn_allreduce_dp1_mb7
	layer6_router_dp0_mb7_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_router_dp1_mb7_gpu4 [label="Layer 6 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer6_attn_allreduce_dp0_mb7 -> layer6_router_dp0_mb7_gpu4
	layer6_attn_allreduce_dp1_mb7 -> layer6_router_dp1_mb7_gpu4
	layer6_expert0_mlp1_dp0_mb7_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp1_mb7_gpu4 [label="Layer 6 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb7_gpu4 -> layer6_expert0_mlp1_dp0_mb7_gpu4
	layer6_router_dp1_mb7_gpu4 -> layer6_expert0_mlp1_dp1_mb7_gpu4
	layer6_expert0_mlp2_dp0_mb7_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp2_dp1_mb7_gpu4 [label="Layer 6 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert0_mlp1_dp0_mb7_gpu4 -> layer6_expert0_mlp2_dp0_mb7_gpu4
	layer6_expert0_mlp1_dp1_mb7_gpu4 -> layer6_expert0_mlp2_dp1_mb7_gpu4
	layer6_expert1_mlp1_dp0_mb7_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp1_mb7_gpu5 [label="Layer 6 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb7_gpu4 -> layer6_expert1_mlp1_dp0_mb7_gpu5
	layer6_router_dp1_mb7_gpu4 -> layer6_expert1_mlp1_dp1_mb7_gpu5
	layer6_expert1_mlp2_dp0_mb7_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp2_dp1_mb7_gpu5 [label="Layer 6 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert1_mlp1_dp0_mb7_gpu5 -> layer6_expert1_mlp2_dp0_mb7_gpu5
	layer6_expert1_mlp1_dp1_mb7_gpu5 -> layer6_expert1_mlp2_dp1_mb7_gpu5
	layer6_expert2_mlp1_dp0_mb7_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp1_mb7_gpu6 [label="Layer 6 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb7_gpu4 -> layer6_expert2_mlp1_dp0_mb7_gpu6
	layer6_router_dp1_mb7_gpu4 -> layer6_expert2_mlp1_dp1_mb7_gpu6
	layer6_expert2_mlp2_dp0_mb7_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp2_dp1_mb7_gpu6 [label="Layer 6 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert2_mlp1_dp0_mb7_gpu6 -> layer6_expert2_mlp2_dp0_mb7_gpu6
	layer6_expert2_mlp1_dp1_mb7_gpu6 -> layer6_expert2_mlp2_dp1_mb7_gpu6
	layer6_expert3_mlp1_dp0_mb7_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp1_mb7_gpu7 [label="Layer 6 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_router_dp0_mb7_gpu4 -> layer6_expert3_mlp1_dp0_mb7_gpu7
	layer6_router_dp1_mb7_gpu4 -> layer6_expert3_mlp1_dp1_mb7_gpu7
	layer6_expert3_mlp2_dp0_mb7_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp2_dp1_mb7_gpu7 [label="Layer 6 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert3_mlp1_dp0_mb7_gpu7 -> layer6_expert3_mlp2_dp0_mb7_gpu7
	layer6_expert3_mlp1_dp1_mb7_gpu7 -> layer6_expert3_mlp2_dp1_mb7_gpu7
	layer6_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer6_expert0_mlp2_dp0_mb7_gpu4 -> layer6_expert_agg_dp0_mb7
	layer6_expert0_mlp2_dp1_mb7_gpu4 -> layer6_expert_agg_dp1_mb7
	layer6_expert1_mlp2_dp0_mb7_gpu5 -> layer6_expert_agg_dp0_mb7
	layer6_expert1_mlp2_dp1_mb7_gpu5 -> layer6_expert_agg_dp1_mb7
	layer6_expert2_mlp2_dp0_mb7_gpu6 -> layer6_expert_agg_dp0_mb7
	layer6_expert2_mlp2_dp1_mb7_gpu6 -> layer6_expert_agg_dp1_mb7
	layer6_expert3_mlp2_dp0_mb7_gpu7 -> layer6_expert_agg_dp0_mb7
	layer6_expert3_mlp2_dp1_mb7_gpu7 -> layer6_expert_agg_dp1_mb7
	layer7_qkv_dp0_mb7_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb7_gpu4 [label="Layer 7 QKV Projection\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb7 -> layer7_qkv_dp0_mb7_gpu4
	layer6_expert_agg_dp1_mb7 -> layer7_qkv_dp1_mb7_gpu4
	layer7_attn_scores_dp0_mb7_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb7_gpu4 [label="Layer 7 Attention Scores\nGPU 4\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb7_gpu4 -> layer7_attn_scores_dp0_mb7_gpu4
	layer7_qkv_dp1_mb7_gpu4 -> layer7_attn_scores_dp1_mb7_gpu4
	layer7_softmax_dp0_mb7_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb7_gpu4 [label="Layer 7 Attention Softmax\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb7_gpu4 -> layer7_softmax_dp0_mb7_gpu4
	layer7_attn_scores_dp1_mb7_gpu4 -> layer7_softmax_dp1_mb7_gpu4
	layer7_attn_out_dp0_mb7_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb7_gpu4 [label="Layer 7 Attention Output\nGPU 4\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb7_gpu4 -> layer7_attn_out_dp0_mb7_gpu4
	layer7_softmax_dp1_mb7_gpu4 -> layer7_attn_out_dp1_mb7_gpu4
	layer7_qkv_dp0_mb7_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb7_gpu5 [label="Layer 7 QKV Projection\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb7 -> layer7_qkv_dp0_mb7_gpu5
	layer6_expert_agg_dp1_mb7 -> layer7_qkv_dp1_mb7_gpu5
	layer7_attn_scores_dp0_mb7_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb7_gpu5 [label="Layer 7 Attention Scores\nGPU 5\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb7_gpu5 -> layer7_attn_scores_dp0_mb7_gpu5
	layer7_qkv_dp1_mb7_gpu5 -> layer7_attn_scores_dp1_mb7_gpu5
	layer7_softmax_dp0_mb7_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb7_gpu5 [label="Layer 7 Attention Softmax\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb7_gpu5 -> layer7_softmax_dp0_mb7_gpu5
	layer7_attn_scores_dp1_mb7_gpu5 -> layer7_softmax_dp1_mb7_gpu5
	layer7_attn_out_dp0_mb7_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb7_gpu5 [label="Layer 7 Attention Output\nGPU 5\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb7_gpu5 -> layer7_attn_out_dp0_mb7_gpu5
	layer7_softmax_dp1_mb7_gpu5 -> layer7_attn_out_dp1_mb7_gpu5
	layer7_qkv_dp0_mb7_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb7_gpu6 [label="Layer 7 QKV Projection\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb7 -> layer7_qkv_dp0_mb7_gpu6
	layer6_expert_agg_dp1_mb7 -> layer7_qkv_dp1_mb7_gpu6
	layer7_attn_scores_dp0_mb7_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb7_gpu6 [label="Layer 7 Attention Scores\nGPU 6\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb7_gpu6 -> layer7_attn_scores_dp0_mb7_gpu6
	layer7_qkv_dp1_mb7_gpu6 -> layer7_attn_scores_dp1_mb7_gpu6
	layer7_softmax_dp0_mb7_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb7_gpu6 [label="Layer 7 Attention Softmax\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb7_gpu6 -> layer7_softmax_dp0_mb7_gpu6
	layer7_attn_scores_dp1_mb7_gpu6 -> layer7_softmax_dp1_mb7_gpu6
	layer7_attn_out_dp0_mb7_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb7_gpu6 [label="Layer 7 Attention Output\nGPU 6\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb7_gpu6 -> layer7_attn_out_dp0_mb7_gpu6
	layer7_softmax_dp1_mb7_gpu6 -> layer7_attn_out_dp1_mb7_gpu6
	layer7_qkv_dp0_mb7_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp1_mb7_gpu7 [label="Layer 7 QKV Projection\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer6_expert_agg_dp0_mb7 -> layer7_qkv_dp0_mb7_gpu7
	layer6_expert_agg_dp1_mb7 -> layer7_qkv_dp1_mb7_gpu7
	layer7_attn_scores_dp0_mb7_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp1_mb7_gpu7 [label="Layer 7 Attention Scores\nGPU 7\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_qkv_dp0_mb7_gpu7 -> layer7_attn_scores_dp0_mb7_gpu7
	layer7_qkv_dp1_mb7_gpu7 -> layer7_attn_scores_dp1_mb7_gpu7
	layer7_softmax_dp0_mb7_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp1_mb7_gpu7 [label="Layer 7 Attention Softmax\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_scores_dp0_mb7_gpu7 -> layer7_softmax_dp0_mb7_gpu7
	layer7_attn_scores_dp1_mb7_gpu7 -> layer7_softmax_dp1_mb7_gpu7
	layer7_attn_out_dp0_mb7_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_attn_out_dp1_mb7_gpu7 [label="Layer 7 Attention Output\nGPU 7\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_softmax_dp0_mb7_gpu7 -> layer7_attn_out_dp0_mb7_gpu7
	layer7_softmax_dp1_mb7_gpu7 -> layer7_attn_out_dp1_mb7_gpu7
	layer7_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_attn_out_dp0_mb7_gpu4 -> layer7_attn_allreduce_dp0_mb7
	layer7_attn_out_dp1_mb7_gpu4 -> layer7_attn_allreduce_dp1_mb7
	layer7_attn_out_dp0_mb7_gpu5 -> layer7_attn_allreduce_dp0_mb7
	layer7_attn_out_dp1_mb7_gpu5 -> layer7_attn_allreduce_dp1_mb7
	layer7_attn_out_dp0_mb7_gpu6 -> layer7_attn_allreduce_dp0_mb7
	layer7_attn_out_dp1_mb7_gpu6 -> layer7_attn_allreduce_dp1_mb7
	layer7_attn_out_dp0_mb7_gpu7 -> layer7_attn_allreduce_dp0_mb7
	layer7_attn_out_dp1_mb7_gpu7 -> layer7_attn_allreduce_dp1_mb7
	layer7_router_dp0_mb7_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_router_dp1_mb7_gpu4 [label="Layer 7 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer7_attn_allreduce_dp0_mb7 -> layer7_router_dp0_mb7_gpu4
	layer7_attn_allreduce_dp1_mb7 -> layer7_router_dp1_mb7_gpu4
	layer7_expert0_mlp1_dp0_mb7_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp1_mb7_gpu4 [label="Layer 7 Expert 0 MLP1\nGPU 4\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb7_gpu4 -> layer7_expert0_mlp1_dp0_mb7_gpu4
	layer7_router_dp1_mb7_gpu4 -> layer7_expert0_mlp1_dp1_mb7_gpu4
	layer7_expert0_mlp2_dp0_mb7_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp2_dp1_mb7_gpu4 [label="Layer 7 Expert 0 MLP2\nGPU 4\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert0_mlp1_dp0_mb7_gpu4 -> layer7_expert0_mlp2_dp0_mb7_gpu4
	layer7_expert0_mlp1_dp1_mb7_gpu4 -> layer7_expert0_mlp2_dp1_mb7_gpu4
	layer7_expert1_mlp1_dp0_mb7_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp1_mb7_gpu5 [label="Layer 7 Expert 1 MLP1\nGPU 5\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb7_gpu4 -> layer7_expert1_mlp1_dp0_mb7_gpu5
	layer7_router_dp1_mb7_gpu4 -> layer7_expert1_mlp1_dp1_mb7_gpu5
	layer7_expert1_mlp2_dp0_mb7_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp2_dp1_mb7_gpu5 [label="Layer 7 Expert 1 MLP2\nGPU 5\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert1_mlp1_dp0_mb7_gpu5 -> layer7_expert1_mlp2_dp0_mb7_gpu5
	layer7_expert1_mlp1_dp1_mb7_gpu5 -> layer7_expert1_mlp2_dp1_mb7_gpu5
	layer7_expert2_mlp1_dp0_mb7_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp1_mb7_gpu6 [label="Layer 7 Expert 2 MLP1\nGPU 6\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb7_gpu4 -> layer7_expert2_mlp1_dp0_mb7_gpu6
	layer7_router_dp1_mb7_gpu4 -> layer7_expert2_mlp1_dp1_mb7_gpu6
	layer7_expert2_mlp2_dp0_mb7_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp2_dp1_mb7_gpu6 [label="Layer 7 Expert 2 MLP2\nGPU 6\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert2_mlp1_dp0_mb7_gpu6 -> layer7_expert2_mlp2_dp0_mb7_gpu6
	layer7_expert2_mlp1_dp1_mb7_gpu6 -> layer7_expert2_mlp2_dp1_mb7_gpu6
	layer7_expert3_mlp1_dp0_mb7_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp1_mb7_gpu7 [label="Layer 7 Expert 3 MLP1\nGPU 7\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_router_dp0_mb7_gpu4 -> layer7_expert3_mlp1_dp0_mb7_gpu7
	layer7_router_dp1_mb7_gpu4 -> layer7_expert3_mlp1_dp1_mb7_gpu7
	layer7_expert3_mlp2_dp0_mb7_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp2_dp1_mb7_gpu7 [label="Layer 7 Expert 3 MLP2\nGPU 7\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5F3FF" fontsize=9 style=filled]
	layer7_expert3_mlp1_dp0_mb7_gpu7 -> layer7_expert3_mlp2_dp0_mb7_gpu7
	layer7_expert3_mlp1_dp1_mb7_gpu7 -> layer7_expert3_mlp2_dp1_mb7_gpu7
	layer7_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer7_expert0_mlp2_dp0_mb7_gpu4 -> layer7_expert_agg_dp0_mb7
	layer7_expert0_mlp2_dp1_mb7_gpu4 -> layer7_expert_agg_dp1_mb7
	layer7_expert1_mlp2_dp0_mb7_gpu5 -> layer7_expert_agg_dp0_mb7
	layer7_expert1_mlp2_dp1_mb7_gpu5 -> layer7_expert_agg_dp1_mb7
	layer7_expert2_mlp2_dp0_mb7_gpu6 -> layer7_expert_agg_dp0_mb7
	layer7_expert2_mlp2_dp1_mb7_gpu6 -> layer7_expert_agg_dp1_mb7
	layer7_expert3_mlp2_dp0_mb7_gpu7 -> layer7_expert_agg_dp0_mb7
	layer7_expert3_mlp2_dp1_mb7_gpu7 -> layer7_expert_agg_dp1_mb7
	layer8_qkv_dp0_mb0_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb0_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb0 -> layer8_qkv_dp0_mb0_gpu8
	layer7_expert_agg_dp1_mb0 -> layer8_qkv_dp1_mb0_gpu8
	layer8_attn_scores_dp0_mb0_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb0_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb0_gpu8 -> layer8_attn_scores_dp0_mb0_gpu8
	layer8_qkv_dp1_mb0_gpu8 -> layer8_attn_scores_dp1_mb0_gpu8
	layer8_softmax_dp0_mb0_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb0_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb0_gpu8 -> layer8_softmax_dp0_mb0_gpu8
	layer8_attn_scores_dp1_mb0_gpu8 -> layer8_softmax_dp1_mb0_gpu8
	layer8_attn_out_dp0_mb0_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb0_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb0_gpu8 -> layer8_attn_out_dp0_mb0_gpu8
	layer8_softmax_dp1_mb0_gpu8 -> layer8_attn_out_dp1_mb0_gpu8
	layer8_qkv_dp0_mb0_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb0_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb0 -> layer8_qkv_dp0_mb0_gpu9
	layer7_expert_agg_dp1_mb0 -> layer8_qkv_dp1_mb0_gpu9
	layer8_attn_scores_dp0_mb0_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb0_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb0_gpu9 -> layer8_attn_scores_dp0_mb0_gpu9
	layer8_qkv_dp1_mb0_gpu9 -> layer8_attn_scores_dp1_mb0_gpu9
	layer8_softmax_dp0_mb0_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb0_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb0_gpu9 -> layer8_softmax_dp0_mb0_gpu9
	layer8_attn_scores_dp1_mb0_gpu9 -> layer8_softmax_dp1_mb0_gpu9
	layer8_attn_out_dp0_mb0_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb0_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb0_gpu9 -> layer8_attn_out_dp0_mb0_gpu9
	layer8_softmax_dp1_mb0_gpu9 -> layer8_attn_out_dp1_mb0_gpu9
	layer8_qkv_dp0_mb0_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb0_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb0 -> layer8_qkv_dp0_mb0_gpu10
	layer7_expert_agg_dp1_mb0 -> layer8_qkv_dp1_mb0_gpu10
	layer8_attn_scores_dp0_mb0_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb0_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb0_gpu10 -> layer8_attn_scores_dp0_mb0_gpu10
	layer8_qkv_dp1_mb0_gpu10 -> layer8_attn_scores_dp1_mb0_gpu10
	layer8_softmax_dp0_mb0_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb0_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb0_gpu10 -> layer8_softmax_dp0_mb0_gpu10
	layer8_attn_scores_dp1_mb0_gpu10 -> layer8_softmax_dp1_mb0_gpu10
	layer8_attn_out_dp0_mb0_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb0_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb0_gpu10 -> layer8_attn_out_dp0_mb0_gpu10
	layer8_softmax_dp1_mb0_gpu10 -> layer8_attn_out_dp1_mb0_gpu10
	layer8_qkv_dp0_mb0_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb0_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb0 -> layer8_qkv_dp0_mb0_gpu11
	layer7_expert_agg_dp1_mb0 -> layer8_qkv_dp1_mb0_gpu11
	layer8_attn_scores_dp0_mb0_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb0_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb0_gpu11 -> layer8_attn_scores_dp0_mb0_gpu11
	layer8_qkv_dp1_mb0_gpu11 -> layer8_attn_scores_dp1_mb0_gpu11
	layer8_softmax_dp0_mb0_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb0_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb0_gpu11 -> layer8_softmax_dp0_mb0_gpu11
	layer8_attn_scores_dp1_mb0_gpu11 -> layer8_softmax_dp1_mb0_gpu11
	layer8_attn_out_dp0_mb0_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb0_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb0_gpu11 -> layer8_attn_out_dp0_mb0_gpu11
	layer8_softmax_dp1_mb0_gpu11 -> layer8_attn_out_dp1_mb0_gpu11
	layer8_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb0_gpu8 -> layer8_attn_allreduce_dp0_mb0
	layer8_attn_out_dp1_mb0_gpu8 -> layer8_attn_allreduce_dp1_mb0
	layer8_attn_out_dp0_mb0_gpu9 -> layer8_attn_allreduce_dp0_mb0
	layer8_attn_out_dp1_mb0_gpu9 -> layer8_attn_allreduce_dp1_mb0
	layer8_attn_out_dp0_mb0_gpu10 -> layer8_attn_allreduce_dp0_mb0
	layer8_attn_out_dp1_mb0_gpu10 -> layer8_attn_allreduce_dp1_mb0
	layer8_attn_out_dp0_mb0_gpu11 -> layer8_attn_allreduce_dp0_mb0
	layer8_attn_out_dp1_mb0_gpu11 -> layer8_attn_allreduce_dp1_mb0
	layer8_router_dp0_mb0_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb0_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb0 -> layer8_router_dp0_mb0_gpu8
	layer8_attn_allreduce_dp1_mb0 -> layer8_router_dp1_mb0_gpu8
	layer8_expert0_mlp1_dp0_mb0_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb0_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb0_gpu8 -> layer8_expert0_mlp1_dp0_mb0_gpu8
	layer8_router_dp1_mb0_gpu8 -> layer8_expert0_mlp1_dp1_mb0_gpu8
	layer8_expert0_mlp2_dp0_mb0_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb0_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb0_gpu8 -> layer8_expert0_mlp2_dp0_mb0_gpu8
	layer8_expert0_mlp1_dp1_mb0_gpu8 -> layer8_expert0_mlp2_dp1_mb0_gpu8
	layer8_expert1_mlp1_dp0_mb0_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb0_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb0_gpu8 -> layer8_expert1_mlp1_dp0_mb0_gpu9
	layer8_router_dp1_mb0_gpu8 -> layer8_expert1_mlp1_dp1_mb0_gpu9
	layer8_expert1_mlp2_dp0_mb0_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb0_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb0_gpu9 -> layer8_expert1_mlp2_dp0_mb0_gpu9
	layer8_expert1_mlp1_dp1_mb0_gpu9 -> layer8_expert1_mlp2_dp1_mb0_gpu9
	layer8_expert2_mlp1_dp0_mb0_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb0_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb0_gpu8 -> layer8_expert2_mlp1_dp0_mb0_gpu10
	layer8_router_dp1_mb0_gpu8 -> layer8_expert2_mlp1_dp1_mb0_gpu10
	layer8_expert2_mlp2_dp0_mb0_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb0_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb0_gpu10 -> layer8_expert2_mlp2_dp0_mb0_gpu10
	layer8_expert2_mlp1_dp1_mb0_gpu10 -> layer8_expert2_mlp2_dp1_mb0_gpu10
	layer8_expert3_mlp1_dp0_mb0_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb0_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb0_gpu8 -> layer8_expert3_mlp1_dp0_mb0_gpu11
	layer8_router_dp1_mb0_gpu8 -> layer8_expert3_mlp1_dp1_mb0_gpu11
	layer8_expert3_mlp2_dp0_mb0_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb0_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb0_gpu11 -> layer8_expert3_mlp2_dp0_mb0_gpu11
	layer8_expert3_mlp1_dp1_mb0_gpu11 -> layer8_expert3_mlp2_dp1_mb0_gpu11
	layer8_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb0_gpu8 -> layer8_expert_agg_dp0_mb0
	layer8_expert0_mlp2_dp1_mb0_gpu8 -> layer8_expert_agg_dp1_mb0
	layer8_expert1_mlp2_dp0_mb0_gpu9 -> layer8_expert_agg_dp0_mb0
	layer8_expert1_mlp2_dp1_mb0_gpu9 -> layer8_expert_agg_dp1_mb0
	layer8_expert2_mlp2_dp0_mb0_gpu10 -> layer8_expert_agg_dp0_mb0
	layer8_expert2_mlp2_dp1_mb0_gpu10 -> layer8_expert_agg_dp1_mb0
	layer8_expert3_mlp2_dp0_mb0_gpu11 -> layer8_expert_agg_dp0_mb0
	layer8_expert3_mlp2_dp1_mb0_gpu11 -> layer8_expert_agg_dp1_mb0
	layer9_qkv_dp0_mb0_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb0_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb0 -> layer9_qkv_dp0_mb0_gpu8
	layer8_expert_agg_dp1_mb0 -> layer9_qkv_dp1_mb0_gpu8
	layer9_attn_scores_dp0_mb0_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb0_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb0_gpu8 -> layer9_attn_scores_dp0_mb0_gpu8
	layer9_qkv_dp1_mb0_gpu8 -> layer9_attn_scores_dp1_mb0_gpu8
	layer9_softmax_dp0_mb0_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb0_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb0_gpu8 -> layer9_softmax_dp0_mb0_gpu8
	layer9_attn_scores_dp1_mb0_gpu8 -> layer9_softmax_dp1_mb0_gpu8
	layer9_attn_out_dp0_mb0_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb0_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb0_gpu8 -> layer9_attn_out_dp0_mb0_gpu8
	layer9_softmax_dp1_mb0_gpu8 -> layer9_attn_out_dp1_mb0_gpu8
	layer9_qkv_dp0_mb0_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb0_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb0 -> layer9_qkv_dp0_mb0_gpu9
	layer8_expert_agg_dp1_mb0 -> layer9_qkv_dp1_mb0_gpu9
	layer9_attn_scores_dp0_mb0_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb0_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb0_gpu9 -> layer9_attn_scores_dp0_mb0_gpu9
	layer9_qkv_dp1_mb0_gpu9 -> layer9_attn_scores_dp1_mb0_gpu9
	layer9_softmax_dp0_mb0_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb0_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb0_gpu9 -> layer9_softmax_dp0_mb0_gpu9
	layer9_attn_scores_dp1_mb0_gpu9 -> layer9_softmax_dp1_mb0_gpu9
	layer9_attn_out_dp0_mb0_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb0_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb0_gpu9 -> layer9_attn_out_dp0_mb0_gpu9
	layer9_softmax_dp1_mb0_gpu9 -> layer9_attn_out_dp1_mb0_gpu9
	layer9_qkv_dp0_mb0_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb0_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb0 -> layer9_qkv_dp0_mb0_gpu10
	layer8_expert_agg_dp1_mb0 -> layer9_qkv_dp1_mb0_gpu10
	layer9_attn_scores_dp0_mb0_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb0_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb0_gpu10 -> layer9_attn_scores_dp0_mb0_gpu10
	layer9_qkv_dp1_mb0_gpu10 -> layer9_attn_scores_dp1_mb0_gpu10
	layer9_softmax_dp0_mb0_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb0_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb0_gpu10 -> layer9_softmax_dp0_mb0_gpu10
	layer9_attn_scores_dp1_mb0_gpu10 -> layer9_softmax_dp1_mb0_gpu10
	layer9_attn_out_dp0_mb0_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb0_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb0_gpu10 -> layer9_attn_out_dp0_mb0_gpu10
	layer9_softmax_dp1_mb0_gpu10 -> layer9_attn_out_dp1_mb0_gpu10
	layer9_qkv_dp0_mb0_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb0_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb0 -> layer9_qkv_dp0_mb0_gpu11
	layer8_expert_agg_dp1_mb0 -> layer9_qkv_dp1_mb0_gpu11
	layer9_attn_scores_dp0_mb0_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb0_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb0_gpu11 -> layer9_attn_scores_dp0_mb0_gpu11
	layer9_qkv_dp1_mb0_gpu11 -> layer9_attn_scores_dp1_mb0_gpu11
	layer9_softmax_dp0_mb0_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb0_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb0_gpu11 -> layer9_softmax_dp0_mb0_gpu11
	layer9_attn_scores_dp1_mb0_gpu11 -> layer9_softmax_dp1_mb0_gpu11
	layer9_attn_out_dp0_mb0_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb0_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb0_gpu11 -> layer9_attn_out_dp0_mb0_gpu11
	layer9_softmax_dp1_mb0_gpu11 -> layer9_attn_out_dp1_mb0_gpu11
	layer9_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb0_gpu8 -> layer9_attn_allreduce_dp0_mb0
	layer9_attn_out_dp1_mb0_gpu8 -> layer9_attn_allreduce_dp1_mb0
	layer9_attn_out_dp0_mb0_gpu9 -> layer9_attn_allreduce_dp0_mb0
	layer9_attn_out_dp1_mb0_gpu9 -> layer9_attn_allreduce_dp1_mb0
	layer9_attn_out_dp0_mb0_gpu10 -> layer9_attn_allreduce_dp0_mb0
	layer9_attn_out_dp1_mb0_gpu10 -> layer9_attn_allreduce_dp1_mb0
	layer9_attn_out_dp0_mb0_gpu11 -> layer9_attn_allreduce_dp0_mb0
	layer9_attn_out_dp1_mb0_gpu11 -> layer9_attn_allreduce_dp1_mb0
	layer9_router_dp0_mb0_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb0_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb0 -> layer9_router_dp0_mb0_gpu8
	layer9_attn_allreduce_dp1_mb0 -> layer9_router_dp1_mb0_gpu8
	layer9_expert0_mlp1_dp0_mb0_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb0_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb0_gpu8 -> layer9_expert0_mlp1_dp0_mb0_gpu8
	layer9_router_dp1_mb0_gpu8 -> layer9_expert0_mlp1_dp1_mb0_gpu8
	layer9_expert0_mlp2_dp0_mb0_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb0_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb0_gpu8 -> layer9_expert0_mlp2_dp0_mb0_gpu8
	layer9_expert0_mlp1_dp1_mb0_gpu8 -> layer9_expert0_mlp2_dp1_mb0_gpu8
	layer9_expert1_mlp1_dp0_mb0_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb0_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb0_gpu8 -> layer9_expert1_mlp1_dp0_mb0_gpu9
	layer9_router_dp1_mb0_gpu8 -> layer9_expert1_mlp1_dp1_mb0_gpu9
	layer9_expert1_mlp2_dp0_mb0_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb0_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb0_gpu9 -> layer9_expert1_mlp2_dp0_mb0_gpu9
	layer9_expert1_mlp1_dp1_mb0_gpu9 -> layer9_expert1_mlp2_dp1_mb0_gpu9
	layer9_expert2_mlp1_dp0_mb0_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb0_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb0_gpu8 -> layer9_expert2_mlp1_dp0_mb0_gpu10
	layer9_router_dp1_mb0_gpu8 -> layer9_expert2_mlp1_dp1_mb0_gpu10
	layer9_expert2_mlp2_dp0_mb0_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb0_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb0_gpu10 -> layer9_expert2_mlp2_dp0_mb0_gpu10
	layer9_expert2_mlp1_dp1_mb0_gpu10 -> layer9_expert2_mlp2_dp1_mb0_gpu10
	layer9_expert3_mlp1_dp0_mb0_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb0_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb0_gpu8 -> layer9_expert3_mlp1_dp0_mb0_gpu11
	layer9_router_dp1_mb0_gpu8 -> layer9_expert3_mlp1_dp1_mb0_gpu11
	layer9_expert3_mlp2_dp0_mb0_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb0_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb0_gpu11 -> layer9_expert3_mlp2_dp0_mb0_gpu11
	layer9_expert3_mlp1_dp1_mb0_gpu11 -> layer9_expert3_mlp2_dp1_mb0_gpu11
	layer9_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb0_gpu8 -> layer9_expert_agg_dp0_mb0
	layer9_expert0_mlp2_dp1_mb0_gpu8 -> layer9_expert_agg_dp1_mb0
	layer9_expert1_mlp2_dp0_mb0_gpu9 -> layer9_expert_agg_dp0_mb0
	layer9_expert1_mlp2_dp1_mb0_gpu9 -> layer9_expert_agg_dp1_mb0
	layer9_expert2_mlp2_dp0_mb0_gpu10 -> layer9_expert_agg_dp0_mb0
	layer9_expert2_mlp2_dp1_mb0_gpu10 -> layer9_expert_agg_dp1_mb0
	layer9_expert3_mlp2_dp0_mb0_gpu11 -> layer9_expert_agg_dp0_mb0
	layer9_expert3_mlp2_dp1_mb0_gpu11 -> layer9_expert_agg_dp1_mb0
	layer10_qkv_dp0_mb0_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb0_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb0 -> layer10_qkv_dp0_mb0_gpu8
	layer9_expert_agg_dp1_mb0 -> layer10_qkv_dp1_mb0_gpu8
	layer10_attn_scores_dp0_mb0_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb0_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb0_gpu8 -> layer10_attn_scores_dp0_mb0_gpu8
	layer10_qkv_dp1_mb0_gpu8 -> layer10_attn_scores_dp1_mb0_gpu8
	layer10_softmax_dp0_mb0_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb0_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb0_gpu8 -> layer10_softmax_dp0_mb0_gpu8
	layer10_attn_scores_dp1_mb0_gpu8 -> layer10_softmax_dp1_mb0_gpu8
	layer10_attn_out_dp0_mb0_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb0_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb0_gpu8 -> layer10_attn_out_dp0_mb0_gpu8
	layer10_softmax_dp1_mb0_gpu8 -> layer10_attn_out_dp1_mb0_gpu8
	layer10_qkv_dp0_mb0_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb0_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb0 -> layer10_qkv_dp0_mb0_gpu9
	layer9_expert_agg_dp1_mb0 -> layer10_qkv_dp1_mb0_gpu9
	layer10_attn_scores_dp0_mb0_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb0_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb0_gpu9 -> layer10_attn_scores_dp0_mb0_gpu9
	layer10_qkv_dp1_mb0_gpu9 -> layer10_attn_scores_dp1_mb0_gpu9
	layer10_softmax_dp0_mb0_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb0_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb0_gpu9 -> layer10_softmax_dp0_mb0_gpu9
	layer10_attn_scores_dp1_mb0_gpu9 -> layer10_softmax_dp1_mb0_gpu9
	layer10_attn_out_dp0_mb0_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb0_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb0_gpu9 -> layer10_attn_out_dp0_mb0_gpu9
	layer10_softmax_dp1_mb0_gpu9 -> layer10_attn_out_dp1_mb0_gpu9
	layer10_qkv_dp0_mb0_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb0_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb0 -> layer10_qkv_dp0_mb0_gpu10
	layer9_expert_agg_dp1_mb0 -> layer10_qkv_dp1_mb0_gpu10
	layer10_attn_scores_dp0_mb0_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb0_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb0_gpu10 -> layer10_attn_scores_dp0_mb0_gpu10
	layer10_qkv_dp1_mb0_gpu10 -> layer10_attn_scores_dp1_mb0_gpu10
	layer10_softmax_dp0_mb0_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb0_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb0_gpu10 -> layer10_softmax_dp0_mb0_gpu10
	layer10_attn_scores_dp1_mb0_gpu10 -> layer10_softmax_dp1_mb0_gpu10
	layer10_attn_out_dp0_mb0_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb0_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb0_gpu10 -> layer10_attn_out_dp0_mb0_gpu10
	layer10_softmax_dp1_mb0_gpu10 -> layer10_attn_out_dp1_mb0_gpu10
	layer10_qkv_dp0_mb0_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb0_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb0 -> layer10_qkv_dp0_mb0_gpu11
	layer9_expert_agg_dp1_mb0 -> layer10_qkv_dp1_mb0_gpu11
	layer10_attn_scores_dp0_mb0_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb0_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb0_gpu11 -> layer10_attn_scores_dp0_mb0_gpu11
	layer10_qkv_dp1_mb0_gpu11 -> layer10_attn_scores_dp1_mb0_gpu11
	layer10_softmax_dp0_mb0_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb0_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb0_gpu11 -> layer10_softmax_dp0_mb0_gpu11
	layer10_attn_scores_dp1_mb0_gpu11 -> layer10_softmax_dp1_mb0_gpu11
	layer10_attn_out_dp0_mb0_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb0_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb0_gpu11 -> layer10_attn_out_dp0_mb0_gpu11
	layer10_softmax_dp1_mb0_gpu11 -> layer10_attn_out_dp1_mb0_gpu11
	layer10_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb0_gpu8 -> layer10_attn_allreduce_dp0_mb0
	layer10_attn_out_dp1_mb0_gpu8 -> layer10_attn_allreduce_dp1_mb0
	layer10_attn_out_dp0_mb0_gpu9 -> layer10_attn_allreduce_dp0_mb0
	layer10_attn_out_dp1_mb0_gpu9 -> layer10_attn_allreduce_dp1_mb0
	layer10_attn_out_dp0_mb0_gpu10 -> layer10_attn_allreduce_dp0_mb0
	layer10_attn_out_dp1_mb0_gpu10 -> layer10_attn_allreduce_dp1_mb0
	layer10_attn_out_dp0_mb0_gpu11 -> layer10_attn_allreduce_dp0_mb0
	layer10_attn_out_dp1_mb0_gpu11 -> layer10_attn_allreduce_dp1_mb0
	layer10_router_dp0_mb0_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb0_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb0 -> layer10_router_dp0_mb0_gpu8
	layer10_attn_allreduce_dp1_mb0 -> layer10_router_dp1_mb0_gpu8
	layer10_expert0_mlp1_dp0_mb0_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb0_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb0_gpu8 -> layer10_expert0_mlp1_dp0_mb0_gpu8
	layer10_router_dp1_mb0_gpu8 -> layer10_expert0_mlp1_dp1_mb0_gpu8
	layer10_expert0_mlp2_dp0_mb0_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb0_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb0_gpu8 -> layer10_expert0_mlp2_dp0_mb0_gpu8
	layer10_expert0_mlp1_dp1_mb0_gpu8 -> layer10_expert0_mlp2_dp1_mb0_gpu8
	layer10_expert1_mlp1_dp0_mb0_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb0_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb0_gpu8 -> layer10_expert1_mlp1_dp0_mb0_gpu9
	layer10_router_dp1_mb0_gpu8 -> layer10_expert1_mlp1_dp1_mb0_gpu9
	layer10_expert1_mlp2_dp0_mb0_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb0_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb0_gpu9 -> layer10_expert1_mlp2_dp0_mb0_gpu9
	layer10_expert1_mlp1_dp1_mb0_gpu9 -> layer10_expert1_mlp2_dp1_mb0_gpu9
	layer10_expert2_mlp1_dp0_mb0_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb0_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb0_gpu8 -> layer10_expert2_mlp1_dp0_mb0_gpu10
	layer10_router_dp1_mb0_gpu8 -> layer10_expert2_mlp1_dp1_mb0_gpu10
	layer10_expert2_mlp2_dp0_mb0_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb0_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb0_gpu10 -> layer10_expert2_mlp2_dp0_mb0_gpu10
	layer10_expert2_mlp1_dp1_mb0_gpu10 -> layer10_expert2_mlp2_dp1_mb0_gpu10
	layer10_expert3_mlp1_dp0_mb0_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb0_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb0_gpu8 -> layer10_expert3_mlp1_dp0_mb0_gpu11
	layer10_router_dp1_mb0_gpu8 -> layer10_expert3_mlp1_dp1_mb0_gpu11
	layer10_expert3_mlp2_dp0_mb0_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb0_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb0_gpu11 -> layer10_expert3_mlp2_dp0_mb0_gpu11
	layer10_expert3_mlp1_dp1_mb0_gpu11 -> layer10_expert3_mlp2_dp1_mb0_gpu11
	layer10_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb0_gpu8 -> layer10_expert_agg_dp0_mb0
	layer10_expert0_mlp2_dp1_mb0_gpu8 -> layer10_expert_agg_dp1_mb0
	layer10_expert1_mlp2_dp0_mb0_gpu9 -> layer10_expert_agg_dp0_mb0
	layer10_expert1_mlp2_dp1_mb0_gpu9 -> layer10_expert_agg_dp1_mb0
	layer10_expert2_mlp2_dp0_mb0_gpu10 -> layer10_expert_agg_dp0_mb0
	layer10_expert2_mlp2_dp1_mb0_gpu10 -> layer10_expert_agg_dp1_mb0
	layer10_expert3_mlp2_dp0_mb0_gpu11 -> layer10_expert_agg_dp0_mb0
	layer10_expert3_mlp2_dp1_mb0_gpu11 -> layer10_expert_agg_dp1_mb0
	layer11_qkv_dp0_mb0_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb0_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb0 -> layer11_qkv_dp0_mb0_gpu8
	layer10_expert_agg_dp1_mb0 -> layer11_qkv_dp1_mb0_gpu8
	layer11_attn_scores_dp0_mb0_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb0_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb0_gpu8 -> layer11_attn_scores_dp0_mb0_gpu8
	layer11_qkv_dp1_mb0_gpu8 -> layer11_attn_scores_dp1_mb0_gpu8
	layer11_softmax_dp0_mb0_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb0_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb0_gpu8 -> layer11_softmax_dp0_mb0_gpu8
	layer11_attn_scores_dp1_mb0_gpu8 -> layer11_softmax_dp1_mb0_gpu8
	layer11_attn_out_dp0_mb0_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb0_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb0_gpu8 -> layer11_attn_out_dp0_mb0_gpu8
	layer11_softmax_dp1_mb0_gpu8 -> layer11_attn_out_dp1_mb0_gpu8
	layer11_qkv_dp0_mb0_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb0_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb0 -> layer11_qkv_dp0_mb0_gpu9
	layer10_expert_agg_dp1_mb0 -> layer11_qkv_dp1_mb0_gpu9
	layer11_attn_scores_dp0_mb0_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb0_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb0_gpu9 -> layer11_attn_scores_dp0_mb0_gpu9
	layer11_qkv_dp1_mb0_gpu9 -> layer11_attn_scores_dp1_mb0_gpu9
	layer11_softmax_dp0_mb0_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb0_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb0_gpu9 -> layer11_softmax_dp0_mb0_gpu9
	layer11_attn_scores_dp1_mb0_gpu9 -> layer11_softmax_dp1_mb0_gpu9
	layer11_attn_out_dp0_mb0_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb0_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb0_gpu9 -> layer11_attn_out_dp0_mb0_gpu9
	layer11_softmax_dp1_mb0_gpu9 -> layer11_attn_out_dp1_mb0_gpu9
	layer11_qkv_dp0_mb0_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb0_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb0 -> layer11_qkv_dp0_mb0_gpu10
	layer10_expert_agg_dp1_mb0 -> layer11_qkv_dp1_mb0_gpu10
	layer11_attn_scores_dp0_mb0_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb0_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb0_gpu10 -> layer11_attn_scores_dp0_mb0_gpu10
	layer11_qkv_dp1_mb0_gpu10 -> layer11_attn_scores_dp1_mb0_gpu10
	layer11_softmax_dp0_mb0_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb0_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb0_gpu10 -> layer11_softmax_dp0_mb0_gpu10
	layer11_attn_scores_dp1_mb0_gpu10 -> layer11_softmax_dp1_mb0_gpu10
	layer11_attn_out_dp0_mb0_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb0_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb0_gpu10 -> layer11_attn_out_dp0_mb0_gpu10
	layer11_softmax_dp1_mb0_gpu10 -> layer11_attn_out_dp1_mb0_gpu10
	layer11_qkv_dp0_mb0_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb0_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb0 -> layer11_qkv_dp0_mb0_gpu11
	layer10_expert_agg_dp1_mb0 -> layer11_qkv_dp1_mb0_gpu11
	layer11_attn_scores_dp0_mb0_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb0_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb0_gpu11 -> layer11_attn_scores_dp0_mb0_gpu11
	layer11_qkv_dp1_mb0_gpu11 -> layer11_attn_scores_dp1_mb0_gpu11
	layer11_softmax_dp0_mb0_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb0_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb0_gpu11 -> layer11_softmax_dp0_mb0_gpu11
	layer11_attn_scores_dp1_mb0_gpu11 -> layer11_softmax_dp1_mb0_gpu11
	layer11_attn_out_dp0_mb0_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb0_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb0_gpu11 -> layer11_attn_out_dp0_mb0_gpu11
	layer11_softmax_dp1_mb0_gpu11 -> layer11_attn_out_dp1_mb0_gpu11
	layer11_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb0_gpu8 -> layer11_attn_allreduce_dp0_mb0
	layer11_attn_out_dp1_mb0_gpu8 -> layer11_attn_allreduce_dp1_mb0
	layer11_attn_out_dp0_mb0_gpu9 -> layer11_attn_allreduce_dp0_mb0
	layer11_attn_out_dp1_mb0_gpu9 -> layer11_attn_allreduce_dp1_mb0
	layer11_attn_out_dp0_mb0_gpu10 -> layer11_attn_allreduce_dp0_mb0
	layer11_attn_out_dp1_mb0_gpu10 -> layer11_attn_allreduce_dp1_mb0
	layer11_attn_out_dp0_mb0_gpu11 -> layer11_attn_allreduce_dp0_mb0
	layer11_attn_out_dp1_mb0_gpu11 -> layer11_attn_allreduce_dp1_mb0
	layer11_router_dp0_mb0_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb0_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb0 -> layer11_router_dp0_mb0_gpu8
	layer11_attn_allreduce_dp1_mb0 -> layer11_router_dp1_mb0_gpu8
	layer11_expert0_mlp1_dp0_mb0_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb0_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb0_gpu8 -> layer11_expert0_mlp1_dp0_mb0_gpu8
	layer11_router_dp1_mb0_gpu8 -> layer11_expert0_mlp1_dp1_mb0_gpu8
	layer11_expert0_mlp2_dp0_mb0_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb0_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb0_gpu8 -> layer11_expert0_mlp2_dp0_mb0_gpu8
	layer11_expert0_mlp1_dp1_mb0_gpu8 -> layer11_expert0_mlp2_dp1_mb0_gpu8
	layer11_expert1_mlp1_dp0_mb0_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb0_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb0_gpu8 -> layer11_expert1_mlp1_dp0_mb0_gpu9
	layer11_router_dp1_mb0_gpu8 -> layer11_expert1_mlp1_dp1_mb0_gpu9
	layer11_expert1_mlp2_dp0_mb0_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb0_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb0_gpu9 -> layer11_expert1_mlp2_dp0_mb0_gpu9
	layer11_expert1_mlp1_dp1_mb0_gpu9 -> layer11_expert1_mlp2_dp1_mb0_gpu9
	layer11_expert2_mlp1_dp0_mb0_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb0_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb0_gpu8 -> layer11_expert2_mlp1_dp0_mb0_gpu10
	layer11_router_dp1_mb0_gpu8 -> layer11_expert2_mlp1_dp1_mb0_gpu10
	layer11_expert2_mlp2_dp0_mb0_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb0_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb0_gpu10 -> layer11_expert2_mlp2_dp0_mb0_gpu10
	layer11_expert2_mlp1_dp1_mb0_gpu10 -> layer11_expert2_mlp2_dp1_mb0_gpu10
	layer11_expert3_mlp1_dp0_mb0_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb0_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb0_gpu8 -> layer11_expert3_mlp1_dp0_mb0_gpu11
	layer11_router_dp1_mb0_gpu8 -> layer11_expert3_mlp1_dp1_mb0_gpu11
	layer11_expert3_mlp2_dp0_mb0_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb0_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb0_gpu11 -> layer11_expert3_mlp2_dp0_mb0_gpu11
	layer11_expert3_mlp1_dp1_mb0_gpu11 -> layer11_expert3_mlp2_dp1_mb0_gpu11
	layer11_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb0_gpu8 -> layer11_expert_agg_dp0_mb0
	layer11_expert0_mlp2_dp1_mb0_gpu8 -> layer11_expert_agg_dp1_mb0
	layer11_expert1_mlp2_dp0_mb0_gpu9 -> layer11_expert_agg_dp0_mb0
	layer11_expert1_mlp2_dp1_mb0_gpu9 -> layer11_expert_agg_dp1_mb0
	layer11_expert2_mlp2_dp0_mb0_gpu10 -> layer11_expert_agg_dp0_mb0
	layer11_expert2_mlp2_dp1_mb0_gpu10 -> layer11_expert_agg_dp1_mb0
	layer11_expert3_mlp2_dp0_mb0_gpu11 -> layer11_expert_agg_dp0_mb0
	layer11_expert3_mlp2_dp1_mb0_gpu11 -> layer11_expert_agg_dp1_mb0
	layer8_qkv_dp0_mb1_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb1_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb1 -> layer8_qkv_dp0_mb1_gpu8
	layer7_expert_agg_dp1_mb1 -> layer8_qkv_dp1_mb1_gpu8
	layer8_attn_scores_dp0_mb1_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb1_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb1_gpu8 -> layer8_attn_scores_dp0_mb1_gpu8
	layer8_qkv_dp1_mb1_gpu8 -> layer8_attn_scores_dp1_mb1_gpu8
	layer8_softmax_dp0_mb1_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb1_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb1_gpu8 -> layer8_softmax_dp0_mb1_gpu8
	layer8_attn_scores_dp1_mb1_gpu8 -> layer8_softmax_dp1_mb1_gpu8
	layer8_attn_out_dp0_mb1_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb1_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb1_gpu8 -> layer8_attn_out_dp0_mb1_gpu8
	layer8_softmax_dp1_mb1_gpu8 -> layer8_attn_out_dp1_mb1_gpu8
	layer8_qkv_dp0_mb1_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb1_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb1 -> layer8_qkv_dp0_mb1_gpu9
	layer7_expert_agg_dp1_mb1 -> layer8_qkv_dp1_mb1_gpu9
	layer8_attn_scores_dp0_mb1_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb1_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb1_gpu9 -> layer8_attn_scores_dp0_mb1_gpu9
	layer8_qkv_dp1_mb1_gpu9 -> layer8_attn_scores_dp1_mb1_gpu9
	layer8_softmax_dp0_mb1_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb1_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb1_gpu9 -> layer8_softmax_dp0_mb1_gpu9
	layer8_attn_scores_dp1_mb1_gpu9 -> layer8_softmax_dp1_mb1_gpu9
	layer8_attn_out_dp0_mb1_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb1_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb1_gpu9 -> layer8_attn_out_dp0_mb1_gpu9
	layer8_softmax_dp1_mb1_gpu9 -> layer8_attn_out_dp1_mb1_gpu9
	layer8_qkv_dp0_mb1_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb1_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb1 -> layer8_qkv_dp0_mb1_gpu10
	layer7_expert_agg_dp1_mb1 -> layer8_qkv_dp1_mb1_gpu10
	layer8_attn_scores_dp0_mb1_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb1_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb1_gpu10 -> layer8_attn_scores_dp0_mb1_gpu10
	layer8_qkv_dp1_mb1_gpu10 -> layer8_attn_scores_dp1_mb1_gpu10
	layer8_softmax_dp0_mb1_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb1_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb1_gpu10 -> layer8_softmax_dp0_mb1_gpu10
	layer8_attn_scores_dp1_mb1_gpu10 -> layer8_softmax_dp1_mb1_gpu10
	layer8_attn_out_dp0_mb1_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb1_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb1_gpu10 -> layer8_attn_out_dp0_mb1_gpu10
	layer8_softmax_dp1_mb1_gpu10 -> layer8_attn_out_dp1_mb1_gpu10
	layer8_qkv_dp0_mb1_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb1_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb1 -> layer8_qkv_dp0_mb1_gpu11
	layer7_expert_agg_dp1_mb1 -> layer8_qkv_dp1_mb1_gpu11
	layer8_attn_scores_dp0_mb1_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb1_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb1_gpu11 -> layer8_attn_scores_dp0_mb1_gpu11
	layer8_qkv_dp1_mb1_gpu11 -> layer8_attn_scores_dp1_mb1_gpu11
	layer8_softmax_dp0_mb1_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb1_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb1_gpu11 -> layer8_softmax_dp0_mb1_gpu11
	layer8_attn_scores_dp1_mb1_gpu11 -> layer8_softmax_dp1_mb1_gpu11
	layer8_attn_out_dp0_mb1_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb1_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb1_gpu11 -> layer8_attn_out_dp0_mb1_gpu11
	layer8_softmax_dp1_mb1_gpu11 -> layer8_attn_out_dp1_mb1_gpu11
	layer8_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb1_gpu8 -> layer8_attn_allreduce_dp0_mb1
	layer8_attn_out_dp1_mb1_gpu8 -> layer8_attn_allreduce_dp1_mb1
	layer8_attn_out_dp0_mb1_gpu9 -> layer8_attn_allreduce_dp0_mb1
	layer8_attn_out_dp1_mb1_gpu9 -> layer8_attn_allreduce_dp1_mb1
	layer8_attn_out_dp0_mb1_gpu10 -> layer8_attn_allreduce_dp0_mb1
	layer8_attn_out_dp1_mb1_gpu10 -> layer8_attn_allreduce_dp1_mb1
	layer8_attn_out_dp0_mb1_gpu11 -> layer8_attn_allreduce_dp0_mb1
	layer8_attn_out_dp1_mb1_gpu11 -> layer8_attn_allreduce_dp1_mb1
	layer8_router_dp0_mb1_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb1_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb1 -> layer8_router_dp0_mb1_gpu8
	layer8_attn_allreduce_dp1_mb1 -> layer8_router_dp1_mb1_gpu8
	layer8_expert0_mlp1_dp0_mb1_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb1_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb1_gpu8 -> layer8_expert0_mlp1_dp0_mb1_gpu8
	layer8_router_dp1_mb1_gpu8 -> layer8_expert0_mlp1_dp1_mb1_gpu8
	layer8_expert0_mlp2_dp0_mb1_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb1_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb1_gpu8 -> layer8_expert0_mlp2_dp0_mb1_gpu8
	layer8_expert0_mlp1_dp1_mb1_gpu8 -> layer8_expert0_mlp2_dp1_mb1_gpu8
	layer8_expert1_mlp1_dp0_mb1_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb1_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb1_gpu8 -> layer8_expert1_mlp1_dp0_mb1_gpu9
	layer8_router_dp1_mb1_gpu8 -> layer8_expert1_mlp1_dp1_mb1_gpu9
	layer8_expert1_mlp2_dp0_mb1_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb1_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb1_gpu9 -> layer8_expert1_mlp2_dp0_mb1_gpu9
	layer8_expert1_mlp1_dp1_mb1_gpu9 -> layer8_expert1_mlp2_dp1_mb1_gpu9
	layer8_expert2_mlp1_dp0_mb1_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb1_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb1_gpu8 -> layer8_expert2_mlp1_dp0_mb1_gpu10
	layer8_router_dp1_mb1_gpu8 -> layer8_expert2_mlp1_dp1_mb1_gpu10
	layer8_expert2_mlp2_dp0_mb1_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb1_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb1_gpu10 -> layer8_expert2_mlp2_dp0_mb1_gpu10
	layer8_expert2_mlp1_dp1_mb1_gpu10 -> layer8_expert2_mlp2_dp1_mb1_gpu10
	layer8_expert3_mlp1_dp0_mb1_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb1_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb1_gpu8 -> layer8_expert3_mlp1_dp0_mb1_gpu11
	layer8_router_dp1_mb1_gpu8 -> layer8_expert3_mlp1_dp1_mb1_gpu11
	layer8_expert3_mlp2_dp0_mb1_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb1_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb1_gpu11 -> layer8_expert3_mlp2_dp0_mb1_gpu11
	layer8_expert3_mlp1_dp1_mb1_gpu11 -> layer8_expert3_mlp2_dp1_mb1_gpu11
	layer8_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb1_gpu8 -> layer8_expert_agg_dp0_mb1
	layer8_expert0_mlp2_dp1_mb1_gpu8 -> layer8_expert_agg_dp1_mb1
	layer8_expert1_mlp2_dp0_mb1_gpu9 -> layer8_expert_agg_dp0_mb1
	layer8_expert1_mlp2_dp1_mb1_gpu9 -> layer8_expert_agg_dp1_mb1
	layer8_expert2_mlp2_dp0_mb1_gpu10 -> layer8_expert_agg_dp0_mb1
	layer8_expert2_mlp2_dp1_mb1_gpu10 -> layer8_expert_agg_dp1_mb1
	layer8_expert3_mlp2_dp0_mb1_gpu11 -> layer8_expert_agg_dp0_mb1
	layer8_expert3_mlp2_dp1_mb1_gpu11 -> layer8_expert_agg_dp1_mb1
	layer9_qkv_dp0_mb1_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb1_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb1 -> layer9_qkv_dp0_mb1_gpu8
	layer8_expert_agg_dp1_mb1 -> layer9_qkv_dp1_mb1_gpu8
	layer9_attn_scores_dp0_mb1_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb1_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb1_gpu8 -> layer9_attn_scores_dp0_mb1_gpu8
	layer9_qkv_dp1_mb1_gpu8 -> layer9_attn_scores_dp1_mb1_gpu8
	layer9_softmax_dp0_mb1_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb1_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb1_gpu8 -> layer9_softmax_dp0_mb1_gpu8
	layer9_attn_scores_dp1_mb1_gpu8 -> layer9_softmax_dp1_mb1_gpu8
	layer9_attn_out_dp0_mb1_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb1_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb1_gpu8 -> layer9_attn_out_dp0_mb1_gpu8
	layer9_softmax_dp1_mb1_gpu8 -> layer9_attn_out_dp1_mb1_gpu8
	layer9_qkv_dp0_mb1_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb1_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb1 -> layer9_qkv_dp0_mb1_gpu9
	layer8_expert_agg_dp1_mb1 -> layer9_qkv_dp1_mb1_gpu9
	layer9_attn_scores_dp0_mb1_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb1_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb1_gpu9 -> layer9_attn_scores_dp0_mb1_gpu9
	layer9_qkv_dp1_mb1_gpu9 -> layer9_attn_scores_dp1_mb1_gpu9
	layer9_softmax_dp0_mb1_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb1_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb1_gpu9 -> layer9_softmax_dp0_mb1_gpu9
	layer9_attn_scores_dp1_mb1_gpu9 -> layer9_softmax_dp1_mb1_gpu9
	layer9_attn_out_dp0_mb1_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb1_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb1_gpu9 -> layer9_attn_out_dp0_mb1_gpu9
	layer9_softmax_dp1_mb1_gpu9 -> layer9_attn_out_dp1_mb1_gpu9
	layer9_qkv_dp0_mb1_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb1_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb1 -> layer9_qkv_dp0_mb1_gpu10
	layer8_expert_agg_dp1_mb1 -> layer9_qkv_dp1_mb1_gpu10
	layer9_attn_scores_dp0_mb1_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb1_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb1_gpu10 -> layer9_attn_scores_dp0_mb1_gpu10
	layer9_qkv_dp1_mb1_gpu10 -> layer9_attn_scores_dp1_mb1_gpu10
	layer9_softmax_dp0_mb1_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb1_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb1_gpu10 -> layer9_softmax_dp0_mb1_gpu10
	layer9_attn_scores_dp1_mb1_gpu10 -> layer9_softmax_dp1_mb1_gpu10
	layer9_attn_out_dp0_mb1_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb1_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb1_gpu10 -> layer9_attn_out_dp0_mb1_gpu10
	layer9_softmax_dp1_mb1_gpu10 -> layer9_attn_out_dp1_mb1_gpu10
	layer9_qkv_dp0_mb1_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb1_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb1 -> layer9_qkv_dp0_mb1_gpu11
	layer8_expert_agg_dp1_mb1 -> layer9_qkv_dp1_mb1_gpu11
	layer9_attn_scores_dp0_mb1_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb1_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb1_gpu11 -> layer9_attn_scores_dp0_mb1_gpu11
	layer9_qkv_dp1_mb1_gpu11 -> layer9_attn_scores_dp1_mb1_gpu11
	layer9_softmax_dp0_mb1_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb1_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb1_gpu11 -> layer9_softmax_dp0_mb1_gpu11
	layer9_attn_scores_dp1_mb1_gpu11 -> layer9_softmax_dp1_mb1_gpu11
	layer9_attn_out_dp0_mb1_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb1_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb1_gpu11 -> layer9_attn_out_dp0_mb1_gpu11
	layer9_softmax_dp1_mb1_gpu11 -> layer9_attn_out_dp1_mb1_gpu11
	layer9_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb1_gpu8 -> layer9_attn_allreduce_dp0_mb1
	layer9_attn_out_dp1_mb1_gpu8 -> layer9_attn_allreduce_dp1_mb1
	layer9_attn_out_dp0_mb1_gpu9 -> layer9_attn_allreduce_dp0_mb1
	layer9_attn_out_dp1_mb1_gpu9 -> layer9_attn_allreduce_dp1_mb1
	layer9_attn_out_dp0_mb1_gpu10 -> layer9_attn_allreduce_dp0_mb1
	layer9_attn_out_dp1_mb1_gpu10 -> layer9_attn_allreduce_dp1_mb1
	layer9_attn_out_dp0_mb1_gpu11 -> layer9_attn_allreduce_dp0_mb1
	layer9_attn_out_dp1_mb1_gpu11 -> layer9_attn_allreduce_dp1_mb1
	layer9_router_dp0_mb1_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb1_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb1 -> layer9_router_dp0_mb1_gpu8
	layer9_attn_allreduce_dp1_mb1 -> layer9_router_dp1_mb1_gpu8
	layer9_expert0_mlp1_dp0_mb1_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb1_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb1_gpu8 -> layer9_expert0_mlp1_dp0_mb1_gpu8
	layer9_router_dp1_mb1_gpu8 -> layer9_expert0_mlp1_dp1_mb1_gpu8
	layer9_expert0_mlp2_dp0_mb1_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb1_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb1_gpu8 -> layer9_expert0_mlp2_dp0_mb1_gpu8
	layer9_expert0_mlp1_dp1_mb1_gpu8 -> layer9_expert0_mlp2_dp1_mb1_gpu8
	layer9_expert1_mlp1_dp0_mb1_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb1_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb1_gpu8 -> layer9_expert1_mlp1_dp0_mb1_gpu9
	layer9_router_dp1_mb1_gpu8 -> layer9_expert1_mlp1_dp1_mb1_gpu9
	layer9_expert1_mlp2_dp0_mb1_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb1_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb1_gpu9 -> layer9_expert1_mlp2_dp0_mb1_gpu9
	layer9_expert1_mlp1_dp1_mb1_gpu9 -> layer9_expert1_mlp2_dp1_mb1_gpu9
	layer9_expert2_mlp1_dp0_mb1_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb1_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb1_gpu8 -> layer9_expert2_mlp1_dp0_mb1_gpu10
	layer9_router_dp1_mb1_gpu8 -> layer9_expert2_mlp1_dp1_mb1_gpu10
	layer9_expert2_mlp2_dp0_mb1_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb1_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb1_gpu10 -> layer9_expert2_mlp2_dp0_mb1_gpu10
	layer9_expert2_mlp1_dp1_mb1_gpu10 -> layer9_expert2_mlp2_dp1_mb1_gpu10
	layer9_expert3_mlp1_dp0_mb1_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb1_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb1_gpu8 -> layer9_expert3_mlp1_dp0_mb1_gpu11
	layer9_router_dp1_mb1_gpu8 -> layer9_expert3_mlp1_dp1_mb1_gpu11
	layer9_expert3_mlp2_dp0_mb1_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb1_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb1_gpu11 -> layer9_expert3_mlp2_dp0_mb1_gpu11
	layer9_expert3_mlp1_dp1_mb1_gpu11 -> layer9_expert3_mlp2_dp1_mb1_gpu11
	layer9_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb1_gpu8 -> layer9_expert_agg_dp0_mb1
	layer9_expert0_mlp2_dp1_mb1_gpu8 -> layer9_expert_agg_dp1_mb1
	layer9_expert1_mlp2_dp0_mb1_gpu9 -> layer9_expert_agg_dp0_mb1
	layer9_expert1_mlp2_dp1_mb1_gpu9 -> layer9_expert_agg_dp1_mb1
	layer9_expert2_mlp2_dp0_mb1_gpu10 -> layer9_expert_agg_dp0_mb1
	layer9_expert2_mlp2_dp1_mb1_gpu10 -> layer9_expert_agg_dp1_mb1
	layer9_expert3_mlp2_dp0_mb1_gpu11 -> layer9_expert_agg_dp0_mb1
	layer9_expert3_mlp2_dp1_mb1_gpu11 -> layer9_expert_agg_dp1_mb1
	layer10_qkv_dp0_mb1_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb1_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb1 -> layer10_qkv_dp0_mb1_gpu8
	layer9_expert_agg_dp1_mb1 -> layer10_qkv_dp1_mb1_gpu8
	layer10_attn_scores_dp0_mb1_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb1_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb1_gpu8 -> layer10_attn_scores_dp0_mb1_gpu8
	layer10_qkv_dp1_mb1_gpu8 -> layer10_attn_scores_dp1_mb1_gpu8
	layer10_softmax_dp0_mb1_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb1_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb1_gpu8 -> layer10_softmax_dp0_mb1_gpu8
	layer10_attn_scores_dp1_mb1_gpu8 -> layer10_softmax_dp1_mb1_gpu8
	layer10_attn_out_dp0_mb1_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb1_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb1_gpu8 -> layer10_attn_out_dp0_mb1_gpu8
	layer10_softmax_dp1_mb1_gpu8 -> layer10_attn_out_dp1_mb1_gpu8
	layer10_qkv_dp0_mb1_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb1_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb1 -> layer10_qkv_dp0_mb1_gpu9
	layer9_expert_agg_dp1_mb1 -> layer10_qkv_dp1_mb1_gpu9
	layer10_attn_scores_dp0_mb1_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb1_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb1_gpu9 -> layer10_attn_scores_dp0_mb1_gpu9
	layer10_qkv_dp1_mb1_gpu9 -> layer10_attn_scores_dp1_mb1_gpu9
	layer10_softmax_dp0_mb1_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb1_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb1_gpu9 -> layer10_softmax_dp0_mb1_gpu9
	layer10_attn_scores_dp1_mb1_gpu9 -> layer10_softmax_dp1_mb1_gpu9
	layer10_attn_out_dp0_mb1_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb1_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb1_gpu9 -> layer10_attn_out_dp0_mb1_gpu9
	layer10_softmax_dp1_mb1_gpu9 -> layer10_attn_out_dp1_mb1_gpu9
	layer10_qkv_dp0_mb1_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb1_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb1 -> layer10_qkv_dp0_mb1_gpu10
	layer9_expert_agg_dp1_mb1 -> layer10_qkv_dp1_mb1_gpu10
	layer10_attn_scores_dp0_mb1_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb1_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb1_gpu10 -> layer10_attn_scores_dp0_mb1_gpu10
	layer10_qkv_dp1_mb1_gpu10 -> layer10_attn_scores_dp1_mb1_gpu10
	layer10_softmax_dp0_mb1_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb1_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb1_gpu10 -> layer10_softmax_dp0_mb1_gpu10
	layer10_attn_scores_dp1_mb1_gpu10 -> layer10_softmax_dp1_mb1_gpu10
	layer10_attn_out_dp0_mb1_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb1_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb1_gpu10 -> layer10_attn_out_dp0_mb1_gpu10
	layer10_softmax_dp1_mb1_gpu10 -> layer10_attn_out_dp1_mb1_gpu10
	layer10_qkv_dp0_mb1_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb1_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb1 -> layer10_qkv_dp0_mb1_gpu11
	layer9_expert_agg_dp1_mb1 -> layer10_qkv_dp1_mb1_gpu11
	layer10_attn_scores_dp0_mb1_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb1_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb1_gpu11 -> layer10_attn_scores_dp0_mb1_gpu11
	layer10_qkv_dp1_mb1_gpu11 -> layer10_attn_scores_dp1_mb1_gpu11
	layer10_softmax_dp0_mb1_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb1_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb1_gpu11 -> layer10_softmax_dp0_mb1_gpu11
	layer10_attn_scores_dp1_mb1_gpu11 -> layer10_softmax_dp1_mb1_gpu11
	layer10_attn_out_dp0_mb1_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb1_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb1_gpu11 -> layer10_attn_out_dp0_mb1_gpu11
	layer10_softmax_dp1_mb1_gpu11 -> layer10_attn_out_dp1_mb1_gpu11
	layer10_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb1_gpu8 -> layer10_attn_allreduce_dp0_mb1
	layer10_attn_out_dp1_mb1_gpu8 -> layer10_attn_allreduce_dp1_mb1
	layer10_attn_out_dp0_mb1_gpu9 -> layer10_attn_allreduce_dp0_mb1
	layer10_attn_out_dp1_mb1_gpu9 -> layer10_attn_allreduce_dp1_mb1
	layer10_attn_out_dp0_mb1_gpu10 -> layer10_attn_allreduce_dp0_mb1
	layer10_attn_out_dp1_mb1_gpu10 -> layer10_attn_allreduce_dp1_mb1
	layer10_attn_out_dp0_mb1_gpu11 -> layer10_attn_allreduce_dp0_mb1
	layer10_attn_out_dp1_mb1_gpu11 -> layer10_attn_allreduce_dp1_mb1
	layer10_router_dp0_mb1_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb1_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb1 -> layer10_router_dp0_mb1_gpu8
	layer10_attn_allreduce_dp1_mb1 -> layer10_router_dp1_mb1_gpu8
	layer10_expert0_mlp1_dp0_mb1_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb1_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb1_gpu8 -> layer10_expert0_mlp1_dp0_mb1_gpu8
	layer10_router_dp1_mb1_gpu8 -> layer10_expert0_mlp1_dp1_mb1_gpu8
	layer10_expert0_mlp2_dp0_mb1_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb1_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb1_gpu8 -> layer10_expert0_mlp2_dp0_mb1_gpu8
	layer10_expert0_mlp1_dp1_mb1_gpu8 -> layer10_expert0_mlp2_dp1_mb1_gpu8
	layer10_expert1_mlp1_dp0_mb1_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb1_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb1_gpu8 -> layer10_expert1_mlp1_dp0_mb1_gpu9
	layer10_router_dp1_mb1_gpu8 -> layer10_expert1_mlp1_dp1_mb1_gpu9
	layer10_expert1_mlp2_dp0_mb1_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb1_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb1_gpu9 -> layer10_expert1_mlp2_dp0_mb1_gpu9
	layer10_expert1_mlp1_dp1_mb1_gpu9 -> layer10_expert1_mlp2_dp1_mb1_gpu9
	layer10_expert2_mlp1_dp0_mb1_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb1_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb1_gpu8 -> layer10_expert2_mlp1_dp0_mb1_gpu10
	layer10_router_dp1_mb1_gpu8 -> layer10_expert2_mlp1_dp1_mb1_gpu10
	layer10_expert2_mlp2_dp0_mb1_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb1_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb1_gpu10 -> layer10_expert2_mlp2_dp0_mb1_gpu10
	layer10_expert2_mlp1_dp1_mb1_gpu10 -> layer10_expert2_mlp2_dp1_mb1_gpu10
	layer10_expert3_mlp1_dp0_mb1_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb1_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb1_gpu8 -> layer10_expert3_mlp1_dp0_mb1_gpu11
	layer10_router_dp1_mb1_gpu8 -> layer10_expert3_mlp1_dp1_mb1_gpu11
	layer10_expert3_mlp2_dp0_mb1_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb1_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb1_gpu11 -> layer10_expert3_mlp2_dp0_mb1_gpu11
	layer10_expert3_mlp1_dp1_mb1_gpu11 -> layer10_expert3_mlp2_dp1_mb1_gpu11
	layer10_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb1_gpu8 -> layer10_expert_agg_dp0_mb1
	layer10_expert0_mlp2_dp1_mb1_gpu8 -> layer10_expert_agg_dp1_mb1
	layer10_expert1_mlp2_dp0_mb1_gpu9 -> layer10_expert_agg_dp0_mb1
	layer10_expert1_mlp2_dp1_mb1_gpu9 -> layer10_expert_agg_dp1_mb1
	layer10_expert2_mlp2_dp0_mb1_gpu10 -> layer10_expert_agg_dp0_mb1
	layer10_expert2_mlp2_dp1_mb1_gpu10 -> layer10_expert_agg_dp1_mb1
	layer10_expert3_mlp2_dp0_mb1_gpu11 -> layer10_expert_agg_dp0_mb1
	layer10_expert3_mlp2_dp1_mb1_gpu11 -> layer10_expert_agg_dp1_mb1
	layer11_qkv_dp0_mb1_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb1_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb1 -> layer11_qkv_dp0_mb1_gpu8
	layer10_expert_agg_dp1_mb1 -> layer11_qkv_dp1_mb1_gpu8
	layer11_attn_scores_dp0_mb1_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb1_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb1_gpu8 -> layer11_attn_scores_dp0_mb1_gpu8
	layer11_qkv_dp1_mb1_gpu8 -> layer11_attn_scores_dp1_mb1_gpu8
	layer11_softmax_dp0_mb1_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb1_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb1_gpu8 -> layer11_softmax_dp0_mb1_gpu8
	layer11_attn_scores_dp1_mb1_gpu8 -> layer11_softmax_dp1_mb1_gpu8
	layer11_attn_out_dp0_mb1_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb1_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb1_gpu8 -> layer11_attn_out_dp0_mb1_gpu8
	layer11_softmax_dp1_mb1_gpu8 -> layer11_attn_out_dp1_mb1_gpu8
	layer11_qkv_dp0_mb1_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb1_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb1 -> layer11_qkv_dp0_mb1_gpu9
	layer10_expert_agg_dp1_mb1 -> layer11_qkv_dp1_mb1_gpu9
	layer11_attn_scores_dp0_mb1_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb1_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb1_gpu9 -> layer11_attn_scores_dp0_mb1_gpu9
	layer11_qkv_dp1_mb1_gpu9 -> layer11_attn_scores_dp1_mb1_gpu9
	layer11_softmax_dp0_mb1_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb1_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb1_gpu9 -> layer11_softmax_dp0_mb1_gpu9
	layer11_attn_scores_dp1_mb1_gpu9 -> layer11_softmax_dp1_mb1_gpu9
	layer11_attn_out_dp0_mb1_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb1_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb1_gpu9 -> layer11_attn_out_dp0_mb1_gpu9
	layer11_softmax_dp1_mb1_gpu9 -> layer11_attn_out_dp1_mb1_gpu9
	layer11_qkv_dp0_mb1_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb1_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb1 -> layer11_qkv_dp0_mb1_gpu10
	layer10_expert_agg_dp1_mb1 -> layer11_qkv_dp1_mb1_gpu10
	layer11_attn_scores_dp0_mb1_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb1_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb1_gpu10 -> layer11_attn_scores_dp0_mb1_gpu10
	layer11_qkv_dp1_mb1_gpu10 -> layer11_attn_scores_dp1_mb1_gpu10
	layer11_softmax_dp0_mb1_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb1_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb1_gpu10 -> layer11_softmax_dp0_mb1_gpu10
	layer11_attn_scores_dp1_mb1_gpu10 -> layer11_softmax_dp1_mb1_gpu10
	layer11_attn_out_dp0_mb1_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb1_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb1_gpu10 -> layer11_attn_out_dp0_mb1_gpu10
	layer11_softmax_dp1_mb1_gpu10 -> layer11_attn_out_dp1_mb1_gpu10
	layer11_qkv_dp0_mb1_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb1_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb1 -> layer11_qkv_dp0_mb1_gpu11
	layer10_expert_agg_dp1_mb1 -> layer11_qkv_dp1_mb1_gpu11
	layer11_attn_scores_dp0_mb1_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb1_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb1_gpu11 -> layer11_attn_scores_dp0_mb1_gpu11
	layer11_qkv_dp1_mb1_gpu11 -> layer11_attn_scores_dp1_mb1_gpu11
	layer11_softmax_dp0_mb1_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb1_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb1_gpu11 -> layer11_softmax_dp0_mb1_gpu11
	layer11_attn_scores_dp1_mb1_gpu11 -> layer11_softmax_dp1_mb1_gpu11
	layer11_attn_out_dp0_mb1_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb1_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb1_gpu11 -> layer11_attn_out_dp0_mb1_gpu11
	layer11_softmax_dp1_mb1_gpu11 -> layer11_attn_out_dp1_mb1_gpu11
	layer11_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb1_gpu8 -> layer11_attn_allreduce_dp0_mb1
	layer11_attn_out_dp1_mb1_gpu8 -> layer11_attn_allreduce_dp1_mb1
	layer11_attn_out_dp0_mb1_gpu9 -> layer11_attn_allreduce_dp0_mb1
	layer11_attn_out_dp1_mb1_gpu9 -> layer11_attn_allreduce_dp1_mb1
	layer11_attn_out_dp0_mb1_gpu10 -> layer11_attn_allreduce_dp0_mb1
	layer11_attn_out_dp1_mb1_gpu10 -> layer11_attn_allreduce_dp1_mb1
	layer11_attn_out_dp0_mb1_gpu11 -> layer11_attn_allreduce_dp0_mb1
	layer11_attn_out_dp1_mb1_gpu11 -> layer11_attn_allreduce_dp1_mb1
	layer11_router_dp0_mb1_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb1_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb1 -> layer11_router_dp0_mb1_gpu8
	layer11_attn_allreduce_dp1_mb1 -> layer11_router_dp1_mb1_gpu8
	layer11_expert0_mlp1_dp0_mb1_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb1_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb1_gpu8 -> layer11_expert0_mlp1_dp0_mb1_gpu8
	layer11_router_dp1_mb1_gpu8 -> layer11_expert0_mlp1_dp1_mb1_gpu8
	layer11_expert0_mlp2_dp0_mb1_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb1_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb1_gpu8 -> layer11_expert0_mlp2_dp0_mb1_gpu8
	layer11_expert0_mlp1_dp1_mb1_gpu8 -> layer11_expert0_mlp2_dp1_mb1_gpu8
	layer11_expert1_mlp1_dp0_mb1_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb1_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb1_gpu8 -> layer11_expert1_mlp1_dp0_mb1_gpu9
	layer11_router_dp1_mb1_gpu8 -> layer11_expert1_mlp1_dp1_mb1_gpu9
	layer11_expert1_mlp2_dp0_mb1_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb1_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb1_gpu9 -> layer11_expert1_mlp2_dp0_mb1_gpu9
	layer11_expert1_mlp1_dp1_mb1_gpu9 -> layer11_expert1_mlp2_dp1_mb1_gpu9
	layer11_expert2_mlp1_dp0_mb1_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb1_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb1_gpu8 -> layer11_expert2_mlp1_dp0_mb1_gpu10
	layer11_router_dp1_mb1_gpu8 -> layer11_expert2_mlp1_dp1_mb1_gpu10
	layer11_expert2_mlp2_dp0_mb1_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb1_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb1_gpu10 -> layer11_expert2_mlp2_dp0_mb1_gpu10
	layer11_expert2_mlp1_dp1_mb1_gpu10 -> layer11_expert2_mlp2_dp1_mb1_gpu10
	layer11_expert3_mlp1_dp0_mb1_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb1_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb1_gpu8 -> layer11_expert3_mlp1_dp0_mb1_gpu11
	layer11_router_dp1_mb1_gpu8 -> layer11_expert3_mlp1_dp1_mb1_gpu11
	layer11_expert3_mlp2_dp0_mb1_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb1_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb1_gpu11 -> layer11_expert3_mlp2_dp0_mb1_gpu11
	layer11_expert3_mlp1_dp1_mb1_gpu11 -> layer11_expert3_mlp2_dp1_mb1_gpu11
	layer11_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb1_gpu8 -> layer11_expert_agg_dp0_mb1
	layer11_expert0_mlp2_dp1_mb1_gpu8 -> layer11_expert_agg_dp1_mb1
	layer11_expert1_mlp2_dp0_mb1_gpu9 -> layer11_expert_agg_dp0_mb1
	layer11_expert1_mlp2_dp1_mb1_gpu9 -> layer11_expert_agg_dp1_mb1
	layer11_expert2_mlp2_dp0_mb1_gpu10 -> layer11_expert_agg_dp0_mb1
	layer11_expert2_mlp2_dp1_mb1_gpu10 -> layer11_expert_agg_dp1_mb1
	layer11_expert3_mlp2_dp0_mb1_gpu11 -> layer11_expert_agg_dp0_mb1
	layer11_expert3_mlp2_dp1_mb1_gpu11 -> layer11_expert_agg_dp1_mb1
	layer8_qkv_dp0_mb2_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb2_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb2 -> layer8_qkv_dp0_mb2_gpu8
	layer7_expert_agg_dp1_mb2 -> layer8_qkv_dp1_mb2_gpu8
	layer8_attn_scores_dp0_mb2_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb2_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb2_gpu8 -> layer8_attn_scores_dp0_mb2_gpu8
	layer8_qkv_dp1_mb2_gpu8 -> layer8_attn_scores_dp1_mb2_gpu8
	layer8_softmax_dp0_mb2_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb2_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb2_gpu8 -> layer8_softmax_dp0_mb2_gpu8
	layer8_attn_scores_dp1_mb2_gpu8 -> layer8_softmax_dp1_mb2_gpu8
	layer8_attn_out_dp0_mb2_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb2_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb2_gpu8 -> layer8_attn_out_dp0_mb2_gpu8
	layer8_softmax_dp1_mb2_gpu8 -> layer8_attn_out_dp1_mb2_gpu8
	layer8_qkv_dp0_mb2_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb2_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb2 -> layer8_qkv_dp0_mb2_gpu9
	layer7_expert_agg_dp1_mb2 -> layer8_qkv_dp1_mb2_gpu9
	layer8_attn_scores_dp0_mb2_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb2_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb2_gpu9 -> layer8_attn_scores_dp0_mb2_gpu9
	layer8_qkv_dp1_mb2_gpu9 -> layer8_attn_scores_dp1_mb2_gpu9
	layer8_softmax_dp0_mb2_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb2_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb2_gpu9 -> layer8_softmax_dp0_mb2_gpu9
	layer8_attn_scores_dp1_mb2_gpu9 -> layer8_softmax_dp1_mb2_gpu9
	layer8_attn_out_dp0_mb2_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb2_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb2_gpu9 -> layer8_attn_out_dp0_mb2_gpu9
	layer8_softmax_dp1_mb2_gpu9 -> layer8_attn_out_dp1_mb2_gpu9
	layer8_qkv_dp0_mb2_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb2_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb2 -> layer8_qkv_dp0_mb2_gpu10
	layer7_expert_agg_dp1_mb2 -> layer8_qkv_dp1_mb2_gpu10
	layer8_attn_scores_dp0_mb2_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb2_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb2_gpu10 -> layer8_attn_scores_dp0_mb2_gpu10
	layer8_qkv_dp1_mb2_gpu10 -> layer8_attn_scores_dp1_mb2_gpu10
	layer8_softmax_dp0_mb2_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb2_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb2_gpu10 -> layer8_softmax_dp0_mb2_gpu10
	layer8_attn_scores_dp1_mb2_gpu10 -> layer8_softmax_dp1_mb2_gpu10
	layer8_attn_out_dp0_mb2_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb2_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb2_gpu10 -> layer8_attn_out_dp0_mb2_gpu10
	layer8_softmax_dp1_mb2_gpu10 -> layer8_attn_out_dp1_mb2_gpu10
	layer8_qkv_dp0_mb2_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb2_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb2 -> layer8_qkv_dp0_mb2_gpu11
	layer7_expert_agg_dp1_mb2 -> layer8_qkv_dp1_mb2_gpu11
	layer8_attn_scores_dp0_mb2_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb2_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb2_gpu11 -> layer8_attn_scores_dp0_mb2_gpu11
	layer8_qkv_dp1_mb2_gpu11 -> layer8_attn_scores_dp1_mb2_gpu11
	layer8_softmax_dp0_mb2_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb2_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb2_gpu11 -> layer8_softmax_dp0_mb2_gpu11
	layer8_attn_scores_dp1_mb2_gpu11 -> layer8_softmax_dp1_mb2_gpu11
	layer8_attn_out_dp0_mb2_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb2_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb2_gpu11 -> layer8_attn_out_dp0_mb2_gpu11
	layer8_softmax_dp1_mb2_gpu11 -> layer8_attn_out_dp1_mb2_gpu11
	layer8_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb2_gpu8 -> layer8_attn_allreduce_dp0_mb2
	layer8_attn_out_dp1_mb2_gpu8 -> layer8_attn_allreduce_dp1_mb2
	layer8_attn_out_dp0_mb2_gpu9 -> layer8_attn_allreduce_dp0_mb2
	layer8_attn_out_dp1_mb2_gpu9 -> layer8_attn_allreduce_dp1_mb2
	layer8_attn_out_dp0_mb2_gpu10 -> layer8_attn_allreduce_dp0_mb2
	layer8_attn_out_dp1_mb2_gpu10 -> layer8_attn_allreduce_dp1_mb2
	layer8_attn_out_dp0_mb2_gpu11 -> layer8_attn_allreduce_dp0_mb2
	layer8_attn_out_dp1_mb2_gpu11 -> layer8_attn_allreduce_dp1_mb2
	layer8_router_dp0_mb2_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb2_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb2 -> layer8_router_dp0_mb2_gpu8
	layer8_attn_allreduce_dp1_mb2 -> layer8_router_dp1_mb2_gpu8
	layer8_expert0_mlp1_dp0_mb2_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb2_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb2_gpu8 -> layer8_expert0_mlp1_dp0_mb2_gpu8
	layer8_router_dp1_mb2_gpu8 -> layer8_expert0_mlp1_dp1_mb2_gpu8
	layer8_expert0_mlp2_dp0_mb2_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb2_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb2_gpu8 -> layer8_expert0_mlp2_dp0_mb2_gpu8
	layer8_expert0_mlp1_dp1_mb2_gpu8 -> layer8_expert0_mlp2_dp1_mb2_gpu8
	layer8_expert1_mlp1_dp0_mb2_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb2_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb2_gpu8 -> layer8_expert1_mlp1_dp0_mb2_gpu9
	layer8_router_dp1_mb2_gpu8 -> layer8_expert1_mlp1_dp1_mb2_gpu9
	layer8_expert1_mlp2_dp0_mb2_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb2_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb2_gpu9 -> layer8_expert1_mlp2_dp0_mb2_gpu9
	layer8_expert1_mlp1_dp1_mb2_gpu9 -> layer8_expert1_mlp2_dp1_mb2_gpu9
	layer8_expert2_mlp1_dp0_mb2_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb2_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb2_gpu8 -> layer8_expert2_mlp1_dp0_mb2_gpu10
	layer8_router_dp1_mb2_gpu8 -> layer8_expert2_mlp1_dp1_mb2_gpu10
	layer8_expert2_mlp2_dp0_mb2_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb2_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb2_gpu10 -> layer8_expert2_mlp2_dp0_mb2_gpu10
	layer8_expert2_mlp1_dp1_mb2_gpu10 -> layer8_expert2_mlp2_dp1_mb2_gpu10
	layer8_expert3_mlp1_dp0_mb2_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb2_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb2_gpu8 -> layer8_expert3_mlp1_dp0_mb2_gpu11
	layer8_router_dp1_mb2_gpu8 -> layer8_expert3_mlp1_dp1_mb2_gpu11
	layer8_expert3_mlp2_dp0_mb2_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb2_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb2_gpu11 -> layer8_expert3_mlp2_dp0_mb2_gpu11
	layer8_expert3_mlp1_dp1_mb2_gpu11 -> layer8_expert3_mlp2_dp1_mb2_gpu11
	layer8_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb2_gpu8 -> layer8_expert_agg_dp0_mb2
	layer8_expert0_mlp2_dp1_mb2_gpu8 -> layer8_expert_agg_dp1_mb2
	layer8_expert1_mlp2_dp0_mb2_gpu9 -> layer8_expert_agg_dp0_mb2
	layer8_expert1_mlp2_dp1_mb2_gpu9 -> layer8_expert_agg_dp1_mb2
	layer8_expert2_mlp2_dp0_mb2_gpu10 -> layer8_expert_agg_dp0_mb2
	layer8_expert2_mlp2_dp1_mb2_gpu10 -> layer8_expert_agg_dp1_mb2
	layer8_expert3_mlp2_dp0_mb2_gpu11 -> layer8_expert_agg_dp0_mb2
	layer8_expert3_mlp2_dp1_mb2_gpu11 -> layer8_expert_agg_dp1_mb2
	layer9_qkv_dp0_mb2_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb2_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb2 -> layer9_qkv_dp0_mb2_gpu8
	layer8_expert_agg_dp1_mb2 -> layer9_qkv_dp1_mb2_gpu8
	layer9_attn_scores_dp0_mb2_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb2_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb2_gpu8 -> layer9_attn_scores_dp0_mb2_gpu8
	layer9_qkv_dp1_mb2_gpu8 -> layer9_attn_scores_dp1_mb2_gpu8
	layer9_softmax_dp0_mb2_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb2_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb2_gpu8 -> layer9_softmax_dp0_mb2_gpu8
	layer9_attn_scores_dp1_mb2_gpu8 -> layer9_softmax_dp1_mb2_gpu8
	layer9_attn_out_dp0_mb2_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb2_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb2_gpu8 -> layer9_attn_out_dp0_mb2_gpu8
	layer9_softmax_dp1_mb2_gpu8 -> layer9_attn_out_dp1_mb2_gpu8
	layer9_qkv_dp0_mb2_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb2_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb2 -> layer9_qkv_dp0_mb2_gpu9
	layer8_expert_agg_dp1_mb2 -> layer9_qkv_dp1_mb2_gpu9
	layer9_attn_scores_dp0_mb2_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb2_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb2_gpu9 -> layer9_attn_scores_dp0_mb2_gpu9
	layer9_qkv_dp1_mb2_gpu9 -> layer9_attn_scores_dp1_mb2_gpu9
	layer9_softmax_dp0_mb2_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb2_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb2_gpu9 -> layer9_softmax_dp0_mb2_gpu9
	layer9_attn_scores_dp1_mb2_gpu9 -> layer9_softmax_dp1_mb2_gpu9
	layer9_attn_out_dp0_mb2_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb2_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb2_gpu9 -> layer9_attn_out_dp0_mb2_gpu9
	layer9_softmax_dp1_mb2_gpu9 -> layer9_attn_out_dp1_mb2_gpu9
	layer9_qkv_dp0_mb2_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb2_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb2 -> layer9_qkv_dp0_mb2_gpu10
	layer8_expert_agg_dp1_mb2 -> layer9_qkv_dp1_mb2_gpu10
	layer9_attn_scores_dp0_mb2_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb2_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb2_gpu10 -> layer9_attn_scores_dp0_mb2_gpu10
	layer9_qkv_dp1_mb2_gpu10 -> layer9_attn_scores_dp1_mb2_gpu10
	layer9_softmax_dp0_mb2_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb2_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb2_gpu10 -> layer9_softmax_dp0_mb2_gpu10
	layer9_attn_scores_dp1_mb2_gpu10 -> layer9_softmax_dp1_mb2_gpu10
	layer9_attn_out_dp0_mb2_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb2_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb2_gpu10 -> layer9_attn_out_dp0_mb2_gpu10
	layer9_softmax_dp1_mb2_gpu10 -> layer9_attn_out_dp1_mb2_gpu10
	layer9_qkv_dp0_mb2_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb2_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb2 -> layer9_qkv_dp0_mb2_gpu11
	layer8_expert_agg_dp1_mb2 -> layer9_qkv_dp1_mb2_gpu11
	layer9_attn_scores_dp0_mb2_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb2_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb2_gpu11 -> layer9_attn_scores_dp0_mb2_gpu11
	layer9_qkv_dp1_mb2_gpu11 -> layer9_attn_scores_dp1_mb2_gpu11
	layer9_softmax_dp0_mb2_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb2_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb2_gpu11 -> layer9_softmax_dp0_mb2_gpu11
	layer9_attn_scores_dp1_mb2_gpu11 -> layer9_softmax_dp1_mb2_gpu11
	layer9_attn_out_dp0_mb2_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb2_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb2_gpu11 -> layer9_attn_out_dp0_mb2_gpu11
	layer9_softmax_dp1_mb2_gpu11 -> layer9_attn_out_dp1_mb2_gpu11
	layer9_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb2_gpu8 -> layer9_attn_allreduce_dp0_mb2
	layer9_attn_out_dp1_mb2_gpu8 -> layer9_attn_allreduce_dp1_mb2
	layer9_attn_out_dp0_mb2_gpu9 -> layer9_attn_allreduce_dp0_mb2
	layer9_attn_out_dp1_mb2_gpu9 -> layer9_attn_allreduce_dp1_mb2
	layer9_attn_out_dp0_mb2_gpu10 -> layer9_attn_allreduce_dp0_mb2
	layer9_attn_out_dp1_mb2_gpu10 -> layer9_attn_allreduce_dp1_mb2
	layer9_attn_out_dp0_mb2_gpu11 -> layer9_attn_allreduce_dp0_mb2
	layer9_attn_out_dp1_mb2_gpu11 -> layer9_attn_allreduce_dp1_mb2
	layer9_router_dp0_mb2_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb2_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb2 -> layer9_router_dp0_mb2_gpu8
	layer9_attn_allreduce_dp1_mb2 -> layer9_router_dp1_mb2_gpu8
	layer9_expert0_mlp1_dp0_mb2_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb2_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb2_gpu8 -> layer9_expert0_mlp1_dp0_mb2_gpu8
	layer9_router_dp1_mb2_gpu8 -> layer9_expert0_mlp1_dp1_mb2_gpu8
	layer9_expert0_mlp2_dp0_mb2_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb2_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb2_gpu8 -> layer9_expert0_mlp2_dp0_mb2_gpu8
	layer9_expert0_mlp1_dp1_mb2_gpu8 -> layer9_expert0_mlp2_dp1_mb2_gpu8
	layer9_expert1_mlp1_dp0_mb2_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb2_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb2_gpu8 -> layer9_expert1_mlp1_dp0_mb2_gpu9
	layer9_router_dp1_mb2_gpu8 -> layer9_expert1_mlp1_dp1_mb2_gpu9
	layer9_expert1_mlp2_dp0_mb2_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb2_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb2_gpu9 -> layer9_expert1_mlp2_dp0_mb2_gpu9
	layer9_expert1_mlp1_dp1_mb2_gpu9 -> layer9_expert1_mlp2_dp1_mb2_gpu9
	layer9_expert2_mlp1_dp0_mb2_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb2_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb2_gpu8 -> layer9_expert2_mlp1_dp0_mb2_gpu10
	layer9_router_dp1_mb2_gpu8 -> layer9_expert2_mlp1_dp1_mb2_gpu10
	layer9_expert2_mlp2_dp0_mb2_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb2_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb2_gpu10 -> layer9_expert2_mlp2_dp0_mb2_gpu10
	layer9_expert2_mlp1_dp1_mb2_gpu10 -> layer9_expert2_mlp2_dp1_mb2_gpu10
	layer9_expert3_mlp1_dp0_mb2_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb2_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb2_gpu8 -> layer9_expert3_mlp1_dp0_mb2_gpu11
	layer9_router_dp1_mb2_gpu8 -> layer9_expert3_mlp1_dp1_mb2_gpu11
	layer9_expert3_mlp2_dp0_mb2_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb2_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb2_gpu11 -> layer9_expert3_mlp2_dp0_mb2_gpu11
	layer9_expert3_mlp1_dp1_mb2_gpu11 -> layer9_expert3_mlp2_dp1_mb2_gpu11
	layer9_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb2_gpu8 -> layer9_expert_agg_dp0_mb2
	layer9_expert0_mlp2_dp1_mb2_gpu8 -> layer9_expert_agg_dp1_mb2
	layer9_expert1_mlp2_dp0_mb2_gpu9 -> layer9_expert_agg_dp0_mb2
	layer9_expert1_mlp2_dp1_mb2_gpu9 -> layer9_expert_agg_dp1_mb2
	layer9_expert2_mlp2_dp0_mb2_gpu10 -> layer9_expert_agg_dp0_mb2
	layer9_expert2_mlp2_dp1_mb2_gpu10 -> layer9_expert_agg_dp1_mb2
	layer9_expert3_mlp2_dp0_mb2_gpu11 -> layer9_expert_agg_dp0_mb2
	layer9_expert3_mlp2_dp1_mb2_gpu11 -> layer9_expert_agg_dp1_mb2
	layer10_qkv_dp0_mb2_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb2_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb2 -> layer10_qkv_dp0_mb2_gpu8
	layer9_expert_agg_dp1_mb2 -> layer10_qkv_dp1_mb2_gpu8
	layer10_attn_scores_dp0_mb2_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb2_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb2_gpu8 -> layer10_attn_scores_dp0_mb2_gpu8
	layer10_qkv_dp1_mb2_gpu8 -> layer10_attn_scores_dp1_mb2_gpu8
	layer10_softmax_dp0_mb2_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb2_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb2_gpu8 -> layer10_softmax_dp0_mb2_gpu8
	layer10_attn_scores_dp1_mb2_gpu8 -> layer10_softmax_dp1_mb2_gpu8
	layer10_attn_out_dp0_mb2_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb2_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb2_gpu8 -> layer10_attn_out_dp0_mb2_gpu8
	layer10_softmax_dp1_mb2_gpu8 -> layer10_attn_out_dp1_mb2_gpu8
	layer10_qkv_dp0_mb2_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb2_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb2 -> layer10_qkv_dp0_mb2_gpu9
	layer9_expert_agg_dp1_mb2 -> layer10_qkv_dp1_mb2_gpu9
	layer10_attn_scores_dp0_mb2_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb2_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb2_gpu9 -> layer10_attn_scores_dp0_mb2_gpu9
	layer10_qkv_dp1_mb2_gpu9 -> layer10_attn_scores_dp1_mb2_gpu9
	layer10_softmax_dp0_mb2_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb2_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb2_gpu9 -> layer10_softmax_dp0_mb2_gpu9
	layer10_attn_scores_dp1_mb2_gpu9 -> layer10_softmax_dp1_mb2_gpu9
	layer10_attn_out_dp0_mb2_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb2_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb2_gpu9 -> layer10_attn_out_dp0_mb2_gpu9
	layer10_softmax_dp1_mb2_gpu9 -> layer10_attn_out_dp1_mb2_gpu9
	layer10_qkv_dp0_mb2_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb2_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb2 -> layer10_qkv_dp0_mb2_gpu10
	layer9_expert_agg_dp1_mb2 -> layer10_qkv_dp1_mb2_gpu10
	layer10_attn_scores_dp0_mb2_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb2_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb2_gpu10 -> layer10_attn_scores_dp0_mb2_gpu10
	layer10_qkv_dp1_mb2_gpu10 -> layer10_attn_scores_dp1_mb2_gpu10
	layer10_softmax_dp0_mb2_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb2_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb2_gpu10 -> layer10_softmax_dp0_mb2_gpu10
	layer10_attn_scores_dp1_mb2_gpu10 -> layer10_softmax_dp1_mb2_gpu10
	layer10_attn_out_dp0_mb2_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb2_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb2_gpu10 -> layer10_attn_out_dp0_mb2_gpu10
	layer10_softmax_dp1_mb2_gpu10 -> layer10_attn_out_dp1_mb2_gpu10
	layer10_qkv_dp0_mb2_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb2_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb2 -> layer10_qkv_dp0_mb2_gpu11
	layer9_expert_agg_dp1_mb2 -> layer10_qkv_dp1_mb2_gpu11
	layer10_attn_scores_dp0_mb2_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb2_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb2_gpu11 -> layer10_attn_scores_dp0_mb2_gpu11
	layer10_qkv_dp1_mb2_gpu11 -> layer10_attn_scores_dp1_mb2_gpu11
	layer10_softmax_dp0_mb2_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb2_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb2_gpu11 -> layer10_softmax_dp0_mb2_gpu11
	layer10_attn_scores_dp1_mb2_gpu11 -> layer10_softmax_dp1_mb2_gpu11
	layer10_attn_out_dp0_mb2_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb2_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb2_gpu11 -> layer10_attn_out_dp0_mb2_gpu11
	layer10_softmax_dp1_mb2_gpu11 -> layer10_attn_out_dp1_mb2_gpu11
	layer10_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb2_gpu8 -> layer10_attn_allreduce_dp0_mb2
	layer10_attn_out_dp1_mb2_gpu8 -> layer10_attn_allreduce_dp1_mb2
	layer10_attn_out_dp0_mb2_gpu9 -> layer10_attn_allreduce_dp0_mb2
	layer10_attn_out_dp1_mb2_gpu9 -> layer10_attn_allreduce_dp1_mb2
	layer10_attn_out_dp0_mb2_gpu10 -> layer10_attn_allreduce_dp0_mb2
	layer10_attn_out_dp1_mb2_gpu10 -> layer10_attn_allreduce_dp1_mb2
	layer10_attn_out_dp0_mb2_gpu11 -> layer10_attn_allreduce_dp0_mb2
	layer10_attn_out_dp1_mb2_gpu11 -> layer10_attn_allreduce_dp1_mb2
	layer10_router_dp0_mb2_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb2_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb2 -> layer10_router_dp0_mb2_gpu8
	layer10_attn_allreduce_dp1_mb2 -> layer10_router_dp1_mb2_gpu8
	layer10_expert0_mlp1_dp0_mb2_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb2_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb2_gpu8 -> layer10_expert0_mlp1_dp0_mb2_gpu8
	layer10_router_dp1_mb2_gpu8 -> layer10_expert0_mlp1_dp1_mb2_gpu8
	layer10_expert0_mlp2_dp0_mb2_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb2_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb2_gpu8 -> layer10_expert0_mlp2_dp0_mb2_gpu8
	layer10_expert0_mlp1_dp1_mb2_gpu8 -> layer10_expert0_mlp2_dp1_mb2_gpu8
	layer10_expert1_mlp1_dp0_mb2_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb2_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb2_gpu8 -> layer10_expert1_mlp1_dp0_mb2_gpu9
	layer10_router_dp1_mb2_gpu8 -> layer10_expert1_mlp1_dp1_mb2_gpu9
	layer10_expert1_mlp2_dp0_mb2_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb2_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb2_gpu9 -> layer10_expert1_mlp2_dp0_mb2_gpu9
	layer10_expert1_mlp1_dp1_mb2_gpu9 -> layer10_expert1_mlp2_dp1_mb2_gpu9
	layer10_expert2_mlp1_dp0_mb2_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb2_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb2_gpu8 -> layer10_expert2_mlp1_dp0_mb2_gpu10
	layer10_router_dp1_mb2_gpu8 -> layer10_expert2_mlp1_dp1_mb2_gpu10
	layer10_expert2_mlp2_dp0_mb2_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb2_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb2_gpu10 -> layer10_expert2_mlp2_dp0_mb2_gpu10
	layer10_expert2_mlp1_dp1_mb2_gpu10 -> layer10_expert2_mlp2_dp1_mb2_gpu10
	layer10_expert3_mlp1_dp0_mb2_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb2_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb2_gpu8 -> layer10_expert3_mlp1_dp0_mb2_gpu11
	layer10_router_dp1_mb2_gpu8 -> layer10_expert3_mlp1_dp1_mb2_gpu11
	layer10_expert3_mlp2_dp0_mb2_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb2_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb2_gpu11 -> layer10_expert3_mlp2_dp0_mb2_gpu11
	layer10_expert3_mlp1_dp1_mb2_gpu11 -> layer10_expert3_mlp2_dp1_mb2_gpu11
	layer10_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb2_gpu8 -> layer10_expert_agg_dp0_mb2
	layer10_expert0_mlp2_dp1_mb2_gpu8 -> layer10_expert_agg_dp1_mb2
	layer10_expert1_mlp2_dp0_mb2_gpu9 -> layer10_expert_agg_dp0_mb2
	layer10_expert1_mlp2_dp1_mb2_gpu9 -> layer10_expert_agg_dp1_mb2
	layer10_expert2_mlp2_dp0_mb2_gpu10 -> layer10_expert_agg_dp0_mb2
	layer10_expert2_mlp2_dp1_mb2_gpu10 -> layer10_expert_agg_dp1_mb2
	layer10_expert3_mlp2_dp0_mb2_gpu11 -> layer10_expert_agg_dp0_mb2
	layer10_expert3_mlp2_dp1_mb2_gpu11 -> layer10_expert_agg_dp1_mb2
	layer11_qkv_dp0_mb2_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb2_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb2 -> layer11_qkv_dp0_mb2_gpu8
	layer10_expert_agg_dp1_mb2 -> layer11_qkv_dp1_mb2_gpu8
	layer11_attn_scores_dp0_mb2_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb2_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb2_gpu8 -> layer11_attn_scores_dp0_mb2_gpu8
	layer11_qkv_dp1_mb2_gpu8 -> layer11_attn_scores_dp1_mb2_gpu8
	layer11_softmax_dp0_mb2_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb2_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb2_gpu8 -> layer11_softmax_dp0_mb2_gpu8
	layer11_attn_scores_dp1_mb2_gpu8 -> layer11_softmax_dp1_mb2_gpu8
	layer11_attn_out_dp0_mb2_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb2_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb2_gpu8 -> layer11_attn_out_dp0_mb2_gpu8
	layer11_softmax_dp1_mb2_gpu8 -> layer11_attn_out_dp1_mb2_gpu8
	layer11_qkv_dp0_mb2_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb2_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb2 -> layer11_qkv_dp0_mb2_gpu9
	layer10_expert_agg_dp1_mb2 -> layer11_qkv_dp1_mb2_gpu9
	layer11_attn_scores_dp0_mb2_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb2_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb2_gpu9 -> layer11_attn_scores_dp0_mb2_gpu9
	layer11_qkv_dp1_mb2_gpu9 -> layer11_attn_scores_dp1_mb2_gpu9
	layer11_softmax_dp0_mb2_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb2_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb2_gpu9 -> layer11_softmax_dp0_mb2_gpu9
	layer11_attn_scores_dp1_mb2_gpu9 -> layer11_softmax_dp1_mb2_gpu9
	layer11_attn_out_dp0_mb2_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb2_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb2_gpu9 -> layer11_attn_out_dp0_mb2_gpu9
	layer11_softmax_dp1_mb2_gpu9 -> layer11_attn_out_dp1_mb2_gpu9
	layer11_qkv_dp0_mb2_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb2_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb2 -> layer11_qkv_dp0_mb2_gpu10
	layer10_expert_agg_dp1_mb2 -> layer11_qkv_dp1_mb2_gpu10
	layer11_attn_scores_dp0_mb2_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb2_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb2_gpu10 -> layer11_attn_scores_dp0_mb2_gpu10
	layer11_qkv_dp1_mb2_gpu10 -> layer11_attn_scores_dp1_mb2_gpu10
	layer11_softmax_dp0_mb2_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb2_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb2_gpu10 -> layer11_softmax_dp0_mb2_gpu10
	layer11_attn_scores_dp1_mb2_gpu10 -> layer11_softmax_dp1_mb2_gpu10
	layer11_attn_out_dp0_mb2_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb2_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb2_gpu10 -> layer11_attn_out_dp0_mb2_gpu10
	layer11_softmax_dp1_mb2_gpu10 -> layer11_attn_out_dp1_mb2_gpu10
	layer11_qkv_dp0_mb2_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb2_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb2 -> layer11_qkv_dp0_mb2_gpu11
	layer10_expert_agg_dp1_mb2 -> layer11_qkv_dp1_mb2_gpu11
	layer11_attn_scores_dp0_mb2_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb2_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb2_gpu11 -> layer11_attn_scores_dp0_mb2_gpu11
	layer11_qkv_dp1_mb2_gpu11 -> layer11_attn_scores_dp1_mb2_gpu11
	layer11_softmax_dp0_mb2_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb2_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb2_gpu11 -> layer11_softmax_dp0_mb2_gpu11
	layer11_attn_scores_dp1_mb2_gpu11 -> layer11_softmax_dp1_mb2_gpu11
	layer11_attn_out_dp0_mb2_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb2_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb2_gpu11 -> layer11_attn_out_dp0_mb2_gpu11
	layer11_softmax_dp1_mb2_gpu11 -> layer11_attn_out_dp1_mb2_gpu11
	layer11_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb2_gpu8 -> layer11_attn_allreduce_dp0_mb2
	layer11_attn_out_dp1_mb2_gpu8 -> layer11_attn_allreduce_dp1_mb2
	layer11_attn_out_dp0_mb2_gpu9 -> layer11_attn_allreduce_dp0_mb2
	layer11_attn_out_dp1_mb2_gpu9 -> layer11_attn_allreduce_dp1_mb2
	layer11_attn_out_dp0_mb2_gpu10 -> layer11_attn_allreduce_dp0_mb2
	layer11_attn_out_dp1_mb2_gpu10 -> layer11_attn_allreduce_dp1_mb2
	layer11_attn_out_dp0_mb2_gpu11 -> layer11_attn_allreduce_dp0_mb2
	layer11_attn_out_dp1_mb2_gpu11 -> layer11_attn_allreduce_dp1_mb2
	layer11_router_dp0_mb2_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb2_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb2 -> layer11_router_dp0_mb2_gpu8
	layer11_attn_allreduce_dp1_mb2 -> layer11_router_dp1_mb2_gpu8
	layer11_expert0_mlp1_dp0_mb2_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb2_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb2_gpu8 -> layer11_expert0_mlp1_dp0_mb2_gpu8
	layer11_router_dp1_mb2_gpu8 -> layer11_expert0_mlp1_dp1_mb2_gpu8
	layer11_expert0_mlp2_dp0_mb2_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb2_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb2_gpu8 -> layer11_expert0_mlp2_dp0_mb2_gpu8
	layer11_expert0_mlp1_dp1_mb2_gpu8 -> layer11_expert0_mlp2_dp1_mb2_gpu8
	layer11_expert1_mlp1_dp0_mb2_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb2_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb2_gpu8 -> layer11_expert1_mlp1_dp0_mb2_gpu9
	layer11_router_dp1_mb2_gpu8 -> layer11_expert1_mlp1_dp1_mb2_gpu9
	layer11_expert1_mlp2_dp0_mb2_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb2_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb2_gpu9 -> layer11_expert1_mlp2_dp0_mb2_gpu9
	layer11_expert1_mlp1_dp1_mb2_gpu9 -> layer11_expert1_mlp2_dp1_mb2_gpu9
	layer11_expert2_mlp1_dp0_mb2_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb2_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb2_gpu8 -> layer11_expert2_mlp1_dp0_mb2_gpu10
	layer11_router_dp1_mb2_gpu8 -> layer11_expert2_mlp1_dp1_mb2_gpu10
	layer11_expert2_mlp2_dp0_mb2_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb2_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb2_gpu10 -> layer11_expert2_mlp2_dp0_mb2_gpu10
	layer11_expert2_mlp1_dp1_mb2_gpu10 -> layer11_expert2_mlp2_dp1_mb2_gpu10
	layer11_expert3_mlp1_dp0_mb2_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb2_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb2_gpu8 -> layer11_expert3_mlp1_dp0_mb2_gpu11
	layer11_router_dp1_mb2_gpu8 -> layer11_expert3_mlp1_dp1_mb2_gpu11
	layer11_expert3_mlp2_dp0_mb2_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb2_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb2_gpu11 -> layer11_expert3_mlp2_dp0_mb2_gpu11
	layer11_expert3_mlp1_dp1_mb2_gpu11 -> layer11_expert3_mlp2_dp1_mb2_gpu11
	layer11_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb2_gpu8 -> layer11_expert_agg_dp0_mb2
	layer11_expert0_mlp2_dp1_mb2_gpu8 -> layer11_expert_agg_dp1_mb2
	layer11_expert1_mlp2_dp0_mb2_gpu9 -> layer11_expert_agg_dp0_mb2
	layer11_expert1_mlp2_dp1_mb2_gpu9 -> layer11_expert_agg_dp1_mb2
	layer11_expert2_mlp2_dp0_mb2_gpu10 -> layer11_expert_agg_dp0_mb2
	layer11_expert2_mlp2_dp1_mb2_gpu10 -> layer11_expert_agg_dp1_mb2
	layer11_expert3_mlp2_dp0_mb2_gpu11 -> layer11_expert_agg_dp0_mb2
	layer11_expert3_mlp2_dp1_mb2_gpu11 -> layer11_expert_agg_dp1_mb2
	layer8_qkv_dp0_mb3_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb3_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb3 -> layer8_qkv_dp0_mb3_gpu8
	layer7_expert_agg_dp1_mb3 -> layer8_qkv_dp1_mb3_gpu8
	layer8_attn_scores_dp0_mb3_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb3_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb3_gpu8 -> layer8_attn_scores_dp0_mb3_gpu8
	layer8_qkv_dp1_mb3_gpu8 -> layer8_attn_scores_dp1_mb3_gpu8
	layer8_softmax_dp0_mb3_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb3_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb3_gpu8 -> layer8_softmax_dp0_mb3_gpu8
	layer8_attn_scores_dp1_mb3_gpu8 -> layer8_softmax_dp1_mb3_gpu8
	layer8_attn_out_dp0_mb3_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb3_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb3_gpu8 -> layer8_attn_out_dp0_mb3_gpu8
	layer8_softmax_dp1_mb3_gpu8 -> layer8_attn_out_dp1_mb3_gpu8
	layer8_qkv_dp0_mb3_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb3_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb3 -> layer8_qkv_dp0_mb3_gpu9
	layer7_expert_agg_dp1_mb3 -> layer8_qkv_dp1_mb3_gpu9
	layer8_attn_scores_dp0_mb3_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb3_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb3_gpu9 -> layer8_attn_scores_dp0_mb3_gpu9
	layer8_qkv_dp1_mb3_gpu9 -> layer8_attn_scores_dp1_mb3_gpu9
	layer8_softmax_dp0_mb3_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb3_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb3_gpu9 -> layer8_softmax_dp0_mb3_gpu9
	layer8_attn_scores_dp1_mb3_gpu9 -> layer8_softmax_dp1_mb3_gpu9
	layer8_attn_out_dp0_mb3_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb3_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb3_gpu9 -> layer8_attn_out_dp0_mb3_gpu9
	layer8_softmax_dp1_mb3_gpu9 -> layer8_attn_out_dp1_mb3_gpu9
	layer8_qkv_dp0_mb3_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb3_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb3 -> layer8_qkv_dp0_mb3_gpu10
	layer7_expert_agg_dp1_mb3 -> layer8_qkv_dp1_mb3_gpu10
	layer8_attn_scores_dp0_mb3_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb3_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb3_gpu10 -> layer8_attn_scores_dp0_mb3_gpu10
	layer8_qkv_dp1_mb3_gpu10 -> layer8_attn_scores_dp1_mb3_gpu10
	layer8_softmax_dp0_mb3_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb3_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb3_gpu10 -> layer8_softmax_dp0_mb3_gpu10
	layer8_attn_scores_dp1_mb3_gpu10 -> layer8_softmax_dp1_mb3_gpu10
	layer8_attn_out_dp0_mb3_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb3_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb3_gpu10 -> layer8_attn_out_dp0_mb3_gpu10
	layer8_softmax_dp1_mb3_gpu10 -> layer8_attn_out_dp1_mb3_gpu10
	layer8_qkv_dp0_mb3_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb3_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb3 -> layer8_qkv_dp0_mb3_gpu11
	layer7_expert_agg_dp1_mb3 -> layer8_qkv_dp1_mb3_gpu11
	layer8_attn_scores_dp0_mb3_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb3_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb3_gpu11 -> layer8_attn_scores_dp0_mb3_gpu11
	layer8_qkv_dp1_mb3_gpu11 -> layer8_attn_scores_dp1_mb3_gpu11
	layer8_softmax_dp0_mb3_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb3_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb3_gpu11 -> layer8_softmax_dp0_mb3_gpu11
	layer8_attn_scores_dp1_mb3_gpu11 -> layer8_softmax_dp1_mb3_gpu11
	layer8_attn_out_dp0_mb3_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb3_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb3_gpu11 -> layer8_attn_out_dp0_mb3_gpu11
	layer8_softmax_dp1_mb3_gpu11 -> layer8_attn_out_dp1_mb3_gpu11
	layer8_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb3_gpu8 -> layer8_attn_allreduce_dp0_mb3
	layer8_attn_out_dp1_mb3_gpu8 -> layer8_attn_allreduce_dp1_mb3
	layer8_attn_out_dp0_mb3_gpu9 -> layer8_attn_allreduce_dp0_mb3
	layer8_attn_out_dp1_mb3_gpu9 -> layer8_attn_allreduce_dp1_mb3
	layer8_attn_out_dp0_mb3_gpu10 -> layer8_attn_allreduce_dp0_mb3
	layer8_attn_out_dp1_mb3_gpu10 -> layer8_attn_allreduce_dp1_mb3
	layer8_attn_out_dp0_mb3_gpu11 -> layer8_attn_allreduce_dp0_mb3
	layer8_attn_out_dp1_mb3_gpu11 -> layer8_attn_allreduce_dp1_mb3
	layer8_router_dp0_mb3_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb3_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb3 -> layer8_router_dp0_mb3_gpu8
	layer8_attn_allreduce_dp1_mb3 -> layer8_router_dp1_mb3_gpu8
	layer8_expert0_mlp1_dp0_mb3_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb3_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb3_gpu8 -> layer8_expert0_mlp1_dp0_mb3_gpu8
	layer8_router_dp1_mb3_gpu8 -> layer8_expert0_mlp1_dp1_mb3_gpu8
	layer8_expert0_mlp2_dp0_mb3_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb3_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb3_gpu8 -> layer8_expert0_mlp2_dp0_mb3_gpu8
	layer8_expert0_mlp1_dp1_mb3_gpu8 -> layer8_expert0_mlp2_dp1_mb3_gpu8
	layer8_expert1_mlp1_dp0_mb3_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb3_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb3_gpu8 -> layer8_expert1_mlp1_dp0_mb3_gpu9
	layer8_router_dp1_mb3_gpu8 -> layer8_expert1_mlp1_dp1_mb3_gpu9
	layer8_expert1_mlp2_dp0_mb3_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb3_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb3_gpu9 -> layer8_expert1_mlp2_dp0_mb3_gpu9
	layer8_expert1_mlp1_dp1_mb3_gpu9 -> layer8_expert1_mlp2_dp1_mb3_gpu9
	layer8_expert2_mlp1_dp0_mb3_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb3_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb3_gpu8 -> layer8_expert2_mlp1_dp0_mb3_gpu10
	layer8_router_dp1_mb3_gpu8 -> layer8_expert2_mlp1_dp1_mb3_gpu10
	layer8_expert2_mlp2_dp0_mb3_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb3_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb3_gpu10 -> layer8_expert2_mlp2_dp0_mb3_gpu10
	layer8_expert2_mlp1_dp1_mb3_gpu10 -> layer8_expert2_mlp2_dp1_mb3_gpu10
	layer8_expert3_mlp1_dp0_mb3_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb3_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb3_gpu8 -> layer8_expert3_mlp1_dp0_mb3_gpu11
	layer8_router_dp1_mb3_gpu8 -> layer8_expert3_mlp1_dp1_mb3_gpu11
	layer8_expert3_mlp2_dp0_mb3_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb3_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb3_gpu11 -> layer8_expert3_mlp2_dp0_mb3_gpu11
	layer8_expert3_mlp1_dp1_mb3_gpu11 -> layer8_expert3_mlp2_dp1_mb3_gpu11
	layer8_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb3_gpu8 -> layer8_expert_agg_dp0_mb3
	layer8_expert0_mlp2_dp1_mb3_gpu8 -> layer8_expert_agg_dp1_mb3
	layer8_expert1_mlp2_dp0_mb3_gpu9 -> layer8_expert_agg_dp0_mb3
	layer8_expert1_mlp2_dp1_mb3_gpu9 -> layer8_expert_agg_dp1_mb3
	layer8_expert2_mlp2_dp0_mb3_gpu10 -> layer8_expert_agg_dp0_mb3
	layer8_expert2_mlp2_dp1_mb3_gpu10 -> layer8_expert_agg_dp1_mb3
	layer8_expert3_mlp2_dp0_mb3_gpu11 -> layer8_expert_agg_dp0_mb3
	layer8_expert3_mlp2_dp1_mb3_gpu11 -> layer8_expert_agg_dp1_mb3
	layer9_qkv_dp0_mb3_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb3_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb3 -> layer9_qkv_dp0_mb3_gpu8
	layer8_expert_agg_dp1_mb3 -> layer9_qkv_dp1_mb3_gpu8
	layer9_attn_scores_dp0_mb3_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb3_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb3_gpu8 -> layer9_attn_scores_dp0_mb3_gpu8
	layer9_qkv_dp1_mb3_gpu8 -> layer9_attn_scores_dp1_mb3_gpu8
	layer9_softmax_dp0_mb3_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb3_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb3_gpu8 -> layer9_softmax_dp0_mb3_gpu8
	layer9_attn_scores_dp1_mb3_gpu8 -> layer9_softmax_dp1_mb3_gpu8
	layer9_attn_out_dp0_mb3_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb3_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb3_gpu8 -> layer9_attn_out_dp0_mb3_gpu8
	layer9_softmax_dp1_mb3_gpu8 -> layer9_attn_out_dp1_mb3_gpu8
	layer9_qkv_dp0_mb3_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb3_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb3 -> layer9_qkv_dp0_mb3_gpu9
	layer8_expert_agg_dp1_mb3 -> layer9_qkv_dp1_mb3_gpu9
	layer9_attn_scores_dp0_mb3_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb3_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb3_gpu9 -> layer9_attn_scores_dp0_mb3_gpu9
	layer9_qkv_dp1_mb3_gpu9 -> layer9_attn_scores_dp1_mb3_gpu9
	layer9_softmax_dp0_mb3_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb3_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb3_gpu9 -> layer9_softmax_dp0_mb3_gpu9
	layer9_attn_scores_dp1_mb3_gpu9 -> layer9_softmax_dp1_mb3_gpu9
	layer9_attn_out_dp0_mb3_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb3_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb3_gpu9 -> layer9_attn_out_dp0_mb3_gpu9
	layer9_softmax_dp1_mb3_gpu9 -> layer9_attn_out_dp1_mb3_gpu9
	layer9_qkv_dp0_mb3_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb3_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb3 -> layer9_qkv_dp0_mb3_gpu10
	layer8_expert_agg_dp1_mb3 -> layer9_qkv_dp1_mb3_gpu10
	layer9_attn_scores_dp0_mb3_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb3_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb3_gpu10 -> layer9_attn_scores_dp0_mb3_gpu10
	layer9_qkv_dp1_mb3_gpu10 -> layer9_attn_scores_dp1_mb3_gpu10
	layer9_softmax_dp0_mb3_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb3_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb3_gpu10 -> layer9_softmax_dp0_mb3_gpu10
	layer9_attn_scores_dp1_mb3_gpu10 -> layer9_softmax_dp1_mb3_gpu10
	layer9_attn_out_dp0_mb3_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb3_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb3_gpu10 -> layer9_attn_out_dp0_mb3_gpu10
	layer9_softmax_dp1_mb3_gpu10 -> layer9_attn_out_dp1_mb3_gpu10
	layer9_qkv_dp0_mb3_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb3_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb3 -> layer9_qkv_dp0_mb3_gpu11
	layer8_expert_agg_dp1_mb3 -> layer9_qkv_dp1_mb3_gpu11
	layer9_attn_scores_dp0_mb3_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb3_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb3_gpu11 -> layer9_attn_scores_dp0_mb3_gpu11
	layer9_qkv_dp1_mb3_gpu11 -> layer9_attn_scores_dp1_mb3_gpu11
	layer9_softmax_dp0_mb3_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb3_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb3_gpu11 -> layer9_softmax_dp0_mb3_gpu11
	layer9_attn_scores_dp1_mb3_gpu11 -> layer9_softmax_dp1_mb3_gpu11
	layer9_attn_out_dp0_mb3_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb3_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb3_gpu11 -> layer9_attn_out_dp0_mb3_gpu11
	layer9_softmax_dp1_mb3_gpu11 -> layer9_attn_out_dp1_mb3_gpu11
	layer9_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb3_gpu8 -> layer9_attn_allreduce_dp0_mb3
	layer9_attn_out_dp1_mb3_gpu8 -> layer9_attn_allreduce_dp1_mb3
	layer9_attn_out_dp0_mb3_gpu9 -> layer9_attn_allreduce_dp0_mb3
	layer9_attn_out_dp1_mb3_gpu9 -> layer9_attn_allreduce_dp1_mb3
	layer9_attn_out_dp0_mb3_gpu10 -> layer9_attn_allreduce_dp0_mb3
	layer9_attn_out_dp1_mb3_gpu10 -> layer9_attn_allreduce_dp1_mb3
	layer9_attn_out_dp0_mb3_gpu11 -> layer9_attn_allreduce_dp0_mb3
	layer9_attn_out_dp1_mb3_gpu11 -> layer9_attn_allreduce_dp1_mb3
	layer9_router_dp0_mb3_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb3_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb3 -> layer9_router_dp0_mb3_gpu8
	layer9_attn_allreduce_dp1_mb3 -> layer9_router_dp1_mb3_gpu8
	layer9_expert0_mlp1_dp0_mb3_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb3_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb3_gpu8 -> layer9_expert0_mlp1_dp0_mb3_gpu8
	layer9_router_dp1_mb3_gpu8 -> layer9_expert0_mlp1_dp1_mb3_gpu8
	layer9_expert0_mlp2_dp0_mb3_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb3_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb3_gpu8 -> layer9_expert0_mlp2_dp0_mb3_gpu8
	layer9_expert0_mlp1_dp1_mb3_gpu8 -> layer9_expert0_mlp2_dp1_mb3_gpu8
	layer9_expert1_mlp1_dp0_mb3_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb3_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb3_gpu8 -> layer9_expert1_mlp1_dp0_mb3_gpu9
	layer9_router_dp1_mb3_gpu8 -> layer9_expert1_mlp1_dp1_mb3_gpu9
	layer9_expert1_mlp2_dp0_mb3_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb3_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb3_gpu9 -> layer9_expert1_mlp2_dp0_mb3_gpu9
	layer9_expert1_mlp1_dp1_mb3_gpu9 -> layer9_expert1_mlp2_dp1_mb3_gpu9
	layer9_expert2_mlp1_dp0_mb3_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb3_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb3_gpu8 -> layer9_expert2_mlp1_dp0_mb3_gpu10
	layer9_router_dp1_mb3_gpu8 -> layer9_expert2_mlp1_dp1_mb3_gpu10
	layer9_expert2_mlp2_dp0_mb3_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb3_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb3_gpu10 -> layer9_expert2_mlp2_dp0_mb3_gpu10
	layer9_expert2_mlp1_dp1_mb3_gpu10 -> layer9_expert2_mlp2_dp1_mb3_gpu10
	layer9_expert3_mlp1_dp0_mb3_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb3_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb3_gpu8 -> layer9_expert3_mlp1_dp0_mb3_gpu11
	layer9_router_dp1_mb3_gpu8 -> layer9_expert3_mlp1_dp1_mb3_gpu11
	layer9_expert3_mlp2_dp0_mb3_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb3_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb3_gpu11 -> layer9_expert3_mlp2_dp0_mb3_gpu11
	layer9_expert3_mlp1_dp1_mb3_gpu11 -> layer9_expert3_mlp2_dp1_mb3_gpu11
	layer9_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb3_gpu8 -> layer9_expert_agg_dp0_mb3
	layer9_expert0_mlp2_dp1_mb3_gpu8 -> layer9_expert_agg_dp1_mb3
	layer9_expert1_mlp2_dp0_mb3_gpu9 -> layer9_expert_agg_dp0_mb3
	layer9_expert1_mlp2_dp1_mb3_gpu9 -> layer9_expert_agg_dp1_mb3
	layer9_expert2_mlp2_dp0_mb3_gpu10 -> layer9_expert_agg_dp0_mb3
	layer9_expert2_mlp2_dp1_mb3_gpu10 -> layer9_expert_agg_dp1_mb3
	layer9_expert3_mlp2_dp0_mb3_gpu11 -> layer9_expert_agg_dp0_mb3
	layer9_expert3_mlp2_dp1_mb3_gpu11 -> layer9_expert_agg_dp1_mb3
	layer10_qkv_dp0_mb3_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb3_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb3 -> layer10_qkv_dp0_mb3_gpu8
	layer9_expert_agg_dp1_mb3 -> layer10_qkv_dp1_mb3_gpu8
	layer10_attn_scores_dp0_mb3_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb3_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb3_gpu8 -> layer10_attn_scores_dp0_mb3_gpu8
	layer10_qkv_dp1_mb3_gpu8 -> layer10_attn_scores_dp1_mb3_gpu8
	layer10_softmax_dp0_mb3_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb3_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb3_gpu8 -> layer10_softmax_dp0_mb3_gpu8
	layer10_attn_scores_dp1_mb3_gpu8 -> layer10_softmax_dp1_mb3_gpu8
	layer10_attn_out_dp0_mb3_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb3_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb3_gpu8 -> layer10_attn_out_dp0_mb3_gpu8
	layer10_softmax_dp1_mb3_gpu8 -> layer10_attn_out_dp1_mb3_gpu8
	layer10_qkv_dp0_mb3_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb3_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb3 -> layer10_qkv_dp0_mb3_gpu9
	layer9_expert_agg_dp1_mb3 -> layer10_qkv_dp1_mb3_gpu9
	layer10_attn_scores_dp0_mb3_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb3_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb3_gpu9 -> layer10_attn_scores_dp0_mb3_gpu9
	layer10_qkv_dp1_mb3_gpu9 -> layer10_attn_scores_dp1_mb3_gpu9
	layer10_softmax_dp0_mb3_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb3_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb3_gpu9 -> layer10_softmax_dp0_mb3_gpu9
	layer10_attn_scores_dp1_mb3_gpu9 -> layer10_softmax_dp1_mb3_gpu9
	layer10_attn_out_dp0_mb3_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb3_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb3_gpu9 -> layer10_attn_out_dp0_mb3_gpu9
	layer10_softmax_dp1_mb3_gpu9 -> layer10_attn_out_dp1_mb3_gpu9
	layer10_qkv_dp0_mb3_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb3_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb3 -> layer10_qkv_dp0_mb3_gpu10
	layer9_expert_agg_dp1_mb3 -> layer10_qkv_dp1_mb3_gpu10
	layer10_attn_scores_dp0_mb3_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb3_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb3_gpu10 -> layer10_attn_scores_dp0_mb3_gpu10
	layer10_qkv_dp1_mb3_gpu10 -> layer10_attn_scores_dp1_mb3_gpu10
	layer10_softmax_dp0_mb3_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb3_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb3_gpu10 -> layer10_softmax_dp0_mb3_gpu10
	layer10_attn_scores_dp1_mb3_gpu10 -> layer10_softmax_dp1_mb3_gpu10
	layer10_attn_out_dp0_mb3_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb3_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb3_gpu10 -> layer10_attn_out_dp0_mb3_gpu10
	layer10_softmax_dp1_mb3_gpu10 -> layer10_attn_out_dp1_mb3_gpu10
	layer10_qkv_dp0_mb3_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb3_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb3 -> layer10_qkv_dp0_mb3_gpu11
	layer9_expert_agg_dp1_mb3 -> layer10_qkv_dp1_mb3_gpu11
	layer10_attn_scores_dp0_mb3_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb3_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb3_gpu11 -> layer10_attn_scores_dp0_mb3_gpu11
	layer10_qkv_dp1_mb3_gpu11 -> layer10_attn_scores_dp1_mb3_gpu11
	layer10_softmax_dp0_mb3_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb3_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb3_gpu11 -> layer10_softmax_dp0_mb3_gpu11
	layer10_attn_scores_dp1_mb3_gpu11 -> layer10_softmax_dp1_mb3_gpu11
	layer10_attn_out_dp0_mb3_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb3_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb3_gpu11 -> layer10_attn_out_dp0_mb3_gpu11
	layer10_softmax_dp1_mb3_gpu11 -> layer10_attn_out_dp1_mb3_gpu11
	layer10_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb3_gpu8 -> layer10_attn_allreduce_dp0_mb3
	layer10_attn_out_dp1_mb3_gpu8 -> layer10_attn_allreduce_dp1_mb3
	layer10_attn_out_dp0_mb3_gpu9 -> layer10_attn_allreduce_dp0_mb3
	layer10_attn_out_dp1_mb3_gpu9 -> layer10_attn_allreduce_dp1_mb3
	layer10_attn_out_dp0_mb3_gpu10 -> layer10_attn_allreduce_dp0_mb3
	layer10_attn_out_dp1_mb3_gpu10 -> layer10_attn_allreduce_dp1_mb3
	layer10_attn_out_dp0_mb3_gpu11 -> layer10_attn_allreduce_dp0_mb3
	layer10_attn_out_dp1_mb3_gpu11 -> layer10_attn_allreduce_dp1_mb3
	layer10_router_dp0_mb3_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb3_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb3 -> layer10_router_dp0_mb3_gpu8
	layer10_attn_allreduce_dp1_mb3 -> layer10_router_dp1_mb3_gpu8
	layer10_expert0_mlp1_dp0_mb3_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb3_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb3_gpu8 -> layer10_expert0_mlp1_dp0_mb3_gpu8
	layer10_router_dp1_mb3_gpu8 -> layer10_expert0_mlp1_dp1_mb3_gpu8
	layer10_expert0_mlp2_dp0_mb3_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb3_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb3_gpu8 -> layer10_expert0_mlp2_dp0_mb3_gpu8
	layer10_expert0_mlp1_dp1_mb3_gpu8 -> layer10_expert0_mlp2_dp1_mb3_gpu8
	layer10_expert1_mlp1_dp0_mb3_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb3_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb3_gpu8 -> layer10_expert1_mlp1_dp0_mb3_gpu9
	layer10_router_dp1_mb3_gpu8 -> layer10_expert1_mlp1_dp1_mb3_gpu9
	layer10_expert1_mlp2_dp0_mb3_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb3_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb3_gpu9 -> layer10_expert1_mlp2_dp0_mb3_gpu9
	layer10_expert1_mlp1_dp1_mb3_gpu9 -> layer10_expert1_mlp2_dp1_mb3_gpu9
	layer10_expert2_mlp1_dp0_mb3_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb3_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb3_gpu8 -> layer10_expert2_mlp1_dp0_mb3_gpu10
	layer10_router_dp1_mb3_gpu8 -> layer10_expert2_mlp1_dp1_mb3_gpu10
	layer10_expert2_mlp2_dp0_mb3_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb3_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb3_gpu10 -> layer10_expert2_mlp2_dp0_mb3_gpu10
	layer10_expert2_mlp1_dp1_mb3_gpu10 -> layer10_expert2_mlp2_dp1_mb3_gpu10
	layer10_expert3_mlp1_dp0_mb3_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb3_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb3_gpu8 -> layer10_expert3_mlp1_dp0_mb3_gpu11
	layer10_router_dp1_mb3_gpu8 -> layer10_expert3_mlp1_dp1_mb3_gpu11
	layer10_expert3_mlp2_dp0_mb3_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb3_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb3_gpu11 -> layer10_expert3_mlp2_dp0_mb3_gpu11
	layer10_expert3_mlp1_dp1_mb3_gpu11 -> layer10_expert3_mlp2_dp1_mb3_gpu11
	layer10_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb3_gpu8 -> layer10_expert_agg_dp0_mb3
	layer10_expert0_mlp2_dp1_mb3_gpu8 -> layer10_expert_agg_dp1_mb3
	layer10_expert1_mlp2_dp0_mb3_gpu9 -> layer10_expert_agg_dp0_mb3
	layer10_expert1_mlp2_dp1_mb3_gpu9 -> layer10_expert_agg_dp1_mb3
	layer10_expert2_mlp2_dp0_mb3_gpu10 -> layer10_expert_agg_dp0_mb3
	layer10_expert2_mlp2_dp1_mb3_gpu10 -> layer10_expert_agg_dp1_mb3
	layer10_expert3_mlp2_dp0_mb3_gpu11 -> layer10_expert_agg_dp0_mb3
	layer10_expert3_mlp2_dp1_mb3_gpu11 -> layer10_expert_agg_dp1_mb3
	layer11_qkv_dp0_mb3_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb3_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb3 -> layer11_qkv_dp0_mb3_gpu8
	layer10_expert_agg_dp1_mb3 -> layer11_qkv_dp1_mb3_gpu8
	layer11_attn_scores_dp0_mb3_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb3_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb3_gpu8 -> layer11_attn_scores_dp0_mb3_gpu8
	layer11_qkv_dp1_mb3_gpu8 -> layer11_attn_scores_dp1_mb3_gpu8
	layer11_softmax_dp0_mb3_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb3_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb3_gpu8 -> layer11_softmax_dp0_mb3_gpu8
	layer11_attn_scores_dp1_mb3_gpu8 -> layer11_softmax_dp1_mb3_gpu8
	layer11_attn_out_dp0_mb3_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb3_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb3_gpu8 -> layer11_attn_out_dp0_mb3_gpu8
	layer11_softmax_dp1_mb3_gpu8 -> layer11_attn_out_dp1_mb3_gpu8
	layer11_qkv_dp0_mb3_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb3_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb3 -> layer11_qkv_dp0_mb3_gpu9
	layer10_expert_agg_dp1_mb3 -> layer11_qkv_dp1_mb3_gpu9
	layer11_attn_scores_dp0_mb3_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb3_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb3_gpu9 -> layer11_attn_scores_dp0_mb3_gpu9
	layer11_qkv_dp1_mb3_gpu9 -> layer11_attn_scores_dp1_mb3_gpu9
	layer11_softmax_dp0_mb3_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb3_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb3_gpu9 -> layer11_softmax_dp0_mb3_gpu9
	layer11_attn_scores_dp1_mb3_gpu9 -> layer11_softmax_dp1_mb3_gpu9
	layer11_attn_out_dp0_mb3_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb3_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb3_gpu9 -> layer11_attn_out_dp0_mb3_gpu9
	layer11_softmax_dp1_mb3_gpu9 -> layer11_attn_out_dp1_mb3_gpu9
	layer11_qkv_dp0_mb3_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb3_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb3 -> layer11_qkv_dp0_mb3_gpu10
	layer10_expert_agg_dp1_mb3 -> layer11_qkv_dp1_mb3_gpu10
	layer11_attn_scores_dp0_mb3_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb3_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb3_gpu10 -> layer11_attn_scores_dp0_mb3_gpu10
	layer11_qkv_dp1_mb3_gpu10 -> layer11_attn_scores_dp1_mb3_gpu10
	layer11_softmax_dp0_mb3_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb3_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb3_gpu10 -> layer11_softmax_dp0_mb3_gpu10
	layer11_attn_scores_dp1_mb3_gpu10 -> layer11_softmax_dp1_mb3_gpu10
	layer11_attn_out_dp0_mb3_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb3_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb3_gpu10 -> layer11_attn_out_dp0_mb3_gpu10
	layer11_softmax_dp1_mb3_gpu10 -> layer11_attn_out_dp1_mb3_gpu10
	layer11_qkv_dp0_mb3_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb3_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb3 -> layer11_qkv_dp0_mb3_gpu11
	layer10_expert_agg_dp1_mb3 -> layer11_qkv_dp1_mb3_gpu11
	layer11_attn_scores_dp0_mb3_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb3_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb3_gpu11 -> layer11_attn_scores_dp0_mb3_gpu11
	layer11_qkv_dp1_mb3_gpu11 -> layer11_attn_scores_dp1_mb3_gpu11
	layer11_softmax_dp0_mb3_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb3_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb3_gpu11 -> layer11_softmax_dp0_mb3_gpu11
	layer11_attn_scores_dp1_mb3_gpu11 -> layer11_softmax_dp1_mb3_gpu11
	layer11_attn_out_dp0_mb3_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb3_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb3_gpu11 -> layer11_attn_out_dp0_mb3_gpu11
	layer11_softmax_dp1_mb3_gpu11 -> layer11_attn_out_dp1_mb3_gpu11
	layer11_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb3_gpu8 -> layer11_attn_allreduce_dp0_mb3
	layer11_attn_out_dp1_mb3_gpu8 -> layer11_attn_allreduce_dp1_mb3
	layer11_attn_out_dp0_mb3_gpu9 -> layer11_attn_allreduce_dp0_mb3
	layer11_attn_out_dp1_mb3_gpu9 -> layer11_attn_allreduce_dp1_mb3
	layer11_attn_out_dp0_mb3_gpu10 -> layer11_attn_allreduce_dp0_mb3
	layer11_attn_out_dp1_mb3_gpu10 -> layer11_attn_allreduce_dp1_mb3
	layer11_attn_out_dp0_mb3_gpu11 -> layer11_attn_allreduce_dp0_mb3
	layer11_attn_out_dp1_mb3_gpu11 -> layer11_attn_allreduce_dp1_mb3
	layer11_router_dp0_mb3_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb3_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb3 -> layer11_router_dp0_mb3_gpu8
	layer11_attn_allreduce_dp1_mb3 -> layer11_router_dp1_mb3_gpu8
	layer11_expert0_mlp1_dp0_mb3_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb3_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb3_gpu8 -> layer11_expert0_mlp1_dp0_mb3_gpu8
	layer11_router_dp1_mb3_gpu8 -> layer11_expert0_mlp1_dp1_mb3_gpu8
	layer11_expert0_mlp2_dp0_mb3_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb3_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb3_gpu8 -> layer11_expert0_mlp2_dp0_mb3_gpu8
	layer11_expert0_mlp1_dp1_mb3_gpu8 -> layer11_expert0_mlp2_dp1_mb3_gpu8
	layer11_expert1_mlp1_dp0_mb3_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb3_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb3_gpu8 -> layer11_expert1_mlp1_dp0_mb3_gpu9
	layer11_router_dp1_mb3_gpu8 -> layer11_expert1_mlp1_dp1_mb3_gpu9
	layer11_expert1_mlp2_dp0_mb3_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb3_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb3_gpu9 -> layer11_expert1_mlp2_dp0_mb3_gpu9
	layer11_expert1_mlp1_dp1_mb3_gpu9 -> layer11_expert1_mlp2_dp1_mb3_gpu9
	layer11_expert2_mlp1_dp0_mb3_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb3_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb3_gpu8 -> layer11_expert2_mlp1_dp0_mb3_gpu10
	layer11_router_dp1_mb3_gpu8 -> layer11_expert2_mlp1_dp1_mb3_gpu10
	layer11_expert2_mlp2_dp0_mb3_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb3_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb3_gpu10 -> layer11_expert2_mlp2_dp0_mb3_gpu10
	layer11_expert2_mlp1_dp1_mb3_gpu10 -> layer11_expert2_mlp2_dp1_mb3_gpu10
	layer11_expert3_mlp1_dp0_mb3_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb3_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb3_gpu8 -> layer11_expert3_mlp1_dp0_mb3_gpu11
	layer11_router_dp1_mb3_gpu8 -> layer11_expert3_mlp1_dp1_mb3_gpu11
	layer11_expert3_mlp2_dp0_mb3_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb3_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb3_gpu11 -> layer11_expert3_mlp2_dp0_mb3_gpu11
	layer11_expert3_mlp1_dp1_mb3_gpu11 -> layer11_expert3_mlp2_dp1_mb3_gpu11
	layer11_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb3_gpu8 -> layer11_expert_agg_dp0_mb3
	layer11_expert0_mlp2_dp1_mb3_gpu8 -> layer11_expert_agg_dp1_mb3
	layer11_expert1_mlp2_dp0_mb3_gpu9 -> layer11_expert_agg_dp0_mb3
	layer11_expert1_mlp2_dp1_mb3_gpu9 -> layer11_expert_agg_dp1_mb3
	layer11_expert2_mlp2_dp0_mb3_gpu10 -> layer11_expert_agg_dp0_mb3
	layer11_expert2_mlp2_dp1_mb3_gpu10 -> layer11_expert_agg_dp1_mb3
	layer11_expert3_mlp2_dp0_mb3_gpu11 -> layer11_expert_agg_dp0_mb3
	layer11_expert3_mlp2_dp1_mb3_gpu11 -> layer11_expert_agg_dp1_mb3
	layer8_qkv_dp0_mb4_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb4_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb4 -> layer8_qkv_dp0_mb4_gpu8
	layer7_expert_agg_dp1_mb4 -> layer8_qkv_dp1_mb4_gpu8
	layer8_attn_scores_dp0_mb4_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb4_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb4_gpu8 -> layer8_attn_scores_dp0_mb4_gpu8
	layer8_qkv_dp1_mb4_gpu8 -> layer8_attn_scores_dp1_mb4_gpu8
	layer8_softmax_dp0_mb4_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb4_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb4_gpu8 -> layer8_softmax_dp0_mb4_gpu8
	layer8_attn_scores_dp1_mb4_gpu8 -> layer8_softmax_dp1_mb4_gpu8
	layer8_attn_out_dp0_mb4_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb4_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb4_gpu8 -> layer8_attn_out_dp0_mb4_gpu8
	layer8_softmax_dp1_mb4_gpu8 -> layer8_attn_out_dp1_mb4_gpu8
	layer8_qkv_dp0_mb4_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb4_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb4 -> layer8_qkv_dp0_mb4_gpu9
	layer7_expert_agg_dp1_mb4 -> layer8_qkv_dp1_mb4_gpu9
	layer8_attn_scores_dp0_mb4_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb4_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb4_gpu9 -> layer8_attn_scores_dp0_mb4_gpu9
	layer8_qkv_dp1_mb4_gpu9 -> layer8_attn_scores_dp1_mb4_gpu9
	layer8_softmax_dp0_mb4_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb4_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb4_gpu9 -> layer8_softmax_dp0_mb4_gpu9
	layer8_attn_scores_dp1_mb4_gpu9 -> layer8_softmax_dp1_mb4_gpu9
	layer8_attn_out_dp0_mb4_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb4_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb4_gpu9 -> layer8_attn_out_dp0_mb4_gpu9
	layer8_softmax_dp1_mb4_gpu9 -> layer8_attn_out_dp1_mb4_gpu9
	layer8_qkv_dp0_mb4_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb4_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb4 -> layer8_qkv_dp0_mb4_gpu10
	layer7_expert_agg_dp1_mb4 -> layer8_qkv_dp1_mb4_gpu10
	layer8_attn_scores_dp0_mb4_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb4_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb4_gpu10 -> layer8_attn_scores_dp0_mb4_gpu10
	layer8_qkv_dp1_mb4_gpu10 -> layer8_attn_scores_dp1_mb4_gpu10
	layer8_softmax_dp0_mb4_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb4_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb4_gpu10 -> layer8_softmax_dp0_mb4_gpu10
	layer8_attn_scores_dp1_mb4_gpu10 -> layer8_softmax_dp1_mb4_gpu10
	layer8_attn_out_dp0_mb4_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb4_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb4_gpu10 -> layer8_attn_out_dp0_mb4_gpu10
	layer8_softmax_dp1_mb4_gpu10 -> layer8_attn_out_dp1_mb4_gpu10
	layer8_qkv_dp0_mb4_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb4_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb4 -> layer8_qkv_dp0_mb4_gpu11
	layer7_expert_agg_dp1_mb4 -> layer8_qkv_dp1_mb4_gpu11
	layer8_attn_scores_dp0_mb4_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb4_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb4_gpu11 -> layer8_attn_scores_dp0_mb4_gpu11
	layer8_qkv_dp1_mb4_gpu11 -> layer8_attn_scores_dp1_mb4_gpu11
	layer8_softmax_dp0_mb4_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb4_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb4_gpu11 -> layer8_softmax_dp0_mb4_gpu11
	layer8_attn_scores_dp1_mb4_gpu11 -> layer8_softmax_dp1_mb4_gpu11
	layer8_attn_out_dp0_mb4_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb4_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb4_gpu11 -> layer8_attn_out_dp0_mb4_gpu11
	layer8_softmax_dp1_mb4_gpu11 -> layer8_attn_out_dp1_mb4_gpu11
	layer8_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb4_gpu8 -> layer8_attn_allreduce_dp0_mb4
	layer8_attn_out_dp1_mb4_gpu8 -> layer8_attn_allreduce_dp1_mb4
	layer8_attn_out_dp0_mb4_gpu9 -> layer8_attn_allreduce_dp0_mb4
	layer8_attn_out_dp1_mb4_gpu9 -> layer8_attn_allreduce_dp1_mb4
	layer8_attn_out_dp0_mb4_gpu10 -> layer8_attn_allreduce_dp0_mb4
	layer8_attn_out_dp1_mb4_gpu10 -> layer8_attn_allreduce_dp1_mb4
	layer8_attn_out_dp0_mb4_gpu11 -> layer8_attn_allreduce_dp0_mb4
	layer8_attn_out_dp1_mb4_gpu11 -> layer8_attn_allreduce_dp1_mb4
	layer8_router_dp0_mb4_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb4_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb4 -> layer8_router_dp0_mb4_gpu8
	layer8_attn_allreduce_dp1_mb4 -> layer8_router_dp1_mb4_gpu8
	layer8_expert0_mlp1_dp0_mb4_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb4_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb4_gpu8 -> layer8_expert0_mlp1_dp0_mb4_gpu8
	layer8_router_dp1_mb4_gpu8 -> layer8_expert0_mlp1_dp1_mb4_gpu8
	layer8_expert0_mlp2_dp0_mb4_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb4_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb4_gpu8 -> layer8_expert0_mlp2_dp0_mb4_gpu8
	layer8_expert0_mlp1_dp1_mb4_gpu8 -> layer8_expert0_mlp2_dp1_mb4_gpu8
	layer8_expert1_mlp1_dp0_mb4_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb4_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb4_gpu8 -> layer8_expert1_mlp1_dp0_mb4_gpu9
	layer8_router_dp1_mb4_gpu8 -> layer8_expert1_mlp1_dp1_mb4_gpu9
	layer8_expert1_mlp2_dp0_mb4_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb4_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb4_gpu9 -> layer8_expert1_mlp2_dp0_mb4_gpu9
	layer8_expert1_mlp1_dp1_mb4_gpu9 -> layer8_expert1_mlp2_dp1_mb4_gpu9
	layer8_expert2_mlp1_dp0_mb4_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb4_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb4_gpu8 -> layer8_expert2_mlp1_dp0_mb4_gpu10
	layer8_router_dp1_mb4_gpu8 -> layer8_expert2_mlp1_dp1_mb4_gpu10
	layer8_expert2_mlp2_dp0_mb4_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb4_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb4_gpu10 -> layer8_expert2_mlp2_dp0_mb4_gpu10
	layer8_expert2_mlp1_dp1_mb4_gpu10 -> layer8_expert2_mlp2_dp1_mb4_gpu10
	layer8_expert3_mlp1_dp0_mb4_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb4_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb4_gpu8 -> layer8_expert3_mlp1_dp0_mb4_gpu11
	layer8_router_dp1_mb4_gpu8 -> layer8_expert3_mlp1_dp1_mb4_gpu11
	layer8_expert3_mlp2_dp0_mb4_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb4_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb4_gpu11 -> layer8_expert3_mlp2_dp0_mb4_gpu11
	layer8_expert3_mlp1_dp1_mb4_gpu11 -> layer8_expert3_mlp2_dp1_mb4_gpu11
	layer8_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb4_gpu8 -> layer8_expert_agg_dp0_mb4
	layer8_expert0_mlp2_dp1_mb4_gpu8 -> layer8_expert_agg_dp1_mb4
	layer8_expert1_mlp2_dp0_mb4_gpu9 -> layer8_expert_agg_dp0_mb4
	layer8_expert1_mlp2_dp1_mb4_gpu9 -> layer8_expert_agg_dp1_mb4
	layer8_expert2_mlp2_dp0_mb4_gpu10 -> layer8_expert_agg_dp0_mb4
	layer8_expert2_mlp2_dp1_mb4_gpu10 -> layer8_expert_agg_dp1_mb4
	layer8_expert3_mlp2_dp0_mb4_gpu11 -> layer8_expert_agg_dp0_mb4
	layer8_expert3_mlp2_dp1_mb4_gpu11 -> layer8_expert_agg_dp1_mb4
	layer9_qkv_dp0_mb4_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb4_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb4 -> layer9_qkv_dp0_mb4_gpu8
	layer8_expert_agg_dp1_mb4 -> layer9_qkv_dp1_mb4_gpu8
	layer9_attn_scores_dp0_mb4_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb4_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb4_gpu8 -> layer9_attn_scores_dp0_mb4_gpu8
	layer9_qkv_dp1_mb4_gpu8 -> layer9_attn_scores_dp1_mb4_gpu8
	layer9_softmax_dp0_mb4_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb4_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb4_gpu8 -> layer9_softmax_dp0_mb4_gpu8
	layer9_attn_scores_dp1_mb4_gpu8 -> layer9_softmax_dp1_mb4_gpu8
	layer9_attn_out_dp0_mb4_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb4_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb4_gpu8 -> layer9_attn_out_dp0_mb4_gpu8
	layer9_softmax_dp1_mb4_gpu8 -> layer9_attn_out_dp1_mb4_gpu8
	layer9_qkv_dp0_mb4_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb4_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb4 -> layer9_qkv_dp0_mb4_gpu9
	layer8_expert_agg_dp1_mb4 -> layer9_qkv_dp1_mb4_gpu9
	layer9_attn_scores_dp0_mb4_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb4_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb4_gpu9 -> layer9_attn_scores_dp0_mb4_gpu9
	layer9_qkv_dp1_mb4_gpu9 -> layer9_attn_scores_dp1_mb4_gpu9
	layer9_softmax_dp0_mb4_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb4_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb4_gpu9 -> layer9_softmax_dp0_mb4_gpu9
	layer9_attn_scores_dp1_mb4_gpu9 -> layer9_softmax_dp1_mb4_gpu9
	layer9_attn_out_dp0_mb4_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb4_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb4_gpu9 -> layer9_attn_out_dp0_mb4_gpu9
	layer9_softmax_dp1_mb4_gpu9 -> layer9_attn_out_dp1_mb4_gpu9
	layer9_qkv_dp0_mb4_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb4_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb4 -> layer9_qkv_dp0_mb4_gpu10
	layer8_expert_agg_dp1_mb4 -> layer9_qkv_dp1_mb4_gpu10
	layer9_attn_scores_dp0_mb4_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb4_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb4_gpu10 -> layer9_attn_scores_dp0_mb4_gpu10
	layer9_qkv_dp1_mb4_gpu10 -> layer9_attn_scores_dp1_mb4_gpu10
	layer9_softmax_dp0_mb4_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb4_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb4_gpu10 -> layer9_softmax_dp0_mb4_gpu10
	layer9_attn_scores_dp1_mb4_gpu10 -> layer9_softmax_dp1_mb4_gpu10
	layer9_attn_out_dp0_mb4_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb4_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb4_gpu10 -> layer9_attn_out_dp0_mb4_gpu10
	layer9_softmax_dp1_mb4_gpu10 -> layer9_attn_out_dp1_mb4_gpu10
	layer9_qkv_dp0_mb4_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb4_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb4 -> layer9_qkv_dp0_mb4_gpu11
	layer8_expert_agg_dp1_mb4 -> layer9_qkv_dp1_mb4_gpu11
	layer9_attn_scores_dp0_mb4_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb4_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb4_gpu11 -> layer9_attn_scores_dp0_mb4_gpu11
	layer9_qkv_dp1_mb4_gpu11 -> layer9_attn_scores_dp1_mb4_gpu11
	layer9_softmax_dp0_mb4_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb4_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb4_gpu11 -> layer9_softmax_dp0_mb4_gpu11
	layer9_attn_scores_dp1_mb4_gpu11 -> layer9_softmax_dp1_mb4_gpu11
	layer9_attn_out_dp0_mb4_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb4_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb4_gpu11 -> layer9_attn_out_dp0_mb4_gpu11
	layer9_softmax_dp1_mb4_gpu11 -> layer9_attn_out_dp1_mb4_gpu11
	layer9_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb4_gpu8 -> layer9_attn_allreduce_dp0_mb4
	layer9_attn_out_dp1_mb4_gpu8 -> layer9_attn_allreduce_dp1_mb4
	layer9_attn_out_dp0_mb4_gpu9 -> layer9_attn_allreduce_dp0_mb4
	layer9_attn_out_dp1_mb4_gpu9 -> layer9_attn_allreduce_dp1_mb4
	layer9_attn_out_dp0_mb4_gpu10 -> layer9_attn_allreduce_dp0_mb4
	layer9_attn_out_dp1_mb4_gpu10 -> layer9_attn_allreduce_dp1_mb4
	layer9_attn_out_dp0_mb4_gpu11 -> layer9_attn_allreduce_dp0_mb4
	layer9_attn_out_dp1_mb4_gpu11 -> layer9_attn_allreduce_dp1_mb4
	layer9_router_dp0_mb4_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb4_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb4 -> layer9_router_dp0_mb4_gpu8
	layer9_attn_allreduce_dp1_mb4 -> layer9_router_dp1_mb4_gpu8
	layer9_expert0_mlp1_dp0_mb4_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb4_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb4_gpu8 -> layer9_expert0_mlp1_dp0_mb4_gpu8
	layer9_router_dp1_mb4_gpu8 -> layer9_expert0_mlp1_dp1_mb4_gpu8
	layer9_expert0_mlp2_dp0_mb4_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb4_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb4_gpu8 -> layer9_expert0_mlp2_dp0_mb4_gpu8
	layer9_expert0_mlp1_dp1_mb4_gpu8 -> layer9_expert0_mlp2_dp1_mb4_gpu8
	layer9_expert1_mlp1_dp0_mb4_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb4_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb4_gpu8 -> layer9_expert1_mlp1_dp0_mb4_gpu9
	layer9_router_dp1_mb4_gpu8 -> layer9_expert1_mlp1_dp1_mb4_gpu9
	layer9_expert1_mlp2_dp0_mb4_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb4_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb4_gpu9 -> layer9_expert1_mlp2_dp0_mb4_gpu9
	layer9_expert1_mlp1_dp1_mb4_gpu9 -> layer9_expert1_mlp2_dp1_mb4_gpu9
	layer9_expert2_mlp1_dp0_mb4_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb4_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb4_gpu8 -> layer9_expert2_mlp1_dp0_mb4_gpu10
	layer9_router_dp1_mb4_gpu8 -> layer9_expert2_mlp1_dp1_mb4_gpu10
	layer9_expert2_mlp2_dp0_mb4_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb4_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb4_gpu10 -> layer9_expert2_mlp2_dp0_mb4_gpu10
	layer9_expert2_mlp1_dp1_mb4_gpu10 -> layer9_expert2_mlp2_dp1_mb4_gpu10
	layer9_expert3_mlp1_dp0_mb4_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb4_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb4_gpu8 -> layer9_expert3_mlp1_dp0_mb4_gpu11
	layer9_router_dp1_mb4_gpu8 -> layer9_expert3_mlp1_dp1_mb4_gpu11
	layer9_expert3_mlp2_dp0_mb4_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb4_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb4_gpu11 -> layer9_expert3_mlp2_dp0_mb4_gpu11
	layer9_expert3_mlp1_dp1_mb4_gpu11 -> layer9_expert3_mlp2_dp1_mb4_gpu11
	layer9_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb4_gpu8 -> layer9_expert_agg_dp0_mb4
	layer9_expert0_mlp2_dp1_mb4_gpu8 -> layer9_expert_agg_dp1_mb4
	layer9_expert1_mlp2_dp0_mb4_gpu9 -> layer9_expert_agg_dp0_mb4
	layer9_expert1_mlp2_dp1_mb4_gpu9 -> layer9_expert_agg_dp1_mb4
	layer9_expert2_mlp2_dp0_mb4_gpu10 -> layer9_expert_agg_dp0_mb4
	layer9_expert2_mlp2_dp1_mb4_gpu10 -> layer9_expert_agg_dp1_mb4
	layer9_expert3_mlp2_dp0_mb4_gpu11 -> layer9_expert_agg_dp0_mb4
	layer9_expert3_mlp2_dp1_mb4_gpu11 -> layer9_expert_agg_dp1_mb4
	layer10_qkv_dp0_mb4_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb4_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb4 -> layer10_qkv_dp0_mb4_gpu8
	layer9_expert_agg_dp1_mb4 -> layer10_qkv_dp1_mb4_gpu8
	layer10_attn_scores_dp0_mb4_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb4_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb4_gpu8 -> layer10_attn_scores_dp0_mb4_gpu8
	layer10_qkv_dp1_mb4_gpu8 -> layer10_attn_scores_dp1_mb4_gpu8
	layer10_softmax_dp0_mb4_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb4_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb4_gpu8 -> layer10_softmax_dp0_mb4_gpu8
	layer10_attn_scores_dp1_mb4_gpu8 -> layer10_softmax_dp1_mb4_gpu8
	layer10_attn_out_dp0_mb4_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb4_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb4_gpu8 -> layer10_attn_out_dp0_mb4_gpu8
	layer10_softmax_dp1_mb4_gpu8 -> layer10_attn_out_dp1_mb4_gpu8
	layer10_qkv_dp0_mb4_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb4_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb4 -> layer10_qkv_dp0_mb4_gpu9
	layer9_expert_agg_dp1_mb4 -> layer10_qkv_dp1_mb4_gpu9
	layer10_attn_scores_dp0_mb4_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb4_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb4_gpu9 -> layer10_attn_scores_dp0_mb4_gpu9
	layer10_qkv_dp1_mb4_gpu9 -> layer10_attn_scores_dp1_mb4_gpu9
	layer10_softmax_dp0_mb4_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb4_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb4_gpu9 -> layer10_softmax_dp0_mb4_gpu9
	layer10_attn_scores_dp1_mb4_gpu9 -> layer10_softmax_dp1_mb4_gpu9
	layer10_attn_out_dp0_mb4_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb4_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb4_gpu9 -> layer10_attn_out_dp0_mb4_gpu9
	layer10_softmax_dp1_mb4_gpu9 -> layer10_attn_out_dp1_mb4_gpu9
	layer10_qkv_dp0_mb4_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb4_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb4 -> layer10_qkv_dp0_mb4_gpu10
	layer9_expert_agg_dp1_mb4 -> layer10_qkv_dp1_mb4_gpu10
	layer10_attn_scores_dp0_mb4_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb4_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb4_gpu10 -> layer10_attn_scores_dp0_mb4_gpu10
	layer10_qkv_dp1_mb4_gpu10 -> layer10_attn_scores_dp1_mb4_gpu10
	layer10_softmax_dp0_mb4_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb4_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb4_gpu10 -> layer10_softmax_dp0_mb4_gpu10
	layer10_attn_scores_dp1_mb4_gpu10 -> layer10_softmax_dp1_mb4_gpu10
	layer10_attn_out_dp0_mb4_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb4_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb4_gpu10 -> layer10_attn_out_dp0_mb4_gpu10
	layer10_softmax_dp1_mb4_gpu10 -> layer10_attn_out_dp1_mb4_gpu10
	layer10_qkv_dp0_mb4_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb4_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb4 -> layer10_qkv_dp0_mb4_gpu11
	layer9_expert_agg_dp1_mb4 -> layer10_qkv_dp1_mb4_gpu11
	layer10_attn_scores_dp0_mb4_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb4_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb4_gpu11 -> layer10_attn_scores_dp0_mb4_gpu11
	layer10_qkv_dp1_mb4_gpu11 -> layer10_attn_scores_dp1_mb4_gpu11
	layer10_softmax_dp0_mb4_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb4_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb4_gpu11 -> layer10_softmax_dp0_mb4_gpu11
	layer10_attn_scores_dp1_mb4_gpu11 -> layer10_softmax_dp1_mb4_gpu11
	layer10_attn_out_dp0_mb4_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb4_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb4_gpu11 -> layer10_attn_out_dp0_mb4_gpu11
	layer10_softmax_dp1_mb4_gpu11 -> layer10_attn_out_dp1_mb4_gpu11
	layer10_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb4_gpu8 -> layer10_attn_allreduce_dp0_mb4
	layer10_attn_out_dp1_mb4_gpu8 -> layer10_attn_allreduce_dp1_mb4
	layer10_attn_out_dp0_mb4_gpu9 -> layer10_attn_allreduce_dp0_mb4
	layer10_attn_out_dp1_mb4_gpu9 -> layer10_attn_allreduce_dp1_mb4
	layer10_attn_out_dp0_mb4_gpu10 -> layer10_attn_allreduce_dp0_mb4
	layer10_attn_out_dp1_mb4_gpu10 -> layer10_attn_allreduce_dp1_mb4
	layer10_attn_out_dp0_mb4_gpu11 -> layer10_attn_allreduce_dp0_mb4
	layer10_attn_out_dp1_mb4_gpu11 -> layer10_attn_allreduce_dp1_mb4
	layer10_router_dp0_mb4_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb4_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb4 -> layer10_router_dp0_mb4_gpu8
	layer10_attn_allreduce_dp1_mb4 -> layer10_router_dp1_mb4_gpu8
	layer10_expert0_mlp1_dp0_mb4_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb4_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb4_gpu8 -> layer10_expert0_mlp1_dp0_mb4_gpu8
	layer10_router_dp1_mb4_gpu8 -> layer10_expert0_mlp1_dp1_mb4_gpu8
	layer10_expert0_mlp2_dp0_mb4_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb4_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb4_gpu8 -> layer10_expert0_mlp2_dp0_mb4_gpu8
	layer10_expert0_mlp1_dp1_mb4_gpu8 -> layer10_expert0_mlp2_dp1_mb4_gpu8
	layer10_expert1_mlp1_dp0_mb4_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb4_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb4_gpu8 -> layer10_expert1_mlp1_dp0_mb4_gpu9
	layer10_router_dp1_mb4_gpu8 -> layer10_expert1_mlp1_dp1_mb4_gpu9
	layer10_expert1_mlp2_dp0_mb4_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb4_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb4_gpu9 -> layer10_expert1_mlp2_dp0_mb4_gpu9
	layer10_expert1_mlp1_dp1_mb4_gpu9 -> layer10_expert1_mlp2_dp1_mb4_gpu9
	layer10_expert2_mlp1_dp0_mb4_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb4_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb4_gpu8 -> layer10_expert2_mlp1_dp0_mb4_gpu10
	layer10_router_dp1_mb4_gpu8 -> layer10_expert2_mlp1_dp1_mb4_gpu10
	layer10_expert2_mlp2_dp0_mb4_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb4_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb4_gpu10 -> layer10_expert2_mlp2_dp0_mb4_gpu10
	layer10_expert2_mlp1_dp1_mb4_gpu10 -> layer10_expert2_mlp2_dp1_mb4_gpu10
	layer10_expert3_mlp1_dp0_mb4_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb4_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb4_gpu8 -> layer10_expert3_mlp1_dp0_mb4_gpu11
	layer10_router_dp1_mb4_gpu8 -> layer10_expert3_mlp1_dp1_mb4_gpu11
	layer10_expert3_mlp2_dp0_mb4_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb4_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb4_gpu11 -> layer10_expert3_mlp2_dp0_mb4_gpu11
	layer10_expert3_mlp1_dp1_mb4_gpu11 -> layer10_expert3_mlp2_dp1_mb4_gpu11
	layer10_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb4_gpu8 -> layer10_expert_agg_dp0_mb4
	layer10_expert0_mlp2_dp1_mb4_gpu8 -> layer10_expert_agg_dp1_mb4
	layer10_expert1_mlp2_dp0_mb4_gpu9 -> layer10_expert_agg_dp0_mb4
	layer10_expert1_mlp2_dp1_mb4_gpu9 -> layer10_expert_agg_dp1_mb4
	layer10_expert2_mlp2_dp0_mb4_gpu10 -> layer10_expert_agg_dp0_mb4
	layer10_expert2_mlp2_dp1_mb4_gpu10 -> layer10_expert_agg_dp1_mb4
	layer10_expert3_mlp2_dp0_mb4_gpu11 -> layer10_expert_agg_dp0_mb4
	layer10_expert3_mlp2_dp1_mb4_gpu11 -> layer10_expert_agg_dp1_mb4
	layer11_qkv_dp0_mb4_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb4_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb4 -> layer11_qkv_dp0_mb4_gpu8
	layer10_expert_agg_dp1_mb4 -> layer11_qkv_dp1_mb4_gpu8
	layer11_attn_scores_dp0_mb4_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb4_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb4_gpu8 -> layer11_attn_scores_dp0_mb4_gpu8
	layer11_qkv_dp1_mb4_gpu8 -> layer11_attn_scores_dp1_mb4_gpu8
	layer11_softmax_dp0_mb4_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb4_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb4_gpu8 -> layer11_softmax_dp0_mb4_gpu8
	layer11_attn_scores_dp1_mb4_gpu8 -> layer11_softmax_dp1_mb4_gpu8
	layer11_attn_out_dp0_mb4_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb4_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb4_gpu8 -> layer11_attn_out_dp0_mb4_gpu8
	layer11_softmax_dp1_mb4_gpu8 -> layer11_attn_out_dp1_mb4_gpu8
	layer11_qkv_dp0_mb4_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb4_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb4 -> layer11_qkv_dp0_mb4_gpu9
	layer10_expert_agg_dp1_mb4 -> layer11_qkv_dp1_mb4_gpu9
	layer11_attn_scores_dp0_mb4_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb4_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb4_gpu9 -> layer11_attn_scores_dp0_mb4_gpu9
	layer11_qkv_dp1_mb4_gpu9 -> layer11_attn_scores_dp1_mb4_gpu9
	layer11_softmax_dp0_mb4_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb4_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb4_gpu9 -> layer11_softmax_dp0_mb4_gpu9
	layer11_attn_scores_dp1_mb4_gpu9 -> layer11_softmax_dp1_mb4_gpu9
	layer11_attn_out_dp0_mb4_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb4_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb4_gpu9 -> layer11_attn_out_dp0_mb4_gpu9
	layer11_softmax_dp1_mb4_gpu9 -> layer11_attn_out_dp1_mb4_gpu9
	layer11_qkv_dp0_mb4_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb4_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb4 -> layer11_qkv_dp0_mb4_gpu10
	layer10_expert_agg_dp1_mb4 -> layer11_qkv_dp1_mb4_gpu10
	layer11_attn_scores_dp0_mb4_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb4_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb4_gpu10 -> layer11_attn_scores_dp0_mb4_gpu10
	layer11_qkv_dp1_mb4_gpu10 -> layer11_attn_scores_dp1_mb4_gpu10
	layer11_softmax_dp0_mb4_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb4_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb4_gpu10 -> layer11_softmax_dp0_mb4_gpu10
	layer11_attn_scores_dp1_mb4_gpu10 -> layer11_softmax_dp1_mb4_gpu10
	layer11_attn_out_dp0_mb4_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb4_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb4_gpu10 -> layer11_attn_out_dp0_mb4_gpu10
	layer11_softmax_dp1_mb4_gpu10 -> layer11_attn_out_dp1_mb4_gpu10
	layer11_qkv_dp0_mb4_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb4_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb4 -> layer11_qkv_dp0_mb4_gpu11
	layer10_expert_agg_dp1_mb4 -> layer11_qkv_dp1_mb4_gpu11
	layer11_attn_scores_dp0_mb4_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb4_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb4_gpu11 -> layer11_attn_scores_dp0_mb4_gpu11
	layer11_qkv_dp1_mb4_gpu11 -> layer11_attn_scores_dp1_mb4_gpu11
	layer11_softmax_dp0_mb4_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb4_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb4_gpu11 -> layer11_softmax_dp0_mb4_gpu11
	layer11_attn_scores_dp1_mb4_gpu11 -> layer11_softmax_dp1_mb4_gpu11
	layer11_attn_out_dp0_mb4_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb4_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb4_gpu11 -> layer11_attn_out_dp0_mb4_gpu11
	layer11_softmax_dp1_mb4_gpu11 -> layer11_attn_out_dp1_mb4_gpu11
	layer11_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb4_gpu8 -> layer11_attn_allreduce_dp0_mb4
	layer11_attn_out_dp1_mb4_gpu8 -> layer11_attn_allreduce_dp1_mb4
	layer11_attn_out_dp0_mb4_gpu9 -> layer11_attn_allreduce_dp0_mb4
	layer11_attn_out_dp1_mb4_gpu9 -> layer11_attn_allreduce_dp1_mb4
	layer11_attn_out_dp0_mb4_gpu10 -> layer11_attn_allreduce_dp0_mb4
	layer11_attn_out_dp1_mb4_gpu10 -> layer11_attn_allreduce_dp1_mb4
	layer11_attn_out_dp0_mb4_gpu11 -> layer11_attn_allreduce_dp0_mb4
	layer11_attn_out_dp1_mb4_gpu11 -> layer11_attn_allreduce_dp1_mb4
	layer11_router_dp0_mb4_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb4_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb4 -> layer11_router_dp0_mb4_gpu8
	layer11_attn_allreduce_dp1_mb4 -> layer11_router_dp1_mb4_gpu8
	layer11_expert0_mlp1_dp0_mb4_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb4_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb4_gpu8 -> layer11_expert0_mlp1_dp0_mb4_gpu8
	layer11_router_dp1_mb4_gpu8 -> layer11_expert0_mlp1_dp1_mb4_gpu8
	layer11_expert0_mlp2_dp0_mb4_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb4_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb4_gpu8 -> layer11_expert0_mlp2_dp0_mb4_gpu8
	layer11_expert0_mlp1_dp1_mb4_gpu8 -> layer11_expert0_mlp2_dp1_mb4_gpu8
	layer11_expert1_mlp1_dp0_mb4_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb4_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb4_gpu8 -> layer11_expert1_mlp1_dp0_mb4_gpu9
	layer11_router_dp1_mb4_gpu8 -> layer11_expert1_mlp1_dp1_mb4_gpu9
	layer11_expert1_mlp2_dp0_mb4_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb4_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb4_gpu9 -> layer11_expert1_mlp2_dp0_mb4_gpu9
	layer11_expert1_mlp1_dp1_mb4_gpu9 -> layer11_expert1_mlp2_dp1_mb4_gpu9
	layer11_expert2_mlp1_dp0_mb4_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb4_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb4_gpu8 -> layer11_expert2_mlp1_dp0_mb4_gpu10
	layer11_router_dp1_mb4_gpu8 -> layer11_expert2_mlp1_dp1_mb4_gpu10
	layer11_expert2_mlp2_dp0_mb4_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb4_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb4_gpu10 -> layer11_expert2_mlp2_dp0_mb4_gpu10
	layer11_expert2_mlp1_dp1_mb4_gpu10 -> layer11_expert2_mlp2_dp1_mb4_gpu10
	layer11_expert3_mlp1_dp0_mb4_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb4_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb4_gpu8 -> layer11_expert3_mlp1_dp0_mb4_gpu11
	layer11_router_dp1_mb4_gpu8 -> layer11_expert3_mlp1_dp1_mb4_gpu11
	layer11_expert3_mlp2_dp0_mb4_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb4_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb4_gpu11 -> layer11_expert3_mlp2_dp0_mb4_gpu11
	layer11_expert3_mlp1_dp1_mb4_gpu11 -> layer11_expert3_mlp2_dp1_mb4_gpu11
	layer11_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb4_gpu8 -> layer11_expert_agg_dp0_mb4
	layer11_expert0_mlp2_dp1_mb4_gpu8 -> layer11_expert_agg_dp1_mb4
	layer11_expert1_mlp2_dp0_mb4_gpu9 -> layer11_expert_agg_dp0_mb4
	layer11_expert1_mlp2_dp1_mb4_gpu9 -> layer11_expert_agg_dp1_mb4
	layer11_expert2_mlp2_dp0_mb4_gpu10 -> layer11_expert_agg_dp0_mb4
	layer11_expert2_mlp2_dp1_mb4_gpu10 -> layer11_expert_agg_dp1_mb4
	layer11_expert3_mlp2_dp0_mb4_gpu11 -> layer11_expert_agg_dp0_mb4
	layer11_expert3_mlp2_dp1_mb4_gpu11 -> layer11_expert_agg_dp1_mb4
	layer8_qkv_dp0_mb5_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb5_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb5 -> layer8_qkv_dp0_mb5_gpu8
	layer7_expert_agg_dp1_mb5 -> layer8_qkv_dp1_mb5_gpu8
	layer8_attn_scores_dp0_mb5_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb5_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb5_gpu8 -> layer8_attn_scores_dp0_mb5_gpu8
	layer8_qkv_dp1_mb5_gpu8 -> layer8_attn_scores_dp1_mb5_gpu8
	layer8_softmax_dp0_mb5_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb5_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb5_gpu8 -> layer8_softmax_dp0_mb5_gpu8
	layer8_attn_scores_dp1_mb5_gpu8 -> layer8_softmax_dp1_mb5_gpu8
	layer8_attn_out_dp0_mb5_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb5_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb5_gpu8 -> layer8_attn_out_dp0_mb5_gpu8
	layer8_softmax_dp1_mb5_gpu8 -> layer8_attn_out_dp1_mb5_gpu8
	layer8_qkv_dp0_mb5_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb5_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb5 -> layer8_qkv_dp0_mb5_gpu9
	layer7_expert_agg_dp1_mb5 -> layer8_qkv_dp1_mb5_gpu9
	layer8_attn_scores_dp0_mb5_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb5_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb5_gpu9 -> layer8_attn_scores_dp0_mb5_gpu9
	layer8_qkv_dp1_mb5_gpu9 -> layer8_attn_scores_dp1_mb5_gpu9
	layer8_softmax_dp0_mb5_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb5_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb5_gpu9 -> layer8_softmax_dp0_mb5_gpu9
	layer8_attn_scores_dp1_mb5_gpu9 -> layer8_softmax_dp1_mb5_gpu9
	layer8_attn_out_dp0_mb5_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb5_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb5_gpu9 -> layer8_attn_out_dp0_mb5_gpu9
	layer8_softmax_dp1_mb5_gpu9 -> layer8_attn_out_dp1_mb5_gpu9
	layer8_qkv_dp0_mb5_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb5_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb5 -> layer8_qkv_dp0_mb5_gpu10
	layer7_expert_agg_dp1_mb5 -> layer8_qkv_dp1_mb5_gpu10
	layer8_attn_scores_dp0_mb5_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb5_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb5_gpu10 -> layer8_attn_scores_dp0_mb5_gpu10
	layer8_qkv_dp1_mb5_gpu10 -> layer8_attn_scores_dp1_mb5_gpu10
	layer8_softmax_dp0_mb5_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb5_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb5_gpu10 -> layer8_softmax_dp0_mb5_gpu10
	layer8_attn_scores_dp1_mb5_gpu10 -> layer8_softmax_dp1_mb5_gpu10
	layer8_attn_out_dp0_mb5_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb5_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb5_gpu10 -> layer8_attn_out_dp0_mb5_gpu10
	layer8_softmax_dp1_mb5_gpu10 -> layer8_attn_out_dp1_mb5_gpu10
	layer8_qkv_dp0_mb5_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb5_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb5 -> layer8_qkv_dp0_mb5_gpu11
	layer7_expert_agg_dp1_mb5 -> layer8_qkv_dp1_mb5_gpu11
	layer8_attn_scores_dp0_mb5_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb5_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb5_gpu11 -> layer8_attn_scores_dp0_mb5_gpu11
	layer8_qkv_dp1_mb5_gpu11 -> layer8_attn_scores_dp1_mb5_gpu11
	layer8_softmax_dp0_mb5_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb5_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb5_gpu11 -> layer8_softmax_dp0_mb5_gpu11
	layer8_attn_scores_dp1_mb5_gpu11 -> layer8_softmax_dp1_mb5_gpu11
	layer8_attn_out_dp0_mb5_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb5_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb5_gpu11 -> layer8_attn_out_dp0_mb5_gpu11
	layer8_softmax_dp1_mb5_gpu11 -> layer8_attn_out_dp1_mb5_gpu11
	layer8_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb5_gpu8 -> layer8_attn_allreduce_dp0_mb5
	layer8_attn_out_dp1_mb5_gpu8 -> layer8_attn_allreduce_dp1_mb5
	layer8_attn_out_dp0_mb5_gpu9 -> layer8_attn_allreduce_dp0_mb5
	layer8_attn_out_dp1_mb5_gpu9 -> layer8_attn_allreduce_dp1_mb5
	layer8_attn_out_dp0_mb5_gpu10 -> layer8_attn_allreduce_dp0_mb5
	layer8_attn_out_dp1_mb5_gpu10 -> layer8_attn_allreduce_dp1_mb5
	layer8_attn_out_dp0_mb5_gpu11 -> layer8_attn_allreduce_dp0_mb5
	layer8_attn_out_dp1_mb5_gpu11 -> layer8_attn_allreduce_dp1_mb5
	layer8_router_dp0_mb5_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb5_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb5 -> layer8_router_dp0_mb5_gpu8
	layer8_attn_allreduce_dp1_mb5 -> layer8_router_dp1_mb5_gpu8
	layer8_expert0_mlp1_dp0_mb5_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb5_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb5_gpu8 -> layer8_expert0_mlp1_dp0_mb5_gpu8
	layer8_router_dp1_mb5_gpu8 -> layer8_expert0_mlp1_dp1_mb5_gpu8
	layer8_expert0_mlp2_dp0_mb5_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb5_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb5_gpu8 -> layer8_expert0_mlp2_dp0_mb5_gpu8
	layer8_expert0_mlp1_dp1_mb5_gpu8 -> layer8_expert0_mlp2_dp1_mb5_gpu8
	layer8_expert1_mlp1_dp0_mb5_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb5_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb5_gpu8 -> layer8_expert1_mlp1_dp0_mb5_gpu9
	layer8_router_dp1_mb5_gpu8 -> layer8_expert1_mlp1_dp1_mb5_gpu9
	layer8_expert1_mlp2_dp0_mb5_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb5_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb5_gpu9 -> layer8_expert1_mlp2_dp0_mb5_gpu9
	layer8_expert1_mlp1_dp1_mb5_gpu9 -> layer8_expert1_mlp2_dp1_mb5_gpu9
	layer8_expert2_mlp1_dp0_mb5_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb5_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb5_gpu8 -> layer8_expert2_mlp1_dp0_mb5_gpu10
	layer8_router_dp1_mb5_gpu8 -> layer8_expert2_mlp1_dp1_mb5_gpu10
	layer8_expert2_mlp2_dp0_mb5_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb5_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb5_gpu10 -> layer8_expert2_mlp2_dp0_mb5_gpu10
	layer8_expert2_mlp1_dp1_mb5_gpu10 -> layer8_expert2_mlp2_dp1_mb5_gpu10
	layer8_expert3_mlp1_dp0_mb5_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb5_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb5_gpu8 -> layer8_expert3_mlp1_dp0_mb5_gpu11
	layer8_router_dp1_mb5_gpu8 -> layer8_expert3_mlp1_dp1_mb5_gpu11
	layer8_expert3_mlp2_dp0_mb5_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb5_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb5_gpu11 -> layer8_expert3_mlp2_dp0_mb5_gpu11
	layer8_expert3_mlp1_dp1_mb5_gpu11 -> layer8_expert3_mlp2_dp1_mb5_gpu11
	layer8_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb5_gpu8 -> layer8_expert_agg_dp0_mb5
	layer8_expert0_mlp2_dp1_mb5_gpu8 -> layer8_expert_agg_dp1_mb5
	layer8_expert1_mlp2_dp0_mb5_gpu9 -> layer8_expert_agg_dp0_mb5
	layer8_expert1_mlp2_dp1_mb5_gpu9 -> layer8_expert_agg_dp1_mb5
	layer8_expert2_mlp2_dp0_mb5_gpu10 -> layer8_expert_agg_dp0_mb5
	layer8_expert2_mlp2_dp1_mb5_gpu10 -> layer8_expert_agg_dp1_mb5
	layer8_expert3_mlp2_dp0_mb5_gpu11 -> layer8_expert_agg_dp0_mb5
	layer8_expert3_mlp2_dp1_mb5_gpu11 -> layer8_expert_agg_dp1_mb5
	layer9_qkv_dp0_mb5_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb5_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb5 -> layer9_qkv_dp0_mb5_gpu8
	layer8_expert_agg_dp1_mb5 -> layer9_qkv_dp1_mb5_gpu8
	layer9_attn_scores_dp0_mb5_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb5_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb5_gpu8 -> layer9_attn_scores_dp0_mb5_gpu8
	layer9_qkv_dp1_mb5_gpu8 -> layer9_attn_scores_dp1_mb5_gpu8
	layer9_softmax_dp0_mb5_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb5_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb5_gpu8 -> layer9_softmax_dp0_mb5_gpu8
	layer9_attn_scores_dp1_mb5_gpu8 -> layer9_softmax_dp1_mb5_gpu8
	layer9_attn_out_dp0_mb5_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb5_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb5_gpu8 -> layer9_attn_out_dp0_mb5_gpu8
	layer9_softmax_dp1_mb5_gpu8 -> layer9_attn_out_dp1_mb5_gpu8
	layer9_qkv_dp0_mb5_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb5_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb5 -> layer9_qkv_dp0_mb5_gpu9
	layer8_expert_agg_dp1_mb5 -> layer9_qkv_dp1_mb5_gpu9
	layer9_attn_scores_dp0_mb5_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb5_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb5_gpu9 -> layer9_attn_scores_dp0_mb5_gpu9
	layer9_qkv_dp1_mb5_gpu9 -> layer9_attn_scores_dp1_mb5_gpu9
	layer9_softmax_dp0_mb5_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb5_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb5_gpu9 -> layer9_softmax_dp0_mb5_gpu9
	layer9_attn_scores_dp1_mb5_gpu9 -> layer9_softmax_dp1_mb5_gpu9
	layer9_attn_out_dp0_mb5_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb5_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb5_gpu9 -> layer9_attn_out_dp0_mb5_gpu9
	layer9_softmax_dp1_mb5_gpu9 -> layer9_attn_out_dp1_mb5_gpu9
	layer9_qkv_dp0_mb5_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb5_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb5 -> layer9_qkv_dp0_mb5_gpu10
	layer8_expert_agg_dp1_mb5 -> layer9_qkv_dp1_mb5_gpu10
	layer9_attn_scores_dp0_mb5_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb5_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb5_gpu10 -> layer9_attn_scores_dp0_mb5_gpu10
	layer9_qkv_dp1_mb5_gpu10 -> layer9_attn_scores_dp1_mb5_gpu10
	layer9_softmax_dp0_mb5_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb5_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb5_gpu10 -> layer9_softmax_dp0_mb5_gpu10
	layer9_attn_scores_dp1_mb5_gpu10 -> layer9_softmax_dp1_mb5_gpu10
	layer9_attn_out_dp0_mb5_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb5_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb5_gpu10 -> layer9_attn_out_dp0_mb5_gpu10
	layer9_softmax_dp1_mb5_gpu10 -> layer9_attn_out_dp1_mb5_gpu10
	layer9_qkv_dp0_mb5_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb5_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb5 -> layer9_qkv_dp0_mb5_gpu11
	layer8_expert_agg_dp1_mb5 -> layer9_qkv_dp1_mb5_gpu11
	layer9_attn_scores_dp0_mb5_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb5_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb5_gpu11 -> layer9_attn_scores_dp0_mb5_gpu11
	layer9_qkv_dp1_mb5_gpu11 -> layer9_attn_scores_dp1_mb5_gpu11
	layer9_softmax_dp0_mb5_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb5_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb5_gpu11 -> layer9_softmax_dp0_mb5_gpu11
	layer9_attn_scores_dp1_mb5_gpu11 -> layer9_softmax_dp1_mb5_gpu11
	layer9_attn_out_dp0_mb5_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb5_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb5_gpu11 -> layer9_attn_out_dp0_mb5_gpu11
	layer9_softmax_dp1_mb5_gpu11 -> layer9_attn_out_dp1_mb5_gpu11
	layer9_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb5_gpu8 -> layer9_attn_allreduce_dp0_mb5
	layer9_attn_out_dp1_mb5_gpu8 -> layer9_attn_allreduce_dp1_mb5
	layer9_attn_out_dp0_mb5_gpu9 -> layer9_attn_allreduce_dp0_mb5
	layer9_attn_out_dp1_mb5_gpu9 -> layer9_attn_allreduce_dp1_mb5
	layer9_attn_out_dp0_mb5_gpu10 -> layer9_attn_allreduce_dp0_mb5
	layer9_attn_out_dp1_mb5_gpu10 -> layer9_attn_allreduce_dp1_mb5
	layer9_attn_out_dp0_mb5_gpu11 -> layer9_attn_allreduce_dp0_mb5
	layer9_attn_out_dp1_mb5_gpu11 -> layer9_attn_allreduce_dp1_mb5
	layer9_router_dp0_mb5_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb5_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb5 -> layer9_router_dp0_mb5_gpu8
	layer9_attn_allreduce_dp1_mb5 -> layer9_router_dp1_mb5_gpu8
	layer9_expert0_mlp1_dp0_mb5_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb5_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb5_gpu8 -> layer9_expert0_mlp1_dp0_mb5_gpu8
	layer9_router_dp1_mb5_gpu8 -> layer9_expert0_mlp1_dp1_mb5_gpu8
	layer9_expert0_mlp2_dp0_mb5_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb5_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb5_gpu8 -> layer9_expert0_mlp2_dp0_mb5_gpu8
	layer9_expert0_mlp1_dp1_mb5_gpu8 -> layer9_expert0_mlp2_dp1_mb5_gpu8
	layer9_expert1_mlp1_dp0_mb5_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb5_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb5_gpu8 -> layer9_expert1_mlp1_dp0_mb5_gpu9
	layer9_router_dp1_mb5_gpu8 -> layer9_expert1_mlp1_dp1_mb5_gpu9
	layer9_expert1_mlp2_dp0_mb5_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb5_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb5_gpu9 -> layer9_expert1_mlp2_dp0_mb5_gpu9
	layer9_expert1_mlp1_dp1_mb5_gpu9 -> layer9_expert1_mlp2_dp1_mb5_gpu9
	layer9_expert2_mlp1_dp0_mb5_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb5_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb5_gpu8 -> layer9_expert2_mlp1_dp0_mb5_gpu10
	layer9_router_dp1_mb5_gpu8 -> layer9_expert2_mlp1_dp1_mb5_gpu10
	layer9_expert2_mlp2_dp0_mb5_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb5_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb5_gpu10 -> layer9_expert2_mlp2_dp0_mb5_gpu10
	layer9_expert2_mlp1_dp1_mb5_gpu10 -> layer9_expert2_mlp2_dp1_mb5_gpu10
	layer9_expert3_mlp1_dp0_mb5_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb5_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb5_gpu8 -> layer9_expert3_mlp1_dp0_mb5_gpu11
	layer9_router_dp1_mb5_gpu8 -> layer9_expert3_mlp1_dp1_mb5_gpu11
	layer9_expert3_mlp2_dp0_mb5_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb5_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb5_gpu11 -> layer9_expert3_mlp2_dp0_mb5_gpu11
	layer9_expert3_mlp1_dp1_mb5_gpu11 -> layer9_expert3_mlp2_dp1_mb5_gpu11
	layer9_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb5_gpu8 -> layer9_expert_agg_dp0_mb5
	layer9_expert0_mlp2_dp1_mb5_gpu8 -> layer9_expert_agg_dp1_mb5
	layer9_expert1_mlp2_dp0_mb5_gpu9 -> layer9_expert_agg_dp0_mb5
	layer9_expert1_mlp2_dp1_mb5_gpu9 -> layer9_expert_agg_dp1_mb5
	layer9_expert2_mlp2_dp0_mb5_gpu10 -> layer9_expert_agg_dp0_mb5
	layer9_expert2_mlp2_dp1_mb5_gpu10 -> layer9_expert_agg_dp1_mb5
	layer9_expert3_mlp2_dp0_mb5_gpu11 -> layer9_expert_agg_dp0_mb5
	layer9_expert3_mlp2_dp1_mb5_gpu11 -> layer9_expert_agg_dp1_mb5
	layer10_qkv_dp0_mb5_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb5_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb5 -> layer10_qkv_dp0_mb5_gpu8
	layer9_expert_agg_dp1_mb5 -> layer10_qkv_dp1_mb5_gpu8
	layer10_attn_scores_dp0_mb5_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb5_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb5_gpu8 -> layer10_attn_scores_dp0_mb5_gpu8
	layer10_qkv_dp1_mb5_gpu8 -> layer10_attn_scores_dp1_mb5_gpu8
	layer10_softmax_dp0_mb5_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb5_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb5_gpu8 -> layer10_softmax_dp0_mb5_gpu8
	layer10_attn_scores_dp1_mb5_gpu8 -> layer10_softmax_dp1_mb5_gpu8
	layer10_attn_out_dp0_mb5_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb5_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb5_gpu8 -> layer10_attn_out_dp0_mb5_gpu8
	layer10_softmax_dp1_mb5_gpu8 -> layer10_attn_out_dp1_mb5_gpu8
	layer10_qkv_dp0_mb5_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb5_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb5 -> layer10_qkv_dp0_mb5_gpu9
	layer9_expert_agg_dp1_mb5 -> layer10_qkv_dp1_mb5_gpu9
	layer10_attn_scores_dp0_mb5_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb5_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb5_gpu9 -> layer10_attn_scores_dp0_mb5_gpu9
	layer10_qkv_dp1_mb5_gpu9 -> layer10_attn_scores_dp1_mb5_gpu9
	layer10_softmax_dp0_mb5_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb5_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb5_gpu9 -> layer10_softmax_dp0_mb5_gpu9
	layer10_attn_scores_dp1_mb5_gpu9 -> layer10_softmax_dp1_mb5_gpu9
	layer10_attn_out_dp0_mb5_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb5_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb5_gpu9 -> layer10_attn_out_dp0_mb5_gpu9
	layer10_softmax_dp1_mb5_gpu9 -> layer10_attn_out_dp1_mb5_gpu9
	layer10_qkv_dp0_mb5_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb5_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb5 -> layer10_qkv_dp0_mb5_gpu10
	layer9_expert_agg_dp1_mb5 -> layer10_qkv_dp1_mb5_gpu10
	layer10_attn_scores_dp0_mb5_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb5_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb5_gpu10 -> layer10_attn_scores_dp0_mb5_gpu10
	layer10_qkv_dp1_mb5_gpu10 -> layer10_attn_scores_dp1_mb5_gpu10
	layer10_softmax_dp0_mb5_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb5_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb5_gpu10 -> layer10_softmax_dp0_mb5_gpu10
	layer10_attn_scores_dp1_mb5_gpu10 -> layer10_softmax_dp1_mb5_gpu10
	layer10_attn_out_dp0_mb5_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb5_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb5_gpu10 -> layer10_attn_out_dp0_mb5_gpu10
	layer10_softmax_dp1_mb5_gpu10 -> layer10_attn_out_dp1_mb5_gpu10
	layer10_qkv_dp0_mb5_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb5_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb5 -> layer10_qkv_dp0_mb5_gpu11
	layer9_expert_agg_dp1_mb5 -> layer10_qkv_dp1_mb5_gpu11
	layer10_attn_scores_dp0_mb5_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb5_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb5_gpu11 -> layer10_attn_scores_dp0_mb5_gpu11
	layer10_qkv_dp1_mb5_gpu11 -> layer10_attn_scores_dp1_mb5_gpu11
	layer10_softmax_dp0_mb5_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb5_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb5_gpu11 -> layer10_softmax_dp0_mb5_gpu11
	layer10_attn_scores_dp1_mb5_gpu11 -> layer10_softmax_dp1_mb5_gpu11
	layer10_attn_out_dp0_mb5_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb5_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb5_gpu11 -> layer10_attn_out_dp0_mb5_gpu11
	layer10_softmax_dp1_mb5_gpu11 -> layer10_attn_out_dp1_mb5_gpu11
	layer10_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb5_gpu8 -> layer10_attn_allreduce_dp0_mb5
	layer10_attn_out_dp1_mb5_gpu8 -> layer10_attn_allreduce_dp1_mb5
	layer10_attn_out_dp0_mb5_gpu9 -> layer10_attn_allreduce_dp0_mb5
	layer10_attn_out_dp1_mb5_gpu9 -> layer10_attn_allreduce_dp1_mb5
	layer10_attn_out_dp0_mb5_gpu10 -> layer10_attn_allreduce_dp0_mb5
	layer10_attn_out_dp1_mb5_gpu10 -> layer10_attn_allreduce_dp1_mb5
	layer10_attn_out_dp0_mb5_gpu11 -> layer10_attn_allreduce_dp0_mb5
	layer10_attn_out_dp1_mb5_gpu11 -> layer10_attn_allreduce_dp1_mb5
	layer10_router_dp0_mb5_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb5_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb5 -> layer10_router_dp0_mb5_gpu8
	layer10_attn_allreduce_dp1_mb5 -> layer10_router_dp1_mb5_gpu8
	layer10_expert0_mlp1_dp0_mb5_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb5_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb5_gpu8 -> layer10_expert0_mlp1_dp0_mb5_gpu8
	layer10_router_dp1_mb5_gpu8 -> layer10_expert0_mlp1_dp1_mb5_gpu8
	layer10_expert0_mlp2_dp0_mb5_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb5_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb5_gpu8 -> layer10_expert0_mlp2_dp0_mb5_gpu8
	layer10_expert0_mlp1_dp1_mb5_gpu8 -> layer10_expert0_mlp2_dp1_mb5_gpu8
	layer10_expert1_mlp1_dp0_mb5_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb5_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb5_gpu8 -> layer10_expert1_mlp1_dp0_mb5_gpu9
	layer10_router_dp1_mb5_gpu8 -> layer10_expert1_mlp1_dp1_mb5_gpu9
	layer10_expert1_mlp2_dp0_mb5_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb5_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb5_gpu9 -> layer10_expert1_mlp2_dp0_mb5_gpu9
	layer10_expert1_mlp1_dp1_mb5_gpu9 -> layer10_expert1_mlp2_dp1_mb5_gpu9
	layer10_expert2_mlp1_dp0_mb5_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb5_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb5_gpu8 -> layer10_expert2_mlp1_dp0_mb5_gpu10
	layer10_router_dp1_mb5_gpu8 -> layer10_expert2_mlp1_dp1_mb5_gpu10
	layer10_expert2_mlp2_dp0_mb5_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb5_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb5_gpu10 -> layer10_expert2_mlp2_dp0_mb5_gpu10
	layer10_expert2_mlp1_dp1_mb5_gpu10 -> layer10_expert2_mlp2_dp1_mb5_gpu10
	layer10_expert3_mlp1_dp0_mb5_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb5_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb5_gpu8 -> layer10_expert3_mlp1_dp0_mb5_gpu11
	layer10_router_dp1_mb5_gpu8 -> layer10_expert3_mlp1_dp1_mb5_gpu11
	layer10_expert3_mlp2_dp0_mb5_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb5_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb5_gpu11 -> layer10_expert3_mlp2_dp0_mb5_gpu11
	layer10_expert3_mlp1_dp1_mb5_gpu11 -> layer10_expert3_mlp2_dp1_mb5_gpu11
	layer10_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb5_gpu8 -> layer10_expert_agg_dp0_mb5
	layer10_expert0_mlp2_dp1_mb5_gpu8 -> layer10_expert_agg_dp1_mb5
	layer10_expert1_mlp2_dp0_mb5_gpu9 -> layer10_expert_agg_dp0_mb5
	layer10_expert1_mlp2_dp1_mb5_gpu9 -> layer10_expert_agg_dp1_mb5
	layer10_expert2_mlp2_dp0_mb5_gpu10 -> layer10_expert_agg_dp0_mb5
	layer10_expert2_mlp2_dp1_mb5_gpu10 -> layer10_expert_agg_dp1_mb5
	layer10_expert3_mlp2_dp0_mb5_gpu11 -> layer10_expert_agg_dp0_mb5
	layer10_expert3_mlp2_dp1_mb5_gpu11 -> layer10_expert_agg_dp1_mb5
	layer11_qkv_dp0_mb5_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb5_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb5 -> layer11_qkv_dp0_mb5_gpu8
	layer10_expert_agg_dp1_mb5 -> layer11_qkv_dp1_mb5_gpu8
	layer11_attn_scores_dp0_mb5_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb5_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb5_gpu8 -> layer11_attn_scores_dp0_mb5_gpu8
	layer11_qkv_dp1_mb5_gpu8 -> layer11_attn_scores_dp1_mb5_gpu8
	layer11_softmax_dp0_mb5_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb5_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb5_gpu8 -> layer11_softmax_dp0_mb5_gpu8
	layer11_attn_scores_dp1_mb5_gpu8 -> layer11_softmax_dp1_mb5_gpu8
	layer11_attn_out_dp0_mb5_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb5_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb5_gpu8 -> layer11_attn_out_dp0_mb5_gpu8
	layer11_softmax_dp1_mb5_gpu8 -> layer11_attn_out_dp1_mb5_gpu8
	layer11_qkv_dp0_mb5_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb5_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb5 -> layer11_qkv_dp0_mb5_gpu9
	layer10_expert_agg_dp1_mb5 -> layer11_qkv_dp1_mb5_gpu9
	layer11_attn_scores_dp0_mb5_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb5_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb5_gpu9 -> layer11_attn_scores_dp0_mb5_gpu9
	layer11_qkv_dp1_mb5_gpu9 -> layer11_attn_scores_dp1_mb5_gpu9
	layer11_softmax_dp0_mb5_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb5_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb5_gpu9 -> layer11_softmax_dp0_mb5_gpu9
	layer11_attn_scores_dp1_mb5_gpu9 -> layer11_softmax_dp1_mb5_gpu9
	layer11_attn_out_dp0_mb5_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb5_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb5_gpu9 -> layer11_attn_out_dp0_mb5_gpu9
	layer11_softmax_dp1_mb5_gpu9 -> layer11_attn_out_dp1_mb5_gpu9
	layer11_qkv_dp0_mb5_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb5_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb5 -> layer11_qkv_dp0_mb5_gpu10
	layer10_expert_agg_dp1_mb5 -> layer11_qkv_dp1_mb5_gpu10
	layer11_attn_scores_dp0_mb5_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb5_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb5_gpu10 -> layer11_attn_scores_dp0_mb5_gpu10
	layer11_qkv_dp1_mb5_gpu10 -> layer11_attn_scores_dp1_mb5_gpu10
	layer11_softmax_dp0_mb5_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb5_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb5_gpu10 -> layer11_softmax_dp0_mb5_gpu10
	layer11_attn_scores_dp1_mb5_gpu10 -> layer11_softmax_dp1_mb5_gpu10
	layer11_attn_out_dp0_mb5_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb5_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb5_gpu10 -> layer11_attn_out_dp0_mb5_gpu10
	layer11_softmax_dp1_mb5_gpu10 -> layer11_attn_out_dp1_mb5_gpu10
	layer11_qkv_dp0_mb5_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb5_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb5 -> layer11_qkv_dp0_mb5_gpu11
	layer10_expert_agg_dp1_mb5 -> layer11_qkv_dp1_mb5_gpu11
	layer11_attn_scores_dp0_mb5_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb5_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb5_gpu11 -> layer11_attn_scores_dp0_mb5_gpu11
	layer11_qkv_dp1_mb5_gpu11 -> layer11_attn_scores_dp1_mb5_gpu11
	layer11_softmax_dp0_mb5_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb5_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb5_gpu11 -> layer11_softmax_dp0_mb5_gpu11
	layer11_attn_scores_dp1_mb5_gpu11 -> layer11_softmax_dp1_mb5_gpu11
	layer11_attn_out_dp0_mb5_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb5_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb5_gpu11 -> layer11_attn_out_dp0_mb5_gpu11
	layer11_softmax_dp1_mb5_gpu11 -> layer11_attn_out_dp1_mb5_gpu11
	layer11_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb5_gpu8 -> layer11_attn_allreduce_dp0_mb5
	layer11_attn_out_dp1_mb5_gpu8 -> layer11_attn_allreduce_dp1_mb5
	layer11_attn_out_dp0_mb5_gpu9 -> layer11_attn_allreduce_dp0_mb5
	layer11_attn_out_dp1_mb5_gpu9 -> layer11_attn_allreduce_dp1_mb5
	layer11_attn_out_dp0_mb5_gpu10 -> layer11_attn_allreduce_dp0_mb5
	layer11_attn_out_dp1_mb5_gpu10 -> layer11_attn_allreduce_dp1_mb5
	layer11_attn_out_dp0_mb5_gpu11 -> layer11_attn_allreduce_dp0_mb5
	layer11_attn_out_dp1_mb5_gpu11 -> layer11_attn_allreduce_dp1_mb5
	layer11_router_dp0_mb5_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb5_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb5 -> layer11_router_dp0_mb5_gpu8
	layer11_attn_allreduce_dp1_mb5 -> layer11_router_dp1_mb5_gpu8
	layer11_expert0_mlp1_dp0_mb5_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb5_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb5_gpu8 -> layer11_expert0_mlp1_dp0_mb5_gpu8
	layer11_router_dp1_mb5_gpu8 -> layer11_expert0_mlp1_dp1_mb5_gpu8
	layer11_expert0_mlp2_dp0_mb5_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb5_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb5_gpu8 -> layer11_expert0_mlp2_dp0_mb5_gpu8
	layer11_expert0_mlp1_dp1_mb5_gpu8 -> layer11_expert0_mlp2_dp1_mb5_gpu8
	layer11_expert1_mlp1_dp0_mb5_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb5_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb5_gpu8 -> layer11_expert1_mlp1_dp0_mb5_gpu9
	layer11_router_dp1_mb5_gpu8 -> layer11_expert1_mlp1_dp1_mb5_gpu9
	layer11_expert1_mlp2_dp0_mb5_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb5_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb5_gpu9 -> layer11_expert1_mlp2_dp0_mb5_gpu9
	layer11_expert1_mlp1_dp1_mb5_gpu9 -> layer11_expert1_mlp2_dp1_mb5_gpu9
	layer11_expert2_mlp1_dp0_mb5_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb5_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb5_gpu8 -> layer11_expert2_mlp1_dp0_mb5_gpu10
	layer11_router_dp1_mb5_gpu8 -> layer11_expert2_mlp1_dp1_mb5_gpu10
	layer11_expert2_mlp2_dp0_mb5_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb5_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb5_gpu10 -> layer11_expert2_mlp2_dp0_mb5_gpu10
	layer11_expert2_mlp1_dp1_mb5_gpu10 -> layer11_expert2_mlp2_dp1_mb5_gpu10
	layer11_expert3_mlp1_dp0_mb5_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb5_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb5_gpu8 -> layer11_expert3_mlp1_dp0_mb5_gpu11
	layer11_router_dp1_mb5_gpu8 -> layer11_expert3_mlp1_dp1_mb5_gpu11
	layer11_expert3_mlp2_dp0_mb5_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb5_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb5_gpu11 -> layer11_expert3_mlp2_dp0_mb5_gpu11
	layer11_expert3_mlp1_dp1_mb5_gpu11 -> layer11_expert3_mlp2_dp1_mb5_gpu11
	layer11_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb5_gpu8 -> layer11_expert_agg_dp0_mb5
	layer11_expert0_mlp2_dp1_mb5_gpu8 -> layer11_expert_agg_dp1_mb5
	layer11_expert1_mlp2_dp0_mb5_gpu9 -> layer11_expert_agg_dp0_mb5
	layer11_expert1_mlp2_dp1_mb5_gpu9 -> layer11_expert_agg_dp1_mb5
	layer11_expert2_mlp2_dp0_mb5_gpu10 -> layer11_expert_agg_dp0_mb5
	layer11_expert2_mlp2_dp1_mb5_gpu10 -> layer11_expert_agg_dp1_mb5
	layer11_expert3_mlp2_dp0_mb5_gpu11 -> layer11_expert_agg_dp0_mb5
	layer11_expert3_mlp2_dp1_mb5_gpu11 -> layer11_expert_agg_dp1_mb5
	layer8_qkv_dp0_mb6_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb6_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb6 -> layer8_qkv_dp0_mb6_gpu8
	layer7_expert_agg_dp1_mb6 -> layer8_qkv_dp1_mb6_gpu8
	layer8_attn_scores_dp0_mb6_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb6_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb6_gpu8 -> layer8_attn_scores_dp0_mb6_gpu8
	layer8_qkv_dp1_mb6_gpu8 -> layer8_attn_scores_dp1_mb6_gpu8
	layer8_softmax_dp0_mb6_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb6_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb6_gpu8 -> layer8_softmax_dp0_mb6_gpu8
	layer8_attn_scores_dp1_mb6_gpu8 -> layer8_softmax_dp1_mb6_gpu8
	layer8_attn_out_dp0_mb6_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb6_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb6_gpu8 -> layer8_attn_out_dp0_mb6_gpu8
	layer8_softmax_dp1_mb6_gpu8 -> layer8_attn_out_dp1_mb6_gpu8
	layer8_qkv_dp0_mb6_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb6_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb6 -> layer8_qkv_dp0_mb6_gpu9
	layer7_expert_agg_dp1_mb6 -> layer8_qkv_dp1_mb6_gpu9
	layer8_attn_scores_dp0_mb6_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb6_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb6_gpu9 -> layer8_attn_scores_dp0_mb6_gpu9
	layer8_qkv_dp1_mb6_gpu9 -> layer8_attn_scores_dp1_mb6_gpu9
	layer8_softmax_dp0_mb6_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb6_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb6_gpu9 -> layer8_softmax_dp0_mb6_gpu9
	layer8_attn_scores_dp1_mb6_gpu9 -> layer8_softmax_dp1_mb6_gpu9
	layer8_attn_out_dp0_mb6_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb6_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb6_gpu9 -> layer8_attn_out_dp0_mb6_gpu9
	layer8_softmax_dp1_mb6_gpu9 -> layer8_attn_out_dp1_mb6_gpu9
	layer8_qkv_dp0_mb6_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb6_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb6 -> layer8_qkv_dp0_mb6_gpu10
	layer7_expert_agg_dp1_mb6 -> layer8_qkv_dp1_mb6_gpu10
	layer8_attn_scores_dp0_mb6_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb6_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb6_gpu10 -> layer8_attn_scores_dp0_mb6_gpu10
	layer8_qkv_dp1_mb6_gpu10 -> layer8_attn_scores_dp1_mb6_gpu10
	layer8_softmax_dp0_mb6_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb6_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb6_gpu10 -> layer8_softmax_dp0_mb6_gpu10
	layer8_attn_scores_dp1_mb6_gpu10 -> layer8_softmax_dp1_mb6_gpu10
	layer8_attn_out_dp0_mb6_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb6_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb6_gpu10 -> layer8_attn_out_dp0_mb6_gpu10
	layer8_softmax_dp1_mb6_gpu10 -> layer8_attn_out_dp1_mb6_gpu10
	layer8_qkv_dp0_mb6_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb6_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb6 -> layer8_qkv_dp0_mb6_gpu11
	layer7_expert_agg_dp1_mb6 -> layer8_qkv_dp1_mb6_gpu11
	layer8_attn_scores_dp0_mb6_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb6_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb6_gpu11 -> layer8_attn_scores_dp0_mb6_gpu11
	layer8_qkv_dp1_mb6_gpu11 -> layer8_attn_scores_dp1_mb6_gpu11
	layer8_softmax_dp0_mb6_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb6_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb6_gpu11 -> layer8_softmax_dp0_mb6_gpu11
	layer8_attn_scores_dp1_mb6_gpu11 -> layer8_softmax_dp1_mb6_gpu11
	layer8_attn_out_dp0_mb6_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb6_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb6_gpu11 -> layer8_attn_out_dp0_mb6_gpu11
	layer8_softmax_dp1_mb6_gpu11 -> layer8_attn_out_dp1_mb6_gpu11
	layer8_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb6_gpu8 -> layer8_attn_allreduce_dp0_mb6
	layer8_attn_out_dp1_mb6_gpu8 -> layer8_attn_allreduce_dp1_mb6
	layer8_attn_out_dp0_mb6_gpu9 -> layer8_attn_allreduce_dp0_mb6
	layer8_attn_out_dp1_mb6_gpu9 -> layer8_attn_allreduce_dp1_mb6
	layer8_attn_out_dp0_mb6_gpu10 -> layer8_attn_allreduce_dp0_mb6
	layer8_attn_out_dp1_mb6_gpu10 -> layer8_attn_allreduce_dp1_mb6
	layer8_attn_out_dp0_mb6_gpu11 -> layer8_attn_allreduce_dp0_mb6
	layer8_attn_out_dp1_mb6_gpu11 -> layer8_attn_allreduce_dp1_mb6
	layer8_router_dp0_mb6_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb6_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb6 -> layer8_router_dp0_mb6_gpu8
	layer8_attn_allreduce_dp1_mb6 -> layer8_router_dp1_mb6_gpu8
	layer8_expert0_mlp1_dp0_mb6_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb6_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb6_gpu8 -> layer8_expert0_mlp1_dp0_mb6_gpu8
	layer8_router_dp1_mb6_gpu8 -> layer8_expert0_mlp1_dp1_mb6_gpu8
	layer8_expert0_mlp2_dp0_mb6_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb6_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb6_gpu8 -> layer8_expert0_mlp2_dp0_mb6_gpu8
	layer8_expert0_mlp1_dp1_mb6_gpu8 -> layer8_expert0_mlp2_dp1_mb6_gpu8
	layer8_expert1_mlp1_dp0_mb6_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb6_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb6_gpu8 -> layer8_expert1_mlp1_dp0_mb6_gpu9
	layer8_router_dp1_mb6_gpu8 -> layer8_expert1_mlp1_dp1_mb6_gpu9
	layer8_expert1_mlp2_dp0_mb6_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb6_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb6_gpu9 -> layer8_expert1_mlp2_dp0_mb6_gpu9
	layer8_expert1_mlp1_dp1_mb6_gpu9 -> layer8_expert1_mlp2_dp1_mb6_gpu9
	layer8_expert2_mlp1_dp0_mb6_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb6_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb6_gpu8 -> layer8_expert2_mlp1_dp0_mb6_gpu10
	layer8_router_dp1_mb6_gpu8 -> layer8_expert2_mlp1_dp1_mb6_gpu10
	layer8_expert2_mlp2_dp0_mb6_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb6_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb6_gpu10 -> layer8_expert2_mlp2_dp0_mb6_gpu10
	layer8_expert2_mlp1_dp1_mb6_gpu10 -> layer8_expert2_mlp2_dp1_mb6_gpu10
	layer8_expert3_mlp1_dp0_mb6_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb6_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb6_gpu8 -> layer8_expert3_mlp1_dp0_mb6_gpu11
	layer8_router_dp1_mb6_gpu8 -> layer8_expert3_mlp1_dp1_mb6_gpu11
	layer8_expert3_mlp2_dp0_mb6_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb6_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb6_gpu11 -> layer8_expert3_mlp2_dp0_mb6_gpu11
	layer8_expert3_mlp1_dp1_mb6_gpu11 -> layer8_expert3_mlp2_dp1_mb6_gpu11
	layer8_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb6_gpu8 -> layer8_expert_agg_dp0_mb6
	layer8_expert0_mlp2_dp1_mb6_gpu8 -> layer8_expert_agg_dp1_mb6
	layer8_expert1_mlp2_dp0_mb6_gpu9 -> layer8_expert_agg_dp0_mb6
	layer8_expert1_mlp2_dp1_mb6_gpu9 -> layer8_expert_agg_dp1_mb6
	layer8_expert2_mlp2_dp0_mb6_gpu10 -> layer8_expert_agg_dp0_mb6
	layer8_expert2_mlp2_dp1_mb6_gpu10 -> layer8_expert_agg_dp1_mb6
	layer8_expert3_mlp2_dp0_mb6_gpu11 -> layer8_expert_agg_dp0_mb6
	layer8_expert3_mlp2_dp1_mb6_gpu11 -> layer8_expert_agg_dp1_mb6
	layer9_qkv_dp0_mb6_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb6_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb6 -> layer9_qkv_dp0_mb6_gpu8
	layer8_expert_agg_dp1_mb6 -> layer9_qkv_dp1_mb6_gpu8
	layer9_attn_scores_dp0_mb6_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb6_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb6_gpu8 -> layer9_attn_scores_dp0_mb6_gpu8
	layer9_qkv_dp1_mb6_gpu8 -> layer9_attn_scores_dp1_mb6_gpu8
	layer9_softmax_dp0_mb6_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb6_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb6_gpu8 -> layer9_softmax_dp0_mb6_gpu8
	layer9_attn_scores_dp1_mb6_gpu8 -> layer9_softmax_dp1_mb6_gpu8
	layer9_attn_out_dp0_mb6_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb6_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb6_gpu8 -> layer9_attn_out_dp0_mb6_gpu8
	layer9_softmax_dp1_mb6_gpu8 -> layer9_attn_out_dp1_mb6_gpu8
	layer9_qkv_dp0_mb6_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb6_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb6 -> layer9_qkv_dp0_mb6_gpu9
	layer8_expert_agg_dp1_mb6 -> layer9_qkv_dp1_mb6_gpu9
	layer9_attn_scores_dp0_mb6_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb6_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb6_gpu9 -> layer9_attn_scores_dp0_mb6_gpu9
	layer9_qkv_dp1_mb6_gpu9 -> layer9_attn_scores_dp1_mb6_gpu9
	layer9_softmax_dp0_mb6_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb6_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb6_gpu9 -> layer9_softmax_dp0_mb6_gpu9
	layer9_attn_scores_dp1_mb6_gpu9 -> layer9_softmax_dp1_mb6_gpu9
	layer9_attn_out_dp0_mb6_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb6_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb6_gpu9 -> layer9_attn_out_dp0_mb6_gpu9
	layer9_softmax_dp1_mb6_gpu9 -> layer9_attn_out_dp1_mb6_gpu9
	layer9_qkv_dp0_mb6_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb6_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb6 -> layer9_qkv_dp0_mb6_gpu10
	layer8_expert_agg_dp1_mb6 -> layer9_qkv_dp1_mb6_gpu10
	layer9_attn_scores_dp0_mb6_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb6_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb6_gpu10 -> layer9_attn_scores_dp0_mb6_gpu10
	layer9_qkv_dp1_mb6_gpu10 -> layer9_attn_scores_dp1_mb6_gpu10
	layer9_softmax_dp0_mb6_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb6_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb6_gpu10 -> layer9_softmax_dp0_mb6_gpu10
	layer9_attn_scores_dp1_mb6_gpu10 -> layer9_softmax_dp1_mb6_gpu10
	layer9_attn_out_dp0_mb6_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb6_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb6_gpu10 -> layer9_attn_out_dp0_mb6_gpu10
	layer9_softmax_dp1_mb6_gpu10 -> layer9_attn_out_dp1_mb6_gpu10
	layer9_qkv_dp0_mb6_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb6_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb6 -> layer9_qkv_dp0_mb6_gpu11
	layer8_expert_agg_dp1_mb6 -> layer9_qkv_dp1_mb6_gpu11
	layer9_attn_scores_dp0_mb6_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb6_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb6_gpu11 -> layer9_attn_scores_dp0_mb6_gpu11
	layer9_qkv_dp1_mb6_gpu11 -> layer9_attn_scores_dp1_mb6_gpu11
	layer9_softmax_dp0_mb6_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb6_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb6_gpu11 -> layer9_softmax_dp0_mb6_gpu11
	layer9_attn_scores_dp1_mb6_gpu11 -> layer9_softmax_dp1_mb6_gpu11
	layer9_attn_out_dp0_mb6_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb6_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb6_gpu11 -> layer9_attn_out_dp0_mb6_gpu11
	layer9_softmax_dp1_mb6_gpu11 -> layer9_attn_out_dp1_mb6_gpu11
	layer9_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb6_gpu8 -> layer9_attn_allreduce_dp0_mb6
	layer9_attn_out_dp1_mb6_gpu8 -> layer9_attn_allreduce_dp1_mb6
	layer9_attn_out_dp0_mb6_gpu9 -> layer9_attn_allreduce_dp0_mb6
	layer9_attn_out_dp1_mb6_gpu9 -> layer9_attn_allreduce_dp1_mb6
	layer9_attn_out_dp0_mb6_gpu10 -> layer9_attn_allreduce_dp0_mb6
	layer9_attn_out_dp1_mb6_gpu10 -> layer9_attn_allreduce_dp1_mb6
	layer9_attn_out_dp0_mb6_gpu11 -> layer9_attn_allreduce_dp0_mb6
	layer9_attn_out_dp1_mb6_gpu11 -> layer9_attn_allreduce_dp1_mb6
	layer9_router_dp0_mb6_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb6_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb6 -> layer9_router_dp0_mb6_gpu8
	layer9_attn_allreduce_dp1_mb6 -> layer9_router_dp1_mb6_gpu8
	layer9_expert0_mlp1_dp0_mb6_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb6_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb6_gpu8 -> layer9_expert0_mlp1_dp0_mb6_gpu8
	layer9_router_dp1_mb6_gpu8 -> layer9_expert0_mlp1_dp1_mb6_gpu8
	layer9_expert0_mlp2_dp0_mb6_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb6_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb6_gpu8 -> layer9_expert0_mlp2_dp0_mb6_gpu8
	layer9_expert0_mlp1_dp1_mb6_gpu8 -> layer9_expert0_mlp2_dp1_mb6_gpu8
	layer9_expert1_mlp1_dp0_mb6_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb6_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb6_gpu8 -> layer9_expert1_mlp1_dp0_mb6_gpu9
	layer9_router_dp1_mb6_gpu8 -> layer9_expert1_mlp1_dp1_mb6_gpu9
	layer9_expert1_mlp2_dp0_mb6_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb6_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb6_gpu9 -> layer9_expert1_mlp2_dp0_mb6_gpu9
	layer9_expert1_mlp1_dp1_mb6_gpu9 -> layer9_expert1_mlp2_dp1_mb6_gpu9
	layer9_expert2_mlp1_dp0_mb6_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb6_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb6_gpu8 -> layer9_expert2_mlp1_dp0_mb6_gpu10
	layer9_router_dp1_mb6_gpu8 -> layer9_expert2_mlp1_dp1_mb6_gpu10
	layer9_expert2_mlp2_dp0_mb6_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb6_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb6_gpu10 -> layer9_expert2_mlp2_dp0_mb6_gpu10
	layer9_expert2_mlp1_dp1_mb6_gpu10 -> layer9_expert2_mlp2_dp1_mb6_gpu10
	layer9_expert3_mlp1_dp0_mb6_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb6_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb6_gpu8 -> layer9_expert3_mlp1_dp0_mb6_gpu11
	layer9_router_dp1_mb6_gpu8 -> layer9_expert3_mlp1_dp1_mb6_gpu11
	layer9_expert3_mlp2_dp0_mb6_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb6_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb6_gpu11 -> layer9_expert3_mlp2_dp0_mb6_gpu11
	layer9_expert3_mlp1_dp1_mb6_gpu11 -> layer9_expert3_mlp2_dp1_mb6_gpu11
	layer9_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb6_gpu8 -> layer9_expert_agg_dp0_mb6
	layer9_expert0_mlp2_dp1_mb6_gpu8 -> layer9_expert_agg_dp1_mb6
	layer9_expert1_mlp2_dp0_mb6_gpu9 -> layer9_expert_agg_dp0_mb6
	layer9_expert1_mlp2_dp1_mb6_gpu9 -> layer9_expert_agg_dp1_mb6
	layer9_expert2_mlp2_dp0_mb6_gpu10 -> layer9_expert_agg_dp0_mb6
	layer9_expert2_mlp2_dp1_mb6_gpu10 -> layer9_expert_agg_dp1_mb6
	layer9_expert3_mlp2_dp0_mb6_gpu11 -> layer9_expert_agg_dp0_mb6
	layer9_expert3_mlp2_dp1_mb6_gpu11 -> layer9_expert_agg_dp1_mb6
	layer10_qkv_dp0_mb6_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb6_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb6 -> layer10_qkv_dp0_mb6_gpu8
	layer9_expert_agg_dp1_mb6 -> layer10_qkv_dp1_mb6_gpu8
	layer10_attn_scores_dp0_mb6_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb6_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb6_gpu8 -> layer10_attn_scores_dp0_mb6_gpu8
	layer10_qkv_dp1_mb6_gpu8 -> layer10_attn_scores_dp1_mb6_gpu8
	layer10_softmax_dp0_mb6_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb6_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb6_gpu8 -> layer10_softmax_dp0_mb6_gpu8
	layer10_attn_scores_dp1_mb6_gpu8 -> layer10_softmax_dp1_mb6_gpu8
	layer10_attn_out_dp0_mb6_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb6_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb6_gpu8 -> layer10_attn_out_dp0_mb6_gpu8
	layer10_softmax_dp1_mb6_gpu8 -> layer10_attn_out_dp1_mb6_gpu8
	layer10_qkv_dp0_mb6_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb6_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb6 -> layer10_qkv_dp0_mb6_gpu9
	layer9_expert_agg_dp1_mb6 -> layer10_qkv_dp1_mb6_gpu9
	layer10_attn_scores_dp0_mb6_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb6_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb6_gpu9 -> layer10_attn_scores_dp0_mb6_gpu9
	layer10_qkv_dp1_mb6_gpu9 -> layer10_attn_scores_dp1_mb6_gpu9
	layer10_softmax_dp0_mb6_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb6_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb6_gpu9 -> layer10_softmax_dp0_mb6_gpu9
	layer10_attn_scores_dp1_mb6_gpu9 -> layer10_softmax_dp1_mb6_gpu9
	layer10_attn_out_dp0_mb6_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb6_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb6_gpu9 -> layer10_attn_out_dp0_mb6_gpu9
	layer10_softmax_dp1_mb6_gpu9 -> layer10_attn_out_dp1_mb6_gpu9
	layer10_qkv_dp0_mb6_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb6_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb6 -> layer10_qkv_dp0_mb6_gpu10
	layer9_expert_agg_dp1_mb6 -> layer10_qkv_dp1_mb6_gpu10
	layer10_attn_scores_dp0_mb6_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb6_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb6_gpu10 -> layer10_attn_scores_dp0_mb6_gpu10
	layer10_qkv_dp1_mb6_gpu10 -> layer10_attn_scores_dp1_mb6_gpu10
	layer10_softmax_dp0_mb6_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb6_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb6_gpu10 -> layer10_softmax_dp0_mb6_gpu10
	layer10_attn_scores_dp1_mb6_gpu10 -> layer10_softmax_dp1_mb6_gpu10
	layer10_attn_out_dp0_mb6_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb6_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb6_gpu10 -> layer10_attn_out_dp0_mb6_gpu10
	layer10_softmax_dp1_mb6_gpu10 -> layer10_attn_out_dp1_mb6_gpu10
	layer10_qkv_dp0_mb6_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb6_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb6 -> layer10_qkv_dp0_mb6_gpu11
	layer9_expert_agg_dp1_mb6 -> layer10_qkv_dp1_mb6_gpu11
	layer10_attn_scores_dp0_mb6_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb6_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb6_gpu11 -> layer10_attn_scores_dp0_mb6_gpu11
	layer10_qkv_dp1_mb6_gpu11 -> layer10_attn_scores_dp1_mb6_gpu11
	layer10_softmax_dp0_mb6_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb6_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb6_gpu11 -> layer10_softmax_dp0_mb6_gpu11
	layer10_attn_scores_dp1_mb6_gpu11 -> layer10_softmax_dp1_mb6_gpu11
	layer10_attn_out_dp0_mb6_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb6_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb6_gpu11 -> layer10_attn_out_dp0_mb6_gpu11
	layer10_softmax_dp1_mb6_gpu11 -> layer10_attn_out_dp1_mb6_gpu11
	layer10_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb6_gpu8 -> layer10_attn_allreduce_dp0_mb6
	layer10_attn_out_dp1_mb6_gpu8 -> layer10_attn_allreduce_dp1_mb6
	layer10_attn_out_dp0_mb6_gpu9 -> layer10_attn_allreduce_dp0_mb6
	layer10_attn_out_dp1_mb6_gpu9 -> layer10_attn_allreduce_dp1_mb6
	layer10_attn_out_dp0_mb6_gpu10 -> layer10_attn_allreduce_dp0_mb6
	layer10_attn_out_dp1_mb6_gpu10 -> layer10_attn_allreduce_dp1_mb6
	layer10_attn_out_dp0_mb6_gpu11 -> layer10_attn_allreduce_dp0_mb6
	layer10_attn_out_dp1_mb6_gpu11 -> layer10_attn_allreduce_dp1_mb6
	layer10_router_dp0_mb6_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb6_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb6 -> layer10_router_dp0_mb6_gpu8
	layer10_attn_allreduce_dp1_mb6 -> layer10_router_dp1_mb6_gpu8
	layer10_expert0_mlp1_dp0_mb6_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb6_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb6_gpu8 -> layer10_expert0_mlp1_dp0_mb6_gpu8
	layer10_router_dp1_mb6_gpu8 -> layer10_expert0_mlp1_dp1_mb6_gpu8
	layer10_expert0_mlp2_dp0_mb6_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb6_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb6_gpu8 -> layer10_expert0_mlp2_dp0_mb6_gpu8
	layer10_expert0_mlp1_dp1_mb6_gpu8 -> layer10_expert0_mlp2_dp1_mb6_gpu8
	layer10_expert1_mlp1_dp0_mb6_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb6_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb6_gpu8 -> layer10_expert1_mlp1_dp0_mb6_gpu9
	layer10_router_dp1_mb6_gpu8 -> layer10_expert1_mlp1_dp1_mb6_gpu9
	layer10_expert1_mlp2_dp0_mb6_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb6_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb6_gpu9 -> layer10_expert1_mlp2_dp0_mb6_gpu9
	layer10_expert1_mlp1_dp1_mb6_gpu9 -> layer10_expert1_mlp2_dp1_mb6_gpu9
	layer10_expert2_mlp1_dp0_mb6_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb6_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb6_gpu8 -> layer10_expert2_mlp1_dp0_mb6_gpu10
	layer10_router_dp1_mb6_gpu8 -> layer10_expert2_mlp1_dp1_mb6_gpu10
	layer10_expert2_mlp2_dp0_mb6_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb6_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb6_gpu10 -> layer10_expert2_mlp2_dp0_mb6_gpu10
	layer10_expert2_mlp1_dp1_mb6_gpu10 -> layer10_expert2_mlp2_dp1_mb6_gpu10
	layer10_expert3_mlp1_dp0_mb6_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb6_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb6_gpu8 -> layer10_expert3_mlp1_dp0_mb6_gpu11
	layer10_router_dp1_mb6_gpu8 -> layer10_expert3_mlp1_dp1_mb6_gpu11
	layer10_expert3_mlp2_dp0_mb6_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb6_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb6_gpu11 -> layer10_expert3_mlp2_dp0_mb6_gpu11
	layer10_expert3_mlp1_dp1_mb6_gpu11 -> layer10_expert3_mlp2_dp1_mb6_gpu11
	layer10_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb6_gpu8 -> layer10_expert_agg_dp0_mb6
	layer10_expert0_mlp2_dp1_mb6_gpu8 -> layer10_expert_agg_dp1_mb6
	layer10_expert1_mlp2_dp0_mb6_gpu9 -> layer10_expert_agg_dp0_mb6
	layer10_expert1_mlp2_dp1_mb6_gpu9 -> layer10_expert_agg_dp1_mb6
	layer10_expert2_mlp2_dp0_mb6_gpu10 -> layer10_expert_agg_dp0_mb6
	layer10_expert2_mlp2_dp1_mb6_gpu10 -> layer10_expert_agg_dp1_mb6
	layer10_expert3_mlp2_dp0_mb6_gpu11 -> layer10_expert_agg_dp0_mb6
	layer10_expert3_mlp2_dp1_mb6_gpu11 -> layer10_expert_agg_dp1_mb6
	layer11_qkv_dp0_mb6_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb6_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb6 -> layer11_qkv_dp0_mb6_gpu8
	layer10_expert_agg_dp1_mb6 -> layer11_qkv_dp1_mb6_gpu8
	layer11_attn_scores_dp0_mb6_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb6_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb6_gpu8 -> layer11_attn_scores_dp0_mb6_gpu8
	layer11_qkv_dp1_mb6_gpu8 -> layer11_attn_scores_dp1_mb6_gpu8
	layer11_softmax_dp0_mb6_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb6_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb6_gpu8 -> layer11_softmax_dp0_mb6_gpu8
	layer11_attn_scores_dp1_mb6_gpu8 -> layer11_softmax_dp1_mb6_gpu8
	layer11_attn_out_dp0_mb6_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb6_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb6_gpu8 -> layer11_attn_out_dp0_mb6_gpu8
	layer11_softmax_dp1_mb6_gpu8 -> layer11_attn_out_dp1_mb6_gpu8
	layer11_qkv_dp0_mb6_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb6_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb6 -> layer11_qkv_dp0_mb6_gpu9
	layer10_expert_agg_dp1_mb6 -> layer11_qkv_dp1_mb6_gpu9
	layer11_attn_scores_dp0_mb6_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb6_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb6_gpu9 -> layer11_attn_scores_dp0_mb6_gpu9
	layer11_qkv_dp1_mb6_gpu9 -> layer11_attn_scores_dp1_mb6_gpu9
	layer11_softmax_dp0_mb6_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb6_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb6_gpu9 -> layer11_softmax_dp0_mb6_gpu9
	layer11_attn_scores_dp1_mb6_gpu9 -> layer11_softmax_dp1_mb6_gpu9
	layer11_attn_out_dp0_mb6_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb6_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb6_gpu9 -> layer11_attn_out_dp0_mb6_gpu9
	layer11_softmax_dp1_mb6_gpu9 -> layer11_attn_out_dp1_mb6_gpu9
	layer11_qkv_dp0_mb6_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb6_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb6 -> layer11_qkv_dp0_mb6_gpu10
	layer10_expert_agg_dp1_mb6 -> layer11_qkv_dp1_mb6_gpu10
	layer11_attn_scores_dp0_mb6_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb6_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb6_gpu10 -> layer11_attn_scores_dp0_mb6_gpu10
	layer11_qkv_dp1_mb6_gpu10 -> layer11_attn_scores_dp1_mb6_gpu10
	layer11_softmax_dp0_mb6_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb6_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb6_gpu10 -> layer11_softmax_dp0_mb6_gpu10
	layer11_attn_scores_dp1_mb6_gpu10 -> layer11_softmax_dp1_mb6_gpu10
	layer11_attn_out_dp0_mb6_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb6_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb6_gpu10 -> layer11_attn_out_dp0_mb6_gpu10
	layer11_softmax_dp1_mb6_gpu10 -> layer11_attn_out_dp1_mb6_gpu10
	layer11_qkv_dp0_mb6_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb6_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb6 -> layer11_qkv_dp0_mb6_gpu11
	layer10_expert_agg_dp1_mb6 -> layer11_qkv_dp1_mb6_gpu11
	layer11_attn_scores_dp0_mb6_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb6_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb6_gpu11 -> layer11_attn_scores_dp0_mb6_gpu11
	layer11_qkv_dp1_mb6_gpu11 -> layer11_attn_scores_dp1_mb6_gpu11
	layer11_softmax_dp0_mb6_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb6_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb6_gpu11 -> layer11_softmax_dp0_mb6_gpu11
	layer11_attn_scores_dp1_mb6_gpu11 -> layer11_softmax_dp1_mb6_gpu11
	layer11_attn_out_dp0_mb6_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb6_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb6_gpu11 -> layer11_attn_out_dp0_mb6_gpu11
	layer11_softmax_dp1_mb6_gpu11 -> layer11_attn_out_dp1_mb6_gpu11
	layer11_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb6_gpu8 -> layer11_attn_allreduce_dp0_mb6
	layer11_attn_out_dp1_mb6_gpu8 -> layer11_attn_allreduce_dp1_mb6
	layer11_attn_out_dp0_mb6_gpu9 -> layer11_attn_allreduce_dp0_mb6
	layer11_attn_out_dp1_mb6_gpu9 -> layer11_attn_allreduce_dp1_mb6
	layer11_attn_out_dp0_mb6_gpu10 -> layer11_attn_allreduce_dp0_mb6
	layer11_attn_out_dp1_mb6_gpu10 -> layer11_attn_allreduce_dp1_mb6
	layer11_attn_out_dp0_mb6_gpu11 -> layer11_attn_allreduce_dp0_mb6
	layer11_attn_out_dp1_mb6_gpu11 -> layer11_attn_allreduce_dp1_mb6
	layer11_router_dp0_mb6_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb6_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb6 -> layer11_router_dp0_mb6_gpu8
	layer11_attn_allreduce_dp1_mb6 -> layer11_router_dp1_mb6_gpu8
	layer11_expert0_mlp1_dp0_mb6_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb6_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb6_gpu8 -> layer11_expert0_mlp1_dp0_mb6_gpu8
	layer11_router_dp1_mb6_gpu8 -> layer11_expert0_mlp1_dp1_mb6_gpu8
	layer11_expert0_mlp2_dp0_mb6_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb6_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb6_gpu8 -> layer11_expert0_mlp2_dp0_mb6_gpu8
	layer11_expert0_mlp1_dp1_mb6_gpu8 -> layer11_expert0_mlp2_dp1_mb6_gpu8
	layer11_expert1_mlp1_dp0_mb6_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb6_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb6_gpu8 -> layer11_expert1_mlp1_dp0_mb6_gpu9
	layer11_router_dp1_mb6_gpu8 -> layer11_expert1_mlp1_dp1_mb6_gpu9
	layer11_expert1_mlp2_dp0_mb6_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb6_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb6_gpu9 -> layer11_expert1_mlp2_dp0_mb6_gpu9
	layer11_expert1_mlp1_dp1_mb6_gpu9 -> layer11_expert1_mlp2_dp1_mb6_gpu9
	layer11_expert2_mlp1_dp0_mb6_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb6_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb6_gpu8 -> layer11_expert2_mlp1_dp0_mb6_gpu10
	layer11_router_dp1_mb6_gpu8 -> layer11_expert2_mlp1_dp1_mb6_gpu10
	layer11_expert2_mlp2_dp0_mb6_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb6_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb6_gpu10 -> layer11_expert2_mlp2_dp0_mb6_gpu10
	layer11_expert2_mlp1_dp1_mb6_gpu10 -> layer11_expert2_mlp2_dp1_mb6_gpu10
	layer11_expert3_mlp1_dp0_mb6_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb6_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb6_gpu8 -> layer11_expert3_mlp1_dp0_mb6_gpu11
	layer11_router_dp1_mb6_gpu8 -> layer11_expert3_mlp1_dp1_mb6_gpu11
	layer11_expert3_mlp2_dp0_mb6_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb6_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb6_gpu11 -> layer11_expert3_mlp2_dp0_mb6_gpu11
	layer11_expert3_mlp1_dp1_mb6_gpu11 -> layer11_expert3_mlp2_dp1_mb6_gpu11
	layer11_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb6_gpu8 -> layer11_expert_agg_dp0_mb6
	layer11_expert0_mlp2_dp1_mb6_gpu8 -> layer11_expert_agg_dp1_mb6
	layer11_expert1_mlp2_dp0_mb6_gpu9 -> layer11_expert_agg_dp0_mb6
	layer11_expert1_mlp2_dp1_mb6_gpu9 -> layer11_expert_agg_dp1_mb6
	layer11_expert2_mlp2_dp0_mb6_gpu10 -> layer11_expert_agg_dp0_mb6
	layer11_expert2_mlp2_dp1_mb6_gpu10 -> layer11_expert_agg_dp1_mb6
	layer11_expert3_mlp2_dp0_mb6_gpu11 -> layer11_expert_agg_dp0_mb6
	layer11_expert3_mlp2_dp1_mb6_gpu11 -> layer11_expert_agg_dp1_mb6
	layer8_qkv_dp0_mb7_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb7_gpu8 [label="Layer 8 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb7 -> layer8_qkv_dp0_mb7_gpu8
	layer7_expert_agg_dp1_mb7 -> layer8_qkv_dp1_mb7_gpu8
	layer8_attn_scores_dp0_mb7_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb7_gpu8 [label="Layer 8 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb7_gpu8 -> layer8_attn_scores_dp0_mb7_gpu8
	layer8_qkv_dp1_mb7_gpu8 -> layer8_attn_scores_dp1_mb7_gpu8
	layer8_softmax_dp0_mb7_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb7_gpu8 [label="Layer 8 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb7_gpu8 -> layer8_softmax_dp0_mb7_gpu8
	layer8_attn_scores_dp1_mb7_gpu8 -> layer8_softmax_dp1_mb7_gpu8
	layer8_attn_out_dp0_mb7_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb7_gpu8 [label="Layer 8 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb7_gpu8 -> layer8_attn_out_dp0_mb7_gpu8
	layer8_softmax_dp1_mb7_gpu8 -> layer8_attn_out_dp1_mb7_gpu8
	layer8_qkv_dp0_mb7_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb7_gpu9 [label="Layer 8 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb7 -> layer8_qkv_dp0_mb7_gpu9
	layer7_expert_agg_dp1_mb7 -> layer8_qkv_dp1_mb7_gpu9
	layer8_attn_scores_dp0_mb7_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb7_gpu9 [label="Layer 8 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb7_gpu9 -> layer8_attn_scores_dp0_mb7_gpu9
	layer8_qkv_dp1_mb7_gpu9 -> layer8_attn_scores_dp1_mb7_gpu9
	layer8_softmax_dp0_mb7_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb7_gpu9 [label="Layer 8 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb7_gpu9 -> layer8_softmax_dp0_mb7_gpu9
	layer8_attn_scores_dp1_mb7_gpu9 -> layer8_softmax_dp1_mb7_gpu9
	layer8_attn_out_dp0_mb7_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb7_gpu9 [label="Layer 8 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb7_gpu9 -> layer8_attn_out_dp0_mb7_gpu9
	layer8_softmax_dp1_mb7_gpu9 -> layer8_attn_out_dp1_mb7_gpu9
	layer8_qkv_dp0_mb7_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb7_gpu10 [label="Layer 8 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb7 -> layer8_qkv_dp0_mb7_gpu10
	layer7_expert_agg_dp1_mb7 -> layer8_qkv_dp1_mb7_gpu10
	layer8_attn_scores_dp0_mb7_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb7_gpu10 [label="Layer 8 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb7_gpu10 -> layer8_attn_scores_dp0_mb7_gpu10
	layer8_qkv_dp1_mb7_gpu10 -> layer8_attn_scores_dp1_mb7_gpu10
	layer8_softmax_dp0_mb7_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb7_gpu10 [label="Layer 8 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb7_gpu10 -> layer8_softmax_dp0_mb7_gpu10
	layer8_attn_scores_dp1_mb7_gpu10 -> layer8_softmax_dp1_mb7_gpu10
	layer8_attn_out_dp0_mb7_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb7_gpu10 [label="Layer 8 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb7_gpu10 -> layer8_attn_out_dp0_mb7_gpu10
	layer8_softmax_dp1_mb7_gpu10 -> layer8_attn_out_dp1_mb7_gpu10
	layer8_qkv_dp0_mb7_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp1_mb7_gpu11 [label="Layer 8 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer7_expert_agg_dp0_mb7 -> layer8_qkv_dp0_mb7_gpu11
	layer7_expert_agg_dp1_mb7 -> layer8_qkv_dp1_mb7_gpu11
	layer8_attn_scores_dp0_mb7_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp1_mb7_gpu11 [label="Layer 8 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_qkv_dp0_mb7_gpu11 -> layer8_attn_scores_dp0_mb7_gpu11
	layer8_qkv_dp1_mb7_gpu11 -> layer8_attn_scores_dp1_mb7_gpu11
	layer8_softmax_dp0_mb7_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp1_mb7_gpu11 [label="Layer 8 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_scores_dp0_mb7_gpu11 -> layer8_softmax_dp0_mb7_gpu11
	layer8_attn_scores_dp1_mb7_gpu11 -> layer8_softmax_dp1_mb7_gpu11
	layer8_attn_out_dp0_mb7_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_attn_out_dp1_mb7_gpu11 [label="Layer 8 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_softmax_dp0_mb7_gpu11 -> layer8_attn_out_dp0_mb7_gpu11
	layer8_softmax_dp1_mb7_gpu11 -> layer8_attn_out_dp1_mb7_gpu11
	layer8_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_attn_out_dp0_mb7_gpu8 -> layer8_attn_allreduce_dp0_mb7
	layer8_attn_out_dp1_mb7_gpu8 -> layer8_attn_allreduce_dp1_mb7
	layer8_attn_out_dp0_mb7_gpu9 -> layer8_attn_allreduce_dp0_mb7
	layer8_attn_out_dp1_mb7_gpu9 -> layer8_attn_allreduce_dp1_mb7
	layer8_attn_out_dp0_mb7_gpu10 -> layer8_attn_allreduce_dp0_mb7
	layer8_attn_out_dp1_mb7_gpu10 -> layer8_attn_allreduce_dp1_mb7
	layer8_attn_out_dp0_mb7_gpu11 -> layer8_attn_allreduce_dp0_mb7
	layer8_attn_out_dp1_mb7_gpu11 -> layer8_attn_allreduce_dp1_mb7
	layer8_router_dp0_mb7_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_router_dp1_mb7_gpu8 [label="Layer 8 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer8_attn_allreduce_dp0_mb7 -> layer8_router_dp0_mb7_gpu8
	layer8_attn_allreduce_dp1_mb7 -> layer8_router_dp1_mb7_gpu8
	layer8_expert0_mlp1_dp0_mb7_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp1_mb7_gpu8 [label="Layer 8 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb7_gpu8 -> layer8_expert0_mlp1_dp0_mb7_gpu8
	layer8_router_dp1_mb7_gpu8 -> layer8_expert0_mlp1_dp1_mb7_gpu8
	layer8_expert0_mlp2_dp0_mb7_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp2_dp1_mb7_gpu8 [label="Layer 8 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert0_mlp1_dp0_mb7_gpu8 -> layer8_expert0_mlp2_dp0_mb7_gpu8
	layer8_expert0_mlp1_dp1_mb7_gpu8 -> layer8_expert0_mlp2_dp1_mb7_gpu8
	layer8_expert1_mlp1_dp0_mb7_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp1_mb7_gpu9 [label="Layer 8 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb7_gpu8 -> layer8_expert1_mlp1_dp0_mb7_gpu9
	layer8_router_dp1_mb7_gpu8 -> layer8_expert1_mlp1_dp1_mb7_gpu9
	layer8_expert1_mlp2_dp0_mb7_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp2_dp1_mb7_gpu9 [label="Layer 8 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert1_mlp1_dp0_mb7_gpu9 -> layer8_expert1_mlp2_dp0_mb7_gpu9
	layer8_expert1_mlp1_dp1_mb7_gpu9 -> layer8_expert1_mlp2_dp1_mb7_gpu9
	layer8_expert2_mlp1_dp0_mb7_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp1_mb7_gpu10 [label="Layer 8 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb7_gpu8 -> layer8_expert2_mlp1_dp0_mb7_gpu10
	layer8_router_dp1_mb7_gpu8 -> layer8_expert2_mlp1_dp1_mb7_gpu10
	layer8_expert2_mlp2_dp0_mb7_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp2_dp1_mb7_gpu10 [label="Layer 8 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert2_mlp1_dp0_mb7_gpu10 -> layer8_expert2_mlp2_dp0_mb7_gpu10
	layer8_expert2_mlp1_dp1_mb7_gpu10 -> layer8_expert2_mlp2_dp1_mb7_gpu10
	layer8_expert3_mlp1_dp0_mb7_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp1_mb7_gpu11 [label="Layer 8 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_router_dp0_mb7_gpu8 -> layer8_expert3_mlp1_dp0_mb7_gpu11
	layer8_router_dp1_mb7_gpu8 -> layer8_expert3_mlp1_dp1_mb7_gpu11
	layer8_expert3_mlp2_dp0_mb7_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp2_dp1_mb7_gpu11 [label="Layer 8 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert3_mlp1_dp0_mb7_gpu11 -> layer8_expert3_mlp2_dp0_mb7_gpu11
	layer8_expert3_mlp1_dp1_mb7_gpu11 -> layer8_expert3_mlp2_dp1_mb7_gpu11
	layer8_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer8_expert0_mlp2_dp0_mb7_gpu8 -> layer8_expert_agg_dp0_mb7
	layer8_expert0_mlp2_dp1_mb7_gpu8 -> layer8_expert_agg_dp1_mb7
	layer8_expert1_mlp2_dp0_mb7_gpu9 -> layer8_expert_agg_dp0_mb7
	layer8_expert1_mlp2_dp1_mb7_gpu9 -> layer8_expert_agg_dp1_mb7
	layer8_expert2_mlp2_dp0_mb7_gpu10 -> layer8_expert_agg_dp0_mb7
	layer8_expert2_mlp2_dp1_mb7_gpu10 -> layer8_expert_agg_dp1_mb7
	layer8_expert3_mlp2_dp0_mb7_gpu11 -> layer8_expert_agg_dp0_mb7
	layer8_expert3_mlp2_dp1_mb7_gpu11 -> layer8_expert_agg_dp1_mb7
	layer9_qkv_dp0_mb7_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb7_gpu8 [label="Layer 9 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb7 -> layer9_qkv_dp0_mb7_gpu8
	layer8_expert_agg_dp1_mb7 -> layer9_qkv_dp1_mb7_gpu8
	layer9_attn_scores_dp0_mb7_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb7_gpu8 [label="Layer 9 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb7_gpu8 -> layer9_attn_scores_dp0_mb7_gpu8
	layer9_qkv_dp1_mb7_gpu8 -> layer9_attn_scores_dp1_mb7_gpu8
	layer9_softmax_dp0_mb7_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb7_gpu8 [label="Layer 9 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb7_gpu8 -> layer9_softmax_dp0_mb7_gpu8
	layer9_attn_scores_dp1_mb7_gpu8 -> layer9_softmax_dp1_mb7_gpu8
	layer9_attn_out_dp0_mb7_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb7_gpu8 [label="Layer 9 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb7_gpu8 -> layer9_attn_out_dp0_mb7_gpu8
	layer9_softmax_dp1_mb7_gpu8 -> layer9_attn_out_dp1_mb7_gpu8
	layer9_qkv_dp0_mb7_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb7_gpu9 [label="Layer 9 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb7 -> layer9_qkv_dp0_mb7_gpu9
	layer8_expert_agg_dp1_mb7 -> layer9_qkv_dp1_mb7_gpu9
	layer9_attn_scores_dp0_mb7_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb7_gpu9 [label="Layer 9 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb7_gpu9 -> layer9_attn_scores_dp0_mb7_gpu9
	layer9_qkv_dp1_mb7_gpu9 -> layer9_attn_scores_dp1_mb7_gpu9
	layer9_softmax_dp0_mb7_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb7_gpu9 [label="Layer 9 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb7_gpu9 -> layer9_softmax_dp0_mb7_gpu9
	layer9_attn_scores_dp1_mb7_gpu9 -> layer9_softmax_dp1_mb7_gpu9
	layer9_attn_out_dp0_mb7_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb7_gpu9 [label="Layer 9 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb7_gpu9 -> layer9_attn_out_dp0_mb7_gpu9
	layer9_softmax_dp1_mb7_gpu9 -> layer9_attn_out_dp1_mb7_gpu9
	layer9_qkv_dp0_mb7_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb7_gpu10 [label="Layer 9 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb7 -> layer9_qkv_dp0_mb7_gpu10
	layer8_expert_agg_dp1_mb7 -> layer9_qkv_dp1_mb7_gpu10
	layer9_attn_scores_dp0_mb7_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb7_gpu10 [label="Layer 9 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb7_gpu10 -> layer9_attn_scores_dp0_mb7_gpu10
	layer9_qkv_dp1_mb7_gpu10 -> layer9_attn_scores_dp1_mb7_gpu10
	layer9_softmax_dp0_mb7_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb7_gpu10 [label="Layer 9 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb7_gpu10 -> layer9_softmax_dp0_mb7_gpu10
	layer9_attn_scores_dp1_mb7_gpu10 -> layer9_softmax_dp1_mb7_gpu10
	layer9_attn_out_dp0_mb7_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb7_gpu10 [label="Layer 9 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb7_gpu10 -> layer9_attn_out_dp0_mb7_gpu10
	layer9_softmax_dp1_mb7_gpu10 -> layer9_attn_out_dp1_mb7_gpu10
	layer9_qkv_dp0_mb7_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp1_mb7_gpu11 [label="Layer 9 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer8_expert_agg_dp0_mb7 -> layer9_qkv_dp0_mb7_gpu11
	layer8_expert_agg_dp1_mb7 -> layer9_qkv_dp1_mb7_gpu11
	layer9_attn_scores_dp0_mb7_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp1_mb7_gpu11 [label="Layer 9 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_qkv_dp0_mb7_gpu11 -> layer9_attn_scores_dp0_mb7_gpu11
	layer9_qkv_dp1_mb7_gpu11 -> layer9_attn_scores_dp1_mb7_gpu11
	layer9_softmax_dp0_mb7_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp1_mb7_gpu11 [label="Layer 9 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_scores_dp0_mb7_gpu11 -> layer9_softmax_dp0_mb7_gpu11
	layer9_attn_scores_dp1_mb7_gpu11 -> layer9_softmax_dp1_mb7_gpu11
	layer9_attn_out_dp0_mb7_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_attn_out_dp1_mb7_gpu11 [label="Layer 9 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_softmax_dp0_mb7_gpu11 -> layer9_attn_out_dp0_mb7_gpu11
	layer9_softmax_dp1_mb7_gpu11 -> layer9_attn_out_dp1_mb7_gpu11
	layer9_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_attn_out_dp0_mb7_gpu8 -> layer9_attn_allreduce_dp0_mb7
	layer9_attn_out_dp1_mb7_gpu8 -> layer9_attn_allreduce_dp1_mb7
	layer9_attn_out_dp0_mb7_gpu9 -> layer9_attn_allreduce_dp0_mb7
	layer9_attn_out_dp1_mb7_gpu9 -> layer9_attn_allreduce_dp1_mb7
	layer9_attn_out_dp0_mb7_gpu10 -> layer9_attn_allreduce_dp0_mb7
	layer9_attn_out_dp1_mb7_gpu10 -> layer9_attn_allreduce_dp1_mb7
	layer9_attn_out_dp0_mb7_gpu11 -> layer9_attn_allreduce_dp0_mb7
	layer9_attn_out_dp1_mb7_gpu11 -> layer9_attn_allreduce_dp1_mb7
	layer9_router_dp0_mb7_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_router_dp1_mb7_gpu8 [label="Layer 9 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer9_attn_allreduce_dp0_mb7 -> layer9_router_dp0_mb7_gpu8
	layer9_attn_allreduce_dp1_mb7 -> layer9_router_dp1_mb7_gpu8
	layer9_expert0_mlp1_dp0_mb7_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp1_mb7_gpu8 [label="Layer 9 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb7_gpu8 -> layer9_expert0_mlp1_dp0_mb7_gpu8
	layer9_router_dp1_mb7_gpu8 -> layer9_expert0_mlp1_dp1_mb7_gpu8
	layer9_expert0_mlp2_dp0_mb7_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp2_dp1_mb7_gpu8 [label="Layer 9 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert0_mlp1_dp0_mb7_gpu8 -> layer9_expert0_mlp2_dp0_mb7_gpu8
	layer9_expert0_mlp1_dp1_mb7_gpu8 -> layer9_expert0_mlp2_dp1_mb7_gpu8
	layer9_expert1_mlp1_dp0_mb7_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp1_mb7_gpu9 [label="Layer 9 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb7_gpu8 -> layer9_expert1_mlp1_dp0_mb7_gpu9
	layer9_router_dp1_mb7_gpu8 -> layer9_expert1_mlp1_dp1_mb7_gpu9
	layer9_expert1_mlp2_dp0_mb7_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp2_dp1_mb7_gpu9 [label="Layer 9 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert1_mlp1_dp0_mb7_gpu9 -> layer9_expert1_mlp2_dp0_mb7_gpu9
	layer9_expert1_mlp1_dp1_mb7_gpu9 -> layer9_expert1_mlp2_dp1_mb7_gpu9
	layer9_expert2_mlp1_dp0_mb7_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp1_mb7_gpu10 [label="Layer 9 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb7_gpu8 -> layer9_expert2_mlp1_dp0_mb7_gpu10
	layer9_router_dp1_mb7_gpu8 -> layer9_expert2_mlp1_dp1_mb7_gpu10
	layer9_expert2_mlp2_dp0_mb7_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp2_dp1_mb7_gpu10 [label="Layer 9 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert2_mlp1_dp0_mb7_gpu10 -> layer9_expert2_mlp2_dp0_mb7_gpu10
	layer9_expert2_mlp1_dp1_mb7_gpu10 -> layer9_expert2_mlp2_dp1_mb7_gpu10
	layer9_expert3_mlp1_dp0_mb7_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp1_mb7_gpu11 [label="Layer 9 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_router_dp0_mb7_gpu8 -> layer9_expert3_mlp1_dp0_mb7_gpu11
	layer9_router_dp1_mb7_gpu8 -> layer9_expert3_mlp1_dp1_mb7_gpu11
	layer9_expert3_mlp2_dp0_mb7_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp2_dp1_mb7_gpu11 [label="Layer 9 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert3_mlp1_dp0_mb7_gpu11 -> layer9_expert3_mlp2_dp0_mb7_gpu11
	layer9_expert3_mlp1_dp1_mb7_gpu11 -> layer9_expert3_mlp2_dp1_mb7_gpu11
	layer9_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer9_expert0_mlp2_dp0_mb7_gpu8 -> layer9_expert_agg_dp0_mb7
	layer9_expert0_mlp2_dp1_mb7_gpu8 -> layer9_expert_agg_dp1_mb7
	layer9_expert1_mlp2_dp0_mb7_gpu9 -> layer9_expert_agg_dp0_mb7
	layer9_expert1_mlp2_dp1_mb7_gpu9 -> layer9_expert_agg_dp1_mb7
	layer9_expert2_mlp2_dp0_mb7_gpu10 -> layer9_expert_agg_dp0_mb7
	layer9_expert2_mlp2_dp1_mb7_gpu10 -> layer9_expert_agg_dp1_mb7
	layer9_expert3_mlp2_dp0_mb7_gpu11 -> layer9_expert_agg_dp0_mb7
	layer9_expert3_mlp2_dp1_mb7_gpu11 -> layer9_expert_agg_dp1_mb7
	layer10_qkv_dp0_mb7_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb7_gpu8 [label="Layer 10 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb7 -> layer10_qkv_dp0_mb7_gpu8
	layer9_expert_agg_dp1_mb7 -> layer10_qkv_dp1_mb7_gpu8
	layer10_attn_scores_dp0_mb7_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb7_gpu8 [label="Layer 10 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb7_gpu8 -> layer10_attn_scores_dp0_mb7_gpu8
	layer10_qkv_dp1_mb7_gpu8 -> layer10_attn_scores_dp1_mb7_gpu8
	layer10_softmax_dp0_mb7_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb7_gpu8 [label="Layer 10 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb7_gpu8 -> layer10_softmax_dp0_mb7_gpu8
	layer10_attn_scores_dp1_mb7_gpu8 -> layer10_softmax_dp1_mb7_gpu8
	layer10_attn_out_dp0_mb7_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb7_gpu8 [label="Layer 10 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb7_gpu8 -> layer10_attn_out_dp0_mb7_gpu8
	layer10_softmax_dp1_mb7_gpu8 -> layer10_attn_out_dp1_mb7_gpu8
	layer10_qkv_dp0_mb7_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb7_gpu9 [label="Layer 10 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb7 -> layer10_qkv_dp0_mb7_gpu9
	layer9_expert_agg_dp1_mb7 -> layer10_qkv_dp1_mb7_gpu9
	layer10_attn_scores_dp0_mb7_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb7_gpu9 [label="Layer 10 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb7_gpu9 -> layer10_attn_scores_dp0_mb7_gpu9
	layer10_qkv_dp1_mb7_gpu9 -> layer10_attn_scores_dp1_mb7_gpu9
	layer10_softmax_dp0_mb7_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb7_gpu9 [label="Layer 10 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb7_gpu9 -> layer10_softmax_dp0_mb7_gpu9
	layer10_attn_scores_dp1_mb7_gpu9 -> layer10_softmax_dp1_mb7_gpu9
	layer10_attn_out_dp0_mb7_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb7_gpu9 [label="Layer 10 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb7_gpu9 -> layer10_attn_out_dp0_mb7_gpu9
	layer10_softmax_dp1_mb7_gpu9 -> layer10_attn_out_dp1_mb7_gpu9
	layer10_qkv_dp0_mb7_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb7_gpu10 [label="Layer 10 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb7 -> layer10_qkv_dp0_mb7_gpu10
	layer9_expert_agg_dp1_mb7 -> layer10_qkv_dp1_mb7_gpu10
	layer10_attn_scores_dp0_mb7_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb7_gpu10 [label="Layer 10 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb7_gpu10 -> layer10_attn_scores_dp0_mb7_gpu10
	layer10_qkv_dp1_mb7_gpu10 -> layer10_attn_scores_dp1_mb7_gpu10
	layer10_softmax_dp0_mb7_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb7_gpu10 [label="Layer 10 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb7_gpu10 -> layer10_softmax_dp0_mb7_gpu10
	layer10_attn_scores_dp1_mb7_gpu10 -> layer10_softmax_dp1_mb7_gpu10
	layer10_attn_out_dp0_mb7_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb7_gpu10 [label="Layer 10 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb7_gpu10 -> layer10_attn_out_dp0_mb7_gpu10
	layer10_softmax_dp1_mb7_gpu10 -> layer10_attn_out_dp1_mb7_gpu10
	layer10_qkv_dp0_mb7_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp1_mb7_gpu11 [label="Layer 10 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer9_expert_agg_dp0_mb7 -> layer10_qkv_dp0_mb7_gpu11
	layer9_expert_agg_dp1_mb7 -> layer10_qkv_dp1_mb7_gpu11
	layer10_attn_scores_dp0_mb7_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp1_mb7_gpu11 [label="Layer 10 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_qkv_dp0_mb7_gpu11 -> layer10_attn_scores_dp0_mb7_gpu11
	layer10_qkv_dp1_mb7_gpu11 -> layer10_attn_scores_dp1_mb7_gpu11
	layer10_softmax_dp0_mb7_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp1_mb7_gpu11 [label="Layer 10 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_scores_dp0_mb7_gpu11 -> layer10_softmax_dp0_mb7_gpu11
	layer10_attn_scores_dp1_mb7_gpu11 -> layer10_softmax_dp1_mb7_gpu11
	layer10_attn_out_dp0_mb7_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_attn_out_dp1_mb7_gpu11 [label="Layer 10 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_softmax_dp0_mb7_gpu11 -> layer10_attn_out_dp0_mb7_gpu11
	layer10_softmax_dp1_mb7_gpu11 -> layer10_attn_out_dp1_mb7_gpu11
	layer10_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_attn_out_dp0_mb7_gpu8 -> layer10_attn_allreduce_dp0_mb7
	layer10_attn_out_dp1_mb7_gpu8 -> layer10_attn_allreduce_dp1_mb7
	layer10_attn_out_dp0_mb7_gpu9 -> layer10_attn_allreduce_dp0_mb7
	layer10_attn_out_dp1_mb7_gpu9 -> layer10_attn_allreduce_dp1_mb7
	layer10_attn_out_dp0_mb7_gpu10 -> layer10_attn_allreduce_dp0_mb7
	layer10_attn_out_dp1_mb7_gpu10 -> layer10_attn_allreduce_dp1_mb7
	layer10_attn_out_dp0_mb7_gpu11 -> layer10_attn_allreduce_dp0_mb7
	layer10_attn_out_dp1_mb7_gpu11 -> layer10_attn_allreduce_dp1_mb7
	layer10_router_dp0_mb7_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_router_dp1_mb7_gpu8 [label="Layer 10 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer10_attn_allreduce_dp0_mb7 -> layer10_router_dp0_mb7_gpu8
	layer10_attn_allreduce_dp1_mb7 -> layer10_router_dp1_mb7_gpu8
	layer10_expert0_mlp1_dp0_mb7_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp1_mb7_gpu8 [label="Layer 10 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb7_gpu8 -> layer10_expert0_mlp1_dp0_mb7_gpu8
	layer10_router_dp1_mb7_gpu8 -> layer10_expert0_mlp1_dp1_mb7_gpu8
	layer10_expert0_mlp2_dp0_mb7_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp2_dp1_mb7_gpu8 [label="Layer 10 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert0_mlp1_dp0_mb7_gpu8 -> layer10_expert0_mlp2_dp0_mb7_gpu8
	layer10_expert0_mlp1_dp1_mb7_gpu8 -> layer10_expert0_mlp2_dp1_mb7_gpu8
	layer10_expert1_mlp1_dp0_mb7_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp1_mb7_gpu9 [label="Layer 10 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb7_gpu8 -> layer10_expert1_mlp1_dp0_mb7_gpu9
	layer10_router_dp1_mb7_gpu8 -> layer10_expert1_mlp1_dp1_mb7_gpu9
	layer10_expert1_mlp2_dp0_mb7_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp2_dp1_mb7_gpu9 [label="Layer 10 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert1_mlp1_dp0_mb7_gpu9 -> layer10_expert1_mlp2_dp0_mb7_gpu9
	layer10_expert1_mlp1_dp1_mb7_gpu9 -> layer10_expert1_mlp2_dp1_mb7_gpu9
	layer10_expert2_mlp1_dp0_mb7_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp1_mb7_gpu10 [label="Layer 10 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb7_gpu8 -> layer10_expert2_mlp1_dp0_mb7_gpu10
	layer10_router_dp1_mb7_gpu8 -> layer10_expert2_mlp1_dp1_mb7_gpu10
	layer10_expert2_mlp2_dp0_mb7_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp2_dp1_mb7_gpu10 [label="Layer 10 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert2_mlp1_dp0_mb7_gpu10 -> layer10_expert2_mlp2_dp0_mb7_gpu10
	layer10_expert2_mlp1_dp1_mb7_gpu10 -> layer10_expert2_mlp2_dp1_mb7_gpu10
	layer10_expert3_mlp1_dp0_mb7_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp1_mb7_gpu11 [label="Layer 10 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_router_dp0_mb7_gpu8 -> layer10_expert3_mlp1_dp0_mb7_gpu11
	layer10_router_dp1_mb7_gpu8 -> layer10_expert3_mlp1_dp1_mb7_gpu11
	layer10_expert3_mlp2_dp0_mb7_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp2_dp1_mb7_gpu11 [label="Layer 10 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert3_mlp1_dp0_mb7_gpu11 -> layer10_expert3_mlp2_dp0_mb7_gpu11
	layer10_expert3_mlp1_dp1_mb7_gpu11 -> layer10_expert3_mlp2_dp1_mb7_gpu11
	layer10_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer10_expert0_mlp2_dp0_mb7_gpu8 -> layer10_expert_agg_dp0_mb7
	layer10_expert0_mlp2_dp1_mb7_gpu8 -> layer10_expert_agg_dp1_mb7
	layer10_expert1_mlp2_dp0_mb7_gpu9 -> layer10_expert_agg_dp0_mb7
	layer10_expert1_mlp2_dp1_mb7_gpu9 -> layer10_expert_agg_dp1_mb7
	layer10_expert2_mlp2_dp0_mb7_gpu10 -> layer10_expert_agg_dp0_mb7
	layer10_expert2_mlp2_dp1_mb7_gpu10 -> layer10_expert_agg_dp1_mb7
	layer10_expert3_mlp2_dp0_mb7_gpu11 -> layer10_expert_agg_dp0_mb7
	layer10_expert3_mlp2_dp1_mb7_gpu11 -> layer10_expert_agg_dp1_mb7
	layer11_qkv_dp0_mb7_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb7_gpu8 [label="Layer 11 QKV Projection\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb7 -> layer11_qkv_dp0_mb7_gpu8
	layer10_expert_agg_dp1_mb7 -> layer11_qkv_dp1_mb7_gpu8
	layer11_attn_scores_dp0_mb7_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb7_gpu8 [label="Layer 11 Attention Scores\nGPU 8\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb7_gpu8 -> layer11_attn_scores_dp0_mb7_gpu8
	layer11_qkv_dp1_mb7_gpu8 -> layer11_attn_scores_dp1_mb7_gpu8
	layer11_softmax_dp0_mb7_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb7_gpu8 [label="Layer 11 Attention Softmax\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb7_gpu8 -> layer11_softmax_dp0_mb7_gpu8
	layer11_attn_scores_dp1_mb7_gpu8 -> layer11_softmax_dp1_mb7_gpu8
	layer11_attn_out_dp0_mb7_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb7_gpu8 [label="Layer 11 Attention Output\nGPU 8\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb7_gpu8 -> layer11_attn_out_dp0_mb7_gpu8
	layer11_softmax_dp1_mb7_gpu8 -> layer11_attn_out_dp1_mb7_gpu8
	layer11_qkv_dp0_mb7_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb7_gpu9 [label="Layer 11 QKV Projection\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb7 -> layer11_qkv_dp0_mb7_gpu9
	layer10_expert_agg_dp1_mb7 -> layer11_qkv_dp1_mb7_gpu9
	layer11_attn_scores_dp0_mb7_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb7_gpu9 [label="Layer 11 Attention Scores\nGPU 9\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb7_gpu9 -> layer11_attn_scores_dp0_mb7_gpu9
	layer11_qkv_dp1_mb7_gpu9 -> layer11_attn_scores_dp1_mb7_gpu9
	layer11_softmax_dp0_mb7_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb7_gpu9 [label="Layer 11 Attention Softmax\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb7_gpu9 -> layer11_softmax_dp0_mb7_gpu9
	layer11_attn_scores_dp1_mb7_gpu9 -> layer11_softmax_dp1_mb7_gpu9
	layer11_attn_out_dp0_mb7_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb7_gpu9 [label="Layer 11 Attention Output\nGPU 9\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb7_gpu9 -> layer11_attn_out_dp0_mb7_gpu9
	layer11_softmax_dp1_mb7_gpu9 -> layer11_attn_out_dp1_mb7_gpu9
	layer11_qkv_dp0_mb7_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb7_gpu10 [label="Layer 11 QKV Projection\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb7 -> layer11_qkv_dp0_mb7_gpu10
	layer10_expert_agg_dp1_mb7 -> layer11_qkv_dp1_mb7_gpu10
	layer11_attn_scores_dp0_mb7_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb7_gpu10 [label="Layer 11 Attention Scores\nGPU 10\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb7_gpu10 -> layer11_attn_scores_dp0_mb7_gpu10
	layer11_qkv_dp1_mb7_gpu10 -> layer11_attn_scores_dp1_mb7_gpu10
	layer11_softmax_dp0_mb7_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb7_gpu10 [label="Layer 11 Attention Softmax\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb7_gpu10 -> layer11_softmax_dp0_mb7_gpu10
	layer11_attn_scores_dp1_mb7_gpu10 -> layer11_softmax_dp1_mb7_gpu10
	layer11_attn_out_dp0_mb7_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb7_gpu10 [label="Layer 11 Attention Output\nGPU 10\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb7_gpu10 -> layer11_attn_out_dp0_mb7_gpu10
	layer11_softmax_dp1_mb7_gpu10 -> layer11_attn_out_dp1_mb7_gpu10
	layer11_qkv_dp0_mb7_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp1_mb7_gpu11 [label="Layer 11 QKV Projection\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer10_expert_agg_dp0_mb7 -> layer11_qkv_dp0_mb7_gpu11
	layer10_expert_agg_dp1_mb7 -> layer11_qkv_dp1_mb7_gpu11
	layer11_attn_scores_dp0_mb7_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp1_mb7_gpu11 [label="Layer 11 Attention Scores\nGPU 11\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_qkv_dp0_mb7_gpu11 -> layer11_attn_scores_dp0_mb7_gpu11
	layer11_qkv_dp1_mb7_gpu11 -> layer11_attn_scores_dp1_mb7_gpu11
	layer11_softmax_dp0_mb7_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp1_mb7_gpu11 [label="Layer 11 Attention Softmax\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_scores_dp0_mb7_gpu11 -> layer11_softmax_dp0_mb7_gpu11
	layer11_attn_scores_dp1_mb7_gpu11 -> layer11_softmax_dp1_mb7_gpu11
	layer11_attn_out_dp0_mb7_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_attn_out_dp1_mb7_gpu11 [label="Layer 11 Attention Output\nGPU 11\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_softmax_dp0_mb7_gpu11 -> layer11_attn_out_dp0_mb7_gpu11
	layer11_softmax_dp1_mb7_gpu11 -> layer11_attn_out_dp1_mb7_gpu11
	layer11_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_attn_out_dp0_mb7_gpu8 -> layer11_attn_allreduce_dp0_mb7
	layer11_attn_out_dp1_mb7_gpu8 -> layer11_attn_allreduce_dp1_mb7
	layer11_attn_out_dp0_mb7_gpu9 -> layer11_attn_allreduce_dp0_mb7
	layer11_attn_out_dp1_mb7_gpu9 -> layer11_attn_allreduce_dp1_mb7
	layer11_attn_out_dp0_mb7_gpu10 -> layer11_attn_allreduce_dp0_mb7
	layer11_attn_out_dp1_mb7_gpu10 -> layer11_attn_allreduce_dp1_mb7
	layer11_attn_out_dp0_mb7_gpu11 -> layer11_attn_allreduce_dp0_mb7
	layer11_attn_out_dp1_mb7_gpu11 -> layer11_attn_allreduce_dp1_mb7
	layer11_router_dp0_mb7_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_router_dp1_mb7_gpu8 [label="Layer 11 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer11_attn_allreduce_dp0_mb7 -> layer11_router_dp0_mb7_gpu8
	layer11_attn_allreduce_dp1_mb7 -> layer11_router_dp1_mb7_gpu8
	layer11_expert0_mlp1_dp0_mb7_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp1_mb7_gpu8 [label="Layer 11 Expert 0 MLP1\nGPU 8\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb7_gpu8 -> layer11_expert0_mlp1_dp0_mb7_gpu8
	layer11_router_dp1_mb7_gpu8 -> layer11_expert0_mlp1_dp1_mb7_gpu8
	layer11_expert0_mlp2_dp0_mb7_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp2_dp1_mb7_gpu8 [label="Layer 11 Expert 0 MLP2\nGPU 8\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert0_mlp1_dp0_mb7_gpu8 -> layer11_expert0_mlp2_dp0_mb7_gpu8
	layer11_expert0_mlp1_dp1_mb7_gpu8 -> layer11_expert0_mlp2_dp1_mb7_gpu8
	layer11_expert1_mlp1_dp0_mb7_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp1_mb7_gpu9 [label="Layer 11 Expert 1 MLP1\nGPU 9\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb7_gpu8 -> layer11_expert1_mlp1_dp0_mb7_gpu9
	layer11_router_dp1_mb7_gpu8 -> layer11_expert1_mlp1_dp1_mb7_gpu9
	layer11_expert1_mlp2_dp0_mb7_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp2_dp1_mb7_gpu9 [label="Layer 11 Expert 1 MLP2\nGPU 9\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert1_mlp1_dp0_mb7_gpu9 -> layer11_expert1_mlp2_dp0_mb7_gpu9
	layer11_expert1_mlp1_dp1_mb7_gpu9 -> layer11_expert1_mlp2_dp1_mb7_gpu9
	layer11_expert2_mlp1_dp0_mb7_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp1_mb7_gpu10 [label="Layer 11 Expert 2 MLP1\nGPU 10\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb7_gpu8 -> layer11_expert2_mlp1_dp0_mb7_gpu10
	layer11_router_dp1_mb7_gpu8 -> layer11_expert2_mlp1_dp1_mb7_gpu10
	layer11_expert2_mlp2_dp0_mb7_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp2_dp1_mb7_gpu10 [label="Layer 11 Expert 2 MLP2\nGPU 10\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert2_mlp1_dp0_mb7_gpu10 -> layer11_expert2_mlp2_dp0_mb7_gpu10
	layer11_expert2_mlp1_dp1_mb7_gpu10 -> layer11_expert2_mlp2_dp1_mb7_gpu10
	layer11_expert3_mlp1_dp0_mb7_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp1_mb7_gpu11 [label="Layer 11 Expert 3 MLP1\nGPU 11\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_router_dp0_mb7_gpu8 -> layer11_expert3_mlp1_dp0_mb7_gpu11
	layer11_router_dp1_mb7_gpu8 -> layer11_expert3_mlp1_dp1_mb7_gpu11
	layer11_expert3_mlp2_dp0_mb7_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp2_dp1_mb7_gpu11 [label="Layer 11 Expert 3 MLP2\nGPU 11\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#E5FFE5" fontsize=9 style=filled]
	layer11_expert3_mlp1_dp0_mb7_gpu11 -> layer11_expert3_mlp2_dp0_mb7_gpu11
	layer11_expert3_mlp1_dp1_mb7_gpu11 -> layer11_expert3_mlp2_dp1_mb7_gpu11
	layer11_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer11_expert0_mlp2_dp0_mb7_gpu8 -> layer11_expert_agg_dp0_mb7
	layer11_expert0_mlp2_dp1_mb7_gpu8 -> layer11_expert_agg_dp1_mb7
	layer11_expert1_mlp2_dp0_mb7_gpu9 -> layer11_expert_agg_dp0_mb7
	layer11_expert1_mlp2_dp1_mb7_gpu9 -> layer11_expert_agg_dp1_mb7
	layer11_expert2_mlp2_dp0_mb7_gpu10 -> layer11_expert_agg_dp0_mb7
	layer11_expert2_mlp2_dp1_mb7_gpu10 -> layer11_expert_agg_dp1_mb7
	layer11_expert3_mlp2_dp0_mb7_gpu11 -> layer11_expert_agg_dp0_mb7
	layer11_expert3_mlp2_dp1_mb7_gpu11 -> layer11_expert_agg_dp1_mb7
	layer12_qkv_dp0_mb0_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb0_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb0 -> layer12_qkv_dp0_mb0_gpu12
	layer11_expert_agg_dp1_mb0 -> layer12_qkv_dp1_mb0_gpu12
	layer12_attn_scores_dp0_mb0_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb0_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb0_gpu12 -> layer12_attn_scores_dp0_mb0_gpu12
	layer12_qkv_dp1_mb0_gpu12 -> layer12_attn_scores_dp1_mb0_gpu12
	layer12_softmax_dp0_mb0_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb0_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb0_gpu12 -> layer12_softmax_dp0_mb0_gpu12
	layer12_attn_scores_dp1_mb0_gpu12 -> layer12_softmax_dp1_mb0_gpu12
	layer12_attn_out_dp0_mb0_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb0_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb0_gpu12 -> layer12_attn_out_dp0_mb0_gpu12
	layer12_softmax_dp1_mb0_gpu12 -> layer12_attn_out_dp1_mb0_gpu12
	layer12_qkv_dp0_mb0_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb0_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb0 -> layer12_qkv_dp0_mb0_gpu13
	layer11_expert_agg_dp1_mb0 -> layer12_qkv_dp1_mb0_gpu13
	layer12_attn_scores_dp0_mb0_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb0_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb0_gpu13 -> layer12_attn_scores_dp0_mb0_gpu13
	layer12_qkv_dp1_mb0_gpu13 -> layer12_attn_scores_dp1_mb0_gpu13
	layer12_softmax_dp0_mb0_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb0_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb0_gpu13 -> layer12_softmax_dp0_mb0_gpu13
	layer12_attn_scores_dp1_mb0_gpu13 -> layer12_softmax_dp1_mb0_gpu13
	layer12_attn_out_dp0_mb0_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb0_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb0_gpu13 -> layer12_attn_out_dp0_mb0_gpu13
	layer12_softmax_dp1_mb0_gpu13 -> layer12_attn_out_dp1_mb0_gpu13
	layer12_qkv_dp0_mb0_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb0_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb0 -> layer12_qkv_dp0_mb0_gpu14
	layer11_expert_agg_dp1_mb0 -> layer12_qkv_dp1_mb0_gpu14
	layer12_attn_scores_dp0_mb0_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb0_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb0_gpu14 -> layer12_attn_scores_dp0_mb0_gpu14
	layer12_qkv_dp1_mb0_gpu14 -> layer12_attn_scores_dp1_mb0_gpu14
	layer12_softmax_dp0_mb0_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb0_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb0_gpu14 -> layer12_softmax_dp0_mb0_gpu14
	layer12_attn_scores_dp1_mb0_gpu14 -> layer12_softmax_dp1_mb0_gpu14
	layer12_attn_out_dp0_mb0_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb0_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb0_gpu14 -> layer12_attn_out_dp0_mb0_gpu14
	layer12_softmax_dp1_mb0_gpu14 -> layer12_attn_out_dp1_mb0_gpu14
	layer12_qkv_dp0_mb0_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb0_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb0 -> layer12_qkv_dp0_mb0_gpu15
	layer11_expert_agg_dp1_mb0 -> layer12_qkv_dp1_mb0_gpu15
	layer12_attn_scores_dp0_mb0_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb0_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb0_gpu15 -> layer12_attn_scores_dp0_mb0_gpu15
	layer12_qkv_dp1_mb0_gpu15 -> layer12_attn_scores_dp1_mb0_gpu15
	layer12_softmax_dp0_mb0_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb0_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb0_gpu15 -> layer12_softmax_dp0_mb0_gpu15
	layer12_attn_scores_dp1_mb0_gpu15 -> layer12_softmax_dp1_mb0_gpu15
	layer12_attn_out_dp0_mb0_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb0_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb0_gpu15 -> layer12_attn_out_dp0_mb0_gpu15
	layer12_softmax_dp1_mb0_gpu15 -> layer12_attn_out_dp1_mb0_gpu15
	layer12_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb0_gpu12 -> layer12_attn_allreduce_dp0_mb0
	layer12_attn_out_dp1_mb0_gpu12 -> layer12_attn_allreduce_dp1_mb0
	layer12_attn_out_dp0_mb0_gpu13 -> layer12_attn_allreduce_dp0_mb0
	layer12_attn_out_dp1_mb0_gpu13 -> layer12_attn_allreduce_dp1_mb0
	layer12_attn_out_dp0_mb0_gpu14 -> layer12_attn_allreduce_dp0_mb0
	layer12_attn_out_dp1_mb0_gpu14 -> layer12_attn_allreduce_dp1_mb0
	layer12_attn_out_dp0_mb0_gpu15 -> layer12_attn_allreduce_dp0_mb0
	layer12_attn_out_dp1_mb0_gpu15 -> layer12_attn_allreduce_dp1_mb0
	layer12_router_dp0_mb0_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb0_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb0 -> layer12_router_dp0_mb0_gpu12
	layer12_attn_allreduce_dp1_mb0 -> layer12_router_dp1_mb0_gpu12
	layer12_expert0_mlp1_dp0_mb0_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb0_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb0_gpu12 -> layer12_expert0_mlp1_dp0_mb0_gpu12
	layer12_router_dp1_mb0_gpu12 -> layer12_expert0_mlp1_dp1_mb0_gpu12
	layer12_expert0_mlp2_dp0_mb0_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb0_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb0_gpu12 -> layer12_expert0_mlp2_dp0_mb0_gpu12
	layer12_expert0_mlp1_dp1_mb0_gpu12 -> layer12_expert0_mlp2_dp1_mb0_gpu12
	layer12_expert1_mlp1_dp0_mb0_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb0_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb0_gpu12 -> layer12_expert1_mlp1_dp0_mb0_gpu13
	layer12_router_dp1_mb0_gpu12 -> layer12_expert1_mlp1_dp1_mb0_gpu13
	layer12_expert1_mlp2_dp0_mb0_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb0_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb0_gpu13 -> layer12_expert1_mlp2_dp0_mb0_gpu13
	layer12_expert1_mlp1_dp1_mb0_gpu13 -> layer12_expert1_mlp2_dp1_mb0_gpu13
	layer12_expert2_mlp1_dp0_mb0_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb0_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb0_gpu12 -> layer12_expert2_mlp1_dp0_mb0_gpu14
	layer12_router_dp1_mb0_gpu12 -> layer12_expert2_mlp1_dp1_mb0_gpu14
	layer12_expert2_mlp2_dp0_mb0_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb0_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb0_gpu14 -> layer12_expert2_mlp2_dp0_mb0_gpu14
	layer12_expert2_mlp1_dp1_mb0_gpu14 -> layer12_expert2_mlp2_dp1_mb0_gpu14
	layer12_expert3_mlp1_dp0_mb0_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb0_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb0_gpu12 -> layer12_expert3_mlp1_dp0_mb0_gpu15
	layer12_router_dp1_mb0_gpu12 -> layer12_expert3_mlp1_dp1_mb0_gpu15
	layer12_expert3_mlp2_dp0_mb0_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb0_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb0_gpu15 -> layer12_expert3_mlp2_dp0_mb0_gpu15
	layer12_expert3_mlp1_dp1_mb0_gpu15 -> layer12_expert3_mlp2_dp1_mb0_gpu15
	layer12_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb0_gpu12 -> layer12_expert_agg_dp0_mb0
	layer12_expert0_mlp2_dp1_mb0_gpu12 -> layer12_expert_agg_dp1_mb0
	layer12_expert1_mlp2_dp0_mb0_gpu13 -> layer12_expert_agg_dp0_mb0
	layer12_expert1_mlp2_dp1_mb0_gpu13 -> layer12_expert_agg_dp1_mb0
	layer12_expert2_mlp2_dp0_mb0_gpu14 -> layer12_expert_agg_dp0_mb0
	layer12_expert2_mlp2_dp1_mb0_gpu14 -> layer12_expert_agg_dp1_mb0
	layer12_expert3_mlp2_dp0_mb0_gpu15 -> layer12_expert_agg_dp0_mb0
	layer12_expert3_mlp2_dp1_mb0_gpu15 -> layer12_expert_agg_dp1_mb0
	layer13_qkv_dp0_mb0_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb0_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb0 -> layer13_qkv_dp0_mb0_gpu12
	layer12_expert_agg_dp1_mb0 -> layer13_qkv_dp1_mb0_gpu12
	layer13_attn_scores_dp0_mb0_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb0_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb0_gpu12 -> layer13_attn_scores_dp0_mb0_gpu12
	layer13_qkv_dp1_mb0_gpu12 -> layer13_attn_scores_dp1_mb0_gpu12
	layer13_softmax_dp0_mb0_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb0_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb0_gpu12 -> layer13_softmax_dp0_mb0_gpu12
	layer13_attn_scores_dp1_mb0_gpu12 -> layer13_softmax_dp1_mb0_gpu12
	layer13_attn_out_dp0_mb0_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb0_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb0_gpu12 -> layer13_attn_out_dp0_mb0_gpu12
	layer13_softmax_dp1_mb0_gpu12 -> layer13_attn_out_dp1_mb0_gpu12
	layer13_qkv_dp0_mb0_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb0_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb0 -> layer13_qkv_dp0_mb0_gpu13
	layer12_expert_agg_dp1_mb0 -> layer13_qkv_dp1_mb0_gpu13
	layer13_attn_scores_dp0_mb0_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb0_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb0_gpu13 -> layer13_attn_scores_dp0_mb0_gpu13
	layer13_qkv_dp1_mb0_gpu13 -> layer13_attn_scores_dp1_mb0_gpu13
	layer13_softmax_dp0_mb0_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb0_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb0_gpu13 -> layer13_softmax_dp0_mb0_gpu13
	layer13_attn_scores_dp1_mb0_gpu13 -> layer13_softmax_dp1_mb0_gpu13
	layer13_attn_out_dp0_mb0_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb0_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb0_gpu13 -> layer13_attn_out_dp0_mb0_gpu13
	layer13_softmax_dp1_mb0_gpu13 -> layer13_attn_out_dp1_mb0_gpu13
	layer13_qkv_dp0_mb0_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb0_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb0 -> layer13_qkv_dp0_mb0_gpu14
	layer12_expert_agg_dp1_mb0 -> layer13_qkv_dp1_mb0_gpu14
	layer13_attn_scores_dp0_mb0_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb0_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb0_gpu14 -> layer13_attn_scores_dp0_mb0_gpu14
	layer13_qkv_dp1_mb0_gpu14 -> layer13_attn_scores_dp1_mb0_gpu14
	layer13_softmax_dp0_mb0_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb0_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb0_gpu14 -> layer13_softmax_dp0_mb0_gpu14
	layer13_attn_scores_dp1_mb0_gpu14 -> layer13_softmax_dp1_mb0_gpu14
	layer13_attn_out_dp0_mb0_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb0_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb0_gpu14 -> layer13_attn_out_dp0_mb0_gpu14
	layer13_softmax_dp1_mb0_gpu14 -> layer13_attn_out_dp1_mb0_gpu14
	layer13_qkv_dp0_mb0_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb0_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb0 -> layer13_qkv_dp0_mb0_gpu15
	layer12_expert_agg_dp1_mb0 -> layer13_qkv_dp1_mb0_gpu15
	layer13_attn_scores_dp0_mb0_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb0_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb0_gpu15 -> layer13_attn_scores_dp0_mb0_gpu15
	layer13_qkv_dp1_mb0_gpu15 -> layer13_attn_scores_dp1_mb0_gpu15
	layer13_softmax_dp0_mb0_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb0_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb0_gpu15 -> layer13_softmax_dp0_mb0_gpu15
	layer13_attn_scores_dp1_mb0_gpu15 -> layer13_softmax_dp1_mb0_gpu15
	layer13_attn_out_dp0_mb0_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb0_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb0_gpu15 -> layer13_attn_out_dp0_mb0_gpu15
	layer13_softmax_dp1_mb0_gpu15 -> layer13_attn_out_dp1_mb0_gpu15
	layer13_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb0_gpu12 -> layer13_attn_allreduce_dp0_mb0
	layer13_attn_out_dp1_mb0_gpu12 -> layer13_attn_allreduce_dp1_mb0
	layer13_attn_out_dp0_mb0_gpu13 -> layer13_attn_allreduce_dp0_mb0
	layer13_attn_out_dp1_mb0_gpu13 -> layer13_attn_allreduce_dp1_mb0
	layer13_attn_out_dp0_mb0_gpu14 -> layer13_attn_allreduce_dp0_mb0
	layer13_attn_out_dp1_mb0_gpu14 -> layer13_attn_allreduce_dp1_mb0
	layer13_attn_out_dp0_mb0_gpu15 -> layer13_attn_allreduce_dp0_mb0
	layer13_attn_out_dp1_mb0_gpu15 -> layer13_attn_allreduce_dp1_mb0
	layer13_router_dp0_mb0_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb0_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb0 -> layer13_router_dp0_mb0_gpu12
	layer13_attn_allreduce_dp1_mb0 -> layer13_router_dp1_mb0_gpu12
	layer13_expert0_mlp1_dp0_mb0_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb0_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb0_gpu12 -> layer13_expert0_mlp1_dp0_mb0_gpu12
	layer13_router_dp1_mb0_gpu12 -> layer13_expert0_mlp1_dp1_mb0_gpu12
	layer13_expert0_mlp2_dp0_mb0_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb0_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb0_gpu12 -> layer13_expert0_mlp2_dp0_mb0_gpu12
	layer13_expert0_mlp1_dp1_mb0_gpu12 -> layer13_expert0_mlp2_dp1_mb0_gpu12
	layer13_expert1_mlp1_dp0_mb0_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb0_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb0_gpu12 -> layer13_expert1_mlp1_dp0_mb0_gpu13
	layer13_router_dp1_mb0_gpu12 -> layer13_expert1_mlp1_dp1_mb0_gpu13
	layer13_expert1_mlp2_dp0_mb0_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb0_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb0_gpu13 -> layer13_expert1_mlp2_dp0_mb0_gpu13
	layer13_expert1_mlp1_dp1_mb0_gpu13 -> layer13_expert1_mlp2_dp1_mb0_gpu13
	layer13_expert2_mlp1_dp0_mb0_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb0_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb0_gpu12 -> layer13_expert2_mlp1_dp0_mb0_gpu14
	layer13_router_dp1_mb0_gpu12 -> layer13_expert2_mlp1_dp1_mb0_gpu14
	layer13_expert2_mlp2_dp0_mb0_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb0_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb0_gpu14 -> layer13_expert2_mlp2_dp0_mb0_gpu14
	layer13_expert2_mlp1_dp1_mb0_gpu14 -> layer13_expert2_mlp2_dp1_mb0_gpu14
	layer13_expert3_mlp1_dp0_mb0_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb0_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb0_gpu12 -> layer13_expert3_mlp1_dp0_mb0_gpu15
	layer13_router_dp1_mb0_gpu12 -> layer13_expert3_mlp1_dp1_mb0_gpu15
	layer13_expert3_mlp2_dp0_mb0_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb0_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb0_gpu15 -> layer13_expert3_mlp2_dp0_mb0_gpu15
	layer13_expert3_mlp1_dp1_mb0_gpu15 -> layer13_expert3_mlp2_dp1_mb0_gpu15
	layer13_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb0_gpu12 -> layer13_expert_agg_dp0_mb0
	layer13_expert0_mlp2_dp1_mb0_gpu12 -> layer13_expert_agg_dp1_mb0
	layer13_expert1_mlp2_dp0_mb0_gpu13 -> layer13_expert_agg_dp0_mb0
	layer13_expert1_mlp2_dp1_mb0_gpu13 -> layer13_expert_agg_dp1_mb0
	layer13_expert2_mlp2_dp0_mb0_gpu14 -> layer13_expert_agg_dp0_mb0
	layer13_expert2_mlp2_dp1_mb0_gpu14 -> layer13_expert_agg_dp1_mb0
	layer13_expert3_mlp2_dp0_mb0_gpu15 -> layer13_expert_agg_dp0_mb0
	layer13_expert3_mlp2_dp1_mb0_gpu15 -> layer13_expert_agg_dp1_mb0
	layer14_qkv_dp0_mb0_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb0_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb0 -> layer14_qkv_dp0_mb0_gpu12
	layer13_expert_agg_dp1_mb0 -> layer14_qkv_dp1_mb0_gpu12
	layer14_attn_scores_dp0_mb0_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb0_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb0_gpu12 -> layer14_attn_scores_dp0_mb0_gpu12
	layer14_qkv_dp1_mb0_gpu12 -> layer14_attn_scores_dp1_mb0_gpu12
	layer14_softmax_dp0_mb0_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb0_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb0_gpu12 -> layer14_softmax_dp0_mb0_gpu12
	layer14_attn_scores_dp1_mb0_gpu12 -> layer14_softmax_dp1_mb0_gpu12
	layer14_attn_out_dp0_mb0_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb0_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb0_gpu12 -> layer14_attn_out_dp0_mb0_gpu12
	layer14_softmax_dp1_mb0_gpu12 -> layer14_attn_out_dp1_mb0_gpu12
	layer14_qkv_dp0_mb0_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb0_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb0 -> layer14_qkv_dp0_mb0_gpu13
	layer13_expert_agg_dp1_mb0 -> layer14_qkv_dp1_mb0_gpu13
	layer14_attn_scores_dp0_mb0_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb0_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb0_gpu13 -> layer14_attn_scores_dp0_mb0_gpu13
	layer14_qkv_dp1_mb0_gpu13 -> layer14_attn_scores_dp1_mb0_gpu13
	layer14_softmax_dp0_mb0_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb0_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb0_gpu13 -> layer14_softmax_dp0_mb0_gpu13
	layer14_attn_scores_dp1_mb0_gpu13 -> layer14_softmax_dp1_mb0_gpu13
	layer14_attn_out_dp0_mb0_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb0_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb0_gpu13 -> layer14_attn_out_dp0_mb0_gpu13
	layer14_softmax_dp1_mb0_gpu13 -> layer14_attn_out_dp1_mb0_gpu13
	layer14_qkv_dp0_mb0_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb0_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb0 -> layer14_qkv_dp0_mb0_gpu14
	layer13_expert_agg_dp1_mb0 -> layer14_qkv_dp1_mb0_gpu14
	layer14_attn_scores_dp0_mb0_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb0_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb0_gpu14 -> layer14_attn_scores_dp0_mb0_gpu14
	layer14_qkv_dp1_mb0_gpu14 -> layer14_attn_scores_dp1_mb0_gpu14
	layer14_softmax_dp0_mb0_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb0_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb0_gpu14 -> layer14_softmax_dp0_mb0_gpu14
	layer14_attn_scores_dp1_mb0_gpu14 -> layer14_softmax_dp1_mb0_gpu14
	layer14_attn_out_dp0_mb0_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb0_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb0_gpu14 -> layer14_attn_out_dp0_mb0_gpu14
	layer14_softmax_dp1_mb0_gpu14 -> layer14_attn_out_dp1_mb0_gpu14
	layer14_qkv_dp0_mb0_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb0_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb0 -> layer14_qkv_dp0_mb0_gpu15
	layer13_expert_agg_dp1_mb0 -> layer14_qkv_dp1_mb0_gpu15
	layer14_attn_scores_dp0_mb0_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb0_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb0_gpu15 -> layer14_attn_scores_dp0_mb0_gpu15
	layer14_qkv_dp1_mb0_gpu15 -> layer14_attn_scores_dp1_mb0_gpu15
	layer14_softmax_dp0_mb0_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb0_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb0_gpu15 -> layer14_softmax_dp0_mb0_gpu15
	layer14_attn_scores_dp1_mb0_gpu15 -> layer14_softmax_dp1_mb0_gpu15
	layer14_attn_out_dp0_mb0_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb0_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb0_gpu15 -> layer14_attn_out_dp0_mb0_gpu15
	layer14_softmax_dp1_mb0_gpu15 -> layer14_attn_out_dp1_mb0_gpu15
	layer14_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb0_gpu12 -> layer14_attn_allreduce_dp0_mb0
	layer14_attn_out_dp1_mb0_gpu12 -> layer14_attn_allreduce_dp1_mb0
	layer14_attn_out_dp0_mb0_gpu13 -> layer14_attn_allreduce_dp0_mb0
	layer14_attn_out_dp1_mb0_gpu13 -> layer14_attn_allreduce_dp1_mb0
	layer14_attn_out_dp0_mb0_gpu14 -> layer14_attn_allreduce_dp0_mb0
	layer14_attn_out_dp1_mb0_gpu14 -> layer14_attn_allreduce_dp1_mb0
	layer14_attn_out_dp0_mb0_gpu15 -> layer14_attn_allreduce_dp0_mb0
	layer14_attn_out_dp1_mb0_gpu15 -> layer14_attn_allreduce_dp1_mb0
	layer14_router_dp0_mb0_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb0_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb0 -> layer14_router_dp0_mb0_gpu12
	layer14_attn_allreduce_dp1_mb0 -> layer14_router_dp1_mb0_gpu12
	layer14_expert0_mlp1_dp0_mb0_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb0_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb0_gpu12 -> layer14_expert0_mlp1_dp0_mb0_gpu12
	layer14_router_dp1_mb0_gpu12 -> layer14_expert0_mlp1_dp1_mb0_gpu12
	layer14_expert0_mlp2_dp0_mb0_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb0_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb0_gpu12 -> layer14_expert0_mlp2_dp0_mb0_gpu12
	layer14_expert0_mlp1_dp1_mb0_gpu12 -> layer14_expert0_mlp2_dp1_mb0_gpu12
	layer14_expert1_mlp1_dp0_mb0_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb0_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb0_gpu12 -> layer14_expert1_mlp1_dp0_mb0_gpu13
	layer14_router_dp1_mb0_gpu12 -> layer14_expert1_mlp1_dp1_mb0_gpu13
	layer14_expert1_mlp2_dp0_mb0_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb0_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb0_gpu13 -> layer14_expert1_mlp2_dp0_mb0_gpu13
	layer14_expert1_mlp1_dp1_mb0_gpu13 -> layer14_expert1_mlp2_dp1_mb0_gpu13
	layer14_expert2_mlp1_dp0_mb0_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb0_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb0_gpu12 -> layer14_expert2_mlp1_dp0_mb0_gpu14
	layer14_router_dp1_mb0_gpu12 -> layer14_expert2_mlp1_dp1_mb0_gpu14
	layer14_expert2_mlp2_dp0_mb0_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb0_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb0_gpu14 -> layer14_expert2_mlp2_dp0_mb0_gpu14
	layer14_expert2_mlp1_dp1_mb0_gpu14 -> layer14_expert2_mlp2_dp1_mb0_gpu14
	layer14_expert3_mlp1_dp0_mb0_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb0_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb0_gpu12 -> layer14_expert3_mlp1_dp0_mb0_gpu15
	layer14_router_dp1_mb0_gpu12 -> layer14_expert3_mlp1_dp1_mb0_gpu15
	layer14_expert3_mlp2_dp0_mb0_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb0_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb0_gpu15 -> layer14_expert3_mlp2_dp0_mb0_gpu15
	layer14_expert3_mlp1_dp1_mb0_gpu15 -> layer14_expert3_mlp2_dp1_mb0_gpu15
	layer14_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb0_gpu12 -> layer14_expert_agg_dp0_mb0
	layer14_expert0_mlp2_dp1_mb0_gpu12 -> layer14_expert_agg_dp1_mb0
	layer14_expert1_mlp2_dp0_mb0_gpu13 -> layer14_expert_agg_dp0_mb0
	layer14_expert1_mlp2_dp1_mb0_gpu13 -> layer14_expert_agg_dp1_mb0
	layer14_expert2_mlp2_dp0_mb0_gpu14 -> layer14_expert_agg_dp0_mb0
	layer14_expert2_mlp2_dp1_mb0_gpu14 -> layer14_expert_agg_dp1_mb0
	layer14_expert3_mlp2_dp0_mb0_gpu15 -> layer14_expert_agg_dp0_mb0
	layer14_expert3_mlp2_dp1_mb0_gpu15 -> layer14_expert_agg_dp1_mb0
	layer15_qkv_dp0_mb0_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb0_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb0 -> layer15_qkv_dp0_mb0_gpu12
	layer14_expert_agg_dp1_mb0 -> layer15_qkv_dp1_mb0_gpu12
	layer15_attn_scores_dp0_mb0_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb0_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb0_gpu12 -> layer15_attn_scores_dp0_mb0_gpu12
	layer15_qkv_dp1_mb0_gpu12 -> layer15_attn_scores_dp1_mb0_gpu12
	layer15_softmax_dp0_mb0_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb0_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb0_gpu12 -> layer15_softmax_dp0_mb0_gpu12
	layer15_attn_scores_dp1_mb0_gpu12 -> layer15_softmax_dp1_mb0_gpu12
	layer15_attn_out_dp0_mb0_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb0_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb0_gpu12 -> layer15_attn_out_dp0_mb0_gpu12
	layer15_softmax_dp1_mb0_gpu12 -> layer15_attn_out_dp1_mb0_gpu12
	layer15_qkv_dp0_mb0_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb0_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb0 -> layer15_qkv_dp0_mb0_gpu13
	layer14_expert_agg_dp1_mb0 -> layer15_qkv_dp1_mb0_gpu13
	layer15_attn_scores_dp0_mb0_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb0_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb0_gpu13 -> layer15_attn_scores_dp0_mb0_gpu13
	layer15_qkv_dp1_mb0_gpu13 -> layer15_attn_scores_dp1_mb0_gpu13
	layer15_softmax_dp0_mb0_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb0_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb0_gpu13 -> layer15_softmax_dp0_mb0_gpu13
	layer15_attn_scores_dp1_mb0_gpu13 -> layer15_softmax_dp1_mb0_gpu13
	layer15_attn_out_dp0_mb0_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb0_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb0_gpu13 -> layer15_attn_out_dp0_mb0_gpu13
	layer15_softmax_dp1_mb0_gpu13 -> layer15_attn_out_dp1_mb0_gpu13
	layer15_qkv_dp0_mb0_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb0_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb0 -> layer15_qkv_dp0_mb0_gpu14
	layer14_expert_agg_dp1_mb0 -> layer15_qkv_dp1_mb0_gpu14
	layer15_attn_scores_dp0_mb0_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb0_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb0_gpu14 -> layer15_attn_scores_dp0_mb0_gpu14
	layer15_qkv_dp1_mb0_gpu14 -> layer15_attn_scores_dp1_mb0_gpu14
	layer15_softmax_dp0_mb0_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb0_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb0_gpu14 -> layer15_softmax_dp0_mb0_gpu14
	layer15_attn_scores_dp1_mb0_gpu14 -> layer15_softmax_dp1_mb0_gpu14
	layer15_attn_out_dp0_mb0_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb0_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb0_gpu14 -> layer15_attn_out_dp0_mb0_gpu14
	layer15_softmax_dp1_mb0_gpu14 -> layer15_attn_out_dp1_mb0_gpu14
	layer15_qkv_dp0_mb0_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb0_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb0 -> layer15_qkv_dp0_mb0_gpu15
	layer14_expert_agg_dp1_mb0 -> layer15_qkv_dp1_mb0_gpu15
	layer15_attn_scores_dp0_mb0_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb0_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb0_gpu15 -> layer15_attn_scores_dp0_mb0_gpu15
	layer15_qkv_dp1_mb0_gpu15 -> layer15_attn_scores_dp1_mb0_gpu15
	layer15_softmax_dp0_mb0_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb0_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb0_gpu15 -> layer15_softmax_dp0_mb0_gpu15
	layer15_attn_scores_dp1_mb0_gpu15 -> layer15_softmax_dp1_mb0_gpu15
	layer15_attn_out_dp0_mb0_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb0_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb0_gpu15 -> layer15_attn_out_dp0_mb0_gpu15
	layer15_softmax_dp1_mb0_gpu15 -> layer15_attn_out_dp1_mb0_gpu15
	layer15_attn_allreduce_dp0_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb0 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb0_gpu12 -> layer15_attn_allreduce_dp0_mb0
	layer15_attn_out_dp1_mb0_gpu12 -> layer15_attn_allreduce_dp1_mb0
	layer15_attn_out_dp0_mb0_gpu13 -> layer15_attn_allreduce_dp0_mb0
	layer15_attn_out_dp1_mb0_gpu13 -> layer15_attn_allreduce_dp1_mb0
	layer15_attn_out_dp0_mb0_gpu14 -> layer15_attn_allreduce_dp0_mb0
	layer15_attn_out_dp1_mb0_gpu14 -> layer15_attn_allreduce_dp1_mb0
	layer15_attn_out_dp0_mb0_gpu15 -> layer15_attn_allreduce_dp0_mb0
	layer15_attn_out_dp1_mb0_gpu15 -> layer15_attn_allreduce_dp1_mb0
	layer15_router_dp0_mb0_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb0_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb0 -> layer15_router_dp0_mb0_gpu12
	layer15_attn_allreduce_dp1_mb0 -> layer15_router_dp1_mb0_gpu12
	layer15_expert0_mlp1_dp0_mb0_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb0_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb0_gpu12 -> layer15_expert0_mlp1_dp0_mb0_gpu12
	layer15_router_dp1_mb0_gpu12 -> layer15_expert0_mlp1_dp1_mb0_gpu12
	layer15_expert0_mlp2_dp0_mb0_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb0_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb0_gpu12 -> layer15_expert0_mlp2_dp0_mb0_gpu12
	layer15_expert0_mlp1_dp1_mb0_gpu12 -> layer15_expert0_mlp2_dp1_mb0_gpu12
	layer15_expert1_mlp1_dp0_mb0_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb0_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb0_gpu12 -> layer15_expert1_mlp1_dp0_mb0_gpu13
	layer15_router_dp1_mb0_gpu12 -> layer15_expert1_mlp1_dp1_mb0_gpu13
	layer15_expert1_mlp2_dp0_mb0_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb0_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb0_gpu13 -> layer15_expert1_mlp2_dp0_mb0_gpu13
	layer15_expert1_mlp1_dp1_mb0_gpu13 -> layer15_expert1_mlp2_dp1_mb0_gpu13
	layer15_expert2_mlp1_dp0_mb0_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb0_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb0_gpu12 -> layer15_expert2_mlp1_dp0_mb0_gpu14
	layer15_router_dp1_mb0_gpu12 -> layer15_expert2_mlp1_dp1_mb0_gpu14
	layer15_expert2_mlp2_dp0_mb0_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb0_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb0_gpu14 -> layer15_expert2_mlp2_dp0_mb0_gpu14
	layer15_expert2_mlp1_dp1_mb0_gpu14 -> layer15_expert2_mlp2_dp1_mb0_gpu14
	layer15_expert3_mlp1_dp0_mb0_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb0_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb0_gpu12 -> layer15_expert3_mlp1_dp0_mb0_gpu15
	layer15_router_dp1_mb0_gpu12 -> layer15_expert3_mlp1_dp1_mb0_gpu15
	layer15_expert3_mlp2_dp0_mb0_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb0_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb0_gpu15 -> layer15_expert3_mlp2_dp0_mb0_gpu15
	layer15_expert3_mlp1_dp1_mb0_gpu15 -> layer15_expert3_mlp2_dp1_mb0_gpu15
	layer15_expert_agg_dp0_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb0 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb0_gpu12 -> layer15_expert_agg_dp0_mb0
	layer15_expert0_mlp2_dp1_mb0_gpu12 -> layer15_expert_agg_dp1_mb0
	layer15_expert1_mlp2_dp0_mb0_gpu13 -> layer15_expert_agg_dp0_mb0
	layer15_expert1_mlp2_dp1_mb0_gpu13 -> layer15_expert_agg_dp1_mb0
	layer15_expert2_mlp2_dp0_mb0_gpu14 -> layer15_expert_agg_dp0_mb0
	layer15_expert2_mlp2_dp1_mb0_gpu14 -> layer15_expert_agg_dp1_mb0
	layer15_expert3_mlp2_dp0_mb0_gpu15 -> layer15_expert_agg_dp0_mb0
	layer15_expert3_mlp2_dp1_mb0_gpu15 -> layer15_expert_agg_dp1_mb0
	layer12_qkv_dp0_mb1_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb1_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb1 -> layer12_qkv_dp0_mb1_gpu12
	layer11_expert_agg_dp1_mb1 -> layer12_qkv_dp1_mb1_gpu12
	layer12_attn_scores_dp0_mb1_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb1_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb1_gpu12 -> layer12_attn_scores_dp0_mb1_gpu12
	layer12_qkv_dp1_mb1_gpu12 -> layer12_attn_scores_dp1_mb1_gpu12
	layer12_softmax_dp0_mb1_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb1_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb1_gpu12 -> layer12_softmax_dp0_mb1_gpu12
	layer12_attn_scores_dp1_mb1_gpu12 -> layer12_softmax_dp1_mb1_gpu12
	layer12_attn_out_dp0_mb1_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb1_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb1_gpu12 -> layer12_attn_out_dp0_mb1_gpu12
	layer12_softmax_dp1_mb1_gpu12 -> layer12_attn_out_dp1_mb1_gpu12
	layer12_qkv_dp0_mb1_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb1_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb1 -> layer12_qkv_dp0_mb1_gpu13
	layer11_expert_agg_dp1_mb1 -> layer12_qkv_dp1_mb1_gpu13
	layer12_attn_scores_dp0_mb1_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb1_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb1_gpu13 -> layer12_attn_scores_dp0_mb1_gpu13
	layer12_qkv_dp1_mb1_gpu13 -> layer12_attn_scores_dp1_mb1_gpu13
	layer12_softmax_dp0_mb1_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb1_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb1_gpu13 -> layer12_softmax_dp0_mb1_gpu13
	layer12_attn_scores_dp1_mb1_gpu13 -> layer12_softmax_dp1_mb1_gpu13
	layer12_attn_out_dp0_mb1_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb1_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb1_gpu13 -> layer12_attn_out_dp0_mb1_gpu13
	layer12_softmax_dp1_mb1_gpu13 -> layer12_attn_out_dp1_mb1_gpu13
	layer12_qkv_dp0_mb1_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb1_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb1 -> layer12_qkv_dp0_mb1_gpu14
	layer11_expert_agg_dp1_mb1 -> layer12_qkv_dp1_mb1_gpu14
	layer12_attn_scores_dp0_mb1_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb1_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb1_gpu14 -> layer12_attn_scores_dp0_mb1_gpu14
	layer12_qkv_dp1_mb1_gpu14 -> layer12_attn_scores_dp1_mb1_gpu14
	layer12_softmax_dp0_mb1_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb1_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb1_gpu14 -> layer12_softmax_dp0_mb1_gpu14
	layer12_attn_scores_dp1_mb1_gpu14 -> layer12_softmax_dp1_mb1_gpu14
	layer12_attn_out_dp0_mb1_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb1_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb1_gpu14 -> layer12_attn_out_dp0_mb1_gpu14
	layer12_softmax_dp1_mb1_gpu14 -> layer12_attn_out_dp1_mb1_gpu14
	layer12_qkv_dp0_mb1_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb1_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb1 -> layer12_qkv_dp0_mb1_gpu15
	layer11_expert_agg_dp1_mb1 -> layer12_qkv_dp1_mb1_gpu15
	layer12_attn_scores_dp0_mb1_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb1_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb1_gpu15 -> layer12_attn_scores_dp0_mb1_gpu15
	layer12_qkv_dp1_mb1_gpu15 -> layer12_attn_scores_dp1_mb1_gpu15
	layer12_softmax_dp0_mb1_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb1_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb1_gpu15 -> layer12_softmax_dp0_mb1_gpu15
	layer12_attn_scores_dp1_mb1_gpu15 -> layer12_softmax_dp1_mb1_gpu15
	layer12_attn_out_dp0_mb1_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb1_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb1_gpu15 -> layer12_attn_out_dp0_mb1_gpu15
	layer12_softmax_dp1_mb1_gpu15 -> layer12_attn_out_dp1_mb1_gpu15
	layer12_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb1_gpu12 -> layer12_attn_allreduce_dp0_mb1
	layer12_attn_out_dp1_mb1_gpu12 -> layer12_attn_allreduce_dp1_mb1
	layer12_attn_out_dp0_mb1_gpu13 -> layer12_attn_allreduce_dp0_mb1
	layer12_attn_out_dp1_mb1_gpu13 -> layer12_attn_allreduce_dp1_mb1
	layer12_attn_out_dp0_mb1_gpu14 -> layer12_attn_allreduce_dp0_mb1
	layer12_attn_out_dp1_mb1_gpu14 -> layer12_attn_allreduce_dp1_mb1
	layer12_attn_out_dp0_mb1_gpu15 -> layer12_attn_allreduce_dp0_mb1
	layer12_attn_out_dp1_mb1_gpu15 -> layer12_attn_allreduce_dp1_mb1
	layer12_router_dp0_mb1_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb1_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb1 -> layer12_router_dp0_mb1_gpu12
	layer12_attn_allreduce_dp1_mb1 -> layer12_router_dp1_mb1_gpu12
	layer12_expert0_mlp1_dp0_mb1_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb1_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb1_gpu12 -> layer12_expert0_mlp1_dp0_mb1_gpu12
	layer12_router_dp1_mb1_gpu12 -> layer12_expert0_mlp1_dp1_mb1_gpu12
	layer12_expert0_mlp2_dp0_mb1_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb1_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb1_gpu12 -> layer12_expert0_mlp2_dp0_mb1_gpu12
	layer12_expert0_mlp1_dp1_mb1_gpu12 -> layer12_expert0_mlp2_dp1_mb1_gpu12
	layer12_expert1_mlp1_dp0_mb1_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb1_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb1_gpu12 -> layer12_expert1_mlp1_dp0_mb1_gpu13
	layer12_router_dp1_mb1_gpu12 -> layer12_expert1_mlp1_dp1_mb1_gpu13
	layer12_expert1_mlp2_dp0_mb1_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb1_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb1_gpu13 -> layer12_expert1_mlp2_dp0_mb1_gpu13
	layer12_expert1_mlp1_dp1_mb1_gpu13 -> layer12_expert1_mlp2_dp1_mb1_gpu13
	layer12_expert2_mlp1_dp0_mb1_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb1_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb1_gpu12 -> layer12_expert2_mlp1_dp0_mb1_gpu14
	layer12_router_dp1_mb1_gpu12 -> layer12_expert2_mlp1_dp1_mb1_gpu14
	layer12_expert2_mlp2_dp0_mb1_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb1_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb1_gpu14 -> layer12_expert2_mlp2_dp0_mb1_gpu14
	layer12_expert2_mlp1_dp1_mb1_gpu14 -> layer12_expert2_mlp2_dp1_mb1_gpu14
	layer12_expert3_mlp1_dp0_mb1_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb1_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb1_gpu12 -> layer12_expert3_mlp1_dp0_mb1_gpu15
	layer12_router_dp1_mb1_gpu12 -> layer12_expert3_mlp1_dp1_mb1_gpu15
	layer12_expert3_mlp2_dp0_mb1_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb1_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb1_gpu15 -> layer12_expert3_mlp2_dp0_mb1_gpu15
	layer12_expert3_mlp1_dp1_mb1_gpu15 -> layer12_expert3_mlp2_dp1_mb1_gpu15
	layer12_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb1_gpu12 -> layer12_expert_agg_dp0_mb1
	layer12_expert0_mlp2_dp1_mb1_gpu12 -> layer12_expert_agg_dp1_mb1
	layer12_expert1_mlp2_dp0_mb1_gpu13 -> layer12_expert_agg_dp0_mb1
	layer12_expert1_mlp2_dp1_mb1_gpu13 -> layer12_expert_agg_dp1_mb1
	layer12_expert2_mlp2_dp0_mb1_gpu14 -> layer12_expert_agg_dp0_mb1
	layer12_expert2_mlp2_dp1_mb1_gpu14 -> layer12_expert_agg_dp1_mb1
	layer12_expert3_mlp2_dp0_mb1_gpu15 -> layer12_expert_agg_dp0_mb1
	layer12_expert3_mlp2_dp1_mb1_gpu15 -> layer12_expert_agg_dp1_mb1
	layer13_qkv_dp0_mb1_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb1_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb1 -> layer13_qkv_dp0_mb1_gpu12
	layer12_expert_agg_dp1_mb1 -> layer13_qkv_dp1_mb1_gpu12
	layer13_attn_scores_dp0_mb1_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb1_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb1_gpu12 -> layer13_attn_scores_dp0_mb1_gpu12
	layer13_qkv_dp1_mb1_gpu12 -> layer13_attn_scores_dp1_mb1_gpu12
	layer13_softmax_dp0_mb1_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb1_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb1_gpu12 -> layer13_softmax_dp0_mb1_gpu12
	layer13_attn_scores_dp1_mb1_gpu12 -> layer13_softmax_dp1_mb1_gpu12
	layer13_attn_out_dp0_mb1_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb1_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb1_gpu12 -> layer13_attn_out_dp0_mb1_gpu12
	layer13_softmax_dp1_mb1_gpu12 -> layer13_attn_out_dp1_mb1_gpu12
	layer13_qkv_dp0_mb1_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb1_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb1 -> layer13_qkv_dp0_mb1_gpu13
	layer12_expert_agg_dp1_mb1 -> layer13_qkv_dp1_mb1_gpu13
	layer13_attn_scores_dp0_mb1_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb1_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb1_gpu13 -> layer13_attn_scores_dp0_mb1_gpu13
	layer13_qkv_dp1_mb1_gpu13 -> layer13_attn_scores_dp1_mb1_gpu13
	layer13_softmax_dp0_mb1_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb1_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb1_gpu13 -> layer13_softmax_dp0_mb1_gpu13
	layer13_attn_scores_dp1_mb1_gpu13 -> layer13_softmax_dp1_mb1_gpu13
	layer13_attn_out_dp0_mb1_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb1_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb1_gpu13 -> layer13_attn_out_dp0_mb1_gpu13
	layer13_softmax_dp1_mb1_gpu13 -> layer13_attn_out_dp1_mb1_gpu13
	layer13_qkv_dp0_mb1_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb1_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb1 -> layer13_qkv_dp0_mb1_gpu14
	layer12_expert_agg_dp1_mb1 -> layer13_qkv_dp1_mb1_gpu14
	layer13_attn_scores_dp0_mb1_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb1_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb1_gpu14 -> layer13_attn_scores_dp0_mb1_gpu14
	layer13_qkv_dp1_mb1_gpu14 -> layer13_attn_scores_dp1_mb1_gpu14
	layer13_softmax_dp0_mb1_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb1_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb1_gpu14 -> layer13_softmax_dp0_mb1_gpu14
	layer13_attn_scores_dp1_mb1_gpu14 -> layer13_softmax_dp1_mb1_gpu14
	layer13_attn_out_dp0_mb1_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb1_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb1_gpu14 -> layer13_attn_out_dp0_mb1_gpu14
	layer13_softmax_dp1_mb1_gpu14 -> layer13_attn_out_dp1_mb1_gpu14
	layer13_qkv_dp0_mb1_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb1_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb1 -> layer13_qkv_dp0_mb1_gpu15
	layer12_expert_agg_dp1_mb1 -> layer13_qkv_dp1_mb1_gpu15
	layer13_attn_scores_dp0_mb1_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb1_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb1_gpu15 -> layer13_attn_scores_dp0_mb1_gpu15
	layer13_qkv_dp1_mb1_gpu15 -> layer13_attn_scores_dp1_mb1_gpu15
	layer13_softmax_dp0_mb1_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb1_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb1_gpu15 -> layer13_softmax_dp0_mb1_gpu15
	layer13_attn_scores_dp1_mb1_gpu15 -> layer13_softmax_dp1_mb1_gpu15
	layer13_attn_out_dp0_mb1_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb1_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb1_gpu15 -> layer13_attn_out_dp0_mb1_gpu15
	layer13_softmax_dp1_mb1_gpu15 -> layer13_attn_out_dp1_mb1_gpu15
	layer13_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb1_gpu12 -> layer13_attn_allreduce_dp0_mb1
	layer13_attn_out_dp1_mb1_gpu12 -> layer13_attn_allreduce_dp1_mb1
	layer13_attn_out_dp0_mb1_gpu13 -> layer13_attn_allreduce_dp0_mb1
	layer13_attn_out_dp1_mb1_gpu13 -> layer13_attn_allreduce_dp1_mb1
	layer13_attn_out_dp0_mb1_gpu14 -> layer13_attn_allreduce_dp0_mb1
	layer13_attn_out_dp1_mb1_gpu14 -> layer13_attn_allreduce_dp1_mb1
	layer13_attn_out_dp0_mb1_gpu15 -> layer13_attn_allreduce_dp0_mb1
	layer13_attn_out_dp1_mb1_gpu15 -> layer13_attn_allreduce_dp1_mb1
	layer13_router_dp0_mb1_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb1_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb1 -> layer13_router_dp0_mb1_gpu12
	layer13_attn_allreduce_dp1_mb1 -> layer13_router_dp1_mb1_gpu12
	layer13_expert0_mlp1_dp0_mb1_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb1_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb1_gpu12 -> layer13_expert0_mlp1_dp0_mb1_gpu12
	layer13_router_dp1_mb1_gpu12 -> layer13_expert0_mlp1_dp1_mb1_gpu12
	layer13_expert0_mlp2_dp0_mb1_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb1_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb1_gpu12 -> layer13_expert0_mlp2_dp0_mb1_gpu12
	layer13_expert0_mlp1_dp1_mb1_gpu12 -> layer13_expert0_mlp2_dp1_mb1_gpu12
	layer13_expert1_mlp1_dp0_mb1_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb1_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb1_gpu12 -> layer13_expert1_mlp1_dp0_mb1_gpu13
	layer13_router_dp1_mb1_gpu12 -> layer13_expert1_mlp1_dp1_mb1_gpu13
	layer13_expert1_mlp2_dp0_mb1_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb1_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb1_gpu13 -> layer13_expert1_mlp2_dp0_mb1_gpu13
	layer13_expert1_mlp1_dp1_mb1_gpu13 -> layer13_expert1_mlp2_dp1_mb1_gpu13
	layer13_expert2_mlp1_dp0_mb1_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb1_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb1_gpu12 -> layer13_expert2_mlp1_dp0_mb1_gpu14
	layer13_router_dp1_mb1_gpu12 -> layer13_expert2_mlp1_dp1_mb1_gpu14
	layer13_expert2_mlp2_dp0_mb1_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb1_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb1_gpu14 -> layer13_expert2_mlp2_dp0_mb1_gpu14
	layer13_expert2_mlp1_dp1_mb1_gpu14 -> layer13_expert2_mlp2_dp1_mb1_gpu14
	layer13_expert3_mlp1_dp0_mb1_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb1_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb1_gpu12 -> layer13_expert3_mlp1_dp0_mb1_gpu15
	layer13_router_dp1_mb1_gpu12 -> layer13_expert3_mlp1_dp1_mb1_gpu15
	layer13_expert3_mlp2_dp0_mb1_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb1_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb1_gpu15 -> layer13_expert3_mlp2_dp0_mb1_gpu15
	layer13_expert3_mlp1_dp1_mb1_gpu15 -> layer13_expert3_mlp2_dp1_mb1_gpu15
	layer13_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb1_gpu12 -> layer13_expert_agg_dp0_mb1
	layer13_expert0_mlp2_dp1_mb1_gpu12 -> layer13_expert_agg_dp1_mb1
	layer13_expert1_mlp2_dp0_mb1_gpu13 -> layer13_expert_agg_dp0_mb1
	layer13_expert1_mlp2_dp1_mb1_gpu13 -> layer13_expert_agg_dp1_mb1
	layer13_expert2_mlp2_dp0_mb1_gpu14 -> layer13_expert_agg_dp0_mb1
	layer13_expert2_mlp2_dp1_mb1_gpu14 -> layer13_expert_agg_dp1_mb1
	layer13_expert3_mlp2_dp0_mb1_gpu15 -> layer13_expert_agg_dp0_mb1
	layer13_expert3_mlp2_dp1_mb1_gpu15 -> layer13_expert_agg_dp1_mb1
	layer14_qkv_dp0_mb1_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb1_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb1 -> layer14_qkv_dp0_mb1_gpu12
	layer13_expert_agg_dp1_mb1 -> layer14_qkv_dp1_mb1_gpu12
	layer14_attn_scores_dp0_mb1_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb1_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb1_gpu12 -> layer14_attn_scores_dp0_mb1_gpu12
	layer14_qkv_dp1_mb1_gpu12 -> layer14_attn_scores_dp1_mb1_gpu12
	layer14_softmax_dp0_mb1_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb1_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb1_gpu12 -> layer14_softmax_dp0_mb1_gpu12
	layer14_attn_scores_dp1_mb1_gpu12 -> layer14_softmax_dp1_mb1_gpu12
	layer14_attn_out_dp0_mb1_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb1_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb1_gpu12 -> layer14_attn_out_dp0_mb1_gpu12
	layer14_softmax_dp1_mb1_gpu12 -> layer14_attn_out_dp1_mb1_gpu12
	layer14_qkv_dp0_mb1_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb1_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb1 -> layer14_qkv_dp0_mb1_gpu13
	layer13_expert_agg_dp1_mb1 -> layer14_qkv_dp1_mb1_gpu13
	layer14_attn_scores_dp0_mb1_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb1_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb1_gpu13 -> layer14_attn_scores_dp0_mb1_gpu13
	layer14_qkv_dp1_mb1_gpu13 -> layer14_attn_scores_dp1_mb1_gpu13
	layer14_softmax_dp0_mb1_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb1_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb1_gpu13 -> layer14_softmax_dp0_mb1_gpu13
	layer14_attn_scores_dp1_mb1_gpu13 -> layer14_softmax_dp1_mb1_gpu13
	layer14_attn_out_dp0_mb1_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb1_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb1_gpu13 -> layer14_attn_out_dp0_mb1_gpu13
	layer14_softmax_dp1_mb1_gpu13 -> layer14_attn_out_dp1_mb1_gpu13
	layer14_qkv_dp0_mb1_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb1_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb1 -> layer14_qkv_dp0_mb1_gpu14
	layer13_expert_agg_dp1_mb1 -> layer14_qkv_dp1_mb1_gpu14
	layer14_attn_scores_dp0_mb1_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb1_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb1_gpu14 -> layer14_attn_scores_dp0_mb1_gpu14
	layer14_qkv_dp1_mb1_gpu14 -> layer14_attn_scores_dp1_mb1_gpu14
	layer14_softmax_dp0_mb1_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb1_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb1_gpu14 -> layer14_softmax_dp0_mb1_gpu14
	layer14_attn_scores_dp1_mb1_gpu14 -> layer14_softmax_dp1_mb1_gpu14
	layer14_attn_out_dp0_mb1_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb1_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb1_gpu14 -> layer14_attn_out_dp0_mb1_gpu14
	layer14_softmax_dp1_mb1_gpu14 -> layer14_attn_out_dp1_mb1_gpu14
	layer14_qkv_dp0_mb1_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb1_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb1 -> layer14_qkv_dp0_mb1_gpu15
	layer13_expert_agg_dp1_mb1 -> layer14_qkv_dp1_mb1_gpu15
	layer14_attn_scores_dp0_mb1_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb1_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb1_gpu15 -> layer14_attn_scores_dp0_mb1_gpu15
	layer14_qkv_dp1_mb1_gpu15 -> layer14_attn_scores_dp1_mb1_gpu15
	layer14_softmax_dp0_mb1_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb1_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb1_gpu15 -> layer14_softmax_dp0_mb1_gpu15
	layer14_attn_scores_dp1_mb1_gpu15 -> layer14_softmax_dp1_mb1_gpu15
	layer14_attn_out_dp0_mb1_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb1_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb1_gpu15 -> layer14_attn_out_dp0_mb1_gpu15
	layer14_softmax_dp1_mb1_gpu15 -> layer14_attn_out_dp1_mb1_gpu15
	layer14_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb1_gpu12 -> layer14_attn_allreduce_dp0_mb1
	layer14_attn_out_dp1_mb1_gpu12 -> layer14_attn_allreduce_dp1_mb1
	layer14_attn_out_dp0_mb1_gpu13 -> layer14_attn_allreduce_dp0_mb1
	layer14_attn_out_dp1_mb1_gpu13 -> layer14_attn_allreduce_dp1_mb1
	layer14_attn_out_dp0_mb1_gpu14 -> layer14_attn_allreduce_dp0_mb1
	layer14_attn_out_dp1_mb1_gpu14 -> layer14_attn_allreduce_dp1_mb1
	layer14_attn_out_dp0_mb1_gpu15 -> layer14_attn_allreduce_dp0_mb1
	layer14_attn_out_dp1_mb1_gpu15 -> layer14_attn_allreduce_dp1_mb1
	layer14_router_dp0_mb1_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb1_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb1 -> layer14_router_dp0_mb1_gpu12
	layer14_attn_allreduce_dp1_mb1 -> layer14_router_dp1_mb1_gpu12
	layer14_expert0_mlp1_dp0_mb1_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb1_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb1_gpu12 -> layer14_expert0_mlp1_dp0_mb1_gpu12
	layer14_router_dp1_mb1_gpu12 -> layer14_expert0_mlp1_dp1_mb1_gpu12
	layer14_expert0_mlp2_dp0_mb1_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb1_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb1_gpu12 -> layer14_expert0_mlp2_dp0_mb1_gpu12
	layer14_expert0_mlp1_dp1_mb1_gpu12 -> layer14_expert0_mlp2_dp1_mb1_gpu12
	layer14_expert1_mlp1_dp0_mb1_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb1_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb1_gpu12 -> layer14_expert1_mlp1_dp0_mb1_gpu13
	layer14_router_dp1_mb1_gpu12 -> layer14_expert1_mlp1_dp1_mb1_gpu13
	layer14_expert1_mlp2_dp0_mb1_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb1_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb1_gpu13 -> layer14_expert1_mlp2_dp0_mb1_gpu13
	layer14_expert1_mlp1_dp1_mb1_gpu13 -> layer14_expert1_mlp2_dp1_mb1_gpu13
	layer14_expert2_mlp1_dp0_mb1_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb1_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb1_gpu12 -> layer14_expert2_mlp1_dp0_mb1_gpu14
	layer14_router_dp1_mb1_gpu12 -> layer14_expert2_mlp1_dp1_mb1_gpu14
	layer14_expert2_mlp2_dp0_mb1_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb1_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb1_gpu14 -> layer14_expert2_mlp2_dp0_mb1_gpu14
	layer14_expert2_mlp1_dp1_mb1_gpu14 -> layer14_expert2_mlp2_dp1_mb1_gpu14
	layer14_expert3_mlp1_dp0_mb1_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb1_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb1_gpu12 -> layer14_expert3_mlp1_dp0_mb1_gpu15
	layer14_router_dp1_mb1_gpu12 -> layer14_expert3_mlp1_dp1_mb1_gpu15
	layer14_expert3_mlp2_dp0_mb1_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb1_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb1_gpu15 -> layer14_expert3_mlp2_dp0_mb1_gpu15
	layer14_expert3_mlp1_dp1_mb1_gpu15 -> layer14_expert3_mlp2_dp1_mb1_gpu15
	layer14_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb1_gpu12 -> layer14_expert_agg_dp0_mb1
	layer14_expert0_mlp2_dp1_mb1_gpu12 -> layer14_expert_agg_dp1_mb1
	layer14_expert1_mlp2_dp0_mb1_gpu13 -> layer14_expert_agg_dp0_mb1
	layer14_expert1_mlp2_dp1_mb1_gpu13 -> layer14_expert_agg_dp1_mb1
	layer14_expert2_mlp2_dp0_mb1_gpu14 -> layer14_expert_agg_dp0_mb1
	layer14_expert2_mlp2_dp1_mb1_gpu14 -> layer14_expert_agg_dp1_mb1
	layer14_expert3_mlp2_dp0_mb1_gpu15 -> layer14_expert_agg_dp0_mb1
	layer14_expert3_mlp2_dp1_mb1_gpu15 -> layer14_expert_agg_dp1_mb1
	layer15_qkv_dp0_mb1_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb1_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb1 -> layer15_qkv_dp0_mb1_gpu12
	layer14_expert_agg_dp1_mb1 -> layer15_qkv_dp1_mb1_gpu12
	layer15_attn_scores_dp0_mb1_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb1_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb1_gpu12 -> layer15_attn_scores_dp0_mb1_gpu12
	layer15_qkv_dp1_mb1_gpu12 -> layer15_attn_scores_dp1_mb1_gpu12
	layer15_softmax_dp0_mb1_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb1_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb1_gpu12 -> layer15_softmax_dp0_mb1_gpu12
	layer15_attn_scores_dp1_mb1_gpu12 -> layer15_softmax_dp1_mb1_gpu12
	layer15_attn_out_dp0_mb1_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb1_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb1_gpu12 -> layer15_attn_out_dp0_mb1_gpu12
	layer15_softmax_dp1_mb1_gpu12 -> layer15_attn_out_dp1_mb1_gpu12
	layer15_qkv_dp0_mb1_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb1_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb1 -> layer15_qkv_dp0_mb1_gpu13
	layer14_expert_agg_dp1_mb1 -> layer15_qkv_dp1_mb1_gpu13
	layer15_attn_scores_dp0_mb1_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb1_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb1_gpu13 -> layer15_attn_scores_dp0_mb1_gpu13
	layer15_qkv_dp1_mb1_gpu13 -> layer15_attn_scores_dp1_mb1_gpu13
	layer15_softmax_dp0_mb1_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb1_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb1_gpu13 -> layer15_softmax_dp0_mb1_gpu13
	layer15_attn_scores_dp1_mb1_gpu13 -> layer15_softmax_dp1_mb1_gpu13
	layer15_attn_out_dp0_mb1_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb1_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb1_gpu13 -> layer15_attn_out_dp0_mb1_gpu13
	layer15_softmax_dp1_mb1_gpu13 -> layer15_attn_out_dp1_mb1_gpu13
	layer15_qkv_dp0_mb1_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb1_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb1 -> layer15_qkv_dp0_mb1_gpu14
	layer14_expert_agg_dp1_mb1 -> layer15_qkv_dp1_mb1_gpu14
	layer15_attn_scores_dp0_mb1_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb1_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb1_gpu14 -> layer15_attn_scores_dp0_mb1_gpu14
	layer15_qkv_dp1_mb1_gpu14 -> layer15_attn_scores_dp1_mb1_gpu14
	layer15_softmax_dp0_mb1_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb1_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb1_gpu14 -> layer15_softmax_dp0_mb1_gpu14
	layer15_attn_scores_dp1_mb1_gpu14 -> layer15_softmax_dp1_mb1_gpu14
	layer15_attn_out_dp0_mb1_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb1_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb1_gpu14 -> layer15_attn_out_dp0_mb1_gpu14
	layer15_softmax_dp1_mb1_gpu14 -> layer15_attn_out_dp1_mb1_gpu14
	layer15_qkv_dp0_mb1_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb1_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb1 -> layer15_qkv_dp0_mb1_gpu15
	layer14_expert_agg_dp1_mb1 -> layer15_qkv_dp1_mb1_gpu15
	layer15_attn_scores_dp0_mb1_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb1_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb1_gpu15 -> layer15_attn_scores_dp0_mb1_gpu15
	layer15_qkv_dp1_mb1_gpu15 -> layer15_attn_scores_dp1_mb1_gpu15
	layer15_softmax_dp0_mb1_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb1_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb1_gpu15 -> layer15_softmax_dp0_mb1_gpu15
	layer15_attn_scores_dp1_mb1_gpu15 -> layer15_softmax_dp1_mb1_gpu15
	layer15_attn_out_dp0_mb1_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb1_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb1_gpu15 -> layer15_attn_out_dp0_mb1_gpu15
	layer15_softmax_dp1_mb1_gpu15 -> layer15_attn_out_dp1_mb1_gpu15
	layer15_attn_allreduce_dp0_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb1 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb1_gpu12 -> layer15_attn_allreduce_dp0_mb1
	layer15_attn_out_dp1_mb1_gpu12 -> layer15_attn_allreduce_dp1_mb1
	layer15_attn_out_dp0_mb1_gpu13 -> layer15_attn_allreduce_dp0_mb1
	layer15_attn_out_dp1_mb1_gpu13 -> layer15_attn_allreduce_dp1_mb1
	layer15_attn_out_dp0_mb1_gpu14 -> layer15_attn_allreduce_dp0_mb1
	layer15_attn_out_dp1_mb1_gpu14 -> layer15_attn_allreduce_dp1_mb1
	layer15_attn_out_dp0_mb1_gpu15 -> layer15_attn_allreduce_dp0_mb1
	layer15_attn_out_dp1_mb1_gpu15 -> layer15_attn_allreduce_dp1_mb1
	layer15_router_dp0_mb1_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb1_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb1 -> layer15_router_dp0_mb1_gpu12
	layer15_attn_allreduce_dp1_mb1 -> layer15_router_dp1_mb1_gpu12
	layer15_expert0_mlp1_dp0_mb1_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb1_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb1_gpu12 -> layer15_expert0_mlp1_dp0_mb1_gpu12
	layer15_router_dp1_mb1_gpu12 -> layer15_expert0_mlp1_dp1_mb1_gpu12
	layer15_expert0_mlp2_dp0_mb1_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb1_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb1_gpu12 -> layer15_expert0_mlp2_dp0_mb1_gpu12
	layer15_expert0_mlp1_dp1_mb1_gpu12 -> layer15_expert0_mlp2_dp1_mb1_gpu12
	layer15_expert1_mlp1_dp0_mb1_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb1_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb1_gpu12 -> layer15_expert1_mlp1_dp0_mb1_gpu13
	layer15_router_dp1_mb1_gpu12 -> layer15_expert1_mlp1_dp1_mb1_gpu13
	layer15_expert1_mlp2_dp0_mb1_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb1_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb1_gpu13 -> layer15_expert1_mlp2_dp0_mb1_gpu13
	layer15_expert1_mlp1_dp1_mb1_gpu13 -> layer15_expert1_mlp2_dp1_mb1_gpu13
	layer15_expert2_mlp1_dp0_mb1_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb1_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb1_gpu12 -> layer15_expert2_mlp1_dp0_mb1_gpu14
	layer15_router_dp1_mb1_gpu12 -> layer15_expert2_mlp1_dp1_mb1_gpu14
	layer15_expert2_mlp2_dp0_mb1_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb1_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb1_gpu14 -> layer15_expert2_mlp2_dp0_mb1_gpu14
	layer15_expert2_mlp1_dp1_mb1_gpu14 -> layer15_expert2_mlp2_dp1_mb1_gpu14
	layer15_expert3_mlp1_dp0_mb1_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb1_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb1_gpu12 -> layer15_expert3_mlp1_dp0_mb1_gpu15
	layer15_router_dp1_mb1_gpu12 -> layer15_expert3_mlp1_dp1_mb1_gpu15
	layer15_expert3_mlp2_dp0_mb1_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb1_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb1_gpu15 -> layer15_expert3_mlp2_dp0_mb1_gpu15
	layer15_expert3_mlp1_dp1_mb1_gpu15 -> layer15_expert3_mlp2_dp1_mb1_gpu15
	layer15_expert_agg_dp0_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb1 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb1_gpu12 -> layer15_expert_agg_dp0_mb1
	layer15_expert0_mlp2_dp1_mb1_gpu12 -> layer15_expert_agg_dp1_mb1
	layer15_expert1_mlp2_dp0_mb1_gpu13 -> layer15_expert_agg_dp0_mb1
	layer15_expert1_mlp2_dp1_mb1_gpu13 -> layer15_expert_agg_dp1_mb1
	layer15_expert2_mlp2_dp0_mb1_gpu14 -> layer15_expert_agg_dp0_mb1
	layer15_expert2_mlp2_dp1_mb1_gpu14 -> layer15_expert_agg_dp1_mb1
	layer15_expert3_mlp2_dp0_mb1_gpu15 -> layer15_expert_agg_dp0_mb1
	layer15_expert3_mlp2_dp1_mb1_gpu15 -> layer15_expert_agg_dp1_mb1
	layer12_qkv_dp0_mb2_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb2_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb2 -> layer12_qkv_dp0_mb2_gpu12
	layer11_expert_agg_dp1_mb2 -> layer12_qkv_dp1_mb2_gpu12
	layer12_attn_scores_dp0_mb2_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb2_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb2_gpu12 -> layer12_attn_scores_dp0_mb2_gpu12
	layer12_qkv_dp1_mb2_gpu12 -> layer12_attn_scores_dp1_mb2_gpu12
	layer12_softmax_dp0_mb2_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb2_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb2_gpu12 -> layer12_softmax_dp0_mb2_gpu12
	layer12_attn_scores_dp1_mb2_gpu12 -> layer12_softmax_dp1_mb2_gpu12
	layer12_attn_out_dp0_mb2_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb2_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb2_gpu12 -> layer12_attn_out_dp0_mb2_gpu12
	layer12_softmax_dp1_mb2_gpu12 -> layer12_attn_out_dp1_mb2_gpu12
	layer12_qkv_dp0_mb2_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb2_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb2 -> layer12_qkv_dp0_mb2_gpu13
	layer11_expert_agg_dp1_mb2 -> layer12_qkv_dp1_mb2_gpu13
	layer12_attn_scores_dp0_mb2_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb2_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb2_gpu13 -> layer12_attn_scores_dp0_mb2_gpu13
	layer12_qkv_dp1_mb2_gpu13 -> layer12_attn_scores_dp1_mb2_gpu13
	layer12_softmax_dp0_mb2_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb2_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb2_gpu13 -> layer12_softmax_dp0_mb2_gpu13
	layer12_attn_scores_dp1_mb2_gpu13 -> layer12_softmax_dp1_mb2_gpu13
	layer12_attn_out_dp0_mb2_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb2_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb2_gpu13 -> layer12_attn_out_dp0_mb2_gpu13
	layer12_softmax_dp1_mb2_gpu13 -> layer12_attn_out_dp1_mb2_gpu13
	layer12_qkv_dp0_mb2_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb2_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb2 -> layer12_qkv_dp0_mb2_gpu14
	layer11_expert_agg_dp1_mb2 -> layer12_qkv_dp1_mb2_gpu14
	layer12_attn_scores_dp0_mb2_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb2_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb2_gpu14 -> layer12_attn_scores_dp0_mb2_gpu14
	layer12_qkv_dp1_mb2_gpu14 -> layer12_attn_scores_dp1_mb2_gpu14
	layer12_softmax_dp0_mb2_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb2_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb2_gpu14 -> layer12_softmax_dp0_mb2_gpu14
	layer12_attn_scores_dp1_mb2_gpu14 -> layer12_softmax_dp1_mb2_gpu14
	layer12_attn_out_dp0_mb2_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb2_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb2_gpu14 -> layer12_attn_out_dp0_mb2_gpu14
	layer12_softmax_dp1_mb2_gpu14 -> layer12_attn_out_dp1_mb2_gpu14
	layer12_qkv_dp0_mb2_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb2_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb2 -> layer12_qkv_dp0_mb2_gpu15
	layer11_expert_agg_dp1_mb2 -> layer12_qkv_dp1_mb2_gpu15
	layer12_attn_scores_dp0_mb2_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb2_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb2_gpu15 -> layer12_attn_scores_dp0_mb2_gpu15
	layer12_qkv_dp1_mb2_gpu15 -> layer12_attn_scores_dp1_mb2_gpu15
	layer12_softmax_dp0_mb2_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb2_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb2_gpu15 -> layer12_softmax_dp0_mb2_gpu15
	layer12_attn_scores_dp1_mb2_gpu15 -> layer12_softmax_dp1_mb2_gpu15
	layer12_attn_out_dp0_mb2_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb2_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb2_gpu15 -> layer12_attn_out_dp0_mb2_gpu15
	layer12_softmax_dp1_mb2_gpu15 -> layer12_attn_out_dp1_mb2_gpu15
	layer12_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb2_gpu12 -> layer12_attn_allreduce_dp0_mb2
	layer12_attn_out_dp1_mb2_gpu12 -> layer12_attn_allreduce_dp1_mb2
	layer12_attn_out_dp0_mb2_gpu13 -> layer12_attn_allreduce_dp0_mb2
	layer12_attn_out_dp1_mb2_gpu13 -> layer12_attn_allreduce_dp1_mb2
	layer12_attn_out_dp0_mb2_gpu14 -> layer12_attn_allreduce_dp0_mb2
	layer12_attn_out_dp1_mb2_gpu14 -> layer12_attn_allreduce_dp1_mb2
	layer12_attn_out_dp0_mb2_gpu15 -> layer12_attn_allreduce_dp0_mb2
	layer12_attn_out_dp1_mb2_gpu15 -> layer12_attn_allreduce_dp1_mb2
	layer12_router_dp0_mb2_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb2_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb2 -> layer12_router_dp0_mb2_gpu12
	layer12_attn_allreduce_dp1_mb2 -> layer12_router_dp1_mb2_gpu12
	layer12_expert0_mlp1_dp0_mb2_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb2_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb2_gpu12 -> layer12_expert0_mlp1_dp0_mb2_gpu12
	layer12_router_dp1_mb2_gpu12 -> layer12_expert0_mlp1_dp1_mb2_gpu12
	layer12_expert0_mlp2_dp0_mb2_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb2_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb2_gpu12 -> layer12_expert0_mlp2_dp0_mb2_gpu12
	layer12_expert0_mlp1_dp1_mb2_gpu12 -> layer12_expert0_mlp2_dp1_mb2_gpu12
	layer12_expert1_mlp1_dp0_mb2_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb2_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb2_gpu12 -> layer12_expert1_mlp1_dp0_mb2_gpu13
	layer12_router_dp1_mb2_gpu12 -> layer12_expert1_mlp1_dp1_mb2_gpu13
	layer12_expert1_mlp2_dp0_mb2_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb2_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb2_gpu13 -> layer12_expert1_mlp2_dp0_mb2_gpu13
	layer12_expert1_mlp1_dp1_mb2_gpu13 -> layer12_expert1_mlp2_dp1_mb2_gpu13
	layer12_expert2_mlp1_dp0_mb2_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb2_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb2_gpu12 -> layer12_expert2_mlp1_dp0_mb2_gpu14
	layer12_router_dp1_mb2_gpu12 -> layer12_expert2_mlp1_dp1_mb2_gpu14
	layer12_expert2_mlp2_dp0_mb2_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb2_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb2_gpu14 -> layer12_expert2_mlp2_dp0_mb2_gpu14
	layer12_expert2_mlp1_dp1_mb2_gpu14 -> layer12_expert2_mlp2_dp1_mb2_gpu14
	layer12_expert3_mlp1_dp0_mb2_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb2_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb2_gpu12 -> layer12_expert3_mlp1_dp0_mb2_gpu15
	layer12_router_dp1_mb2_gpu12 -> layer12_expert3_mlp1_dp1_mb2_gpu15
	layer12_expert3_mlp2_dp0_mb2_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb2_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb2_gpu15 -> layer12_expert3_mlp2_dp0_mb2_gpu15
	layer12_expert3_mlp1_dp1_mb2_gpu15 -> layer12_expert3_mlp2_dp1_mb2_gpu15
	layer12_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb2_gpu12 -> layer12_expert_agg_dp0_mb2
	layer12_expert0_mlp2_dp1_mb2_gpu12 -> layer12_expert_agg_dp1_mb2
	layer12_expert1_mlp2_dp0_mb2_gpu13 -> layer12_expert_agg_dp0_mb2
	layer12_expert1_mlp2_dp1_mb2_gpu13 -> layer12_expert_agg_dp1_mb2
	layer12_expert2_mlp2_dp0_mb2_gpu14 -> layer12_expert_agg_dp0_mb2
	layer12_expert2_mlp2_dp1_mb2_gpu14 -> layer12_expert_agg_dp1_mb2
	layer12_expert3_mlp2_dp0_mb2_gpu15 -> layer12_expert_agg_dp0_mb2
	layer12_expert3_mlp2_dp1_mb2_gpu15 -> layer12_expert_agg_dp1_mb2
	layer13_qkv_dp0_mb2_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb2_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb2 -> layer13_qkv_dp0_mb2_gpu12
	layer12_expert_agg_dp1_mb2 -> layer13_qkv_dp1_mb2_gpu12
	layer13_attn_scores_dp0_mb2_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb2_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb2_gpu12 -> layer13_attn_scores_dp0_mb2_gpu12
	layer13_qkv_dp1_mb2_gpu12 -> layer13_attn_scores_dp1_mb2_gpu12
	layer13_softmax_dp0_mb2_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb2_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb2_gpu12 -> layer13_softmax_dp0_mb2_gpu12
	layer13_attn_scores_dp1_mb2_gpu12 -> layer13_softmax_dp1_mb2_gpu12
	layer13_attn_out_dp0_mb2_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb2_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb2_gpu12 -> layer13_attn_out_dp0_mb2_gpu12
	layer13_softmax_dp1_mb2_gpu12 -> layer13_attn_out_dp1_mb2_gpu12
	layer13_qkv_dp0_mb2_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb2_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb2 -> layer13_qkv_dp0_mb2_gpu13
	layer12_expert_agg_dp1_mb2 -> layer13_qkv_dp1_mb2_gpu13
	layer13_attn_scores_dp0_mb2_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb2_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb2_gpu13 -> layer13_attn_scores_dp0_mb2_gpu13
	layer13_qkv_dp1_mb2_gpu13 -> layer13_attn_scores_dp1_mb2_gpu13
	layer13_softmax_dp0_mb2_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb2_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb2_gpu13 -> layer13_softmax_dp0_mb2_gpu13
	layer13_attn_scores_dp1_mb2_gpu13 -> layer13_softmax_dp1_mb2_gpu13
	layer13_attn_out_dp0_mb2_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb2_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb2_gpu13 -> layer13_attn_out_dp0_mb2_gpu13
	layer13_softmax_dp1_mb2_gpu13 -> layer13_attn_out_dp1_mb2_gpu13
	layer13_qkv_dp0_mb2_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb2_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb2 -> layer13_qkv_dp0_mb2_gpu14
	layer12_expert_agg_dp1_mb2 -> layer13_qkv_dp1_mb2_gpu14
	layer13_attn_scores_dp0_mb2_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb2_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb2_gpu14 -> layer13_attn_scores_dp0_mb2_gpu14
	layer13_qkv_dp1_mb2_gpu14 -> layer13_attn_scores_dp1_mb2_gpu14
	layer13_softmax_dp0_mb2_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb2_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb2_gpu14 -> layer13_softmax_dp0_mb2_gpu14
	layer13_attn_scores_dp1_mb2_gpu14 -> layer13_softmax_dp1_mb2_gpu14
	layer13_attn_out_dp0_mb2_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb2_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb2_gpu14 -> layer13_attn_out_dp0_mb2_gpu14
	layer13_softmax_dp1_mb2_gpu14 -> layer13_attn_out_dp1_mb2_gpu14
	layer13_qkv_dp0_mb2_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb2_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb2 -> layer13_qkv_dp0_mb2_gpu15
	layer12_expert_agg_dp1_mb2 -> layer13_qkv_dp1_mb2_gpu15
	layer13_attn_scores_dp0_mb2_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb2_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb2_gpu15 -> layer13_attn_scores_dp0_mb2_gpu15
	layer13_qkv_dp1_mb2_gpu15 -> layer13_attn_scores_dp1_mb2_gpu15
	layer13_softmax_dp0_mb2_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb2_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb2_gpu15 -> layer13_softmax_dp0_mb2_gpu15
	layer13_attn_scores_dp1_mb2_gpu15 -> layer13_softmax_dp1_mb2_gpu15
	layer13_attn_out_dp0_mb2_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb2_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb2_gpu15 -> layer13_attn_out_dp0_mb2_gpu15
	layer13_softmax_dp1_mb2_gpu15 -> layer13_attn_out_dp1_mb2_gpu15
	layer13_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb2_gpu12 -> layer13_attn_allreduce_dp0_mb2
	layer13_attn_out_dp1_mb2_gpu12 -> layer13_attn_allreduce_dp1_mb2
	layer13_attn_out_dp0_mb2_gpu13 -> layer13_attn_allreduce_dp0_mb2
	layer13_attn_out_dp1_mb2_gpu13 -> layer13_attn_allreduce_dp1_mb2
	layer13_attn_out_dp0_mb2_gpu14 -> layer13_attn_allreduce_dp0_mb2
	layer13_attn_out_dp1_mb2_gpu14 -> layer13_attn_allreduce_dp1_mb2
	layer13_attn_out_dp0_mb2_gpu15 -> layer13_attn_allreduce_dp0_mb2
	layer13_attn_out_dp1_mb2_gpu15 -> layer13_attn_allreduce_dp1_mb2
	layer13_router_dp0_mb2_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb2_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb2 -> layer13_router_dp0_mb2_gpu12
	layer13_attn_allreduce_dp1_mb2 -> layer13_router_dp1_mb2_gpu12
	layer13_expert0_mlp1_dp0_mb2_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb2_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb2_gpu12 -> layer13_expert0_mlp1_dp0_mb2_gpu12
	layer13_router_dp1_mb2_gpu12 -> layer13_expert0_mlp1_dp1_mb2_gpu12
	layer13_expert0_mlp2_dp0_mb2_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb2_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb2_gpu12 -> layer13_expert0_mlp2_dp0_mb2_gpu12
	layer13_expert0_mlp1_dp1_mb2_gpu12 -> layer13_expert0_mlp2_dp1_mb2_gpu12
	layer13_expert1_mlp1_dp0_mb2_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb2_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb2_gpu12 -> layer13_expert1_mlp1_dp0_mb2_gpu13
	layer13_router_dp1_mb2_gpu12 -> layer13_expert1_mlp1_dp1_mb2_gpu13
	layer13_expert1_mlp2_dp0_mb2_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb2_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb2_gpu13 -> layer13_expert1_mlp2_dp0_mb2_gpu13
	layer13_expert1_mlp1_dp1_mb2_gpu13 -> layer13_expert1_mlp2_dp1_mb2_gpu13
	layer13_expert2_mlp1_dp0_mb2_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb2_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb2_gpu12 -> layer13_expert2_mlp1_dp0_mb2_gpu14
	layer13_router_dp1_mb2_gpu12 -> layer13_expert2_mlp1_dp1_mb2_gpu14
	layer13_expert2_mlp2_dp0_mb2_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb2_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb2_gpu14 -> layer13_expert2_mlp2_dp0_mb2_gpu14
	layer13_expert2_mlp1_dp1_mb2_gpu14 -> layer13_expert2_mlp2_dp1_mb2_gpu14
	layer13_expert3_mlp1_dp0_mb2_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb2_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb2_gpu12 -> layer13_expert3_mlp1_dp0_mb2_gpu15
	layer13_router_dp1_mb2_gpu12 -> layer13_expert3_mlp1_dp1_mb2_gpu15
	layer13_expert3_mlp2_dp0_mb2_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb2_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb2_gpu15 -> layer13_expert3_mlp2_dp0_mb2_gpu15
	layer13_expert3_mlp1_dp1_mb2_gpu15 -> layer13_expert3_mlp2_dp1_mb2_gpu15
	layer13_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb2_gpu12 -> layer13_expert_agg_dp0_mb2
	layer13_expert0_mlp2_dp1_mb2_gpu12 -> layer13_expert_agg_dp1_mb2
	layer13_expert1_mlp2_dp0_mb2_gpu13 -> layer13_expert_agg_dp0_mb2
	layer13_expert1_mlp2_dp1_mb2_gpu13 -> layer13_expert_agg_dp1_mb2
	layer13_expert2_mlp2_dp0_mb2_gpu14 -> layer13_expert_agg_dp0_mb2
	layer13_expert2_mlp2_dp1_mb2_gpu14 -> layer13_expert_agg_dp1_mb2
	layer13_expert3_mlp2_dp0_mb2_gpu15 -> layer13_expert_agg_dp0_mb2
	layer13_expert3_mlp2_dp1_mb2_gpu15 -> layer13_expert_agg_dp1_mb2
	layer14_qkv_dp0_mb2_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb2_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb2 -> layer14_qkv_dp0_mb2_gpu12
	layer13_expert_agg_dp1_mb2 -> layer14_qkv_dp1_mb2_gpu12
	layer14_attn_scores_dp0_mb2_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb2_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb2_gpu12 -> layer14_attn_scores_dp0_mb2_gpu12
	layer14_qkv_dp1_mb2_gpu12 -> layer14_attn_scores_dp1_mb2_gpu12
	layer14_softmax_dp0_mb2_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb2_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb2_gpu12 -> layer14_softmax_dp0_mb2_gpu12
	layer14_attn_scores_dp1_mb2_gpu12 -> layer14_softmax_dp1_mb2_gpu12
	layer14_attn_out_dp0_mb2_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb2_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb2_gpu12 -> layer14_attn_out_dp0_mb2_gpu12
	layer14_softmax_dp1_mb2_gpu12 -> layer14_attn_out_dp1_mb2_gpu12
	layer14_qkv_dp0_mb2_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb2_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb2 -> layer14_qkv_dp0_mb2_gpu13
	layer13_expert_agg_dp1_mb2 -> layer14_qkv_dp1_mb2_gpu13
	layer14_attn_scores_dp0_mb2_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb2_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb2_gpu13 -> layer14_attn_scores_dp0_mb2_gpu13
	layer14_qkv_dp1_mb2_gpu13 -> layer14_attn_scores_dp1_mb2_gpu13
	layer14_softmax_dp0_mb2_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb2_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb2_gpu13 -> layer14_softmax_dp0_mb2_gpu13
	layer14_attn_scores_dp1_mb2_gpu13 -> layer14_softmax_dp1_mb2_gpu13
	layer14_attn_out_dp0_mb2_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb2_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb2_gpu13 -> layer14_attn_out_dp0_mb2_gpu13
	layer14_softmax_dp1_mb2_gpu13 -> layer14_attn_out_dp1_mb2_gpu13
	layer14_qkv_dp0_mb2_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb2_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb2 -> layer14_qkv_dp0_mb2_gpu14
	layer13_expert_agg_dp1_mb2 -> layer14_qkv_dp1_mb2_gpu14
	layer14_attn_scores_dp0_mb2_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb2_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb2_gpu14 -> layer14_attn_scores_dp0_mb2_gpu14
	layer14_qkv_dp1_mb2_gpu14 -> layer14_attn_scores_dp1_mb2_gpu14
	layer14_softmax_dp0_mb2_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb2_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb2_gpu14 -> layer14_softmax_dp0_mb2_gpu14
	layer14_attn_scores_dp1_mb2_gpu14 -> layer14_softmax_dp1_mb2_gpu14
	layer14_attn_out_dp0_mb2_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb2_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb2_gpu14 -> layer14_attn_out_dp0_mb2_gpu14
	layer14_softmax_dp1_mb2_gpu14 -> layer14_attn_out_dp1_mb2_gpu14
	layer14_qkv_dp0_mb2_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb2_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb2 -> layer14_qkv_dp0_mb2_gpu15
	layer13_expert_agg_dp1_mb2 -> layer14_qkv_dp1_mb2_gpu15
	layer14_attn_scores_dp0_mb2_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb2_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb2_gpu15 -> layer14_attn_scores_dp0_mb2_gpu15
	layer14_qkv_dp1_mb2_gpu15 -> layer14_attn_scores_dp1_mb2_gpu15
	layer14_softmax_dp0_mb2_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb2_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb2_gpu15 -> layer14_softmax_dp0_mb2_gpu15
	layer14_attn_scores_dp1_mb2_gpu15 -> layer14_softmax_dp1_mb2_gpu15
	layer14_attn_out_dp0_mb2_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb2_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb2_gpu15 -> layer14_attn_out_dp0_mb2_gpu15
	layer14_softmax_dp1_mb2_gpu15 -> layer14_attn_out_dp1_mb2_gpu15
	layer14_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb2_gpu12 -> layer14_attn_allreduce_dp0_mb2
	layer14_attn_out_dp1_mb2_gpu12 -> layer14_attn_allreduce_dp1_mb2
	layer14_attn_out_dp0_mb2_gpu13 -> layer14_attn_allreduce_dp0_mb2
	layer14_attn_out_dp1_mb2_gpu13 -> layer14_attn_allreduce_dp1_mb2
	layer14_attn_out_dp0_mb2_gpu14 -> layer14_attn_allreduce_dp0_mb2
	layer14_attn_out_dp1_mb2_gpu14 -> layer14_attn_allreduce_dp1_mb2
	layer14_attn_out_dp0_mb2_gpu15 -> layer14_attn_allreduce_dp0_mb2
	layer14_attn_out_dp1_mb2_gpu15 -> layer14_attn_allreduce_dp1_mb2
	layer14_router_dp0_mb2_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb2_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb2 -> layer14_router_dp0_mb2_gpu12
	layer14_attn_allreduce_dp1_mb2 -> layer14_router_dp1_mb2_gpu12
	layer14_expert0_mlp1_dp0_mb2_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb2_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb2_gpu12 -> layer14_expert0_mlp1_dp0_mb2_gpu12
	layer14_router_dp1_mb2_gpu12 -> layer14_expert0_mlp1_dp1_mb2_gpu12
	layer14_expert0_mlp2_dp0_mb2_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb2_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb2_gpu12 -> layer14_expert0_mlp2_dp0_mb2_gpu12
	layer14_expert0_mlp1_dp1_mb2_gpu12 -> layer14_expert0_mlp2_dp1_mb2_gpu12
	layer14_expert1_mlp1_dp0_mb2_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb2_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb2_gpu12 -> layer14_expert1_mlp1_dp0_mb2_gpu13
	layer14_router_dp1_mb2_gpu12 -> layer14_expert1_mlp1_dp1_mb2_gpu13
	layer14_expert1_mlp2_dp0_mb2_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb2_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb2_gpu13 -> layer14_expert1_mlp2_dp0_mb2_gpu13
	layer14_expert1_mlp1_dp1_mb2_gpu13 -> layer14_expert1_mlp2_dp1_mb2_gpu13
	layer14_expert2_mlp1_dp0_mb2_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb2_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb2_gpu12 -> layer14_expert2_mlp1_dp0_mb2_gpu14
	layer14_router_dp1_mb2_gpu12 -> layer14_expert2_mlp1_dp1_mb2_gpu14
	layer14_expert2_mlp2_dp0_mb2_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb2_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb2_gpu14 -> layer14_expert2_mlp2_dp0_mb2_gpu14
	layer14_expert2_mlp1_dp1_mb2_gpu14 -> layer14_expert2_mlp2_dp1_mb2_gpu14
	layer14_expert3_mlp1_dp0_mb2_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb2_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb2_gpu12 -> layer14_expert3_mlp1_dp0_mb2_gpu15
	layer14_router_dp1_mb2_gpu12 -> layer14_expert3_mlp1_dp1_mb2_gpu15
	layer14_expert3_mlp2_dp0_mb2_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb2_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb2_gpu15 -> layer14_expert3_mlp2_dp0_mb2_gpu15
	layer14_expert3_mlp1_dp1_mb2_gpu15 -> layer14_expert3_mlp2_dp1_mb2_gpu15
	layer14_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb2_gpu12 -> layer14_expert_agg_dp0_mb2
	layer14_expert0_mlp2_dp1_mb2_gpu12 -> layer14_expert_agg_dp1_mb2
	layer14_expert1_mlp2_dp0_mb2_gpu13 -> layer14_expert_agg_dp0_mb2
	layer14_expert1_mlp2_dp1_mb2_gpu13 -> layer14_expert_agg_dp1_mb2
	layer14_expert2_mlp2_dp0_mb2_gpu14 -> layer14_expert_agg_dp0_mb2
	layer14_expert2_mlp2_dp1_mb2_gpu14 -> layer14_expert_agg_dp1_mb2
	layer14_expert3_mlp2_dp0_mb2_gpu15 -> layer14_expert_agg_dp0_mb2
	layer14_expert3_mlp2_dp1_mb2_gpu15 -> layer14_expert_agg_dp1_mb2
	layer15_qkv_dp0_mb2_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb2_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb2 -> layer15_qkv_dp0_mb2_gpu12
	layer14_expert_agg_dp1_mb2 -> layer15_qkv_dp1_mb2_gpu12
	layer15_attn_scores_dp0_mb2_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb2_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb2_gpu12 -> layer15_attn_scores_dp0_mb2_gpu12
	layer15_qkv_dp1_mb2_gpu12 -> layer15_attn_scores_dp1_mb2_gpu12
	layer15_softmax_dp0_mb2_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb2_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb2_gpu12 -> layer15_softmax_dp0_mb2_gpu12
	layer15_attn_scores_dp1_mb2_gpu12 -> layer15_softmax_dp1_mb2_gpu12
	layer15_attn_out_dp0_mb2_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb2_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb2_gpu12 -> layer15_attn_out_dp0_mb2_gpu12
	layer15_softmax_dp1_mb2_gpu12 -> layer15_attn_out_dp1_mb2_gpu12
	layer15_qkv_dp0_mb2_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb2_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb2 -> layer15_qkv_dp0_mb2_gpu13
	layer14_expert_agg_dp1_mb2 -> layer15_qkv_dp1_mb2_gpu13
	layer15_attn_scores_dp0_mb2_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb2_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb2_gpu13 -> layer15_attn_scores_dp0_mb2_gpu13
	layer15_qkv_dp1_mb2_gpu13 -> layer15_attn_scores_dp1_mb2_gpu13
	layer15_softmax_dp0_mb2_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb2_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb2_gpu13 -> layer15_softmax_dp0_mb2_gpu13
	layer15_attn_scores_dp1_mb2_gpu13 -> layer15_softmax_dp1_mb2_gpu13
	layer15_attn_out_dp0_mb2_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb2_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb2_gpu13 -> layer15_attn_out_dp0_mb2_gpu13
	layer15_softmax_dp1_mb2_gpu13 -> layer15_attn_out_dp1_mb2_gpu13
	layer15_qkv_dp0_mb2_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb2_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb2 -> layer15_qkv_dp0_mb2_gpu14
	layer14_expert_agg_dp1_mb2 -> layer15_qkv_dp1_mb2_gpu14
	layer15_attn_scores_dp0_mb2_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb2_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb2_gpu14 -> layer15_attn_scores_dp0_mb2_gpu14
	layer15_qkv_dp1_mb2_gpu14 -> layer15_attn_scores_dp1_mb2_gpu14
	layer15_softmax_dp0_mb2_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb2_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb2_gpu14 -> layer15_softmax_dp0_mb2_gpu14
	layer15_attn_scores_dp1_mb2_gpu14 -> layer15_softmax_dp1_mb2_gpu14
	layer15_attn_out_dp0_mb2_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb2_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb2_gpu14 -> layer15_attn_out_dp0_mb2_gpu14
	layer15_softmax_dp1_mb2_gpu14 -> layer15_attn_out_dp1_mb2_gpu14
	layer15_qkv_dp0_mb2_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb2_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb2 -> layer15_qkv_dp0_mb2_gpu15
	layer14_expert_agg_dp1_mb2 -> layer15_qkv_dp1_mb2_gpu15
	layer15_attn_scores_dp0_mb2_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb2_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb2_gpu15 -> layer15_attn_scores_dp0_mb2_gpu15
	layer15_qkv_dp1_mb2_gpu15 -> layer15_attn_scores_dp1_mb2_gpu15
	layer15_softmax_dp0_mb2_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb2_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb2_gpu15 -> layer15_softmax_dp0_mb2_gpu15
	layer15_attn_scores_dp1_mb2_gpu15 -> layer15_softmax_dp1_mb2_gpu15
	layer15_attn_out_dp0_mb2_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb2_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb2_gpu15 -> layer15_attn_out_dp0_mb2_gpu15
	layer15_softmax_dp1_mb2_gpu15 -> layer15_attn_out_dp1_mb2_gpu15
	layer15_attn_allreduce_dp0_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb2 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb2_gpu12 -> layer15_attn_allreduce_dp0_mb2
	layer15_attn_out_dp1_mb2_gpu12 -> layer15_attn_allreduce_dp1_mb2
	layer15_attn_out_dp0_mb2_gpu13 -> layer15_attn_allreduce_dp0_mb2
	layer15_attn_out_dp1_mb2_gpu13 -> layer15_attn_allreduce_dp1_mb2
	layer15_attn_out_dp0_mb2_gpu14 -> layer15_attn_allreduce_dp0_mb2
	layer15_attn_out_dp1_mb2_gpu14 -> layer15_attn_allreduce_dp1_mb2
	layer15_attn_out_dp0_mb2_gpu15 -> layer15_attn_allreduce_dp0_mb2
	layer15_attn_out_dp1_mb2_gpu15 -> layer15_attn_allreduce_dp1_mb2
	layer15_router_dp0_mb2_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb2_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb2 -> layer15_router_dp0_mb2_gpu12
	layer15_attn_allreduce_dp1_mb2 -> layer15_router_dp1_mb2_gpu12
	layer15_expert0_mlp1_dp0_mb2_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb2_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb2_gpu12 -> layer15_expert0_mlp1_dp0_mb2_gpu12
	layer15_router_dp1_mb2_gpu12 -> layer15_expert0_mlp1_dp1_mb2_gpu12
	layer15_expert0_mlp2_dp0_mb2_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb2_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb2_gpu12 -> layer15_expert0_mlp2_dp0_mb2_gpu12
	layer15_expert0_mlp1_dp1_mb2_gpu12 -> layer15_expert0_mlp2_dp1_mb2_gpu12
	layer15_expert1_mlp1_dp0_mb2_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb2_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb2_gpu12 -> layer15_expert1_mlp1_dp0_mb2_gpu13
	layer15_router_dp1_mb2_gpu12 -> layer15_expert1_mlp1_dp1_mb2_gpu13
	layer15_expert1_mlp2_dp0_mb2_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb2_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb2_gpu13 -> layer15_expert1_mlp2_dp0_mb2_gpu13
	layer15_expert1_mlp1_dp1_mb2_gpu13 -> layer15_expert1_mlp2_dp1_mb2_gpu13
	layer15_expert2_mlp1_dp0_mb2_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb2_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb2_gpu12 -> layer15_expert2_mlp1_dp0_mb2_gpu14
	layer15_router_dp1_mb2_gpu12 -> layer15_expert2_mlp1_dp1_mb2_gpu14
	layer15_expert2_mlp2_dp0_mb2_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb2_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb2_gpu14 -> layer15_expert2_mlp2_dp0_mb2_gpu14
	layer15_expert2_mlp1_dp1_mb2_gpu14 -> layer15_expert2_mlp2_dp1_mb2_gpu14
	layer15_expert3_mlp1_dp0_mb2_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb2_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb2_gpu12 -> layer15_expert3_mlp1_dp0_mb2_gpu15
	layer15_router_dp1_mb2_gpu12 -> layer15_expert3_mlp1_dp1_mb2_gpu15
	layer15_expert3_mlp2_dp0_mb2_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb2_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb2_gpu15 -> layer15_expert3_mlp2_dp0_mb2_gpu15
	layer15_expert3_mlp1_dp1_mb2_gpu15 -> layer15_expert3_mlp2_dp1_mb2_gpu15
	layer15_expert_agg_dp0_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb2 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb2_gpu12 -> layer15_expert_agg_dp0_mb2
	layer15_expert0_mlp2_dp1_mb2_gpu12 -> layer15_expert_agg_dp1_mb2
	layer15_expert1_mlp2_dp0_mb2_gpu13 -> layer15_expert_agg_dp0_mb2
	layer15_expert1_mlp2_dp1_mb2_gpu13 -> layer15_expert_agg_dp1_mb2
	layer15_expert2_mlp2_dp0_mb2_gpu14 -> layer15_expert_agg_dp0_mb2
	layer15_expert2_mlp2_dp1_mb2_gpu14 -> layer15_expert_agg_dp1_mb2
	layer15_expert3_mlp2_dp0_mb2_gpu15 -> layer15_expert_agg_dp0_mb2
	layer15_expert3_mlp2_dp1_mb2_gpu15 -> layer15_expert_agg_dp1_mb2
	layer12_qkv_dp0_mb3_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb3_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb3 -> layer12_qkv_dp0_mb3_gpu12
	layer11_expert_agg_dp1_mb3 -> layer12_qkv_dp1_mb3_gpu12
	layer12_attn_scores_dp0_mb3_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb3_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb3_gpu12 -> layer12_attn_scores_dp0_mb3_gpu12
	layer12_qkv_dp1_mb3_gpu12 -> layer12_attn_scores_dp1_mb3_gpu12
	layer12_softmax_dp0_mb3_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb3_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb3_gpu12 -> layer12_softmax_dp0_mb3_gpu12
	layer12_attn_scores_dp1_mb3_gpu12 -> layer12_softmax_dp1_mb3_gpu12
	layer12_attn_out_dp0_mb3_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb3_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb3_gpu12 -> layer12_attn_out_dp0_mb3_gpu12
	layer12_softmax_dp1_mb3_gpu12 -> layer12_attn_out_dp1_mb3_gpu12
	layer12_qkv_dp0_mb3_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb3_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb3 -> layer12_qkv_dp0_mb3_gpu13
	layer11_expert_agg_dp1_mb3 -> layer12_qkv_dp1_mb3_gpu13
	layer12_attn_scores_dp0_mb3_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb3_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb3_gpu13 -> layer12_attn_scores_dp0_mb3_gpu13
	layer12_qkv_dp1_mb3_gpu13 -> layer12_attn_scores_dp1_mb3_gpu13
	layer12_softmax_dp0_mb3_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb3_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb3_gpu13 -> layer12_softmax_dp0_mb3_gpu13
	layer12_attn_scores_dp1_mb3_gpu13 -> layer12_softmax_dp1_mb3_gpu13
	layer12_attn_out_dp0_mb3_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb3_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb3_gpu13 -> layer12_attn_out_dp0_mb3_gpu13
	layer12_softmax_dp1_mb3_gpu13 -> layer12_attn_out_dp1_mb3_gpu13
	layer12_qkv_dp0_mb3_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb3_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb3 -> layer12_qkv_dp0_mb3_gpu14
	layer11_expert_agg_dp1_mb3 -> layer12_qkv_dp1_mb3_gpu14
	layer12_attn_scores_dp0_mb3_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb3_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb3_gpu14 -> layer12_attn_scores_dp0_mb3_gpu14
	layer12_qkv_dp1_mb3_gpu14 -> layer12_attn_scores_dp1_mb3_gpu14
	layer12_softmax_dp0_mb3_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb3_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb3_gpu14 -> layer12_softmax_dp0_mb3_gpu14
	layer12_attn_scores_dp1_mb3_gpu14 -> layer12_softmax_dp1_mb3_gpu14
	layer12_attn_out_dp0_mb3_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb3_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb3_gpu14 -> layer12_attn_out_dp0_mb3_gpu14
	layer12_softmax_dp1_mb3_gpu14 -> layer12_attn_out_dp1_mb3_gpu14
	layer12_qkv_dp0_mb3_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb3_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb3 -> layer12_qkv_dp0_mb3_gpu15
	layer11_expert_agg_dp1_mb3 -> layer12_qkv_dp1_mb3_gpu15
	layer12_attn_scores_dp0_mb3_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb3_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb3_gpu15 -> layer12_attn_scores_dp0_mb3_gpu15
	layer12_qkv_dp1_mb3_gpu15 -> layer12_attn_scores_dp1_mb3_gpu15
	layer12_softmax_dp0_mb3_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb3_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb3_gpu15 -> layer12_softmax_dp0_mb3_gpu15
	layer12_attn_scores_dp1_mb3_gpu15 -> layer12_softmax_dp1_mb3_gpu15
	layer12_attn_out_dp0_mb3_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb3_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb3_gpu15 -> layer12_attn_out_dp0_mb3_gpu15
	layer12_softmax_dp1_mb3_gpu15 -> layer12_attn_out_dp1_mb3_gpu15
	layer12_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb3_gpu12 -> layer12_attn_allreduce_dp0_mb3
	layer12_attn_out_dp1_mb3_gpu12 -> layer12_attn_allreduce_dp1_mb3
	layer12_attn_out_dp0_mb3_gpu13 -> layer12_attn_allreduce_dp0_mb3
	layer12_attn_out_dp1_mb3_gpu13 -> layer12_attn_allreduce_dp1_mb3
	layer12_attn_out_dp0_mb3_gpu14 -> layer12_attn_allreduce_dp0_mb3
	layer12_attn_out_dp1_mb3_gpu14 -> layer12_attn_allreduce_dp1_mb3
	layer12_attn_out_dp0_mb3_gpu15 -> layer12_attn_allreduce_dp0_mb3
	layer12_attn_out_dp1_mb3_gpu15 -> layer12_attn_allreduce_dp1_mb3
	layer12_router_dp0_mb3_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb3_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb3 -> layer12_router_dp0_mb3_gpu12
	layer12_attn_allreduce_dp1_mb3 -> layer12_router_dp1_mb3_gpu12
	layer12_expert0_mlp1_dp0_mb3_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb3_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb3_gpu12 -> layer12_expert0_mlp1_dp0_mb3_gpu12
	layer12_router_dp1_mb3_gpu12 -> layer12_expert0_mlp1_dp1_mb3_gpu12
	layer12_expert0_mlp2_dp0_mb3_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb3_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb3_gpu12 -> layer12_expert0_mlp2_dp0_mb3_gpu12
	layer12_expert0_mlp1_dp1_mb3_gpu12 -> layer12_expert0_mlp2_dp1_mb3_gpu12
	layer12_expert1_mlp1_dp0_mb3_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb3_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb3_gpu12 -> layer12_expert1_mlp1_dp0_mb3_gpu13
	layer12_router_dp1_mb3_gpu12 -> layer12_expert1_mlp1_dp1_mb3_gpu13
	layer12_expert1_mlp2_dp0_mb3_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb3_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb3_gpu13 -> layer12_expert1_mlp2_dp0_mb3_gpu13
	layer12_expert1_mlp1_dp1_mb3_gpu13 -> layer12_expert1_mlp2_dp1_mb3_gpu13
	layer12_expert2_mlp1_dp0_mb3_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb3_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb3_gpu12 -> layer12_expert2_mlp1_dp0_mb3_gpu14
	layer12_router_dp1_mb3_gpu12 -> layer12_expert2_mlp1_dp1_mb3_gpu14
	layer12_expert2_mlp2_dp0_mb3_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb3_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb3_gpu14 -> layer12_expert2_mlp2_dp0_mb3_gpu14
	layer12_expert2_mlp1_dp1_mb3_gpu14 -> layer12_expert2_mlp2_dp1_mb3_gpu14
	layer12_expert3_mlp1_dp0_mb3_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb3_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb3_gpu12 -> layer12_expert3_mlp1_dp0_mb3_gpu15
	layer12_router_dp1_mb3_gpu12 -> layer12_expert3_mlp1_dp1_mb3_gpu15
	layer12_expert3_mlp2_dp0_mb3_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb3_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb3_gpu15 -> layer12_expert3_mlp2_dp0_mb3_gpu15
	layer12_expert3_mlp1_dp1_mb3_gpu15 -> layer12_expert3_mlp2_dp1_mb3_gpu15
	layer12_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb3_gpu12 -> layer12_expert_agg_dp0_mb3
	layer12_expert0_mlp2_dp1_mb3_gpu12 -> layer12_expert_agg_dp1_mb3
	layer12_expert1_mlp2_dp0_mb3_gpu13 -> layer12_expert_agg_dp0_mb3
	layer12_expert1_mlp2_dp1_mb3_gpu13 -> layer12_expert_agg_dp1_mb3
	layer12_expert2_mlp2_dp0_mb3_gpu14 -> layer12_expert_agg_dp0_mb3
	layer12_expert2_mlp2_dp1_mb3_gpu14 -> layer12_expert_agg_dp1_mb3
	layer12_expert3_mlp2_dp0_mb3_gpu15 -> layer12_expert_agg_dp0_mb3
	layer12_expert3_mlp2_dp1_mb3_gpu15 -> layer12_expert_agg_dp1_mb3
	layer13_qkv_dp0_mb3_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb3_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb3 -> layer13_qkv_dp0_mb3_gpu12
	layer12_expert_agg_dp1_mb3 -> layer13_qkv_dp1_mb3_gpu12
	layer13_attn_scores_dp0_mb3_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb3_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb3_gpu12 -> layer13_attn_scores_dp0_mb3_gpu12
	layer13_qkv_dp1_mb3_gpu12 -> layer13_attn_scores_dp1_mb3_gpu12
	layer13_softmax_dp0_mb3_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb3_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb3_gpu12 -> layer13_softmax_dp0_mb3_gpu12
	layer13_attn_scores_dp1_mb3_gpu12 -> layer13_softmax_dp1_mb3_gpu12
	layer13_attn_out_dp0_mb3_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb3_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb3_gpu12 -> layer13_attn_out_dp0_mb3_gpu12
	layer13_softmax_dp1_mb3_gpu12 -> layer13_attn_out_dp1_mb3_gpu12
	layer13_qkv_dp0_mb3_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb3_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb3 -> layer13_qkv_dp0_mb3_gpu13
	layer12_expert_agg_dp1_mb3 -> layer13_qkv_dp1_mb3_gpu13
	layer13_attn_scores_dp0_mb3_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb3_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb3_gpu13 -> layer13_attn_scores_dp0_mb3_gpu13
	layer13_qkv_dp1_mb3_gpu13 -> layer13_attn_scores_dp1_mb3_gpu13
	layer13_softmax_dp0_mb3_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb3_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb3_gpu13 -> layer13_softmax_dp0_mb3_gpu13
	layer13_attn_scores_dp1_mb3_gpu13 -> layer13_softmax_dp1_mb3_gpu13
	layer13_attn_out_dp0_mb3_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb3_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb3_gpu13 -> layer13_attn_out_dp0_mb3_gpu13
	layer13_softmax_dp1_mb3_gpu13 -> layer13_attn_out_dp1_mb3_gpu13
	layer13_qkv_dp0_mb3_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb3_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb3 -> layer13_qkv_dp0_mb3_gpu14
	layer12_expert_agg_dp1_mb3 -> layer13_qkv_dp1_mb3_gpu14
	layer13_attn_scores_dp0_mb3_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb3_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb3_gpu14 -> layer13_attn_scores_dp0_mb3_gpu14
	layer13_qkv_dp1_mb3_gpu14 -> layer13_attn_scores_dp1_mb3_gpu14
	layer13_softmax_dp0_mb3_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb3_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb3_gpu14 -> layer13_softmax_dp0_mb3_gpu14
	layer13_attn_scores_dp1_mb3_gpu14 -> layer13_softmax_dp1_mb3_gpu14
	layer13_attn_out_dp0_mb3_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb3_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb3_gpu14 -> layer13_attn_out_dp0_mb3_gpu14
	layer13_softmax_dp1_mb3_gpu14 -> layer13_attn_out_dp1_mb3_gpu14
	layer13_qkv_dp0_mb3_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb3_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb3 -> layer13_qkv_dp0_mb3_gpu15
	layer12_expert_agg_dp1_mb3 -> layer13_qkv_dp1_mb3_gpu15
	layer13_attn_scores_dp0_mb3_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb3_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb3_gpu15 -> layer13_attn_scores_dp0_mb3_gpu15
	layer13_qkv_dp1_mb3_gpu15 -> layer13_attn_scores_dp1_mb3_gpu15
	layer13_softmax_dp0_mb3_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb3_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb3_gpu15 -> layer13_softmax_dp0_mb3_gpu15
	layer13_attn_scores_dp1_mb3_gpu15 -> layer13_softmax_dp1_mb3_gpu15
	layer13_attn_out_dp0_mb3_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb3_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb3_gpu15 -> layer13_attn_out_dp0_mb3_gpu15
	layer13_softmax_dp1_mb3_gpu15 -> layer13_attn_out_dp1_mb3_gpu15
	layer13_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb3_gpu12 -> layer13_attn_allreduce_dp0_mb3
	layer13_attn_out_dp1_mb3_gpu12 -> layer13_attn_allreduce_dp1_mb3
	layer13_attn_out_dp0_mb3_gpu13 -> layer13_attn_allreduce_dp0_mb3
	layer13_attn_out_dp1_mb3_gpu13 -> layer13_attn_allreduce_dp1_mb3
	layer13_attn_out_dp0_mb3_gpu14 -> layer13_attn_allreduce_dp0_mb3
	layer13_attn_out_dp1_mb3_gpu14 -> layer13_attn_allreduce_dp1_mb3
	layer13_attn_out_dp0_mb3_gpu15 -> layer13_attn_allreduce_dp0_mb3
	layer13_attn_out_dp1_mb3_gpu15 -> layer13_attn_allreduce_dp1_mb3
	layer13_router_dp0_mb3_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb3_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb3 -> layer13_router_dp0_mb3_gpu12
	layer13_attn_allreduce_dp1_mb3 -> layer13_router_dp1_mb3_gpu12
	layer13_expert0_mlp1_dp0_mb3_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb3_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb3_gpu12 -> layer13_expert0_mlp1_dp0_mb3_gpu12
	layer13_router_dp1_mb3_gpu12 -> layer13_expert0_mlp1_dp1_mb3_gpu12
	layer13_expert0_mlp2_dp0_mb3_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb3_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb3_gpu12 -> layer13_expert0_mlp2_dp0_mb3_gpu12
	layer13_expert0_mlp1_dp1_mb3_gpu12 -> layer13_expert0_mlp2_dp1_mb3_gpu12
	layer13_expert1_mlp1_dp0_mb3_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb3_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb3_gpu12 -> layer13_expert1_mlp1_dp0_mb3_gpu13
	layer13_router_dp1_mb3_gpu12 -> layer13_expert1_mlp1_dp1_mb3_gpu13
	layer13_expert1_mlp2_dp0_mb3_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb3_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb3_gpu13 -> layer13_expert1_mlp2_dp0_mb3_gpu13
	layer13_expert1_mlp1_dp1_mb3_gpu13 -> layer13_expert1_mlp2_dp1_mb3_gpu13
	layer13_expert2_mlp1_dp0_mb3_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb3_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb3_gpu12 -> layer13_expert2_mlp1_dp0_mb3_gpu14
	layer13_router_dp1_mb3_gpu12 -> layer13_expert2_mlp1_dp1_mb3_gpu14
	layer13_expert2_mlp2_dp0_mb3_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb3_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb3_gpu14 -> layer13_expert2_mlp2_dp0_mb3_gpu14
	layer13_expert2_mlp1_dp1_mb3_gpu14 -> layer13_expert2_mlp2_dp1_mb3_gpu14
	layer13_expert3_mlp1_dp0_mb3_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb3_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb3_gpu12 -> layer13_expert3_mlp1_dp0_mb3_gpu15
	layer13_router_dp1_mb3_gpu12 -> layer13_expert3_mlp1_dp1_mb3_gpu15
	layer13_expert3_mlp2_dp0_mb3_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb3_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb3_gpu15 -> layer13_expert3_mlp2_dp0_mb3_gpu15
	layer13_expert3_mlp1_dp1_mb3_gpu15 -> layer13_expert3_mlp2_dp1_mb3_gpu15
	layer13_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb3_gpu12 -> layer13_expert_agg_dp0_mb3
	layer13_expert0_mlp2_dp1_mb3_gpu12 -> layer13_expert_agg_dp1_mb3
	layer13_expert1_mlp2_dp0_mb3_gpu13 -> layer13_expert_agg_dp0_mb3
	layer13_expert1_mlp2_dp1_mb3_gpu13 -> layer13_expert_agg_dp1_mb3
	layer13_expert2_mlp2_dp0_mb3_gpu14 -> layer13_expert_agg_dp0_mb3
	layer13_expert2_mlp2_dp1_mb3_gpu14 -> layer13_expert_agg_dp1_mb3
	layer13_expert3_mlp2_dp0_mb3_gpu15 -> layer13_expert_agg_dp0_mb3
	layer13_expert3_mlp2_dp1_mb3_gpu15 -> layer13_expert_agg_dp1_mb3
	layer14_qkv_dp0_mb3_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb3_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb3 -> layer14_qkv_dp0_mb3_gpu12
	layer13_expert_agg_dp1_mb3 -> layer14_qkv_dp1_mb3_gpu12
	layer14_attn_scores_dp0_mb3_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb3_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb3_gpu12 -> layer14_attn_scores_dp0_mb3_gpu12
	layer14_qkv_dp1_mb3_gpu12 -> layer14_attn_scores_dp1_mb3_gpu12
	layer14_softmax_dp0_mb3_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb3_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb3_gpu12 -> layer14_softmax_dp0_mb3_gpu12
	layer14_attn_scores_dp1_mb3_gpu12 -> layer14_softmax_dp1_mb3_gpu12
	layer14_attn_out_dp0_mb3_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb3_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb3_gpu12 -> layer14_attn_out_dp0_mb3_gpu12
	layer14_softmax_dp1_mb3_gpu12 -> layer14_attn_out_dp1_mb3_gpu12
	layer14_qkv_dp0_mb3_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb3_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb3 -> layer14_qkv_dp0_mb3_gpu13
	layer13_expert_agg_dp1_mb3 -> layer14_qkv_dp1_mb3_gpu13
	layer14_attn_scores_dp0_mb3_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb3_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb3_gpu13 -> layer14_attn_scores_dp0_mb3_gpu13
	layer14_qkv_dp1_mb3_gpu13 -> layer14_attn_scores_dp1_mb3_gpu13
	layer14_softmax_dp0_mb3_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb3_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb3_gpu13 -> layer14_softmax_dp0_mb3_gpu13
	layer14_attn_scores_dp1_mb3_gpu13 -> layer14_softmax_dp1_mb3_gpu13
	layer14_attn_out_dp0_mb3_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb3_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb3_gpu13 -> layer14_attn_out_dp0_mb3_gpu13
	layer14_softmax_dp1_mb3_gpu13 -> layer14_attn_out_dp1_mb3_gpu13
	layer14_qkv_dp0_mb3_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb3_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb3 -> layer14_qkv_dp0_mb3_gpu14
	layer13_expert_agg_dp1_mb3 -> layer14_qkv_dp1_mb3_gpu14
	layer14_attn_scores_dp0_mb3_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb3_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb3_gpu14 -> layer14_attn_scores_dp0_mb3_gpu14
	layer14_qkv_dp1_mb3_gpu14 -> layer14_attn_scores_dp1_mb3_gpu14
	layer14_softmax_dp0_mb3_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb3_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb3_gpu14 -> layer14_softmax_dp0_mb3_gpu14
	layer14_attn_scores_dp1_mb3_gpu14 -> layer14_softmax_dp1_mb3_gpu14
	layer14_attn_out_dp0_mb3_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb3_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb3_gpu14 -> layer14_attn_out_dp0_mb3_gpu14
	layer14_softmax_dp1_mb3_gpu14 -> layer14_attn_out_dp1_mb3_gpu14
	layer14_qkv_dp0_mb3_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb3_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb3 -> layer14_qkv_dp0_mb3_gpu15
	layer13_expert_agg_dp1_mb3 -> layer14_qkv_dp1_mb3_gpu15
	layer14_attn_scores_dp0_mb3_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb3_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb3_gpu15 -> layer14_attn_scores_dp0_mb3_gpu15
	layer14_qkv_dp1_mb3_gpu15 -> layer14_attn_scores_dp1_mb3_gpu15
	layer14_softmax_dp0_mb3_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb3_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb3_gpu15 -> layer14_softmax_dp0_mb3_gpu15
	layer14_attn_scores_dp1_mb3_gpu15 -> layer14_softmax_dp1_mb3_gpu15
	layer14_attn_out_dp0_mb3_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb3_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb3_gpu15 -> layer14_attn_out_dp0_mb3_gpu15
	layer14_softmax_dp1_mb3_gpu15 -> layer14_attn_out_dp1_mb3_gpu15
	layer14_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb3_gpu12 -> layer14_attn_allreduce_dp0_mb3
	layer14_attn_out_dp1_mb3_gpu12 -> layer14_attn_allreduce_dp1_mb3
	layer14_attn_out_dp0_mb3_gpu13 -> layer14_attn_allreduce_dp0_mb3
	layer14_attn_out_dp1_mb3_gpu13 -> layer14_attn_allreduce_dp1_mb3
	layer14_attn_out_dp0_mb3_gpu14 -> layer14_attn_allreduce_dp0_mb3
	layer14_attn_out_dp1_mb3_gpu14 -> layer14_attn_allreduce_dp1_mb3
	layer14_attn_out_dp0_mb3_gpu15 -> layer14_attn_allreduce_dp0_mb3
	layer14_attn_out_dp1_mb3_gpu15 -> layer14_attn_allreduce_dp1_mb3
	layer14_router_dp0_mb3_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb3_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb3 -> layer14_router_dp0_mb3_gpu12
	layer14_attn_allreduce_dp1_mb3 -> layer14_router_dp1_mb3_gpu12
	layer14_expert0_mlp1_dp0_mb3_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb3_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb3_gpu12 -> layer14_expert0_mlp1_dp0_mb3_gpu12
	layer14_router_dp1_mb3_gpu12 -> layer14_expert0_mlp1_dp1_mb3_gpu12
	layer14_expert0_mlp2_dp0_mb3_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb3_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb3_gpu12 -> layer14_expert0_mlp2_dp0_mb3_gpu12
	layer14_expert0_mlp1_dp1_mb3_gpu12 -> layer14_expert0_mlp2_dp1_mb3_gpu12
	layer14_expert1_mlp1_dp0_mb3_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb3_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb3_gpu12 -> layer14_expert1_mlp1_dp0_mb3_gpu13
	layer14_router_dp1_mb3_gpu12 -> layer14_expert1_mlp1_dp1_mb3_gpu13
	layer14_expert1_mlp2_dp0_mb3_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb3_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb3_gpu13 -> layer14_expert1_mlp2_dp0_mb3_gpu13
	layer14_expert1_mlp1_dp1_mb3_gpu13 -> layer14_expert1_mlp2_dp1_mb3_gpu13
	layer14_expert2_mlp1_dp0_mb3_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb3_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb3_gpu12 -> layer14_expert2_mlp1_dp0_mb3_gpu14
	layer14_router_dp1_mb3_gpu12 -> layer14_expert2_mlp1_dp1_mb3_gpu14
	layer14_expert2_mlp2_dp0_mb3_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb3_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb3_gpu14 -> layer14_expert2_mlp2_dp0_mb3_gpu14
	layer14_expert2_mlp1_dp1_mb3_gpu14 -> layer14_expert2_mlp2_dp1_mb3_gpu14
	layer14_expert3_mlp1_dp0_mb3_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb3_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb3_gpu12 -> layer14_expert3_mlp1_dp0_mb3_gpu15
	layer14_router_dp1_mb3_gpu12 -> layer14_expert3_mlp1_dp1_mb3_gpu15
	layer14_expert3_mlp2_dp0_mb3_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb3_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb3_gpu15 -> layer14_expert3_mlp2_dp0_mb3_gpu15
	layer14_expert3_mlp1_dp1_mb3_gpu15 -> layer14_expert3_mlp2_dp1_mb3_gpu15
	layer14_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb3_gpu12 -> layer14_expert_agg_dp0_mb3
	layer14_expert0_mlp2_dp1_mb3_gpu12 -> layer14_expert_agg_dp1_mb3
	layer14_expert1_mlp2_dp0_mb3_gpu13 -> layer14_expert_agg_dp0_mb3
	layer14_expert1_mlp2_dp1_mb3_gpu13 -> layer14_expert_agg_dp1_mb3
	layer14_expert2_mlp2_dp0_mb3_gpu14 -> layer14_expert_agg_dp0_mb3
	layer14_expert2_mlp2_dp1_mb3_gpu14 -> layer14_expert_agg_dp1_mb3
	layer14_expert3_mlp2_dp0_mb3_gpu15 -> layer14_expert_agg_dp0_mb3
	layer14_expert3_mlp2_dp1_mb3_gpu15 -> layer14_expert_agg_dp1_mb3
	layer15_qkv_dp0_mb3_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb3_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb3 -> layer15_qkv_dp0_mb3_gpu12
	layer14_expert_agg_dp1_mb3 -> layer15_qkv_dp1_mb3_gpu12
	layer15_attn_scores_dp0_mb3_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb3_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb3_gpu12 -> layer15_attn_scores_dp0_mb3_gpu12
	layer15_qkv_dp1_mb3_gpu12 -> layer15_attn_scores_dp1_mb3_gpu12
	layer15_softmax_dp0_mb3_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb3_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb3_gpu12 -> layer15_softmax_dp0_mb3_gpu12
	layer15_attn_scores_dp1_mb3_gpu12 -> layer15_softmax_dp1_mb3_gpu12
	layer15_attn_out_dp0_mb3_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb3_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb3_gpu12 -> layer15_attn_out_dp0_mb3_gpu12
	layer15_softmax_dp1_mb3_gpu12 -> layer15_attn_out_dp1_mb3_gpu12
	layer15_qkv_dp0_mb3_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb3_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb3 -> layer15_qkv_dp0_mb3_gpu13
	layer14_expert_agg_dp1_mb3 -> layer15_qkv_dp1_mb3_gpu13
	layer15_attn_scores_dp0_mb3_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb3_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb3_gpu13 -> layer15_attn_scores_dp0_mb3_gpu13
	layer15_qkv_dp1_mb3_gpu13 -> layer15_attn_scores_dp1_mb3_gpu13
	layer15_softmax_dp0_mb3_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb3_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb3_gpu13 -> layer15_softmax_dp0_mb3_gpu13
	layer15_attn_scores_dp1_mb3_gpu13 -> layer15_softmax_dp1_mb3_gpu13
	layer15_attn_out_dp0_mb3_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb3_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb3_gpu13 -> layer15_attn_out_dp0_mb3_gpu13
	layer15_softmax_dp1_mb3_gpu13 -> layer15_attn_out_dp1_mb3_gpu13
	layer15_qkv_dp0_mb3_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb3_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb3 -> layer15_qkv_dp0_mb3_gpu14
	layer14_expert_agg_dp1_mb3 -> layer15_qkv_dp1_mb3_gpu14
	layer15_attn_scores_dp0_mb3_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb3_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb3_gpu14 -> layer15_attn_scores_dp0_mb3_gpu14
	layer15_qkv_dp1_mb3_gpu14 -> layer15_attn_scores_dp1_mb3_gpu14
	layer15_softmax_dp0_mb3_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb3_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb3_gpu14 -> layer15_softmax_dp0_mb3_gpu14
	layer15_attn_scores_dp1_mb3_gpu14 -> layer15_softmax_dp1_mb3_gpu14
	layer15_attn_out_dp0_mb3_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb3_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb3_gpu14 -> layer15_attn_out_dp0_mb3_gpu14
	layer15_softmax_dp1_mb3_gpu14 -> layer15_attn_out_dp1_mb3_gpu14
	layer15_qkv_dp0_mb3_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb3_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb3 -> layer15_qkv_dp0_mb3_gpu15
	layer14_expert_agg_dp1_mb3 -> layer15_qkv_dp1_mb3_gpu15
	layer15_attn_scores_dp0_mb3_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb3_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb3_gpu15 -> layer15_attn_scores_dp0_mb3_gpu15
	layer15_qkv_dp1_mb3_gpu15 -> layer15_attn_scores_dp1_mb3_gpu15
	layer15_softmax_dp0_mb3_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb3_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb3_gpu15 -> layer15_softmax_dp0_mb3_gpu15
	layer15_attn_scores_dp1_mb3_gpu15 -> layer15_softmax_dp1_mb3_gpu15
	layer15_attn_out_dp0_mb3_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb3_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb3_gpu15 -> layer15_attn_out_dp0_mb3_gpu15
	layer15_softmax_dp1_mb3_gpu15 -> layer15_attn_out_dp1_mb3_gpu15
	layer15_attn_allreduce_dp0_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb3 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb3_gpu12 -> layer15_attn_allreduce_dp0_mb3
	layer15_attn_out_dp1_mb3_gpu12 -> layer15_attn_allreduce_dp1_mb3
	layer15_attn_out_dp0_mb3_gpu13 -> layer15_attn_allreduce_dp0_mb3
	layer15_attn_out_dp1_mb3_gpu13 -> layer15_attn_allreduce_dp1_mb3
	layer15_attn_out_dp0_mb3_gpu14 -> layer15_attn_allreduce_dp0_mb3
	layer15_attn_out_dp1_mb3_gpu14 -> layer15_attn_allreduce_dp1_mb3
	layer15_attn_out_dp0_mb3_gpu15 -> layer15_attn_allreduce_dp0_mb3
	layer15_attn_out_dp1_mb3_gpu15 -> layer15_attn_allreduce_dp1_mb3
	layer15_router_dp0_mb3_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb3_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb3 -> layer15_router_dp0_mb3_gpu12
	layer15_attn_allreduce_dp1_mb3 -> layer15_router_dp1_mb3_gpu12
	layer15_expert0_mlp1_dp0_mb3_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb3_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb3_gpu12 -> layer15_expert0_mlp1_dp0_mb3_gpu12
	layer15_router_dp1_mb3_gpu12 -> layer15_expert0_mlp1_dp1_mb3_gpu12
	layer15_expert0_mlp2_dp0_mb3_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb3_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb3_gpu12 -> layer15_expert0_mlp2_dp0_mb3_gpu12
	layer15_expert0_mlp1_dp1_mb3_gpu12 -> layer15_expert0_mlp2_dp1_mb3_gpu12
	layer15_expert1_mlp1_dp0_mb3_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb3_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb3_gpu12 -> layer15_expert1_mlp1_dp0_mb3_gpu13
	layer15_router_dp1_mb3_gpu12 -> layer15_expert1_mlp1_dp1_mb3_gpu13
	layer15_expert1_mlp2_dp0_mb3_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb3_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb3_gpu13 -> layer15_expert1_mlp2_dp0_mb3_gpu13
	layer15_expert1_mlp1_dp1_mb3_gpu13 -> layer15_expert1_mlp2_dp1_mb3_gpu13
	layer15_expert2_mlp1_dp0_mb3_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb3_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb3_gpu12 -> layer15_expert2_mlp1_dp0_mb3_gpu14
	layer15_router_dp1_mb3_gpu12 -> layer15_expert2_mlp1_dp1_mb3_gpu14
	layer15_expert2_mlp2_dp0_mb3_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb3_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb3_gpu14 -> layer15_expert2_mlp2_dp0_mb3_gpu14
	layer15_expert2_mlp1_dp1_mb3_gpu14 -> layer15_expert2_mlp2_dp1_mb3_gpu14
	layer15_expert3_mlp1_dp0_mb3_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb3_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb3_gpu12 -> layer15_expert3_mlp1_dp0_mb3_gpu15
	layer15_router_dp1_mb3_gpu12 -> layer15_expert3_mlp1_dp1_mb3_gpu15
	layer15_expert3_mlp2_dp0_mb3_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb3_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb3_gpu15 -> layer15_expert3_mlp2_dp0_mb3_gpu15
	layer15_expert3_mlp1_dp1_mb3_gpu15 -> layer15_expert3_mlp2_dp1_mb3_gpu15
	layer15_expert_agg_dp0_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb3 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb3_gpu12 -> layer15_expert_agg_dp0_mb3
	layer15_expert0_mlp2_dp1_mb3_gpu12 -> layer15_expert_agg_dp1_mb3
	layer15_expert1_mlp2_dp0_mb3_gpu13 -> layer15_expert_agg_dp0_mb3
	layer15_expert1_mlp2_dp1_mb3_gpu13 -> layer15_expert_agg_dp1_mb3
	layer15_expert2_mlp2_dp0_mb3_gpu14 -> layer15_expert_agg_dp0_mb3
	layer15_expert2_mlp2_dp1_mb3_gpu14 -> layer15_expert_agg_dp1_mb3
	layer15_expert3_mlp2_dp0_mb3_gpu15 -> layer15_expert_agg_dp0_mb3
	layer15_expert3_mlp2_dp1_mb3_gpu15 -> layer15_expert_agg_dp1_mb3
	layer12_qkv_dp0_mb4_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb4_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb4 -> layer12_qkv_dp0_mb4_gpu12
	layer11_expert_agg_dp1_mb4 -> layer12_qkv_dp1_mb4_gpu12
	layer12_attn_scores_dp0_mb4_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb4_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb4_gpu12 -> layer12_attn_scores_dp0_mb4_gpu12
	layer12_qkv_dp1_mb4_gpu12 -> layer12_attn_scores_dp1_mb4_gpu12
	layer12_softmax_dp0_mb4_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb4_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb4_gpu12 -> layer12_softmax_dp0_mb4_gpu12
	layer12_attn_scores_dp1_mb4_gpu12 -> layer12_softmax_dp1_mb4_gpu12
	layer12_attn_out_dp0_mb4_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb4_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb4_gpu12 -> layer12_attn_out_dp0_mb4_gpu12
	layer12_softmax_dp1_mb4_gpu12 -> layer12_attn_out_dp1_mb4_gpu12
	layer12_qkv_dp0_mb4_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb4_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb4 -> layer12_qkv_dp0_mb4_gpu13
	layer11_expert_agg_dp1_mb4 -> layer12_qkv_dp1_mb4_gpu13
	layer12_attn_scores_dp0_mb4_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb4_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb4_gpu13 -> layer12_attn_scores_dp0_mb4_gpu13
	layer12_qkv_dp1_mb4_gpu13 -> layer12_attn_scores_dp1_mb4_gpu13
	layer12_softmax_dp0_mb4_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb4_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb4_gpu13 -> layer12_softmax_dp0_mb4_gpu13
	layer12_attn_scores_dp1_mb4_gpu13 -> layer12_softmax_dp1_mb4_gpu13
	layer12_attn_out_dp0_mb4_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb4_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb4_gpu13 -> layer12_attn_out_dp0_mb4_gpu13
	layer12_softmax_dp1_mb4_gpu13 -> layer12_attn_out_dp1_mb4_gpu13
	layer12_qkv_dp0_mb4_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb4_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb4 -> layer12_qkv_dp0_mb4_gpu14
	layer11_expert_agg_dp1_mb4 -> layer12_qkv_dp1_mb4_gpu14
	layer12_attn_scores_dp0_mb4_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb4_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb4_gpu14 -> layer12_attn_scores_dp0_mb4_gpu14
	layer12_qkv_dp1_mb4_gpu14 -> layer12_attn_scores_dp1_mb4_gpu14
	layer12_softmax_dp0_mb4_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb4_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb4_gpu14 -> layer12_softmax_dp0_mb4_gpu14
	layer12_attn_scores_dp1_mb4_gpu14 -> layer12_softmax_dp1_mb4_gpu14
	layer12_attn_out_dp0_mb4_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb4_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb4_gpu14 -> layer12_attn_out_dp0_mb4_gpu14
	layer12_softmax_dp1_mb4_gpu14 -> layer12_attn_out_dp1_mb4_gpu14
	layer12_qkv_dp0_mb4_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb4_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb4 -> layer12_qkv_dp0_mb4_gpu15
	layer11_expert_agg_dp1_mb4 -> layer12_qkv_dp1_mb4_gpu15
	layer12_attn_scores_dp0_mb4_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb4_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb4_gpu15 -> layer12_attn_scores_dp0_mb4_gpu15
	layer12_qkv_dp1_mb4_gpu15 -> layer12_attn_scores_dp1_mb4_gpu15
	layer12_softmax_dp0_mb4_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb4_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb4_gpu15 -> layer12_softmax_dp0_mb4_gpu15
	layer12_attn_scores_dp1_mb4_gpu15 -> layer12_softmax_dp1_mb4_gpu15
	layer12_attn_out_dp0_mb4_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb4_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb4_gpu15 -> layer12_attn_out_dp0_mb4_gpu15
	layer12_softmax_dp1_mb4_gpu15 -> layer12_attn_out_dp1_mb4_gpu15
	layer12_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb4_gpu12 -> layer12_attn_allreduce_dp0_mb4
	layer12_attn_out_dp1_mb4_gpu12 -> layer12_attn_allreduce_dp1_mb4
	layer12_attn_out_dp0_mb4_gpu13 -> layer12_attn_allreduce_dp0_mb4
	layer12_attn_out_dp1_mb4_gpu13 -> layer12_attn_allreduce_dp1_mb4
	layer12_attn_out_dp0_mb4_gpu14 -> layer12_attn_allreduce_dp0_mb4
	layer12_attn_out_dp1_mb4_gpu14 -> layer12_attn_allreduce_dp1_mb4
	layer12_attn_out_dp0_mb4_gpu15 -> layer12_attn_allreduce_dp0_mb4
	layer12_attn_out_dp1_mb4_gpu15 -> layer12_attn_allreduce_dp1_mb4
	layer12_router_dp0_mb4_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb4_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb4 -> layer12_router_dp0_mb4_gpu12
	layer12_attn_allreduce_dp1_mb4 -> layer12_router_dp1_mb4_gpu12
	layer12_expert0_mlp1_dp0_mb4_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb4_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb4_gpu12 -> layer12_expert0_mlp1_dp0_mb4_gpu12
	layer12_router_dp1_mb4_gpu12 -> layer12_expert0_mlp1_dp1_mb4_gpu12
	layer12_expert0_mlp2_dp0_mb4_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb4_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb4_gpu12 -> layer12_expert0_mlp2_dp0_mb4_gpu12
	layer12_expert0_mlp1_dp1_mb4_gpu12 -> layer12_expert0_mlp2_dp1_mb4_gpu12
	layer12_expert1_mlp1_dp0_mb4_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb4_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb4_gpu12 -> layer12_expert1_mlp1_dp0_mb4_gpu13
	layer12_router_dp1_mb4_gpu12 -> layer12_expert1_mlp1_dp1_mb4_gpu13
	layer12_expert1_mlp2_dp0_mb4_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb4_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb4_gpu13 -> layer12_expert1_mlp2_dp0_mb4_gpu13
	layer12_expert1_mlp1_dp1_mb4_gpu13 -> layer12_expert1_mlp2_dp1_mb4_gpu13
	layer12_expert2_mlp1_dp0_mb4_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb4_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb4_gpu12 -> layer12_expert2_mlp1_dp0_mb4_gpu14
	layer12_router_dp1_mb4_gpu12 -> layer12_expert2_mlp1_dp1_mb4_gpu14
	layer12_expert2_mlp2_dp0_mb4_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb4_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb4_gpu14 -> layer12_expert2_mlp2_dp0_mb4_gpu14
	layer12_expert2_mlp1_dp1_mb4_gpu14 -> layer12_expert2_mlp2_dp1_mb4_gpu14
	layer12_expert3_mlp1_dp0_mb4_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb4_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb4_gpu12 -> layer12_expert3_mlp1_dp0_mb4_gpu15
	layer12_router_dp1_mb4_gpu12 -> layer12_expert3_mlp1_dp1_mb4_gpu15
	layer12_expert3_mlp2_dp0_mb4_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb4_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb4_gpu15 -> layer12_expert3_mlp2_dp0_mb4_gpu15
	layer12_expert3_mlp1_dp1_mb4_gpu15 -> layer12_expert3_mlp2_dp1_mb4_gpu15
	layer12_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb4_gpu12 -> layer12_expert_agg_dp0_mb4
	layer12_expert0_mlp2_dp1_mb4_gpu12 -> layer12_expert_agg_dp1_mb4
	layer12_expert1_mlp2_dp0_mb4_gpu13 -> layer12_expert_agg_dp0_mb4
	layer12_expert1_mlp2_dp1_mb4_gpu13 -> layer12_expert_agg_dp1_mb4
	layer12_expert2_mlp2_dp0_mb4_gpu14 -> layer12_expert_agg_dp0_mb4
	layer12_expert2_mlp2_dp1_mb4_gpu14 -> layer12_expert_agg_dp1_mb4
	layer12_expert3_mlp2_dp0_mb4_gpu15 -> layer12_expert_agg_dp0_mb4
	layer12_expert3_mlp2_dp1_mb4_gpu15 -> layer12_expert_agg_dp1_mb4
	layer13_qkv_dp0_mb4_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb4_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb4 -> layer13_qkv_dp0_mb4_gpu12
	layer12_expert_agg_dp1_mb4 -> layer13_qkv_dp1_mb4_gpu12
	layer13_attn_scores_dp0_mb4_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb4_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb4_gpu12 -> layer13_attn_scores_dp0_mb4_gpu12
	layer13_qkv_dp1_mb4_gpu12 -> layer13_attn_scores_dp1_mb4_gpu12
	layer13_softmax_dp0_mb4_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb4_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb4_gpu12 -> layer13_softmax_dp0_mb4_gpu12
	layer13_attn_scores_dp1_mb4_gpu12 -> layer13_softmax_dp1_mb4_gpu12
	layer13_attn_out_dp0_mb4_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb4_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb4_gpu12 -> layer13_attn_out_dp0_mb4_gpu12
	layer13_softmax_dp1_mb4_gpu12 -> layer13_attn_out_dp1_mb4_gpu12
	layer13_qkv_dp0_mb4_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb4_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb4 -> layer13_qkv_dp0_mb4_gpu13
	layer12_expert_agg_dp1_mb4 -> layer13_qkv_dp1_mb4_gpu13
	layer13_attn_scores_dp0_mb4_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb4_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb4_gpu13 -> layer13_attn_scores_dp0_mb4_gpu13
	layer13_qkv_dp1_mb4_gpu13 -> layer13_attn_scores_dp1_mb4_gpu13
	layer13_softmax_dp0_mb4_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb4_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb4_gpu13 -> layer13_softmax_dp0_mb4_gpu13
	layer13_attn_scores_dp1_mb4_gpu13 -> layer13_softmax_dp1_mb4_gpu13
	layer13_attn_out_dp0_mb4_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb4_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb4_gpu13 -> layer13_attn_out_dp0_mb4_gpu13
	layer13_softmax_dp1_mb4_gpu13 -> layer13_attn_out_dp1_mb4_gpu13
	layer13_qkv_dp0_mb4_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb4_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb4 -> layer13_qkv_dp0_mb4_gpu14
	layer12_expert_agg_dp1_mb4 -> layer13_qkv_dp1_mb4_gpu14
	layer13_attn_scores_dp0_mb4_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb4_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb4_gpu14 -> layer13_attn_scores_dp0_mb4_gpu14
	layer13_qkv_dp1_mb4_gpu14 -> layer13_attn_scores_dp1_mb4_gpu14
	layer13_softmax_dp0_mb4_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb4_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb4_gpu14 -> layer13_softmax_dp0_mb4_gpu14
	layer13_attn_scores_dp1_mb4_gpu14 -> layer13_softmax_dp1_mb4_gpu14
	layer13_attn_out_dp0_mb4_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb4_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb4_gpu14 -> layer13_attn_out_dp0_mb4_gpu14
	layer13_softmax_dp1_mb4_gpu14 -> layer13_attn_out_dp1_mb4_gpu14
	layer13_qkv_dp0_mb4_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb4_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb4 -> layer13_qkv_dp0_mb4_gpu15
	layer12_expert_agg_dp1_mb4 -> layer13_qkv_dp1_mb4_gpu15
	layer13_attn_scores_dp0_mb4_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb4_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb4_gpu15 -> layer13_attn_scores_dp0_mb4_gpu15
	layer13_qkv_dp1_mb4_gpu15 -> layer13_attn_scores_dp1_mb4_gpu15
	layer13_softmax_dp0_mb4_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb4_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb4_gpu15 -> layer13_softmax_dp0_mb4_gpu15
	layer13_attn_scores_dp1_mb4_gpu15 -> layer13_softmax_dp1_mb4_gpu15
	layer13_attn_out_dp0_mb4_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb4_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb4_gpu15 -> layer13_attn_out_dp0_mb4_gpu15
	layer13_softmax_dp1_mb4_gpu15 -> layer13_attn_out_dp1_mb4_gpu15
	layer13_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb4_gpu12 -> layer13_attn_allreduce_dp0_mb4
	layer13_attn_out_dp1_mb4_gpu12 -> layer13_attn_allreduce_dp1_mb4
	layer13_attn_out_dp0_mb4_gpu13 -> layer13_attn_allreduce_dp0_mb4
	layer13_attn_out_dp1_mb4_gpu13 -> layer13_attn_allreduce_dp1_mb4
	layer13_attn_out_dp0_mb4_gpu14 -> layer13_attn_allreduce_dp0_mb4
	layer13_attn_out_dp1_mb4_gpu14 -> layer13_attn_allreduce_dp1_mb4
	layer13_attn_out_dp0_mb4_gpu15 -> layer13_attn_allreduce_dp0_mb4
	layer13_attn_out_dp1_mb4_gpu15 -> layer13_attn_allreduce_dp1_mb4
	layer13_router_dp0_mb4_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb4_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb4 -> layer13_router_dp0_mb4_gpu12
	layer13_attn_allreduce_dp1_mb4 -> layer13_router_dp1_mb4_gpu12
	layer13_expert0_mlp1_dp0_mb4_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb4_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb4_gpu12 -> layer13_expert0_mlp1_dp0_mb4_gpu12
	layer13_router_dp1_mb4_gpu12 -> layer13_expert0_mlp1_dp1_mb4_gpu12
	layer13_expert0_mlp2_dp0_mb4_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb4_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb4_gpu12 -> layer13_expert0_mlp2_dp0_mb4_gpu12
	layer13_expert0_mlp1_dp1_mb4_gpu12 -> layer13_expert0_mlp2_dp1_mb4_gpu12
	layer13_expert1_mlp1_dp0_mb4_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb4_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb4_gpu12 -> layer13_expert1_mlp1_dp0_mb4_gpu13
	layer13_router_dp1_mb4_gpu12 -> layer13_expert1_mlp1_dp1_mb4_gpu13
	layer13_expert1_mlp2_dp0_mb4_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb4_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb4_gpu13 -> layer13_expert1_mlp2_dp0_mb4_gpu13
	layer13_expert1_mlp1_dp1_mb4_gpu13 -> layer13_expert1_mlp2_dp1_mb4_gpu13
	layer13_expert2_mlp1_dp0_mb4_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb4_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb4_gpu12 -> layer13_expert2_mlp1_dp0_mb4_gpu14
	layer13_router_dp1_mb4_gpu12 -> layer13_expert2_mlp1_dp1_mb4_gpu14
	layer13_expert2_mlp2_dp0_mb4_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb4_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb4_gpu14 -> layer13_expert2_mlp2_dp0_mb4_gpu14
	layer13_expert2_mlp1_dp1_mb4_gpu14 -> layer13_expert2_mlp2_dp1_mb4_gpu14
	layer13_expert3_mlp1_dp0_mb4_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb4_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb4_gpu12 -> layer13_expert3_mlp1_dp0_mb4_gpu15
	layer13_router_dp1_mb4_gpu12 -> layer13_expert3_mlp1_dp1_mb4_gpu15
	layer13_expert3_mlp2_dp0_mb4_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb4_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb4_gpu15 -> layer13_expert3_mlp2_dp0_mb4_gpu15
	layer13_expert3_mlp1_dp1_mb4_gpu15 -> layer13_expert3_mlp2_dp1_mb4_gpu15
	layer13_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb4_gpu12 -> layer13_expert_agg_dp0_mb4
	layer13_expert0_mlp2_dp1_mb4_gpu12 -> layer13_expert_agg_dp1_mb4
	layer13_expert1_mlp2_dp0_mb4_gpu13 -> layer13_expert_agg_dp0_mb4
	layer13_expert1_mlp2_dp1_mb4_gpu13 -> layer13_expert_agg_dp1_mb4
	layer13_expert2_mlp2_dp0_mb4_gpu14 -> layer13_expert_agg_dp0_mb4
	layer13_expert2_mlp2_dp1_mb4_gpu14 -> layer13_expert_agg_dp1_mb4
	layer13_expert3_mlp2_dp0_mb4_gpu15 -> layer13_expert_agg_dp0_mb4
	layer13_expert3_mlp2_dp1_mb4_gpu15 -> layer13_expert_agg_dp1_mb4
	layer14_qkv_dp0_mb4_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb4_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb4 -> layer14_qkv_dp0_mb4_gpu12
	layer13_expert_agg_dp1_mb4 -> layer14_qkv_dp1_mb4_gpu12
	layer14_attn_scores_dp0_mb4_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb4_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb4_gpu12 -> layer14_attn_scores_dp0_mb4_gpu12
	layer14_qkv_dp1_mb4_gpu12 -> layer14_attn_scores_dp1_mb4_gpu12
	layer14_softmax_dp0_mb4_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb4_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb4_gpu12 -> layer14_softmax_dp0_mb4_gpu12
	layer14_attn_scores_dp1_mb4_gpu12 -> layer14_softmax_dp1_mb4_gpu12
	layer14_attn_out_dp0_mb4_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb4_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb4_gpu12 -> layer14_attn_out_dp0_mb4_gpu12
	layer14_softmax_dp1_mb4_gpu12 -> layer14_attn_out_dp1_mb4_gpu12
	layer14_qkv_dp0_mb4_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb4_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb4 -> layer14_qkv_dp0_mb4_gpu13
	layer13_expert_agg_dp1_mb4 -> layer14_qkv_dp1_mb4_gpu13
	layer14_attn_scores_dp0_mb4_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb4_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb4_gpu13 -> layer14_attn_scores_dp0_mb4_gpu13
	layer14_qkv_dp1_mb4_gpu13 -> layer14_attn_scores_dp1_mb4_gpu13
	layer14_softmax_dp0_mb4_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb4_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb4_gpu13 -> layer14_softmax_dp0_mb4_gpu13
	layer14_attn_scores_dp1_mb4_gpu13 -> layer14_softmax_dp1_mb4_gpu13
	layer14_attn_out_dp0_mb4_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb4_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb4_gpu13 -> layer14_attn_out_dp0_mb4_gpu13
	layer14_softmax_dp1_mb4_gpu13 -> layer14_attn_out_dp1_mb4_gpu13
	layer14_qkv_dp0_mb4_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb4_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb4 -> layer14_qkv_dp0_mb4_gpu14
	layer13_expert_agg_dp1_mb4 -> layer14_qkv_dp1_mb4_gpu14
	layer14_attn_scores_dp0_mb4_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb4_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb4_gpu14 -> layer14_attn_scores_dp0_mb4_gpu14
	layer14_qkv_dp1_mb4_gpu14 -> layer14_attn_scores_dp1_mb4_gpu14
	layer14_softmax_dp0_mb4_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb4_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb4_gpu14 -> layer14_softmax_dp0_mb4_gpu14
	layer14_attn_scores_dp1_mb4_gpu14 -> layer14_softmax_dp1_mb4_gpu14
	layer14_attn_out_dp0_mb4_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb4_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb4_gpu14 -> layer14_attn_out_dp0_mb4_gpu14
	layer14_softmax_dp1_mb4_gpu14 -> layer14_attn_out_dp1_mb4_gpu14
	layer14_qkv_dp0_mb4_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb4_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb4 -> layer14_qkv_dp0_mb4_gpu15
	layer13_expert_agg_dp1_mb4 -> layer14_qkv_dp1_mb4_gpu15
	layer14_attn_scores_dp0_mb4_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb4_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb4_gpu15 -> layer14_attn_scores_dp0_mb4_gpu15
	layer14_qkv_dp1_mb4_gpu15 -> layer14_attn_scores_dp1_mb4_gpu15
	layer14_softmax_dp0_mb4_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb4_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb4_gpu15 -> layer14_softmax_dp0_mb4_gpu15
	layer14_attn_scores_dp1_mb4_gpu15 -> layer14_softmax_dp1_mb4_gpu15
	layer14_attn_out_dp0_mb4_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb4_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb4_gpu15 -> layer14_attn_out_dp0_mb4_gpu15
	layer14_softmax_dp1_mb4_gpu15 -> layer14_attn_out_dp1_mb4_gpu15
	layer14_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb4_gpu12 -> layer14_attn_allreduce_dp0_mb4
	layer14_attn_out_dp1_mb4_gpu12 -> layer14_attn_allreduce_dp1_mb4
	layer14_attn_out_dp0_mb4_gpu13 -> layer14_attn_allreduce_dp0_mb4
	layer14_attn_out_dp1_mb4_gpu13 -> layer14_attn_allreduce_dp1_mb4
	layer14_attn_out_dp0_mb4_gpu14 -> layer14_attn_allreduce_dp0_mb4
	layer14_attn_out_dp1_mb4_gpu14 -> layer14_attn_allreduce_dp1_mb4
	layer14_attn_out_dp0_mb4_gpu15 -> layer14_attn_allreduce_dp0_mb4
	layer14_attn_out_dp1_mb4_gpu15 -> layer14_attn_allreduce_dp1_mb4
	layer14_router_dp0_mb4_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb4_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb4 -> layer14_router_dp0_mb4_gpu12
	layer14_attn_allreduce_dp1_mb4 -> layer14_router_dp1_mb4_gpu12
	layer14_expert0_mlp1_dp0_mb4_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb4_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb4_gpu12 -> layer14_expert0_mlp1_dp0_mb4_gpu12
	layer14_router_dp1_mb4_gpu12 -> layer14_expert0_mlp1_dp1_mb4_gpu12
	layer14_expert0_mlp2_dp0_mb4_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb4_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb4_gpu12 -> layer14_expert0_mlp2_dp0_mb4_gpu12
	layer14_expert0_mlp1_dp1_mb4_gpu12 -> layer14_expert0_mlp2_dp1_mb4_gpu12
	layer14_expert1_mlp1_dp0_mb4_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb4_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb4_gpu12 -> layer14_expert1_mlp1_dp0_mb4_gpu13
	layer14_router_dp1_mb4_gpu12 -> layer14_expert1_mlp1_dp1_mb4_gpu13
	layer14_expert1_mlp2_dp0_mb4_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb4_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb4_gpu13 -> layer14_expert1_mlp2_dp0_mb4_gpu13
	layer14_expert1_mlp1_dp1_mb4_gpu13 -> layer14_expert1_mlp2_dp1_mb4_gpu13
	layer14_expert2_mlp1_dp0_mb4_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb4_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb4_gpu12 -> layer14_expert2_mlp1_dp0_mb4_gpu14
	layer14_router_dp1_mb4_gpu12 -> layer14_expert2_mlp1_dp1_mb4_gpu14
	layer14_expert2_mlp2_dp0_mb4_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb4_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb4_gpu14 -> layer14_expert2_mlp2_dp0_mb4_gpu14
	layer14_expert2_mlp1_dp1_mb4_gpu14 -> layer14_expert2_mlp2_dp1_mb4_gpu14
	layer14_expert3_mlp1_dp0_mb4_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb4_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb4_gpu12 -> layer14_expert3_mlp1_dp0_mb4_gpu15
	layer14_router_dp1_mb4_gpu12 -> layer14_expert3_mlp1_dp1_mb4_gpu15
	layer14_expert3_mlp2_dp0_mb4_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb4_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb4_gpu15 -> layer14_expert3_mlp2_dp0_mb4_gpu15
	layer14_expert3_mlp1_dp1_mb4_gpu15 -> layer14_expert3_mlp2_dp1_mb4_gpu15
	layer14_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb4_gpu12 -> layer14_expert_agg_dp0_mb4
	layer14_expert0_mlp2_dp1_mb4_gpu12 -> layer14_expert_agg_dp1_mb4
	layer14_expert1_mlp2_dp0_mb4_gpu13 -> layer14_expert_agg_dp0_mb4
	layer14_expert1_mlp2_dp1_mb4_gpu13 -> layer14_expert_agg_dp1_mb4
	layer14_expert2_mlp2_dp0_mb4_gpu14 -> layer14_expert_agg_dp0_mb4
	layer14_expert2_mlp2_dp1_mb4_gpu14 -> layer14_expert_agg_dp1_mb4
	layer14_expert3_mlp2_dp0_mb4_gpu15 -> layer14_expert_agg_dp0_mb4
	layer14_expert3_mlp2_dp1_mb4_gpu15 -> layer14_expert_agg_dp1_mb4
	layer15_qkv_dp0_mb4_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb4_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb4 -> layer15_qkv_dp0_mb4_gpu12
	layer14_expert_agg_dp1_mb4 -> layer15_qkv_dp1_mb4_gpu12
	layer15_attn_scores_dp0_mb4_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb4_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb4_gpu12 -> layer15_attn_scores_dp0_mb4_gpu12
	layer15_qkv_dp1_mb4_gpu12 -> layer15_attn_scores_dp1_mb4_gpu12
	layer15_softmax_dp0_mb4_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb4_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb4_gpu12 -> layer15_softmax_dp0_mb4_gpu12
	layer15_attn_scores_dp1_mb4_gpu12 -> layer15_softmax_dp1_mb4_gpu12
	layer15_attn_out_dp0_mb4_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb4_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb4_gpu12 -> layer15_attn_out_dp0_mb4_gpu12
	layer15_softmax_dp1_mb4_gpu12 -> layer15_attn_out_dp1_mb4_gpu12
	layer15_qkv_dp0_mb4_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb4_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb4 -> layer15_qkv_dp0_mb4_gpu13
	layer14_expert_agg_dp1_mb4 -> layer15_qkv_dp1_mb4_gpu13
	layer15_attn_scores_dp0_mb4_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb4_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb4_gpu13 -> layer15_attn_scores_dp0_mb4_gpu13
	layer15_qkv_dp1_mb4_gpu13 -> layer15_attn_scores_dp1_mb4_gpu13
	layer15_softmax_dp0_mb4_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb4_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb4_gpu13 -> layer15_softmax_dp0_mb4_gpu13
	layer15_attn_scores_dp1_mb4_gpu13 -> layer15_softmax_dp1_mb4_gpu13
	layer15_attn_out_dp0_mb4_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb4_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb4_gpu13 -> layer15_attn_out_dp0_mb4_gpu13
	layer15_softmax_dp1_mb4_gpu13 -> layer15_attn_out_dp1_mb4_gpu13
	layer15_qkv_dp0_mb4_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb4_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb4 -> layer15_qkv_dp0_mb4_gpu14
	layer14_expert_agg_dp1_mb4 -> layer15_qkv_dp1_mb4_gpu14
	layer15_attn_scores_dp0_mb4_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb4_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb4_gpu14 -> layer15_attn_scores_dp0_mb4_gpu14
	layer15_qkv_dp1_mb4_gpu14 -> layer15_attn_scores_dp1_mb4_gpu14
	layer15_softmax_dp0_mb4_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb4_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb4_gpu14 -> layer15_softmax_dp0_mb4_gpu14
	layer15_attn_scores_dp1_mb4_gpu14 -> layer15_softmax_dp1_mb4_gpu14
	layer15_attn_out_dp0_mb4_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb4_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb4_gpu14 -> layer15_attn_out_dp0_mb4_gpu14
	layer15_softmax_dp1_mb4_gpu14 -> layer15_attn_out_dp1_mb4_gpu14
	layer15_qkv_dp0_mb4_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb4_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb4 -> layer15_qkv_dp0_mb4_gpu15
	layer14_expert_agg_dp1_mb4 -> layer15_qkv_dp1_mb4_gpu15
	layer15_attn_scores_dp0_mb4_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb4_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb4_gpu15 -> layer15_attn_scores_dp0_mb4_gpu15
	layer15_qkv_dp1_mb4_gpu15 -> layer15_attn_scores_dp1_mb4_gpu15
	layer15_softmax_dp0_mb4_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb4_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb4_gpu15 -> layer15_softmax_dp0_mb4_gpu15
	layer15_attn_scores_dp1_mb4_gpu15 -> layer15_softmax_dp1_mb4_gpu15
	layer15_attn_out_dp0_mb4_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb4_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb4_gpu15 -> layer15_attn_out_dp0_mb4_gpu15
	layer15_softmax_dp1_mb4_gpu15 -> layer15_attn_out_dp1_mb4_gpu15
	layer15_attn_allreduce_dp0_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb4 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb4_gpu12 -> layer15_attn_allreduce_dp0_mb4
	layer15_attn_out_dp1_mb4_gpu12 -> layer15_attn_allreduce_dp1_mb4
	layer15_attn_out_dp0_mb4_gpu13 -> layer15_attn_allreduce_dp0_mb4
	layer15_attn_out_dp1_mb4_gpu13 -> layer15_attn_allreduce_dp1_mb4
	layer15_attn_out_dp0_mb4_gpu14 -> layer15_attn_allreduce_dp0_mb4
	layer15_attn_out_dp1_mb4_gpu14 -> layer15_attn_allreduce_dp1_mb4
	layer15_attn_out_dp0_mb4_gpu15 -> layer15_attn_allreduce_dp0_mb4
	layer15_attn_out_dp1_mb4_gpu15 -> layer15_attn_allreduce_dp1_mb4
	layer15_router_dp0_mb4_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb4_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb4 -> layer15_router_dp0_mb4_gpu12
	layer15_attn_allreduce_dp1_mb4 -> layer15_router_dp1_mb4_gpu12
	layer15_expert0_mlp1_dp0_mb4_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb4_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb4_gpu12 -> layer15_expert0_mlp1_dp0_mb4_gpu12
	layer15_router_dp1_mb4_gpu12 -> layer15_expert0_mlp1_dp1_mb4_gpu12
	layer15_expert0_mlp2_dp0_mb4_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb4_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb4_gpu12 -> layer15_expert0_mlp2_dp0_mb4_gpu12
	layer15_expert0_mlp1_dp1_mb4_gpu12 -> layer15_expert0_mlp2_dp1_mb4_gpu12
	layer15_expert1_mlp1_dp0_mb4_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb4_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb4_gpu12 -> layer15_expert1_mlp1_dp0_mb4_gpu13
	layer15_router_dp1_mb4_gpu12 -> layer15_expert1_mlp1_dp1_mb4_gpu13
	layer15_expert1_mlp2_dp0_mb4_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb4_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb4_gpu13 -> layer15_expert1_mlp2_dp0_mb4_gpu13
	layer15_expert1_mlp1_dp1_mb4_gpu13 -> layer15_expert1_mlp2_dp1_mb4_gpu13
	layer15_expert2_mlp1_dp0_mb4_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb4_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb4_gpu12 -> layer15_expert2_mlp1_dp0_mb4_gpu14
	layer15_router_dp1_mb4_gpu12 -> layer15_expert2_mlp1_dp1_mb4_gpu14
	layer15_expert2_mlp2_dp0_mb4_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb4_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb4_gpu14 -> layer15_expert2_mlp2_dp0_mb4_gpu14
	layer15_expert2_mlp1_dp1_mb4_gpu14 -> layer15_expert2_mlp2_dp1_mb4_gpu14
	layer15_expert3_mlp1_dp0_mb4_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb4_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb4_gpu12 -> layer15_expert3_mlp1_dp0_mb4_gpu15
	layer15_router_dp1_mb4_gpu12 -> layer15_expert3_mlp1_dp1_mb4_gpu15
	layer15_expert3_mlp2_dp0_mb4_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb4_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb4_gpu15 -> layer15_expert3_mlp2_dp0_mb4_gpu15
	layer15_expert3_mlp1_dp1_mb4_gpu15 -> layer15_expert3_mlp2_dp1_mb4_gpu15
	layer15_expert_agg_dp0_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb4 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb4_gpu12 -> layer15_expert_agg_dp0_mb4
	layer15_expert0_mlp2_dp1_mb4_gpu12 -> layer15_expert_agg_dp1_mb4
	layer15_expert1_mlp2_dp0_mb4_gpu13 -> layer15_expert_agg_dp0_mb4
	layer15_expert1_mlp2_dp1_mb4_gpu13 -> layer15_expert_agg_dp1_mb4
	layer15_expert2_mlp2_dp0_mb4_gpu14 -> layer15_expert_agg_dp0_mb4
	layer15_expert2_mlp2_dp1_mb4_gpu14 -> layer15_expert_agg_dp1_mb4
	layer15_expert3_mlp2_dp0_mb4_gpu15 -> layer15_expert_agg_dp0_mb4
	layer15_expert3_mlp2_dp1_mb4_gpu15 -> layer15_expert_agg_dp1_mb4
	layer12_qkv_dp0_mb5_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb5_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb5 -> layer12_qkv_dp0_mb5_gpu12
	layer11_expert_agg_dp1_mb5 -> layer12_qkv_dp1_mb5_gpu12
	layer12_attn_scores_dp0_mb5_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb5_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb5_gpu12 -> layer12_attn_scores_dp0_mb5_gpu12
	layer12_qkv_dp1_mb5_gpu12 -> layer12_attn_scores_dp1_mb5_gpu12
	layer12_softmax_dp0_mb5_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb5_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb5_gpu12 -> layer12_softmax_dp0_mb5_gpu12
	layer12_attn_scores_dp1_mb5_gpu12 -> layer12_softmax_dp1_mb5_gpu12
	layer12_attn_out_dp0_mb5_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb5_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb5_gpu12 -> layer12_attn_out_dp0_mb5_gpu12
	layer12_softmax_dp1_mb5_gpu12 -> layer12_attn_out_dp1_mb5_gpu12
	layer12_qkv_dp0_mb5_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb5_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb5 -> layer12_qkv_dp0_mb5_gpu13
	layer11_expert_agg_dp1_mb5 -> layer12_qkv_dp1_mb5_gpu13
	layer12_attn_scores_dp0_mb5_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb5_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb5_gpu13 -> layer12_attn_scores_dp0_mb5_gpu13
	layer12_qkv_dp1_mb5_gpu13 -> layer12_attn_scores_dp1_mb5_gpu13
	layer12_softmax_dp0_mb5_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb5_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb5_gpu13 -> layer12_softmax_dp0_mb5_gpu13
	layer12_attn_scores_dp1_mb5_gpu13 -> layer12_softmax_dp1_mb5_gpu13
	layer12_attn_out_dp0_mb5_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb5_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb5_gpu13 -> layer12_attn_out_dp0_mb5_gpu13
	layer12_softmax_dp1_mb5_gpu13 -> layer12_attn_out_dp1_mb5_gpu13
	layer12_qkv_dp0_mb5_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb5_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb5 -> layer12_qkv_dp0_mb5_gpu14
	layer11_expert_agg_dp1_mb5 -> layer12_qkv_dp1_mb5_gpu14
	layer12_attn_scores_dp0_mb5_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb5_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb5_gpu14 -> layer12_attn_scores_dp0_mb5_gpu14
	layer12_qkv_dp1_mb5_gpu14 -> layer12_attn_scores_dp1_mb5_gpu14
	layer12_softmax_dp0_mb5_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb5_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb5_gpu14 -> layer12_softmax_dp0_mb5_gpu14
	layer12_attn_scores_dp1_mb5_gpu14 -> layer12_softmax_dp1_mb5_gpu14
	layer12_attn_out_dp0_mb5_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb5_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb5_gpu14 -> layer12_attn_out_dp0_mb5_gpu14
	layer12_softmax_dp1_mb5_gpu14 -> layer12_attn_out_dp1_mb5_gpu14
	layer12_qkv_dp0_mb5_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb5_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb5 -> layer12_qkv_dp0_mb5_gpu15
	layer11_expert_agg_dp1_mb5 -> layer12_qkv_dp1_mb5_gpu15
	layer12_attn_scores_dp0_mb5_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb5_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb5_gpu15 -> layer12_attn_scores_dp0_mb5_gpu15
	layer12_qkv_dp1_mb5_gpu15 -> layer12_attn_scores_dp1_mb5_gpu15
	layer12_softmax_dp0_mb5_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb5_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb5_gpu15 -> layer12_softmax_dp0_mb5_gpu15
	layer12_attn_scores_dp1_mb5_gpu15 -> layer12_softmax_dp1_mb5_gpu15
	layer12_attn_out_dp0_mb5_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb5_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb5_gpu15 -> layer12_attn_out_dp0_mb5_gpu15
	layer12_softmax_dp1_mb5_gpu15 -> layer12_attn_out_dp1_mb5_gpu15
	layer12_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb5_gpu12 -> layer12_attn_allreduce_dp0_mb5
	layer12_attn_out_dp1_mb5_gpu12 -> layer12_attn_allreduce_dp1_mb5
	layer12_attn_out_dp0_mb5_gpu13 -> layer12_attn_allreduce_dp0_mb5
	layer12_attn_out_dp1_mb5_gpu13 -> layer12_attn_allreduce_dp1_mb5
	layer12_attn_out_dp0_mb5_gpu14 -> layer12_attn_allreduce_dp0_mb5
	layer12_attn_out_dp1_mb5_gpu14 -> layer12_attn_allreduce_dp1_mb5
	layer12_attn_out_dp0_mb5_gpu15 -> layer12_attn_allreduce_dp0_mb5
	layer12_attn_out_dp1_mb5_gpu15 -> layer12_attn_allreduce_dp1_mb5
	layer12_router_dp0_mb5_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb5_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb5 -> layer12_router_dp0_mb5_gpu12
	layer12_attn_allreduce_dp1_mb5 -> layer12_router_dp1_mb5_gpu12
	layer12_expert0_mlp1_dp0_mb5_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb5_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb5_gpu12 -> layer12_expert0_mlp1_dp0_mb5_gpu12
	layer12_router_dp1_mb5_gpu12 -> layer12_expert0_mlp1_dp1_mb5_gpu12
	layer12_expert0_mlp2_dp0_mb5_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb5_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb5_gpu12 -> layer12_expert0_mlp2_dp0_mb5_gpu12
	layer12_expert0_mlp1_dp1_mb5_gpu12 -> layer12_expert0_mlp2_dp1_mb5_gpu12
	layer12_expert1_mlp1_dp0_mb5_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb5_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb5_gpu12 -> layer12_expert1_mlp1_dp0_mb5_gpu13
	layer12_router_dp1_mb5_gpu12 -> layer12_expert1_mlp1_dp1_mb5_gpu13
	layer12_expert1_mlp2_dp0_mb5_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb5_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb5_gpu13 -> layer12_expert1_mlp2_dp0_mb5_gpu13
	layer12_expert1_mlp1_dp1_mb5_gpu13 -> layer12_expert1_mlp2_dp1_mb5_gpu13
	layer12_expert2_mlp1_dp0_mb5_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb5_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb5_gpu12 -> layer12_expert2_mlp1_dp0_mb5_gpu14
	layer12_router_dp1_mb5_gpu12 -> layer12_expert2_mlp1_dp1_mb5_gpu14
	layer12_expert2_mlp2_dp0_mb5_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb5_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb5_gpu14 -> layer12_expert2_mlp2_dp0_mb5_gpu14
	layer12_expert2_mlp1_dp1_mb5_gpu14 -> layer12_expert2_mlp2_dp1_mb5_gpu14
	layer12_expert3_mlp1_dp0_mb5_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb5_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb5_gpu12 -> layer12_expert3_mlp1_dp0_mb5_gpu15
	layer12_router_dp1_mb5_gpu12 -> layer12_expert3_mlp1_dp1_mb5_gpu15
	layer12_expert3_mlp2_dp0_mb5_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb5_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb5_gpu15 -> layer12_expert3_mlp2_dp0_mb5_gpu15
	layer12_expert3_mlp1_dp1_mb5_gpu15 -> layer12_expert3_mlp2_dp1_mb5_gpu15
	layer12_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb5_gpu12 -> layer12_expert_agg_dp0_mb5
	layer12_expert0_mlp2_dp1_mb5_gpu12 -> layer12_expert_agg_dp1_mb5
	layer12_expert1_mlp2_dp0_mb5_gpu13 -> layer12_expert_agg_dp0_mb5
	layer12_expert1_mlp2_dp1_mb5_gpu13 -> layer12_expert_agg_dp1_mb5
	layer12_expert2_mlp2_dp0_mb5_gpu14 -> layer12_expert_agg_dp0_mb5
	layer12_expert2_mlp2_dp1_mb5_gpu14 -> layer12_expert_agg_dp1_mb5
	layer12_expert3_mlp2_dp0_mb5_gpu15 -> layer12_expert_agg_dp0_mb5
	layer12_expert3_mlp2_dp1_mb5_gpu15 -> layer12_expert_agg_dp1_mb5
	layer13_qkv_dp0_mb5_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb5_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb5 -> layer13_qkv_dp0_mb5_gpu12
	layer12_expert_agg_dp1_mb5 -> layer13_qkv_dp1_mb5_gpu12
	layer13_attn_scores_dp0_mb5_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb5_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb5_gpu12 -> layer13_attn_scores_dp0_mb5_gpu12
	layer13_qkv_dp1_mb5_gpu12 -> layer13_attn_scores_dp1_mb5_gpu12
	layer13_softmax_dp0_mb5_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb5_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb5_gpu12 -> layer13_softmax_dp0_mb5_gpu12
	layer13_attn_scores_dp1_mb5_gpu12 -> layer13_softmax_dp1_mb5_gpu12
	layer13_attn_out_dp0_mb5_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb5_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb5_gpu12 -> layer13_attn_out_dp0_mb5_gpu12
	layer13_softmax_dp1_mb5_gpu12 -> layer13_attn_out_dp1_mb5_gpu12
	layer13_qkv_dp0_mb5_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb5_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb5 -> layer13_qkv_dp0_mb5_gpu13
	layer12_expert_agg_dp1_mb5 -> layer13_qkv_dp1_mb5_gpu13
	layer13_attn_scores_dp0_mb5_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb5_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb5_gpu13 -> layer13_attn_scores_dp0_mb5_gpu13
	layer13_qkv_dp1_mb5_gpu13 -> layer13_attn_scores_dp1_mb5_gpu13
	layer13_softmax_dp0_mb5_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb5_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb5_gpu13 -> layer13_softmax_dp0_mb5_gpu13
	layer13_attn_scores_dp1_mb5_gpu13 -> layer13_softmax_dp1_mb5_gpu13
	layer13_attn_out_dp0_mb5_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb5_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb5_gpu13 -> layer13_attn_out_dp0_mb5_gpu13
	layer13_softmax_dp1_mb5_gpu13 -> layer13_attn_out_dp1_mb5_gpu13
	layer13_qkv_dp0_mb5_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb5_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb5 -> layer13_qkv_dp0_mb5_gpu14
	layer12_expert_agg_dp1_mb5 -> layer13_qkv_dp1_mb5_gpu14
	layer13_attn_scores_dp0_mb5_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb5_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb5_gpu14 -> layer13_attn_scores_dp0_mb5_gpu14
	layer13_qkv_dp1_mb5_gpu14 -> layer13_attn_scores_dp1_mb5_gpu14
	layer13_softmax_dp0_mb5_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb5_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb5_gpu14 -> layer13_softmax_dp0_mb5_gpu14
	layer13_attn_scores_dp1_mb5_gpu14 -> layer13_softmax_dp1_mb5_gpu14
	layer13_attn_out_dp0_mb5_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb5_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb5_gpu14 -> layer13_attn_out_dp0_mb5_gpu14
	layer13_softmax_dp1_mb5_gpu14 -> layer13_attn_out_dp1_mb5_gpu14
	layer13_qkv_dp0_mb5_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb5_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb5 -> layer13_qkv_dp0_mb5_gpu15
	layer12_expert_agg_dp1_mb5 -> layer13_qkv_dp1_mb5_gpu15
	layer13_attn_scores_dp0_mb5_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb5_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb5_gpu15 -> layer13_attn_scores_dp0_mb5_gpu15
	layer13_qkv_dp1_mb5_gpu15 -> layer13_attn_scores_dp1_mb5_gpu15
	layer13_softmax_dp0_mb5_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb5_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb5_gpu15 -> layer13_softmax_dp0_mb5_gpu15
	layer13_attn_scores_dp1_mb5_gpu15 -> layer13_softmax_dp1_mb5_gpu15
	layer13_attn_out_dp0_mb5_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb5_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb5_gpu15 -> layer13_attn_out_dp0_mb5_gpu15
	layer13_softmax_dp1_mb5_gpu15 -> layer13_attn_out_dp1_mb5_gpu15
	layer13_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb5_gpu12 -> layer13_attn_allreduce_dp0_mb5
	layer13_attn_out_dp1_mb5_gpu12 -> layer13_attn_allreduce_dp1_mb5
	layer13_attn_out_dp0_mb5_gpu13 -> layer13_attn_allreduce_dp0_mb5
	layer13_attn_out_dp1_mb5_gpu13 -> layer13_attn_allreduce_dp1_mb5
	layer13_attn_out_dp0_mb5_gpu14 -> layer13_attn_allreduce_dp0_mb5
	layer13_attn_out_dp1_mb5_gpu14 -> layer13_attn_allreduce_dp1_mb5
	layer13_attn_out_dp0_mb5_gpu15 -> layer13_attn_allreduce_dp0_mb5
	layer13_attn_out_dp1_mb5_gpu15 -> layer13_attn_allreduce_dp1_mb5
	layer13_router_dp0_mb5_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb5_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb5 -> layer13_router_dp0_mb5_gpu12
	layer13_attn_allreduce_dp1_mb5 -> layer13_router_dp1_mb5_gpu12
	layer13_expert0_mlp1_dp0_mb5_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb5_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb5_gpu12 -> layer13_expert0_mlp1_dp0_mb5_gpu12
	layer13_router_dp1_mb5_gpu12 -> layer13_expert0_mlp1_dp1_mb5_gpu12
	layer13_expert0_mlp2_dp0_mb5_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb5_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb5_gpu12 -> layer13_expert0_mlp2_dp0_mb5_gpu12
	layer13_expert0_mlp1_dp1_mb5_gpu12 -> layer13_expert0_mlp2_dp1_mb5_gpu12
	layer13_expert1_mlp1_dp0_mb5_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb5_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb5_gpu12 -> layer13_expert1_mlp1_dp0_mb5_gpu13
	layer13_router_dp1_mb5_gpu12 -> layer13_expert1_mlp1_dp1_mb5_gpu13
	layer13_expert1_mlp2_dp0_mb5_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb5_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb5_gpu13 -> layer13_expert1_mlp2_dp0_mb5_gpu13
	layer13_expert1_mlp1_dp1_mb5_gpu13 -> layer13_expert1_mlp2_dp1_mb5_gpu13
	layer13_expert2_mlp1_dp0_mb5_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb5_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb5_gpu12 -> layer13_expert2_mlp1_dp0_mb5_gpu14
	layer13_router_dp1_mb5_gpu12 -> layer13_expert2_mlp1_dp1_mb5_gpu14
	layer13_expert2_mlp2_dp0_mb5_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb5_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb5_gpu14 -> layer13_expert2_mlp2_dp0_mb5_gpu14
	layer13_expert2_mlp1_dp1_mb5_gpu14 -> layer13_expert2_mlp2_dp1_mb5_gpu14
	layer13_expert3_mlp1_dp0_mb5_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb5_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb5_gpu12 -> layer13_expert3_mlp1_dp0_mb5_gpu15
	layer13_router_dp1_mb5_gpu12 -> layer13_expert3_mlp1_dp1_mb5_gpu15
	layer13_expert3_mlp2_dp0_mb5_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb5_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb5_gpu15 -> layer13_expert3_mlp2_dp0_mb5_gpu15
	layer13_expert3_mlp1_dp1_mb5_gpu15 -> layer13_expert3_mlp2_dp1_mb5_gpu15
	layer13_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb5_gpu12 -> layer13_expert_agg_dp0_mb5
	layer13_expert0_mlp2_dp1_mb5_gpu12 -> layer13_expert_agg_dp1_mb5
	layer13_expert1_mlp2_dp0_mb5_gpu13 -> layer13_expert_agg_dp0_mb5
	layer13_expert1_mlp2_dp1_mb5_gpu13 -> layer13_expert_agg_dp1_mb5
	layer13_expert2_mlp2_dp0_mb5_gpu14 -> layer13_expert_agg_dp0_mb5
	layer13_expert2_mlp2_dp1_mb5_gpu14 -> layer13_expert_agg_dp1_mb5
	layer13_expert3_mlp2_dp0_mb5_gpu15 -> layer13_expert_agg_dp0_mb5
	layer13_expert3_mlp2_dp1_mb5_gpu15 -> layer13_expert_agg_dp1_mb5
	layer14_qkv_dp0_mb5_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb5_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb5 -> layer14_qkv_dp0_mb5_gpu12
	layer13_expert_agg_dp1_mb5 -> layer14_qkv_dp1_mb5_gpu12
	layer14_attn_scores_dp0_mb5_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb5_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb5_gpu12 -> layer14_attn_scores_dp0_mb5_gpu12
	layer14_qkv_dp1_mb5_gpu12 -> layer14_attn_scores_dp1_mb5_gpu12
	layer14_softmax_dp0_mb5_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb5_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb5_gpu12 -> layer14_softmax_dp0_mb5_gpu12
	layer14_attn_scores_dp1_mb5_gpu12 -> layer14_softmax_dp1_mb5_gpu12
	layer14_attn_out_dp0_mb5_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb5_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb5_gpu12 -> layer14_attn_out_dp0_mb5_gpu12
	layer14_softmax_dp1_mb5_gpu12 -> layer14_attn_out_dp1_mb5_gpu12
	layer14_qkv_dp0_mb5_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb5_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb5 -> layer14_qkv_dp0_mb5_gpu13
	layer13_expert_agg_dp1_mb5 -> layer14_qkv_dp1_mb5_gpu13
	layer14_attn_scores_dp0_mb5_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb5_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb5_gpu13 -> layer14_attn_scores_dp0_mb5_gpu13
	layer14_qkv_dp1_mb5_gpu13 -> layer14_attn_scores_dp1_mb5_gpu13
	layer14_softmax_dp0_mb5_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb5_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb5_gpu13 -> layer14_softmax_dp0_mb5_gpu13
	layer14_attn_scores_dp1_mb5_gpu13 -> layer14_softmax_dp1_mb5_gpu13
	layer14_attn_out_dp0_mb5_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb5_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb5_gpu13 -> layer14_attn_out_dp0_mb5_gpu13
	layer14_softmax_dp1_mb5_gpu13 -> layer14_attn_out_dp1_mb5_gpu13
	layer14_qkv_dp0_mb5_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb5_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb5 -> layer14_qkv_dp0_mb5_gpu14
	layer13_expert_agg_dp1_mb5 -> layer14_qkv_dp1_mb5_gpu14
	layer14_attn_scores_dp0_mb5_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb5_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb5_gpu14 -> layer14_attn_scores_dp0_mb5_gpu14
	layer14_qkv_dp1_mb5_gpu14 -> layer14_attn_scores_dp1_mb5_gpu14
	layer14_softmax_dp0_mb5_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb5_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb5_gpu14 -> layer14_softmax_dp0_mb5_gpu14
	layer14_attn_scores_dp1_mb5_gpu14 -> layer14_softmax_dp1_mb5_gpu14
	layer14_attn_out_dp0_mb5_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb5_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb5_gpu14 -> layer14_attn_out_dp0_mb5_gpu14
	layer14_softmax_dp1_mb5_gpu14 -> layer14_attn_out_dp1_mb5_gpu14
	layer14_qkv_dp0_mb5_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb5_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb5 -> layer14_qkv_dp0_mb5_gpu15
	layer13_expert_agg_dp1_mb5 -> layer14_qkv_dp1_mb5_gpu15
	layer14_attn_scores_dp0_mb5_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb5_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb5_gpu15 -> layer14_attn_scores_dp0_mb5_gpu15
	layer14_qkv_dp1_mb5_gpu15 -> layer14_attn_scores_dp1_mb5_gpu15
	layer14_softmax_dp0_mb5_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb5_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb5_gpu15 -> layer14_softmax_dp0_mb5_gpu15
	layer14_attn_scores_dp1_mb5_gpu15 -> layer14_softmax_dp1_mb5_gpu15
	layer14_attn_out_dp0_mb5_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb5_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb5_gpu15 -> layer14_attn_out_dp0_mb5_gpu15
	layer14_softmax_dp1_mb5_gpu15 -> layer14_attn_out_dp1_mb5_gpu15
	layer14_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb5_gpu12 -> layer14_attn_allreduce_dp0_mb5
	layer14_attn_out_dp1_mb5_gpu12 -> layer14_attn_allreduce_dp1_mb5
	layer14_attn_out_dp0_mb5_gpu13 -> layer14_attn_allreduce_dp0_mb5
	layer14_attn_out_dp1_mb5_gpu13 -> layer14_attn_allreduce_dp1_mb5
	layer14_attn_out_dp0_mb5_gpu14 -> layer14_attn_allreduce_dp0_mb5
	layer14_attn_out_dp1_mb5_gpu14 -> layer14_attn_allreduce_dp1_mb5
	layer14_attn_out_dp0_mb5_gpu15 -> layer14_attn_allreduce_dp0_mb5
	layer14_attn_out_dp1_mb5_gpu15 -> layer14_attn_allreduce_dp1_mb5
	layer14_router_dp0_mb5_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb5_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb5 -> layer14_router_dp0_mb5_gpu12
	layer14_attn_allreduce_dp1_mb5 -> layer14_router_dp1_mb5_gpu12
	layer14_expert0_mlp1_dp0_mb5_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb5_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb5_gpu12 -> layer14_expert0_mlp1_dp0_mb5_gpu12
	layer14_router_dp1_mb5_gpu12 -> layer14_expert0_mlp1_dp1_mb5_gpu12
	layer14_expert0_mlp2_dp0_mb5_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb5_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb5_gpu12 -> layer14_expert0_mlp2_dp0_mb5_gpu12
	layer14_expert0_mlp1_dp1_mb5_gpu12 -> layer14_expert0_mlp2_dp1_mb5_gpu12
	layer14_expert1_mlp1_dp0_mb5_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb5_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb5_gpu12 -> layer14_expert1_mlp1_dp0_mb5_gpu13
	layer14_router_dp1_mb5_gpu12 -> layer14_expert1_mlp1_dp1_mb5_gpu13
	layer14_expert1_mlp2_dp0_mb5_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb5_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb5_gpu13 -> layer14_expert1_mlp2_dp0_mb5_gpu13
	layer14_expert1_mlp1_dp1_mb5_gpu13 -> layer14_expert1_mlp2_dp1_mb5_gpu13
	layer14_expert2_mlp1_dp0_mb5_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb5_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb5_gpu12 -> layer14_expert2_mlp1_dp0_mb5_gpu14
	layer14_router_dp1_mb5_gpu12 -> layer14_expert2_mlp1_dp1_mb5_gpu14
	layer14_expert2_mlp2_dp0_mb5_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb5_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb5_gpu14 -> layer14_expert2_mlp2_dp0_mb5_gpu14
	layer14_expert2_mlp1_dp1_mb5_gpu14 -> layer14_expert2_mlp2_dp1_mb5_gpu14
	layer14_expert3_mlp1_dp0_mb5_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb5_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb5_gpu12 -> layer14_expert3_mlp1_dp0_mb5_gpu15
	layer14_router_dp1_mb5_gpu12 -> layer14_expert3_mlp1_dp1_mb5_gpu15
	layer14_expert3_mlp2_dp0_mb5_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb5_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb5_gpu15 -> layer14_expert3_mlp2_dp0_mb5_gpu15
	layer14_expert3_mlp1_dp1_mb5_gpu15 -> layer14_expert3_mlp2_dp1_mb5_gpu15
	layer14_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb5_gpu12 -> layer14_expert_agg_dp0_mb5
	layer14_expert0_mlp2_dp1_mb5_gpu12 -> layer14_expert_agg_dp1_mb5
	layer14_expert1_mlp2_dp0_mb5_gpu13 -> layer14_expert_agg_dp0_mb5
	layer14_expert1_mlp2_dp1_mb5_gpu13 -> layer14_expert_agg_dp1_mb5
	layer14_expert2_mlp2_dp0_mb5_gpu14 -> layer14_expert_agg_dp0_mb5
	layer14_expert2_mlp2_dp1_mb5_gpu14 -> layer14_expert_agg_dp1_mb5
	layer14_expert3_mlp2_dp0_mb5_gpu15 -> layer14_expert_agg_dp0_mb5
	layer14_expert3_mlp2_dp1_mb5_gpu15 -> layer14_expert_agg_dp1_mb5
	layer15_qkv_dp0_mb5_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb5_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb5 -> layer15_qkv_dp0_mb5_gpu12
	layer14_expert_agg_dp1_mb5 -> layer15_qkv_dp1_mb5_gpu12
	layer15_attn_scores_dp0_mb5_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb5_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb5_gpu12 -> layer15_attn_scores_dp0_mb5_gpu12
	layer15_qkv_dp1_mb5_gpu12 -> layer15_attn_scores_dp1_mb5_gpu12
	layer15_softmax_dp0_mb5_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb5_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb5_gpu12 -> layer15_softmax_dp0_mb5_gpu12
	layer15_attn_scores_dp1_mb5_gpu12 -> layer15_softmax_dp1_mb5_gpu12
	layer15_attn_out_dp0_mb5_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb5_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb5_gpu12 -> layer15_attn_out_dp0_mb5_gpu12
	layer15_softmax_dp1_mb5_gpu12 -> layer15_attn_out_dp1_mb5_gpu12
	layer15_qkv_dp0_mb5_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb5_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb5 -> layer15_qkv_dp0_mb5_gpu13
	layer14_expert_agg_dp1_mb5 -> layer15_qkv_dp1_mb5_gpu13
	layer15_attn_scores_dp0_mb5_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb5_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb5_gpu13 -> layer15_attn_scores_dp0_mb5_gpu13
	layer15_qkv_dp1_mb5_gpu13 -> layer15_attn_scores_dp1_mb5_gpu13
	layer15_softmax_dp0_mb5_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb5_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb5_gpu13 -> layer15_softmax_dp0_mb5_gpu13
	layer15_attn_scores_dp1_mb5_gpu13 -> layer15_softmax_dp1_mb5_gpu13
	layer15_attn_out_dp0_mb5_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb5_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb5_gpu13 -> layer15_attn_out_dp0_mb5_gpu13
	layer15_softmax_dp1_mb5_gpu13 -> layer15_attn_out_dp1_mb5_gpu13
	layer15_qkv_dp0_mb5_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb5_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb5 -> layer15_qkv_dp0_mb5_gpu14
	layer14_expert_agg_dp1_mb5 -> layer15_qkv_dp1_mb5_gpu14
	layer15_attn_scores_dp0_mb5_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb5_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb5_gpu14 -> layer15_attn_scores_dp0_mb5_gpu14
	layer15_qkv_dp1_mb5_gpu14 -> layer15_attn_scores_dp1_mb5_gpu14
	layer15_softmax_dp0_mb5_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb5_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb5_gpu14 -> layer15_softmax_dp0_mb5_gpu14
	layer15_attn_scores_dp1_mb5_gpu14 -> layer15_softmax_dp1_mb5_gpu14
	layer15_attn_out_dp0_mb5_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb5_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb5_gpu14 -> layer15_attn_out_dp0_mb5_gpu14
	layer15_softmax_dp1_mb5_gpu14 -> layer15_attn_out_dp1_mb5_gpu14
	layer15_qkv_dp0_mb5_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb5_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb5 -> layer15_qkv_dp0_mb5_gpu15
	layer14_expert_agg_dp1_mb5 -> layer15_qkv_dp1_mb5_gpu15
	layer15_attn_scores_dp0_mb5_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb5_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb5_gpu15 -> layer15_attn_scores_dp0_mb5_gpu15
	layer15_qkv_dp1_mb5_gpu15 -> layer15_attn_scores_dp1_mb5_gpu15
	layer15_softmax_dp0_mb5_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb5_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb5_gpu15 -> layer15_softmax_dp0_mb5_gpu15
	layer15_attn_scores_dp1_mb5_gpu15 -> layer15_softmax_dp1_mb5_gpu15
	layer15_attn_out_dp0_mb5_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb5_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb5_gpu15 -> layer15_attn_out_dp0_mb5_gpu15
	layer15_softmax_dp1_mb5_gpu15 -> layer15_attn_out_dp1_mb5_gpu15
	layer15_attn_allreduce_dp0_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb5 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb5_gpu12 -> layer15_attn_allreduce_dp0_mb5
	layer15_attn_out_dp1_mb5_gpu12 -> layer15_attn_allreduce_dp1_mb5
	layer15_attn_out_dp0_mb5_gpu13 -> layer15_attn_allreduce_dp0_mb5
	layer15_attn_out_dp1_mb5_gpu13 -> layer15_attn_allreduce_dp1_mb5
	layer15_attn_out_dp0_mb5_gpu14 -> layer15_attn_allreduce_dp0_mb5
	layer15_attn_out_dp1_mb5_gpu14 -> layer15_attn_allreduce_dp1_mb5
	layer15_attn_out_dp0_mb5_gpu15 -> layer15_attn_allreduce_dp0_mb5
	layer15_attn_out_dp1_mb5_gpu15 -> layer15_attn_allreduce_dp1_mb5
	layer15_router_dp0_mb5_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb5_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb5 -> layer15_router_dp0_mb5_gpu12
	layer15_attn_allreduce_dp1_mb5 -> layer15_router_dp1_mb5_gpu12
	layer15_expert0_mlp1_dp0_mb5_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb5_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb5_gpu12 -> layer15_expert0_mlp1_dp0_mb5_gpu12
	layer15_router_dp1_mb5_gpu12 -> layer15_expert0_mlp1_dp1_mb5_gpu12
	layer15_expert0_mlp2_dp0_mb5_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb5_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb5_gpu12 -> layer15_expert0_mlp2_dp0_mb5_gpu12
	layer15_expert0_mlp1_dp1_mb5_gpu12 -> layer15_expert0_mlp2_dp1_mb5_gpu12
	layer15_expert1_mlp1_dp0_mb5_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb5_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb5_gpu12 -> layer15_expert1_mlp1_dp0_mb5_gpu13
	layer15_router_dp1_mb5_gpu12 -> layer15_expert1_mlp1_dp1_mb5_gpu13
	layer15_expert1_mlp2_dp0_mb5_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb5_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb5_gpu13 -> layer15_expert1_mlp2_dp0_mb5_gpu13
	layer15_expert1_mlp1_dp1_mb5_gpu13 -> layer15_expert1_mlp2_dp1_mb5_gpu13
	layer15_expert2_mlp1_dp0_mb5_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb5_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb5_gpu12 -> layer15_expert2_mlp1_dp0_mb5_gpu14
	layer15_router_dp1_mb5_gpu12 -> layer15_expert2_mlp1_dp1_mb5_gpu14
	layer15_expert2_mlp2_dp0_mb5_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb5_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb5_gpu14 -> layer15_expert2_mlp2_dp0_mb5_gpu14
	layer15_expert2_mlp1_dp1_mb5_gpu14 -> layer15_expert2_mlp2_dp1_mb5_gpu14
	layer15_expert3_mlp1_dp0_mb5_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb5_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb5_gpu12 -> layer15_expert3_mlp1_dp0_mb5_gpu15
	layer15_router_dp1_mb5_gpu12 -> layer15_expert3_mlp1_dp1_mb5_gpu15
	layer15_expert3_mlp2_dp0_mb5_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb5_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb5_gpu15 -> layer15_expert3_mlp2_dp0_mb5_gpu15
	layer15_expert3_mlp1_dp1_mb5_gpu15 -> layer15_expert3_mlp2_dp1_mb5_gpu15
	layer15_expert_agg_dp0_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb5 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb5_gpu12 -> layer15_expert_agg_dp0_mb5
	layer15_expert0_mlp2_dp1_mb5_gpu12 -> layer15_expert_agg_dp1_mb5
	layer15_expert1_mlp2_dp0_mb5_gpu13 -> layer15_expert_agg_dp0_mb5
	layer15_expert1_mlp2_dp1_mb5_gpu13 -> layer15_expert_agg_dp1_mb5
	layer15_expert2_mlp2_dp0_mb5_gpu14 -> layer15_expert_agg_dp0_mb5
	layer15_expert2_mlp2_dp1_mb5_gpu14 -> layer15_expert_agg_dp1_mb5
	layer15_expert3_mlp2_dp0_mb5_gpu15 -> layer15_expert_agg_dp0_mb5
	layer15_expert3_mlp2_dp1_mb5_gpu15 -> layer15_expert_agg_dp1_mb5
	layer12_qkv_dp0_mb6_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb6_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb6 -> layer12_qkv_dp0_mb6_gpu12
	layer11_expert_agg_dp1_mb6 -> layer12_qkv_dp1_mb6_gpu12
	layer12_attn_scores_dp0_mb6_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb6_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb6_gpu12 -> layer12_attn_scores_dp0_mb6_gpu12
	layer12_qkv_dp1_mb6_gpu12 -> layer12_attn_scores_dp1_mb6_gpu12
	layer12_softmax_dp0_mb6_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb6_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb6_gpu12 -> layer12_softmax_dp0_mb6_gpu12
	layer12_attn_scores_dp1_mb6_gpu12 -> layer12_softmax_dp1_mb6_gpu12
	layer12_attn_out_dp0_mb6_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb6_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb6_gpu12 -> layer12_attn_out_dp0_mb6_gpu12
	layer12_softmax_dp1_mb6_gpu12 -> layer12_attn_out_dp1_mb6_gpu12
	layer12_qkv_dp0_mb6_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb6_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb6 -> layer12_qkv_dp0_mb6_gpu13
	layer11_expert_agg_dp1_mb6 -> layer12_qkv_dp1_mb6_gpu13
	layer12_attn_scores_dp0_mb6_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb6_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb6_gpu13 -> layer12_attn_scores_dp0_mb6_gpu13
	layer12_qkv_dp1_mb6_gpu13 -> layer12_attn_scores_dp1_mb6_gpu13
	layer12_softmax_dp0_mb6_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb6_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb6_gpu13 -> layer12_softmax_dp0_mb6_gpu13
	layer12_attn_scores_dp1_mb6_gpu13 -> layer12_softmax_dp1_mb6_gpu13
	layer12_attn_out_dp0_mb6_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb6_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb6_gpu13 -> layer12_attn_out_dp0_mb6_gpu13
	layer12_softmax_dp1_mb6_gpu13 -> layer12_attn_out_dp1_mb6_gpu13
	layer12_qkv_dp0_mb6_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb6_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb6 -> layer12_qkv_dp0_mb6_gpu14
	layer11_expert_agg_dp1_mb6 -> layer12_qkv_dp1_mb6_gpu14
	layer12_attn_scores_dp0_mb6_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb6_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb6_gpu14 -> layer12_attn_scores_dp0_mb6_gpu14
	layer12_qkv_dp1_mb6_gpu14 -> layer12_attn_scores_dp1_mb6_gpu14
	layer12_softmax_dp0_mb6_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb6_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb6_gpu14 -> layer12_softmax_dp0_mb6_gpu14
	layer12_attn_scores_dp1_mb6_gpu14 -> layer12_softmax_dp1_mb6_gpu14
	layer12_attn_out_dp0_mb6_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb6_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb6_gpu14 -> layer12_attn_out_dp0_mb6_gpu14
	layer12_softmax_dp1_mb6_gpu14 -> layer12_attn_out_dp1_mb6_gpu14
	layer12_qkv_dp0_mb6_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb6_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb6 -> layer12_qkv_dp0_mb6_gpu15
	layer11_expert_agg_dp1_mb6 -> layer12_qkv_dp1_mb6_gpu15
	layer12_attn_scores_dp0_mb6_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb6_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb6_gpu15 -> layer12_attn_scores_dp0_mb6_gpu15
	layer12_qkv_dp1_mb6_gpu15 -> layer12_attn_scores_dp1_mb6_gpu15
	layer12_softmax_dp0_mb6_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb6_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb6_gpu15 -> layer12_softmax_dp0_mb6_gpu15
	layer12_attn_scores_dp1_mb6_gpu15 -> layer12_softmax_dp1_mb6_gpu15
	layer12_attn_out_dp0_mb6_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb6_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb6_gpu15 -> layer12_attn_out_dp0_mb6_gpu15
	layer12_softmax_dp1_mb6_gpu15 -> layer12_attn_out_dp1_mb6_gpu15
	layer12_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb6_gpu12 -> layer12_attn_allreduce_dp0_mb6
	layer12_attn_out_dp1_mb6_gpu12 -> layer12_attn_allreduce_dp1_mb6
	layer12_attn_out_dp0_mb6_gpu13 -> layer12_attn_allreduce_dp0_mb6
	layer12_attn_out_dp1_mb6_gpu13 -> layer12_attn_allreduce_dp1_mb6
	layer12_attn_out_dp0_mb6_gpu14 -> layer12_attn_allreduce_dp0_mb6
	layer12_attn_out_dp1_mb6_gpu14 -> layer12_attn_allreduce_dp1_mb6
	layer12_attn_out_dp0_mb6_gpu15 -> layer12_attn_allreduce_dp0_mb6
	layer12_attn_out_dp1_mb6_gpu15 -> layer12_attn_allreduce_dp1_mb6
	layer12_router_dp0_mb6_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb6_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb6 -> layer12_router_dp0_mb6_gpu12
	layer12_attn_allreduce_dp1_mb6 -> layer12_router_dp1_mb6_gpu12
	layer12_expert0_mlp1_dp0_mb6_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb6_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb6_gpu12 -> layer12_expert0_mlp1_dp0_mb6_gpu12
	layer12_router_dp1_mb6_gpu12 -> layer12_expert0_mlp1_dp1_mb6_gpu12
	layer12_expert0_mlp2_dp0_mb6_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb6_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb6_gpu12 -> layer12_expert0_mlp2_dp0_mb6_gpu12
	layer12_expert0_mlp1_dp1_mb6_gpu12 -> layer12_expert0_mlp2_dp1_mb6_gpu12
	layer12_expert1_mlp1_dp0_mb6_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb6_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb6_gpu12 -> layer12_expert1_mlp1_dp0_mb6_gpu13
	layer12_router_dp1_mb6_gpu12 -> layer12_expert1_mlp1_dp1_mb6_gpu13
	layer12_expert1_mlp2_dp0_mb6_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb6_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb6_gpu13 -> layer12_expert1_mlp2_dp0_mb6_gpu13
	layer12_expert1_mlp1_dp1_mb6_gpu13 -> layer12_expert1_mlp2_dp1_mb6_gpu13
	layer12_expert2_mlp1_dp0_mb6_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb6_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb6_gpu12 -> layer12_expert2_mlp1_dp0_mb6_gpu14
	layer12_router_dp1_mb6_gpu12 -> layer12_expert2_mlp1_dp1_mb6_gpu14
	layer12_expert2_mlp2_dp0_mb6_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb6_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb6_gpu14 -> layer12_expert2_mlp2_dp0_mb6_gpu14
	layer12_expert2_mlp1_dp1_mb6_gpu14 -> layer12_expert2_mlp2_dp1_mb6_gpu14
	layer12_expert3_mlp1_dp0_mb6_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb6_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb6_gpu12 -> layer12_expert3_mlp1_dp0_mb6_gpu15
	layer12_router_dp1_mb6_gpu12 -> layer12_expert3_mlp1_dp1_mb6_gpu15
	layer12_expert3_mlp2_dp0_mb6_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb6_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb6_gpu15 -> layer12_expert3_mlp2_dp0_mb6_gpu15
	layer12_expert3_mlp1_dp1_mb6_gpu15 -> layer12_expert3_mlp2_dp1_mb6_gpu15
	layer12_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb6_gpu12 -> layer12_expert_agg_dp0_mb6
	layer12_expert0_mlp2_dp1_mb6_gpu12 -> layer12_expert_agg_dp1_mb6
	layer12_expert1_mlp2_dp0_mb6_gpu13 -> layer12_expert_agg_dp0_mb6
	layer12_expert1_mlp2_dp1_mb6_gpu13 -> layer12_expert_agg_dp1_mb6
	layer12_expert2_mlp2_dp0_mb6_gpu14 -> layer12_expert_agg_dp0_mb6
	layer12_expert2_mlp2_dp1_mb6_gpu14 -> layer12_expert_agg_dp1_mb6
	layer12_expert3_mlp2_dp0_mb6_gpu15 -> layer12_expert_agg_dp0_mb6
	layer12_expert3_mlp2_dp1_mb6_gpu15 -> layer12_expert_agg_dp1_mb6
	layer13_qkv_dp0_mb6_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb6_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb6 -> layer13_qkv_dp0_mb6_gpu12
	layer12_expert_agg_dp1_mb6 -> layer13_qkv_dp1_mb6_gpu12
	layer13_attn_scores_dp0_mb6_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb6_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb6_gpu12 -> layer13_attn_scores_dp0_mb6_gpu12
	layer13_qkv_dp1_mb6_gpu12 -> layer13_attn_scores_dp1_mb6_gpu12
	layer13_softmax_dp0_mb6_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb6_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb6_gpu12 -> layer13_softmax_dp0_mb6_gpu12
	layer13_attn_scores_dp1_mb6_gpu12 -> layer13_softmax_dp1_mb6_gpu12
	layer13_attn_out_dp0_mb6_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb6_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb6_gpu12 -> layer13_attn_out_dp0_mb6_gpu12
	layer13_softmax_dp1_mb6_gpu12 -> layer13_attn_out_dp1_mb6_gpu12
	layer13_qkv_dp0_mb6_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb6_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb6 -> layer13_qkv_dp0_mb6_gpu13
	layer12_expert_agg_dp1_mb6 -> layer13_qkv_dp1_mb6_gpu13
	layer13_attn_scores_dp0_mb6_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb6_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb6_gpu13 -> layer13_attn_scores_dp0_mb6_gpu13
	layer13_qkv_dp1_mb6_gpu13 -> layer13_attn_scores_dp1_mb6_gpu13
	layer13_softmax_dp0_mb6_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb6_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb6_gpu13 -> layer13_softmax_dp0_mb6_gpu13
	layer13_attn_scores_dp1_mb6_gpu13 -> layer13_softmax_dp1_mb6_gpu13
	layer13_attn_out_dp0_mb6_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb6_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb6_gpu13 -> layer13_attn_out_dp0_mb6_gpu13
	layer13_softmax_dp1_mb6_gpu13 -> layer13_attn_out_dp1_mb6_gpu13
	layer13_qkv_dp0_mb6_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb6_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb6 -> layer13_qkv_dp0_mb6_gpu14
	layer12_expert_agg_dp1_mb6 -> layer13_qkv_dp1_mb6_gpu14
	layer13_attn_scores_dp0_mb6_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb6_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb6_gpu14 -> layer13_attn_scores_dp0_mb6_gpu14
	layer13_qkv_dp1_mb6_gpu14 -> layer13_attn_scores_dp1_mb6_gpu14
	layer13_softmax_dp0_mb6_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb6_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb6_gpu14 -> layer13_softmax_dp0_mb6_gpu14
	layer13_attn_scores_dp1_mb6_gpu14 -> layer13_softmax_dp1_mb6_gpu14
	layer13_attn_out_dp0_mb6_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb6_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb6_gpu14 -> layer13_attn_out_dp0_mb6_gpu14
	layer13_softmax_dp1_mb6_gpu14 -> layer13_attn_out_dp1_mb6_gpu14
	layer13_qkv_dp0_mb6_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb6_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb6 -> layer13_qkv_dp0_mb6_gpu15
	layer12_expert_agg_dp1_mb6 -> layer13_qkv_dp1_mb6_gpu15
	layer13_attn_scores_dp0_mb6_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb6_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb6_gpu15 -> layer13_attn_scores_dp0_mb6_gpu15
	layer13_qkv_dp1_mb6_gpu15 -> layer13_attn_scores_dp1_mb6_gpu15
	layer13_softmax_dp0_mb6_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb6_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb6_gpu15 -> layer13_softmax_dp0_mb6_gpu15
	layer13_attn_scores_dp1_mb6_gpu15 -> layer13_softmax_dp1_mb6_gpu15
	layer13_attn_out_dp0_mb6_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb6_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb6_gpu15 -> layer13_attn_out_dp0_mb6_gpu15
	layer13_softmax_dp1_mb6_gpu15 -> layer13_attn_out_dp1_mb6_gpu15
	layer13_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb6_gpu12 -> layer13_attn_allreduce_dp0_mb6
	layer13_attn_out_dp1_mb6_gpu12 -> layer13_attn_allreduce_dp1_mb6
	layer13_attn_out_dp0_mb6_gpu13 -> layer13_attn_allreduce_dp0_mb6
	layer13_attn_out_dp1_mb6_gpu13 -> layer13_attn_allreduce_dp1_mb6
	layer13_attn_out_dp0_mb6_gpu14 -> layer13_attn_allreduce_dp0_mb6
	layer13_attn_out_dp1_mb6_gpu14 -> layer13_attn_allreduce_dp1_mb6
	layer13_attn_out_dp0_mb6_gpu15 -> layer13_attn_allreduce_dp0_mb6
	layer13_attn_out_dp1_mb6_gpu15 -> layer13_attn_allreduce_dp1_mb6
	layer13_router_dp0_mb6_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb6_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb6 -> layer13_router_dp0_mb6_gpu12
	layer13_attn_allreduce_dp1_mb6 -> layer13_router_dp1_mb6_gpu12
	layer13_expert0_mlp1_dp0_mb6_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb6_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb6_gpu12 -> layer13_expert0_mlp1_dp0_mb6_gpu12
	layer13_router_dp1_mb6_gpu12 -> layer13_expert0_mlp1_dp1_mb6_gpu12
	layer13_expert0_mlp2_dp0_mb6_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb6_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb6_gpu12 -> layer13_expert0_mlp2_dp0_mb6_gpu12
	layer13_expert0_mlp1_dp1_mb6_gpu12 -> layer13_expert0_mlp2_dp1_mb6_gpu12
	layer13_expert1_mlp1_dp0_mb6_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb6_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb6_gpu12 -> layer13_expert1_mlp1_dp0_mb6_gpu13
	layer13_router_dp1_mb6_gpu12 -> layer13_expert1_mlp1_dp1_mb6_gpu13
	layer13_expert1_mlp2_dp0_mb6_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb6_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb6_gpu13 -> layer13_expert1_mlp2_dp0_mb6_gpu13
	layer13_expert1_mlp1_dp1_mb6_gpu13 -> layer13_expert1_mlp2_dp1_mb6_gpu13
	layer13_expert2_mlp1_dp0_mb6_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb6_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb6_gpu12 -> layer13_expert2_mlp1_dp0_mb6_gpu14
	layer13_router_dp1_mb6_gpu12 -> layer13_expert2_mlp1_dp1_mb6_gpu14
	layer13_expert2_mlp2_dp0_mb6_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb6_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb6_gpu14 -> layer13_expert2_mlp2_dp0_mb6_gpu14
	layer13_expert2_mlp1_dp1_mb6_gpu14 -> layer13_expert2_mlp2_dp1_mb6_gpu14
	layer13_expert3_mlp1_dp0_mb6_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb6_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb6_gpu12 -> layer13_expert3_mlp1_dp0_mb6_gpu15
	layer13_router_dp1_mb6_gpu12 -> layer13_expert3_mlp1_dp1_mb6_gpu15
	layer13_expert3_mlp2_dp0_mb6_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb6_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb6_gpu15 -> layer13_expert3_mlp2_dp0_mb6_gpu15
	layer13_expert3_mlp1_dp1_mb6_gpu15 -> layer13_expert3_mlp2_dp1_mb6_gpu15
	layer13_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb6_gpu12 -> layer13_expert_agg_dp0_mb6
	layer13_expert0_mlp2_dp1_mb6_gpu12 -> layer13_expert_agg_dp1_mb6
	layer13_expert1_mlp2_dp0_mb6_gpu13 -> layer13_expert_agg_dp0_mb6
	layer13_expert1_mlp2_dp1_mb6_gpu13 -> layer13_expert_agg_dp1_mb6
	layer13_expert2_mlp2_dp0_mb6_gpu14 -> layer13_expert_agg_dp0_mb6
	layer13_expert2_mlp2_dp1_mb6_gpu14 -> layer13_expert_agg_dp1_mb6
	layer13_expert3_mlp2_dp0_mb6_gpu15 -> layer13_expert_agg_dp0_mb6
	layer13_expert3_mlp2_dp1_mb6_gpu15 -> layer13_expert_agg_dp1_mb6
	layer14_qkv_dp0_mb6_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb6_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb6 -> layer14_qkv_dp0_mb6_gpu12
	layer13_expert_agg_dp1_mb6 -> layer14_qkv_dp1_mb6_gpu12
	layer14_attn_scores_dp0_mb6_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb6_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb6_gpu12 -> layer14_attn_scores_dp0_mb6_gpu12
	layer14_qkv_dp1_mb6_gpu12 -> layer14_attn_scores_dp1_mb6_gpu12
	layer14_softmax_dp0_mb6_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb6_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb6_gpu12 -> layer14_softmax_dp0_mb6_gpu12
	layer14_attn_scores_dp1_mb6_gpu12 -> layer14_softmax_dp1_mb6_gpu12
	layer14_attn_out_dp0_mb6_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb6_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb6_gpu12 -> layer14_attn_out_dp0_mb6_gpu12
	layer14_softmax_dp1_mb6_gpu12 -> layer14_attn_out_dp1_mb6_gpu12
	layer14_qkv_dp0_mb6_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb6_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb6 -> layer14_qkv_dp0_mb6_gpu13
	layer13_expert_agg_dp1_mb6 -> layer14_qkv_dp1_mb6_gpu13
	layer14_attn_scores_dp0_mb6_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb6_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb6_gpu13 -> layer14_attn_scores_dp0_mb6_gpu13
	layer14_qkv_dp1_mb6_gpu13 -> layer14_attn_scores_dp1_mb6_gpu13
	layer14_softmax_dp0_mb6_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb6_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb6_gpu13 -> layer14_softmax_dp0_mb6_gpu13
	layer14_attn_scores_dp1_mb6_gpu13 -> layer14_softmax_dp1_mb6_gpu13
	layer14_attn_out_dp0_mb6_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb6_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb6_gpu13 -> layer14_attn_out_dp0_mb6_gpu13
	layer14_softmax_dp1_mb6_gpu13 -> layer14_attn_out_dp1_mb6_gpu13
	layer14_qkv_dp0_mb6_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb6_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb6 -> layer14_qkv_dp0_mb6_gpu14
	layer13_expert_agg_dp1_mb6 -> layer14_qkv_dp1_mb6_gpu14
	layer14_attn_scores_dp0_mb6_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb6_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb6_gpu14 -> layer14_attn_scores_dp0_mb6_gpu14
	layer14_qkv_dp1_mb6_gpu14 -> layer14_attn_scores_dp1_mb6_gpu14
	layer14_softmax_dp0_mb6_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb6_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb6_gpu14 -> layer14_softmax_dp0_mb6_gpu14
	layer14_attn_scores_dp1_mb6_gpu14 -> layer14_softmax_dp1_mb6_gpu14
	layer14_attn_out_dp0_mb6_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb6_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb6_gpu14 -> layer14_attn_out_dp0_mb6_gpu14
	layer14_softmax_dp1_mb6_gpu14 -> layer14_attn_out_dp1_mb6_gpu14
	layer14_qkv_dp0_mb6_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb6_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb6 -> layer14_qkv_dp0_mb6_gpu15
	layer13_expert_agg_dp1_mb6 -> layer14_qkv_dp1_mb6_gpu15
	layer14_attn_scores_dp0_mb6_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb6_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb6_gpu15 -> layer14_attn_scores_dp0_mb6_gpu15
	layer14_qkv_dp1_mb6_gpu15 -> layer14_attn_scores_dp1_mb6_gpu15
	layer14_softmax_dp0_mb6_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb6_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb6_gpu15 -> layer14_softmax_dp0_mb6_gpu15
	layer14_attn_scores_dp1_mb6_gpu15 -> layer14_softmax_dp1_mb6_gpu15
	layer14_attn_out_dp0_mb6_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb6_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb6_gpu15 -> layer14_attn_out_dp0_mb6_gpu15
	layer14_softmax_dp1_mb6_gpu15 -> layer14_attn_out_dp1_mb6_gpu15
	layer14_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb6_gpu12 -> layer14_attn_allreduce_dp0_mb6
	layer14_attn_out_dp1_mb6_gpu12 -> layer14_attn_allreduce_dp1_mb6
	layer14_attn_out_dp0_mb6_gpu13 -> layer14_attn_allreduce_dp0_mb6
	layer14_attn_out_dp1_mb6_gpu13 -> layer14_attn_allreduce_dp1_mb6
	layer14_attn_out_dp0_mb6_gpu14 -> layer14_attn_allreduce_dp0_mb6
	layer14_attn_out_dp1_mb6_gpu14 -> layer14_attn_allreduce_dp1_mb6
	layer14_attn_out_dp0_mb6_gpu15 -> layer14_attn_allreduce_dp0_mb6
	layer14_attn_out_dp1_mb6_gpu15 -> layer14_attn_allreduce_dp1_mb6
	layer14_router_dp0_mb6_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb6_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb6 -> layer14_router_dp0_mb6_gpu12
	layer14_attn_allreduce_dp1_mb6 -> layer14_router_dp1_mb6_gpu12
	layer14_expert0_mlp1_dp0_mb6_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb6_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb6_gpu12 -> layer14_expert0_mlp1_dp0_mb6_gpu12
	layer14_router_dp1_mb6_gpu12 -> layer14_expert0_mlp1_dp1_mb6_gpu12
	layer14_expert0_mlp2_dp0_mb6_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb6_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb6_gpu12 -> layer14_expert0_mlp2_dp0_mb6_gpu12
	layer14_expert0_mlp1_dp1_mb6_gpu12 -> layer14_expert0_mlp2_dp1_mb6_gpu12
	layer14_expert1_mlp1_dp0_mb6_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb6_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb6_gpu12 -> layer14_expert1_mlp1_dp0_mb6_gpu13
	layer14_router_dp1_mb6_gpu12 -> layer14_expert1_mlp1_dp1_mb6_gpu13
	layer14_expert1_mlp2_dp0_mb6_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb6_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb6_gpu13 -> layer14_expert1_mlp2_dp0_mb6_gpu13
	layer14_expert1_mlp1_dp1_mb6_gpu13 -> layer14_expert1_mlp2_dp1_mb6_gpu13
	layer14_expert2_mlp1_dp0_mb6_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb6_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb6_gpu12 -> layer14_expert2_mlp1_dp0_mb6_gpu14
	layer14_router_dp1_mb6_gpu12 -> layer14_expert2_mlp1_dp1_mb6_gpu14
	layer14_expert2_mlp2_dp0_mb6_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb6_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb6_gpu14 -> layer14_expert2_mlp2_dp0_mb6_gpu14
	layer14_expert2_mlp1_dp1_mb6_gpu14 -> layer14_expert2_mlp2_dp1_mb6_gpu14
	layer14_expert3_mlp1_dp0_mb6_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb6_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb6_gpu12 -> layer14_expert3_mlp1_dp0_mb6_gpu15
	layer14_router_dp1_mb6_gpu12 -> layer14_expert3_mlp1_dp1_mb6_gpu15
	layer14_expert3_mlp2_dp0_mb6_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb6_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb6_gpu15 -> layer14_expert3_mlp2_dp0_mb6_gpu15
	layer14_expert3_mlp1_dp1_mb6_gpu15 -> layer14_expert3_mlp2_dp1_mb6_gpu15
	layer14_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb6_gpu12 -> layer14_expert_agg_dp0_mb6
	layer14_expert0_mlp2_dp1_mb6_gpu12 -> layer14_expert_agg_dp1_mb6
	layer14_expert1_mlp2_dp0_mb6_gpu13 -> layer14_expert_agg_dp0_mb6
	layer14_expert1_mlp2_dp1_mb6_gpu13 -> layer14_expert_agg_dp1_mb6
	layer14_expert2_mlp2_dp0_mb6_gpu14 -> layer14_expert_agg_dp0_mb6
	layer14_expert2_mlp2_dp1_mb6_gpu14 -> layer14_expert_agg_dp1_mb6
	layer14_expert3_mlp2_dp0_mb6_gpu15 -> layer14_expert_agg_dp0_mb6
	layer14_expert3_mlp2_dp1_mb6_gpu15 -> layer14_expert_agg_dp1_mb6
	layer15_qkv_dp0_mb6_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb6_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb6 -> layer15_qkv_dp0_mb6_gpu12
	layer14_expert_agg_dp1_mb6 -> layer15_qkv_dp1_mb6_gpu12
	layer15_attn_scores_dp0_mb6_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb6_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb6_gpu12 -> layer15_attn_scores_dp0_mb6_gpu12
	layer15_qkv_dp1_mb6_gpu12 -> layer15_attn_scores_dp1_mb6_gpu12
	layer15_softmax_dp0_mb6_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb6_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb6_gpu12 -> layer15_softmax_dp0_mb6_gpu12
	layer15_attn_scores_dp1_mb6_gpu12 -> layer15_softmax_dp1_mb6_gpu12
	layer15_attn_out_dp0_mb6_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb6_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb6_gpu12 -> layer15_attn_out_dp0_mb6_gpu12
	layer15_softmax_dp1_mb6_gpu12 -> layer15_attn_out_dp1_mb6_gpu12
	layer15_qkv_dp0_mb6_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb6_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb6 -> layer15_qkv_dp0_mb6_gpu13
	layer14_expert_agg_dp1_mb6 -> layer15_qkv_dp1_mb6_gpu13
	layer15_attn_scores_dp0_mb6_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb6_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb6_gpu13 -> layer15_attn_scores_dp0_mb6_gpu13
	layer15_qkv_dp1_mb6_gpu13 -> layer15_attn_scores_dp1_mb6_gpu13
	layer15_softmax_dp0_mb6_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb6_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb6_gpu13 -> layer15_softmax_dp0_mb6_gpu13
	layer15_attn_scores_dp1_mb6_gpu13 -> layer15_softmax_dp1_mb6_gpu13
	layer15_attn_out_dp0_mb6_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb6_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb6_gpu13 -> layer15_attn_out_dp0_mb6_gpu13
	layer15_softmax_dp1_mb6_gpu13 -> layer15_attn_out_dp1_mb6_gpu13
	layer15_qkv_dp0_mb6_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb6_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb6 -> layer15_qkv_dp0_mb6_gpu14
	layer14_expert_agg_dp1_mb6 -> layer15_qkv_dp1_mb6_gpu14
	layer15_attn_scores_dp0_mb6_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb6_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb6_gpu14 -> layer15_attn_scores_dp0_mb6_gpu14
	layer15_qkv_dp1_mb6_gpu14 -> layer15_attn_scores_dp1_mb6_gpu14
	layer15_softmax_dp0_mb6_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb6_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb6_gpu14 -> layer15_softmax_dp0_mb6_gpu14
	layer15_attn_scores_dp1_mb6_gpu14 -> layer15_softmax_dp1_mb6_gpu14
	layer15_attn_out_dp0_mb6_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb6_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb6_gpu14 -> layer15_attn_out_dp0_mb6_gpu14
	layer15_softmax_dp1_mb6_gpu14 -> layer15_attn_out_dp1_mb6_gpu14
	layer15_qkv_dp0_mb6_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb6_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb6 -> layer15_qkv_dp0_mb6_gpu15
	layer14_expert_agg_dp1_mb6 -> layer15_qkv_dp1_mb6_gpu15
	layer15_attn_scores_dp0_mb6_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb6_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb6_gpu15 -> layer15_attn_scores_dp0_mb6_gpu15
	layer15_qkv_dp1_mb6_gpu15 -> layer15_attn_scores_dp1_mb6_gpu15
	layer15_softmax_dp0_mb6_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb6_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb6_gpu15 -> layer15_softmax_dp0_mb6_gpu15
	layer15_attn_scores_dp1_mb6_gpu15 -> layer15_softmax_dp1_mb6_gpu15
	layer15_attn_out_dp0_mb6_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb6_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb6_gpu15 -> layer15_attn_out_dp0_mb6_gpu15
	layer15_softmax_dp1_mb6_gpu15 -> layer15_attn_out_dp1_mb6_gpu15
	layer15_attn_allreduce_dp0_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb6 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb6_gpu12 -> layer15_attn_allreduce_dp0_mb6
	layer15_attn_out_dp1_mb6_gpu12 -> layer15_attn_allreduce_dp1_mb6
	layer15_attn_out_dp0_mb6_gpu13 -> layer15_attn_allreduce_dp0_mb6
	layer15_attn_out_dp1_mb6_gpu13 -> layer15_attn_allreduce_dp1_mb6
	layer15_attn_out_dp0_mb6_gpu14 -> layer15_attn_allreduce_dp0_mb6
	layer15_attn_out_dp1_mb6_gpu14 -> layer15_attn_allreduce_dp1_mb6
	layer15_attn_out_dp0_mb6_gpu15 -> layer15_attn_allreduce_dp0_mb6
	layer15_attn_out_dp1_mb6_gpu15 -> layer15_attn_allreduce_dp1_mb6
	layer15_router_dp0_mb6_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb6_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb6 -> layer15_router_dp0_mb6_gpu12
	layer15_attn_allreduce_dp1_mb6 -> layer15_router_dp1_mb6_gpu12
	layer15_expert0_mlp1_dp0_mb6_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb6_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb6_gpu12 -> layer15_expert0_mlp1_dp0_mb6_gpu12
	layer15_router_dp1_mb6_gpu12 -> layer15_expert0_mlp1_dp1_mb6_gpu12
	layer15_expert0_mlp2_dp0_mb6_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb6_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb6_gpu12 -> layer15_expert0_mlp2_dp0_mb6_gpu12
	layer15_expert0_mlp1_dp1_mb6_gpu12 -> layer15_expert0_mlp2_dp1_mb6_gpu12
	layer15_expert1_mlp1_dp0_mb6_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb6_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb6_gpu12 -> layer15_expert1_mlp1_dp0_mb6_gpu13
	layer15_router_dp1_mb6_gpu12 -> layer15_expert1_mlp1_dp1_mb6_gpu13
	layer15_expert1_mlp2_dp0_mb6_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb6_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb6_gpu13 -> layer15_expert1_mlp2_dp0_mb6_gpu13
	layer15_expert1_mlp1_dp1_mb6_gpu13 -> layer15_expert1_mlp2_dp1_mb6_gpu13
	layer15_expert2_mlp1_dp0_mb6_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb6_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb6_gpu12 -> layer15_expert2_mlp1_dp0_mb6_gpu14
	layer15_router_dp1_mb6_gpu12 -> layer15_expert2_mlp1_dp1_mb6_gpu14
	layer15_expert2_mlp2_dp0_mb6_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb6_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb6_gpu14 -> layer15_expert2_mlp2_dp0_mb6_gpu14
	layer15_expert2_mlp1_dp1_mb6_gpu14 -> layer15_expert2_mlp2_dp1_mb6_gpu14
	layer15_expert3_mlp1_dp0_mb6_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb6_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb6_gpu12 -> layer15_expert3_mlp1_dp0_mb6_gpu15
	layer15_router_dp1_mb6_gpu12 -> layer15_expert3_mlp1_dp1_mb6_gpu15
	layer15_expert3_mlp2_dp0_mb6_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb6_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb6_gpu15 -> layer15_expert3_mlp2_dp0_mb6_gpu15
	layer15_expert3_mlp1_dp1_mb6_gpu15 -> layer15_expert3_mlp2_dp1_mb6_gpu15
	layer15_expert_agg_dp0_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb6 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb6_gpu12 -> layer15_expert_agg_dp0_mb6
	layer15_expert0_mlp2_dp1_mb6_gpu12 -> layer15_expert_agg_dp1_mb6
	layer15_expert1_mlp2_dp0_mb6_gpu13 -> layer15_expert_agg_dp0_mb6
	layer15_expert1_mlp2_dp1_mb6_gpu13 -> layer15_expert_agg_dp1_mb6
	layer15_expert2_mlp2_dp0_mb6_gpu14 -> layer15_expert_agg_dp0_mb6
	layer15_expert2_mlp2_dp1_mb6_gpu14 -> layer15_expert_agg_dp1_mb6
	layer15_expert3_mlp2_dp0_mb6_gpu15 -> layer15_expert_agg_dp0_mb6
	layer15_expert3_mlp2_dp1_mb6_gpu15 -> layer15_expert_agg_dp1_mb6
	layer12_qkv_dp0_mb7_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb7_gpu12 [label="Layer 12 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb7 -> layer12_qkv_dp0_mb7_gpu12
	layer11_expert_agg_dp1_mb7 -> layer12_qkv_dp1_mb7_gpu12
	layer12_attn_scores_dp0_mb7_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb7_gpu12 [label="Layer 12 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb7_gpu12 -> layer12_attn_scores_dp0_mb7_gpu12
	layer12_qkv_dp1_mb7_gpu12 -> layer12_attn_scores_dp1_mb7_gpu12
	layer12_softmax_dp0_mb7_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb7_gpu12 [label="Layer 12 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb7_gpu12 -> layer12_softmax_dp0_mb7_gpu12
	layer12_attn_scores_dp1_mb7_gpu12 -> layer12_softmax_dp1_mb7_gpu12
	layer12_attn_out_dp0_mb7_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb7_gpu12 [label="Layer 12 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb7_gpu12 -> layer12_attn_out_dp0_mb7_gpu12
	layer12_softmax_dp1_mb7_gpu12 -> layer12_attn_out_dp1_mb7_gpu12
	layer12_qkv_dp0_mb7_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb7_gpu13 [label="Layer 12 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb7 -> layer12_qkv_dp0_mb7_gpu13
	layer11_expert_agg_dp1_mb7 -> layer12_qkv_dp1_mb7_gpu13
	layer12_attn_scores_dp0_mb7_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb7_gpu13 [label="Layer 12 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb7_gpu13 -> layer12_attn_scores_dp0_mb7_gpu13
	layer12_qkv_dp1_mb7_gpu13 -> layer12_attn_scores_dp1_mb7_gpu13
	layer12_softmax_dp0_mb7_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb7_gpu13 [label="Layer 12 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb7_gpu13 -> layer12_softmax_dp0_mb7_gpu13
	layer12_attn_scores_dp1_mb7_gpu13 -> layer12_softmax_dp1_mb7_gpu13
	layer12_attn_out_dp0_mb7_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb7_gpu13 [label="Layer 12 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb7_gpu13 -> layer12_attn_out_dp0_mb7_gpu13
	layer12_softmax_dp1_mb7_gpu13 -> layer12_attn_out_dp1_mb7_gpu13
	layer12_qkv_dp0_mb7_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb7_gpu14 [label="Layer 12 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb7 -> layer12_qkv_dp0_mb7_gpu14
	layer11_expert_agg_dp1_mb7 -> layer12_qkv_dp1_mb7_gpu14
	layer12_attn_scores_dp0_mb7_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb7_gpu14 [label="Layer 12 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb7_gpu14 -> layer12_attn_scores_dp0_mb7_gpu14
	layer12_qkv_dp1_mb7_gpu14 -> layer12_attn_scores_dp1_mb7_gpu14
	layer12_softmax_dp0_mb7_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb7_gpu14 [label="Layer 12 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb7_gpu14 -> layer12_softmax_dp0_mb7_gpu14
	layer12_attn_scores_dp1_mb7_gpu14 -> layer12_softmax_dp1_mb7_gpu14
	layer12_attn_out_dp0_mb7_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb7_gpu14 [label="Layer 12 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb7_gpu14 -> layer12_attn_out_dp0_mb7_gpu14
	layer12_softmax_dp1_mb7_gpu14 -> layer12_attn_out_dp1_mb7_gpu14
	layer12_qkv_dp0_mb7_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp1_mb7_gpu15 [label="Layer 12 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer11_expert_agg_dp0_mb7 -> layer12_qkv_dp0_mb7_gpu15
	layer11_expert_agg_dp1_mb7 -> layer12_qkv_dp1_mb7_gpu15
	layer12_attn_scores_dp0_mb7_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp1_mb7_gpu15 [label="Layer 12 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_qkv_dp0_mb7_gpu15 -> layer12_attn_scores_dp0_mb7_gpu15
	layer12_qkv_dp1_mb7_gpu15 -> layer12_attn_scores_dp1_mb7_gpu15
	layer12_softmax_dp0_mb7_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp1_mb7_gpu15 [label="Layer 12 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_scores_dp0_mb7_gpu15 -> layer12_softmax_dp0_mb7_gpu15
	layer12_attn_scores_dp1_mb7_gpu15 -> layer12_softmax_dp1_mb7_gpu15
	layer12_attn_out_dp0_mb7_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_attn_out_dp1_mb7_gpu15 [label="Layer 12 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_softmax_dp0_mb7_gpu15 -> layer12_attn_out_dp0_mb7_gpu15
	layer12_softmax_dp1_mb7_gpu15 -> layer12_attn_out_dp1_mb7_gpu15
	layer12_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_attn_out_dp0_mb7_gpu12 -> layer12_attn_allreduce_dp0_mb7
	layer12_attn_out_dp1_mb7_gpu12 -> layer12_attn_allreduce_dp1_mb7
	layer12_attn_out_dp0_mb7_gpu13 -> layer12_attn_allreduce_dp0_mb7
	layer12_attn_out_dp1_mb7_gpu13 -> layer12_attn_allreduce_dp1_mb7
	layer12_attn_out_dp0_mb7_gpu14 -> layer12_attn_allreduce_dp0_mb7
	layer12_attn_out_dp1_mb7_gpu14 -> layer12_attn_allreduce_dp1_mb7
	layer12_attn_out_dp0_mb7_gpu15 -> layer12_attn_allreduce_dp0_mb7
	layer12_attn_out_dp1_mb7_gpu15 -> layer12_attn_allreduce_dp1_mb7
	layer12_router_dp0_mb7_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_router_dp1_mb7_gpu12 [label="Layer 12 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer12_attn_allreduce_dp0_mb7 -> layer12_router_dp0_mb7_gpu12
	layer12_attn_allreduce_dp1_mb7 -> layer12_router_dp1_mb7_gpu12
	layer12_expert0_mlp1_dp0_mb7_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp1_mb7_gpu12 [label="Layer 12 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb7_gpu12 -> layer12_expert0_mlp1_dp0_mb7_gpu12
	layer12_router_dp1_mb7_gpu12 -> layer12_expert0_mlp1_dp1_mb7_gpu12
	layer12_expert0_mlp2_dp0_mb7_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp2_dp1_mb7_gpu12 [label="Layer 12 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert0_mlp1_dp0_mb7_gpu12 -> layer12_expert0_mlp2_dp0_mb7_gpu12
	layer12_expert0_mlp1_dp1_mb7_gpu12 -> layer12_expert0_mlp2_dp1_mb7_gpu12
	layer12_expert1_mlp1_dp0_mb7_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp1_mb7_gpu13 [label="Layer 12 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb7_gpu12 -> layer12_expert1_mlp1_dp0_mb7_gpu13
	layer12_router_dp1_mb7_gpu12 -> layer12_expert1_mlp1_dp1_mb7_gpu13
	layer12_expert1_mlp2_dp0_mb7_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp2_dp1_mb7_gpu13 [label="Layer 12 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert1_mlp1_dp0_mb7_gpu13 -> layer12_expert1_mlp2_dp0_mb7_gpu13
	layer12_expert1_mlp1_dp1_mb7_gpu13 -> layer12_expert1_mlp2_dp1_mb7_gpu13
	layer12_expert2_mlp1_dp0_mb7_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp1_mb7_gpu14 [label="Layer 12 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb7_gpu12 -> layer12_expert2_mlp1_dp0_mb7_gpu14
	layer12_router_dp1_mb7_gpu12 -> layer12_expert2_mlp1_dp1_mb7_gpu14
	layer12_expert2_mlp2_dp0_mb7_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp2_dp1_mb7_gpu14 [label="Layer 12 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert2_mlp1_dp0_mb7_gpu14 -> layer12_expert2_mlp2_dp0_mb7_gpu14
	layer12_expert2_mlp1_dp1_mb7_gpu14 -> layer12_expert2_mlp2_dp1_mb7_gpu14
	layer12_expert3_mlp1_dp0_mb7_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp1_mb7_gpu15 [label="Layer 12 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_router_dp0_mb7_gpu12 -> layer12_expert3_mlp1_dp0_mb7_gpu15
	layer12_router_dp1_mb7_gpu12 -> layer12_expert3_mlp1_dp1_mb7_gpu15
	layer12_expert3_mlp2_dp0_mb7_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp2_dp1_mb7_gpu15 [label="Layer 12 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert3_mlp1_dp0_mb7_gpu15 -> layer12_expert3_mlp2_dp0_mb7_gpu15
	layer12_expert3_mlp1_dp1_mb7_gpu15 -> layer12_expert3_mlp2_dp1_mb7_gpu15
	layer12_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer12_expert0_mlp2_dp0_mb7_gpu12 -> layer12_expert_agg_dp0_mb7
	layer12_expert0_mlp2_dp1_mb7_gpu12 -> layer12_expert_agg_dp1_mb7
	layer12_expert1_mlp2_dp0_mb7_gpu13 -> layer12_expert_agg_dp0_mb7
	layer12_expert1_mlp2_dp1_mb7_gpu13 -> layer12_expert_agg_dp1_mb7
	layer12_expert2_mlp2_dp0_mb7_gpu14 -> layer12_expert_agg_dp0_mb7
	layer12_expert2_mlp2_dp1_mb7_gpu14 -> layer12_expert_agg_dp1_mb7
	layer12_expert3_mlp2_dp0_mb7_gpu15 -> layer12_expert_agg_dp0_mb7
	layer12_expert3_mlp2_dp1_mb7_gpu15 -> layer12_expert_agg_dp1_mb7
	layer13_qkv_dp0_mb7_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb7_gpu12 [label="Layer 13 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb7 -> layer13_qkv_dp0_mb7_gpu12
	layer12_expert_agg_dp1_mb7 -> layer13_qkv_dp1_mb7_gpu12
	layer13_attn_scores_dp0_mb7_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb7_gpu12 [label="Layer 13 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb7_gpu12 -> layer13_attn_scores_dp0_mb7_gpu12
	layer13_qkv_dp1_mb7_gpu12 -> layer13_attn_scores_dp1_mb7_gpu12
	layer13_softmax_dp0_mb7_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb7_gpu12 [label="Layer 13 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb7_gpu12 -> layer13_softmax_dp0_mb7_gpu12
	layer13_attn_scores_dp1_mb7_gpu12 -> layer13_softmax_dp1_mb7_gpu12
	layer13_attn_out_dp0_mb7_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb7_gpu12 [label="Layer 13 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb7_gpu12 -> layer13_attn_out_dp0_mb7_gpu12
	layer13_softmax_dp1_mb7_gpu12 -> layer13_attn_out_dp1_mb7_gpu12
	layer13_qkv_dp0_mb7_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb7_gpu13 [label="Layer 13 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb7 -> layer13_qkv_dp0_mb7_gpu13
	layer12_expert_agg_dp1_mb7 -> layer13_qkv_dp1_mb7_gpu13
	layer13_attn_scores_dp0_mb7_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb7_gpu13 [label="Layer 13 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb7_gpu13 -> layer13_attn_scores_dp0_mb7_gpu13
	layer13_qkv_dp1_mb7_gpu13 -> layer13_attn_scores_dp1_mb7_gpu13
	layer13_softmax_dp0_mb7_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb7_gpu13 [label="Layer 13 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb7_gpu13 -> layer13_softmax_dp0_mb7_gpu13
	layer13_attn_scores_dp1_mb7_gpu13 -> layer13_softmax_dp1_mb7_gpu13
	layer13_attn_out_dp0_mb7_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb7_gpu13 [label="Layer 13 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb7_gpu13 -> layer13_attn_out_dp0_mb7_gpu13
	layer13_softmax_dp1_mb7_gpu13 -> layer13_attn_out_dp1_mb7_gpu13
	layer13_qkv_dp0_mb7_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb7_gpu14 [label="Layer 13 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb7 -> layer13_qkv_dp0_mb7_gpu14
	layer12_expert_agg_dp1_mb7 -> layer13_qkv_dp1_mb7_gpu14
	layer13_attn_scores_dp0_mb7_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb7_gpu14 [label="Layer 13 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb7_gpu14 -> layer13_attn_scores_dp0_mb7_gpu14
	layer13_qkv_dp1_mb7_gpu14 -> layer13_attn_scores_dp1_mb7_gpu14
	layer13_softmax_dp0_mb7_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb7_gpu14 [label="Layer 13 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb7_gpu14 -> layer13_softmax_dp0_mb7_gpu14
	layer13_attn_scores_dp1_mb7_gpu14 -> layer13_softmax_dp1_mb7_gpu14
	layer13_attn_out_dp0_mb7_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb7_gpu14 [label="Layer 13 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb7_gpu14 -> layer13_attn_out_dp0_mb7_gpu14
	layer13_softmax_dp1_mb7_gpu14 -> layer13_attn_out_dp1_mb7_gpu14
	layer13_qkv_dp0_mb7_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp1_mb7_gpu15 [label="Layer 13 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer12_expert_agg_dp0_mb7 -> layer13_qkv_dp0_mb7_gpu15
	layer12_expert_agg_dp1_mb7 -> layer13_qkv_dp1_mb7_gpu15
	layer13_attn_scores_dp0_mb7_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp1_mb7_gpu15 [label="Layer 13 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_qkv_dp0_mb7_gpu15 -> layer13_attn_scores_dp0_mb7_gpu15
	layer13_qkv_dp1_mb7_gpu15 -> layer13_attn_scores_dp1_mb7_gpu15
	layer13_softmax_dp0_mb7_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp1_mb7_gpu15 [label="Layer 13 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_scores_dp0_mb7_gpu15 -> layer13_softmax_dp0_mb7_gpu15
	layer13_attn_scores_dp1_mb7_gpu15 -> layer13_softmax_dp1_mb7_gpu15
	layer13_attn_out_dp0_mb7_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_attn_out_dp1_mb7_gpu15 [label="Layer 13 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_softmax_dp0_mb7_gpu15 -> layer13_attn_out_dp0_mb7_gpu15
	layer13_softmax_dp1_mb7_gpu15 -> layer13_attn_out_dp1_mb7_gpu15
	layer13_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_attn_out_dp0_mb7_gpu12 -> layer13_attn_allreduce_dp0_mb7
	layer13_attn_out_dp1_mb7_gpu12 -> layer13_attn_allreduce_dp1_mb7
	layer13_attn_out_dp0_mb7_gpu13 -> layer13_attn_allreduce_dp0_mb7
	layer13_attn_out_dp1_mb7_gpu13 -> layer13_attn_allreduce_dp1_mb7
	layer13_attn_out_dp0_mb7_gpu14 -> layer13_attn_allreduce_dp0_mb7
	layer13_attn_out_dp1_mb7_gpu14 -> layer13_attn_allreduce_dp1_mb7
	layer13_attn_out_dp0_mb7_gpu15 -> layer13_attn_allreduce_dp0_mb7
	layer13_attn_out_dp1_mb7_gpu15 -> layer13_attn_allreduce_dp1_mb7
	layer13_router_dp0_mb7_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_router_dp1_mb7_gpu12 [label="Layer 13 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer13_attn_allreduce_dp0_mb7 -> layer13_router_dp0_mb7_gpu12
	layer13_attn_allreduce_dp1_mb7 -> layer13_router_dp1_mb7_gpu12
	layer13_expert0_mlp1_dp0_mb7_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp1_mb7_gpu12 [label="Layer 13 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb7_gpu12 -> layer13_expert0_mlp1_dp0_mb7_gpu12
	layer13_router_dp1_mb7_gpu12 -> layer13_expert0_mlp1_dp1_mb7_gpu12
	layer13_expert0_mlp2_dp0_mb7_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp2_dp1_mb7_gpu12 [label="Layer 13 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert0_mlp1_dp0_mb7_gpu12 -> layer13_expert0_mlp2_dp0_mb7_gpu12
	layer13_expert0_mlp1_dp1_mb7_gpu12 -> layer13_expert0_mlp2_dp1_mb7_gpu12
	layer13_expert1_mlp1_dp0_mb7_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp1_mb7_gpu13 [label="Layer 13 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb7_gpu12 -> layer13_expert1_mlp1_dp0_mb7_gpu13
	layer13_router_dp1_mb7_gpu12 -> layer13_expert1_mlp1_dp1_mb7_gpu13
	layer13_expert1_mlp2_dp0_mb7_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp2_dp1_mb7_gpu13 [label="Layer 13 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert1_mlp1_dp0_mb7_gpu13 -> layer13_expert1_mlp2_dp0_mb7_gpu13
	layer13_expert1_mlp1_dp1_mb7_gpu13 -> layer13_expert1_mlp2_dp1_mb7_gpu13
	layer13_expert2_mlp1_dp0_mb7_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp1_mb7_gpu14 [label="Layer 13 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb7_gpu12 -> layer13_expert2_mlp1_dp0_mb7_gpu14
	layer13_router_dp1_mb7_gpu12 -> layer13_expert2_mlp1_dp1_mb7_gpu14
	layer13_expert2_mlp2_dp0_mb7_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp2_dp1_mb7_gpu14 [label="Layer 13 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert2_mlp1_dp0_mb7_gpu14 -> layer13_expert2_mlp2_dp0_mb7_gpu14
	layer13_expert2_mlp1_dp1_mb7_gpu14 -> layer13_expert2_mlp2_dp1_mb7_gpu14
	layer13_expert3_mlp1_dp0_mb7_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp1_mb7_gpu15 [label="Layer 13 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_router_dp0_mb7_gpu12 -> layer13_expert3_mlp1_dp0_mb7_gpu15
	layer13_router_dp1_mb7_gpu12 -> layer13_expert3_mlp1_dp1_mb7_gpu15
	layer13_expert3_mlp2_dp0_mb7_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp2_dp1_mb7_gpu15 [label="Layer 13 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert3_mlp1_dp0_mb7_gpu15 -> layer13_expert3_mlp2_dp0_mb7_gpu15
	layer13_expert3_mlp1_dp1_mb7_gpu15 -> layer13_expert3_mlp2_dp1_mb7_gpu15
	layer13_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer13_expert0_mlp2_dp0_mb7_gpu12 -> layer13_expert_agg_dp0_mb7
	layer13_expert0_mlp2_dp1_mb7_gpu12 -> layer13_expert_agg_dp1_mb7
	layer13_expert1_mlp2_dp0_mb7_gpu13 -> layer13_expert_agg_dp0_mb7
	layer13_expert1_mlp2_dp1_mb7_gpu13 -> layer13_expert_agg_dp1_mb7
	layer13_expert2_mlp2_dp0_mb7_gpu14 -> layer13_expert_agg_dp0_mb7
	layer13_expert2_mlp2_dp1_mb7_gpu14 -> layer13_expert_agg_dp1_mb7
	layer13_expert3_mlp2_dp0_mb7_gpu15 -> layer13_expert_agg_dp0_mb7
	layer13_expert3_mlp2_dp1_mb7_gpu15 -> layer13_expert_agg_dp1_mb7
	layer14_qkv_dp0_mb7_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb7_gpu12 [label="Layer 14 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb7 -> layer14_qkv_dp0_mb7_gpu12
	layer13_expert_agg_dp1_mb7 -> layer14_qkv_dp1_mb7_gpu12
	layer14_attn_scores_dp0_mb7_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb7_gpu12 [label="Layer 14 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb7_gpu12 -> layer14_attn_scores_dp0_mb7_gpu12
	layer14_qkv_dp1_mb7_gpu12 -> layer14_attn_scores_dp1_mb7_gpu12
	layer14_softmax_dp0_mb7_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb7_gpu12 [label="Layer 14 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb7_gpu12 -> layer14_softmax_dp0_mb7_gpu12
	layer14_attn_scores_dp1_mb7_gpu12 -> layer14_softmax_dp1_mb7_gpu12
	layer14_attn_out_dp0_mb7_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb7_gpu12 [label="Layer 14 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb7_gpu12 -> layer14_attn_out_dp0_mb7_gpu12
	layer14_softmax_dp1_mb7_gpu12 -> layer14_attn_out_dp1_mb7_gpu12
	layer14_qkv_dp0_mb7_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb7_gpu13 [label="Layer 14 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb7 -> layer14_qkv_dp0_mb7_gpu13
	layer13_expert_agg_dp1_mb7 -> layer14_qkv_dp1_mb7_gpu13
	layer14_attn_scores_dp0_mb7_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb7_gpu13 [label="Layer 14 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb7_gpu13 -> layer14_attn_scores_dp0_mb7_gpu13
	layer14_qkv_dp1_mb7_gpu13 -> layer14_attn_scores_dp1_mb7_gpu13
	layer14_softmax_dp0_mb7_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb7_gpu13 [label="Layer 14 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb7_gpu13 -> layer14_softmax_dp0_mb7_gpu13
	layer14_attn_scores_dp1_mb7_gpu13 -> layer14_softmax_dp1_mb7_gpu13
	layer14_attn_out_dp0_mb7_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb7_gpu13 [label="Layer 14 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb7_gpu13 -> layer14_attn_out_dp0_mb7_gpu13
	layer14_softmax_dp1_mb7_gpu13 -> layer14_attn_out_dp1_mb7_gpu13
	layer14_qkv_dp0_mb7_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb7_gpu14 [label="Layer 14 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb7 -> layer14_qkv_dp0_mb7_gpu14
	layer13_expert_agg_dp1_mb7 -> layer14_qkv_dp1_mb7_gpu14
	layer14_attn_scores_dp0_mb7_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb7_gpu14 [label="Layer 14 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb7_gpu14 -> layer14_attn_scores_dp0_mb7_gpu14
	layer14_qkv_dp1_mb7_gpu14 -> layer14_attn_scores_dp1_mb7_gpu14
	layer14_softmax_dp0_mb7_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb7_gpu14 [label="Layer 14 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb7_gpu14 -> layer14_softmax_dp0_mb7_gpu14
	layer14_attn_scores_dp1_mb7_gpu14 -> layer14_softmax_dp1_mb7_gpu14
	layer14_attn_out_dp0_mb7_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb7_gpu14 [label="Layer 14 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb7_gpu14 -> layer14_attn_out_dp0_mb7_gpu14
	layer14_softmax_dp1_mb7_gpu14 -> layer14_attn_out_dp1_mb7_gpu14
	layer14_qkv_dp0_mb7_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp1_mb7_gpu15 [label="Layer 14 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer13_expert_agg_dp0_mb7 -> layer14_qkv_dp0_mb7_gpu15
	layer13_expert_agg_dp1_mb7 -> layer14_qkv_dp1_mb7_gpu15
	layer14_attn_scores_dp0_mb7_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp1_mb7_gpu15 [label="Layer 14 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_qkv_dp0_mb7_gpu15 -> layer14_attn_scores_dp0_mb7_gpu15
	layer14_qkv_dp1_mb7_gpu15 -> layer14_attn_scores_dp1_mb7_gpu15
	layer14_softmax_dp0_mb7_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp1_mb7_gpu15 [label="Layer 14 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_scores_dp0_mb7_gpu15 -> layer14_softmax_dp0_mb7_gpu15
	layer14_attn_scores_dp1_mb7_gpu15 -> layer14_softmax_dp1_mb7_gpu15
	layer14_attn_out_dp0_mb7_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_attn_out_dp1_mb7_gpu15 [label="Layer 14 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_softmax_dp0_mb7_gpu15 -> layer14_attn_out_dp0_mb7_gpu15
	layer14_softmax_dp1_mb7_gpu15 -> layer14_attn_out_dp1_mb7_gpu15
	layer14_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_attn_out_dp0_mb7_gpu12 -> layer14_attn_allreduce_dp0_mb7
	layer14_attn_out_dp1_mb7_gpu12 -> layer14_attn_allreduce_dp1_mb7
	layer14_attn_out_dp0_mb7_gpu13 -> layer14_attn_allreduce_dp0_mb7
	layer14_attn_out_dp1_mb7_gpu13 -> layer14_attn_allreduce_dp1_mb7
	layer14_attn_out_dp0_mb7_gpu14 -> layer14_attn_allreduce_dp0_mb7
	layer14_attn_out_dp1_mb7_gpu14 -> layer14_attn_allreduce_dp1_mb7
	layer14_attn_out_dp0_mb7_gpu15 -> layer14_attn_allreduce_dp0_mb7
	layer14_attn_out_dp1_mb7_gpu15 -> layer14_attn_allreduce_dp1_mb7
	layer14_router_dp0_mb7_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_router_dp1_mb7_gpu12 [label="Layer 14 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer14_attn_allreduce_dp0_mb7 -> layer14_router_dp0_mb7_gpu12
	layer14_attn_allreduce_dp1_mb7 -> layer14_router_dp1_mb7_gpu12
	layer14_expert0_mlp1_dp0_mb7_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp1_mb7_gpu12 [label="Layer 14 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb7_gpu12 -> layer14_expert0_mlp1_dp0_mb7_gpu12
	layer14_router_dp1_mb7_gpu12 -> layer14_expert0_mlp1_dp1_mb7_gpu12
	layer14_expert0_mlp2_dp0_mb7_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp2_dp1_mb7_gpu12 [label="Layer 14 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert0_mlp1_dp0_mb7_gpu12 -> layer14_expert0_mlp2_dp0_mb7_gpu12
	layer14_expert0_mlp1_dp1_mb7_gpu12 -> layer14_expert0_mlp2_dp1_mb7_gpu12
	layer14_expert1_mlp1_dp0_mb7_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp1_mb7_gpu13 [label="Layer 14 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb7_gpu12 -> layer14_expert1_mlp1_dp0_mb7_gpu13
	layer14_router_dp1_mb7_gpu12 -> layer14_expert1_mlp1_dp1_mb7_gpu13
	layer14_expert1_mlp2_dp0_mb7_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp2_dp1_mb7_gpu13 [label="Layer 14 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert1_mlp1_dp0_mb7_gpu13 -> layer14_expert1_mlp2_dp0_mb7_gpu13
	layer14_expert1_mlp1_dp1_mb7_gpu13 -> layer14_expert1_mlp2_dp1_mb7_gpu13
	layer14_expert2_mlp1_dp0_mb7_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp1_mb7_gpu14 [label="Layer 14 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb7_gpu12 -> layer14_expert2_mlp1_dp0_mb7_gpu14
	layer14_router_dp1_mb7_gpu12 -> layer14_expert2_mlp1_dp1_mb7_gpu14
	layer14_expert2_mlp2_dp0_mb7_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp2_dp1_mb7_gpu14 [label="Layer 14 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert2_mlp1_dp0_mb7_gpu14 -> layer14_expert2_mlp2_dp0_mb7_gpu14
	layer14_expert2_mlp1_dp1_mb7_gpu14 -> layer14_expert2_mlp2_dp1_mb7_gpu14
	layer14_expert3_mlp1_dp0_mb7_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp1_mb7_gpu15 [label="Layer 14 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_router_dp0_mb7_gpu12 -> layer14_expert3_mlp1_dp0_mb7_gpu15
	layer14_router_dp1_mb7_gpu12 -> layer14_expert3_mlp1_dp1_mb7_gpu15
	layer14_expert3_mlp2_dp0_mb7_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp2_dp1_mb7_gpu15 [label="Layer 14 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert3_mlp1_dp0_mb7_gpu15 -> layer14_expert3_mlp2_dp0_mb7_gpu15
	layer14_expert3_mlp1_dp1_mb7_gpu15 -> layer14_expert3_mlp2_dp1_mb7_gpu15
	layer14_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer14_expert0_mlp2_dp0_mb7_gpu12 -> layer14_expert_agg_dp0_mb7
	layer14_expert0_mlp2_dp1_mb7_gpu12 -> layer14_expert_agg_dp1_mb7
	layer14_expert1_mlp2_dp0_mb7_gpu13 -> layer14_expert_agg_dp0_mb7
	layer14_expert1_mlp2_dp1_mb7_gpu13 -> layer14_expert_agg_dp1_mb7
	layer14_expert2_mlp2_dp0_mb7_gpu14 -> layer14_expert_agg_dp0_mb7
	layer14_expert2_mlp2_dp1_mb7_gpu14 -> layer14_expert_agg_dp1_mb7
	layer14_expert3_mlp2_dp0_mb7_gpu15 -> layer14_expert_agg_dp0_mb7
	layer14_expert3_mlp2_dp1_mb7_gpu15 -> layer14_expert_agg_dp1_mb7
	layer15_qkv_dp0_mb7_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb7_gpu12 [label="Layer 15 QKV Projection\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb7 -> layer15_qkv_dp0_mb7_gpu12
	layer14_expert_agg_dp1_mb7 -> layer15_qkv_dp1_mb7_gpu12
	layer15_attn_scores_dp0_mb7_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb7_gpu12 [label="Layer 15 Attention Scores\nGPU 12\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb7_gpu12 -> layer15_attn_scores_dp0_mb7_gpu12
	layer15_qkv_dp1_mb7_gpu12 -> layer15_attn_scores_dp1_mb7_gpu12
	layer15_softmax_dp0_mb7_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb7_gpu12 [label="Layer 15 Attention Softmax\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb7_gpu12 -> layer15_softmax_dp0_mb7_gpu12
	layer15_attn_scores_dp1_mb7_gpu12 -> layer15_softmax_dp1_mb7_gpu12
	layer15_attn_out_dp0_mb7_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb7_gpu12 [label="Layer 15 Attention Output\nGPU 12\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb7_gpu12 -> layer15_attn_out_dp0_mb7_gpu12
	layer15_softmax_dp1_mb7_gpu12 -> layer15_attn_out_dp1_mb7_gpu12
	layer15_qkv_dp0_mb7_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb7_gpu13 [label="Layer 15 QKV Projection\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb7 -> layer15_qkv_dp0_mb7_gpu13
	layer14_expert_agg_dp1_mb7 -> layer15_qkv_dp1_mb7_gpu13
	layer15_attn_scores_dp0_mb7_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb7_gpu13 [label="Layer 15 Attention Scores\nGPU 13\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb7_gpu13 -> layer15_attn_scores_dp0_mb7_gpu13
	layer15_qkv_dp1_mb7_gpu13 -> layer15_attn_scores_dp1_mb7_gpu13
	layer15_softmax_dp0_mb7_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb7_gpu13 [label="Layer 15 Attention Softmax\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb7_gpu13 -> layer15_softmax_dp0_mb7_gpu13
	layer15_attn_scores_dp1_mb7_gpu13 -> layer15_softmax_dp1_mb7_gpu13
	layer15_attn_out_dp0_mb7_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb7_gpu13 [label="Layer 15 Attention Output\nGPU 13\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb7_gpu13 -> layer15_attn_out_dp0_mb7_gpu13
	layer15_softmax_dp1_mb7_gpu13 -> layer15_attn_out_dp1_mb7_gpu13
	layer15_qkv_dp0_mb7_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb7_gpu14 [label="Layer 15 QKV Projection\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb7 -> layer15_qkv_dp0_mb7_gpu14
	layer14_expert_agg_dp1_mb7 -> layer15_qkv_dp1_mb7_gpu14
	layer15_attn_scores_dp0_mb7_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb7_gpu14 [label="Layer 15 Attention Scores\nGPU 14\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb7_gpu14 -> layer15_attn_scores_dp0_mb7_gpu14
	layer15_qkv_dp1_mb7_gpu14 -> layer15_attn_scores_dp1_mb7_gpu14
	layer15_softmax_dp0_mb7_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb7_gpu14 [label="Layer 15 Attention Softmax\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb7_gpu14 -> layer15_softmax_dp0_mb7_gpu14
	layer15_attn_scores_dp1_mb7_gpu14 -> layer15_softmax_dp1_mb7_gpu14
	layer15_attn_out_dp0_mb7_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb7_gpu14 [label="Layer 15 Attention Output\nGPU 14\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb7_gpu14 -> layer15_attn_out_dp0_mb7_gpu14
	layer15_softmax_dp1_mb7_gpu14 -> layer15_attn_out_dp1_mb7_gpu14
	layer15_qkv_dp0_mb7_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp1_mb7_gpu15 [label="Layer 15 QKV Projection\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, heads=4, d_k=64]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer14_expert_agg_dp0_mb7 -> layer15_qkv_dp0_mb7_gpu15
	layer14_expert_agg_dp1_mb7 -> layer15_qkv_dp1_mb7_gpu15
	layer15_attn_scores_dp0_mb7_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp1_mb7_gpu15 [label="Layer 15 Attention Scores\nGPU 15\nInput: [batch=8, seq=1024, heads=4, d_k=64]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_qkv_dp0_mb7_gpu15 -> layer15_attn_scores_dp0_mb7_gpu15
	layer15_qkv_dp1_mb7_gpu15 -> layer15_attn_scores_dp1_mb7_gpu15
	layer15_softmax_dp0_mb7_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp1_mb7_gpu15 [label="Layer 15 Attention Softmax\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, heads=4, seq=1024, seq=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_scores_dp0_mb7_gpu15 -> layer15_softmax_dp0_mb7_gpu15
	layer15_attn_scores_dp1_mb7_gpu15 -> layer15_softmax_dp1_mb7_gpu15
	layer15_attn_out_dp0_mb7_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_attn_out_dp1_mb7_gpu15 [label="Layer 15 Attention Output\nGPU 15\nInput: [batch=8, heads=4, seq=1024, seq=1024]\nOutput: [batch=8, seq=1024, hidden=256]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_softmax_dp0_mb7_gpu15 -> layer15_attn_out_dp0_mb7_gpu15
	layer15_softmax_dp1_mb7_gpu15 -> layer15_attn_out_dp1_mb7_gpu15
	layer15_attn_allreduce_dp0_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_allreduce_dp1_mb7 [label="All-Reduce Attention Output\nInput: 4×[batch=8, seq=1024, hidden=256]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_attn_out_dp0_mb7_gpu12 -> layer15_attn_allreduce_dp0_mb7
	layer15_attn_out_dp1_mb7_gpu12 -> layer15_attn_allreduce_dp1_mb7
	layer15_attn_out_dp0_mb7_gpu13 -> layer15_attn_allreduce_dp0_mb7
	layer15_attn_out_dp1_mb7_gpu13 -> layer15_attn_allreduce_dp1_mb7
	layer15_attn_out_dp0_mb7_gpu14 -> layer15_attn_allreduce_dp0_mb7
	layer15_attn_out_dp1_mb7_gpu14 -> layer15_attn_allreduce_dp1_mb7
	layer15_attn_out_dp0_mb7_gpu15 -> layer15_attn_allreduce_dp0_mb7
	layer15_attn_out_dp1_mb7_gpu15 -> layer15_attn_allreduce_dp1_mb7
	layer15_router_dp0_mb7_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_router_dp1_mb7_gpu12 [label="Layer 15 Expert Router\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, top_k=1]" fillcolor="#FFCCCB" fontsize=9 shape=parallelogram style=filled]
	layer15_attn_allreduce_dp0_mb7 -> layer15_router_dp0_mb7_gpu12
	layer15_attn_allreduce_dp1_mb7 -> layer15_router_dp1_mb7_gpu12
	layer15_expert0_mlp1_dp0_mb7_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp1_mb7_gpu12 [label="Layer 15 Expert 0 MLP1\nGPU 12\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb7_gpu12 -> layer15_expert0_mlp1_dp0_mb7_gpu12
	layer15_router_dp1_mb7_gpu12 -> layer15_expert0_mlp1_dp1_mb7_gpu12
	layer15_expert0_mlp2_dp0_mb7_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp2_dp1_mb7_gpu12 [label="Layer 15 Expert 0 MLP2\nGPU 12\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert0_mlp1_dp0_mb7_gpu12 -> layer15_expert0_mlp2_dp0_mb7_gpu12
	layer15_expert0_mlp1_dp1_mb7_gpu12 -> layer15_expert0_mlp2_dp1_mb7_gpu12
	layer15_expert1_mlp1_dp0_mb7_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp1_mb7_gpu13 [label="Layer 15 Expert 1 MLP1\nGPU 13\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb7_gpu12 -> layer15_expert1_mlp1_dp0_mb7_gpu13
	layer15_router_dp1_mb7_gpu12 -> layer15_expert1_mlp1_dp1_mb7_gpu13
	layer15_expert1_mlp2_dp0_mb7_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp2_dp1_mb7_gpu13 [label="Layer 15 Expert 1 MLP2\nGPU 13\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert1_mlp1_dp0_mb7_gpu13 -> layer15_expert1_mlp2_dp0_mb7_gpu13
	layer15_expert1_mlp1_dp1_mb7_gpu13 -> layer15_expert1_mlp2_dp1_mb7_gpu13
	layer15_expert2_mlp1_dp0_mb7_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp1_mb7_gpu14 [label="Layer 15 Expert 2 MLP1\nGPU 14\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb7_gpu12 -> layer15_expert2_mlp1_dp0_mb7_gpu14
	layer15_router_dp1_mb7_gpu12 -> layer15_expert2_mlp1_dp1_mb7_gpu14
	layer15_expert2_mlp2_dp0_mb7_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp2_dp1_mb7_gpu14 [label="Layer 15 Expert 2 MLP2\nGPU 14\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert2_mlp1_dp0_mb7_gpu14 -> layer15_expert2_mlp2_dp0_mb7_gpu14
	layer15_expert2_mlp1_dp1_mb7_gpu14 -> layer15_expert2_mlp2_dp1_mb7_gpu14
	layer15_expert3_mlp1_dp0_mb7_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp1_mb7_gpu15 [label="Layer 15 Expert 3 MLP1\nGPU 15\nInput: [batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, ffn=2048]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_router_dp0_mb7_gpu12 -> layer15_expert3_mlp1_dp0_mb7_gpu15
	layer15_router_dp1_mb7_gpu12 -> layer15_expert3_mlp1_dp1_mb7_gpu15
	layer15_expert3_mlp2_dp0_mb7_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp2_dp1_mb7_gpu15 [label="Layer 15 Expert 3 MLP2\nGPU 15\nInput: [batch=8, seq=1024, ffn=2048]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#F3E5FF" fontsize=9 style=filled]
	layer15_expert3_mlp1_dp0_mb7_gpu15 -> layer15_expert3_mlp2_dp0_mb7_gpu15
	layer15_expert3_mlp1_dp1_mb7_gpu15 -> layer15_expert3_mlp2_dp1_mb7_gpu15
	layer15_expert_agg_dp0_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp1_mb7 [label="All-to-All Expert Aggregation\nInput: 4×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=8, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert0_mlp2_dp0_mb7_gpu12 -> layer15_expert_agg_dp0_mb7
	layer15_expert0_mlp2_dp1_mb7_gpu12 -> layer15_expert_agg_dp1_mb7
	layer15_expert1_mlp2_dp0_mb7_gpu13 -> layer15_expert_agg_dp0_mb7
	layer15_expert1_mlp2_dp1_mb7_gpu13 -> layer15_expert_agg_dp1_mb7
	layer15_expert2_mlp2_dp0_mb7_gpu14 -> layer15_expert_agg_dp0_mb7
	layer15_expert2_mlp2_dp1_mb7_gpu14 -> layer15_expert_agg_dp1_mb7
	layer15_expert3_mlp2_dp0_mb7_gpu15 -> layer15_expert_agg_dp0_mb7
	layer15_expert3_mlp2_dp1_mb7_gpu15 -> layer15_expert_agg_dp1_mb7
	final_agg_dp0 [label="Final Output Aggregation\nInput: 8×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=64, seq=1024, hidden=1024]" fillcolor="#DDA0DD" fontsize=9 shape=parallelogram style=filled]
	final_agg_dp1 [label="Final Output Aggregation\nInput: 8×[batch=8, seq=1024, hidden=1024]\nOutput: [batch=64, seq=1024, hidden=1024]" fillcolor="#DDA0DD" fontsize=9 shape=parallelogram style=filled]
	grad_sync [label="Async All-Reduce Gradients\nInput: 2×[batch=64, seq=1024, hidden=1024]\nOutput: [batch=128, seq=1024, hidden=1024]" fillcolor="#FFD700" fontsize=9 shape=ellipse style=filled]
	layer15_expert_agg_dp0_mb0 -> final_agg_dp0
	layer15_expert_agg_dp1_mb0 -> final_agg_dp1
	layer15_expert_agg_dp0_mb1 -> final_agg_dp0
	layer15_expert_agg_dp1_mb1 -> final_agg_dp1
	layer15_expert_agg_dp0_mb2 -> final_agg_dp0
	layer15_expert_agg_dp1_mb2 -> final_agg_dp1
	layer15_expert_agg_dp0_mb3 -> final_agg_dp0
	layer15_expert_agg_dp1_mb3 -> final_agg_dp1
	layer15_expert_agg_dp0_mb4 -> final_agg_dp0
	layer15_expert_agg_dp1_mb4 -> final_agg_dp1
	layer15_expert_agg_dp0_mb5 -> final_agg_dp0
	layer15_expert_agg_dp1_mb5 -> final_agg_dp1
	layer15_expert_agg_dp0_mb6 -> final_agg_dp0
	layer15_expert_agg_dp1_mb6 -> final_agg_dp1
	layer15_expert_agg_dp0_mb7 -> final_agg_dp0
	layer15_expert_agg_dp1_mb7 -> final_agg_dp1
	final_agg_dp0 -> grad_sync
	final_agg_dp1 -> grad_sync
	output [label="Final Output" fillcolor=white fontname=Arial fontsize=9 shape=ellipse style=filled]
	node [shape=rectangle]
	grad_sync -> output
}
