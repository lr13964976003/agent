digraph {
	graph [bb="0,0,79910,1218.3",
		bgcolor=white,
		rankdir=TB,
		splines=ortho
	];
	node [fillcolor=lightblue,
		label="\N",
		shape=rectangle,
		style=filled
	];
	input	[fillcolor=white,
		height=1.041,
		label="Input\n[batch_size=128, seq_len=1024, hidden=1024]\nGPU: ALL",
		pos="2232,1150.3",
		shape=ellipse,
		width=6.9925];
	dp_split	[fillcolor=lightpink,
		height=1.4722,
		label="Data Parallel Split\n[batch_size=64, seq_len=1024, hidden=1024]\nGPU: Routing",
		pos="2232,963.33",
		shape=parallelogram,
		width=9.9931];
	input -> dp_split	[pos="e,2232,1016.4 2232,1112.5 2232,1112.5 2232,1026.4 2232,1026.4"];
	layer0_attn_qkv_gpu0	[height=1.1528,
		label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="1890,802.83",
		width=2.875];
	dp_split -> layer0_attn_qkv_gpu0	[pos="e,1932.9,844.37 1932.9,909.89 1932.9,909.89 1932.9,854.37 1932.9,854.37"];
	layer0_attn_qkv_gpu1	[height=1.1528,
		label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="2117,802.83",
		width=2.875];
	dp_split -> layer0_attn_qkv_gpu1	[pos="e,2117,844.37 2117,909.89 2117,909.89 2117,854.37 2117,854.37"];
	layer0_attn_qkv_gpu2	[height=1.1528,
		label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="2344,802.83",
		width=2.875];
	dp_split -> layer0_attn_qkv_gpu2	[pos="e,2344,844.37 2344,909.89 2344,909.89 2344,854.37 2344,854.37"];
	layer0_attn_qkv_gpu3	[height=1.1528,
		label="Layer0 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="2573,802.83",
		width=2.875];
	dp_split -> layer0_attn_qkv_gpu3	[pos="e,2530.6,844.7 2530.6,963.22 2530.6,963.22 2530.6,854.7 2530.6,854.7"];
	layer0_attn_score_gpu0	[height=0.94444,
		label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0",
		pos="1886,691.33",
		width=2.9444];
	layer0_attn_qkv_gpu0 -> layer0_attn_score_gpu0	[pos="e,1889.2,725.6 1889.2,761.31 1889.2,761.31 1889.2,735.6 1889.2,735.6"];
	layer0_attn_score_gpu1	[height=0.94444,
		label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1",
		pos="2116,691.33",
		width=2.9444];
	layer0_attn_qkv_gpu1 -> layer0_attn_score_gpu1	[pos="e,2117,725.6 2117,761.31 2117,761.31 2117,735.6 2117,735.6"];
	layer0_attn_score_gpu2	[height=0.94444,
		label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2",
		pos="2346,691.33",
		width=2.9444];
	layer0_attn_qkv_gpu2 -> layer0_attn_score_gpu2	[pos="e,2344,725.6 2344,761.31 2344,761.31 2344,735.6 2344,735.6"];
	layer0_attn_score_gpu3	[height=0.94444,
		label="Layer0 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3",
		pos="2576,691.33",
		width=2.9444];
	layer0_attn_qkv_gpu3 -> layer0_attn_score_gpu3	[pos="e,2573.2,725.6 2573.2,761.31 2573.2,761.31 2573.2,735.6 2573.2,735.6"];
	layer0_attn_out_gpu0	[height=1.1528,
		label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="1901,579.83",
		width=2.6667];
	layer0_attn_score_gpu0 -> layer0_attn_out_gpu0	[pos="e,1898.5,621.55 1898.5,657.27 1898.5,657.27 1898.5,631.55 1898.5,631.55"];
	layer0_attn_out_gpu1	[height=1.1528,
		label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="2121,579.83",
		width=2.6667];
	layer0_attn_score_gpu1 -> layer0_attn_out_gpu1	[pos="e,2121,621.55 2121,657.27 2121,657.27 2121,631.55 2121,631.55"];
	layer0_attn_out_gpu2	[height=1.1528,
		label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="2341,579.83",
		width=2.6667];
	layer0_attn_score_gpu2 -> layer0_attn_out_gpu2	[pos="e,2341,621.55 2341,657.27 2341,657.27 2341,631.55 2341,631.55"];
	layer0_attn_out_gpu3	[height=1.1528,
		label="Layer0 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="2566,579.83",
		width=2.6667];
	layer0_attn_score_gpu3 -> layer0_attn_out_gpu3	[pos="e,2566,621.55 2566,657.27 2566,657.27 2566,631.55 2566,631.55"];
	layer0_attn_allreduce	[fillcolor=lightgray,
		height=1.6303,
		label="Layer0 Attention\nAll-Reduce Sum\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1024]\nGPU: 0,1,2,3",
		pos="2231,443.64",
		shape=ellipse,
		width=3.8105];
	layer0_attn_out_gpu0 -> layer0_attn_allreduce	[pos="e,2101.4,463 1914.5,538.31 1914.5,504.73 1914.5,463 1914.5,463 1914.5,463 2091.4,463 2091.4,463"];
	layer0_attn_out_gpu1 -> layer0_attn_allreduce	[pos="e,2155.4,492.64 2155.4,538.23 2155.4,538.23 2155.4,502.64 2155.4,502.64"];
	layer0_attn_out_gpu2 -> layer0_attn_allreduce	[pos="e,2306.6,492.64 2306.6,538.23 2306.6,538.23 2306.6,502.64 2306.6,502.64"];
	layer0_attn_out_gpu3 -> layer0_attn_allreduce	[pos="e,2360.7,463 2550,538.31 2550,504.73 2550,463 2550,463 2550,463 2370.7,463 2370.7,463"];
	layer0_moe_route_gpu0	[height=0.5,
		pos="1925,330.95",
		width=2.5833];
	layer0_attn_allreduce -> layer0_moe_route_gpu0	[pos="e,2014,349.34 2101.6,424 2055.2,424 2014,424 2014,424 2014,424 2014,359.34 2014,359.34"];
	layer0_moe_route_gpu1	[height=0.5,
		pos="2129,330.95",
		width=2.5833];
	layer0_attn_allreduce -> layer0_moe_route_gpu1	[pos="e,2157.9,349.17 2157.9,393.89 2157.9,393.89 2157.9,359.17 2157.9,359.17"];
	layer0_moe_route_gpu2	[height=0.5,
		pos="2333,330.95",
		width=2.5833];
	layer0_attn_allreduce -> layer0_moe_route_gpu2	[pos="e,2304.1,349.17 2304.1,393.89 2304.1,393.89 2304.1,359.17 2304.1,359.17"];
	layer0_moe_route_gpu3	[height=0.5,
		pos="2537,330.95",
		width=2.5833];
	layer0_attn_allreduce -> layer0_moe_route_gpu3	[pos="e,2448,349.34 2360.4,424 2406.8,424 2448,424 2448,424 2448,424 2448,359.34 2448,359.34"];
	layer0_moe_route	[height=1.8889,
		label="Layer0 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0,1,2,3",
		pos="2690,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer0_expert0	[height=0.94444,
		label="Layer0 Expert 0_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 0",
		pos="103,83",
		width=2.8611];
	layer0_expert1	[height=0.94444,
		label="Layer0 Expert 0_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 1",
		pos="327,83",
		width=2.8611];
	layer0_expert2	[height=0.94444,
		label="Layer0 Expert 0_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 2",
		pos="551,83",
		width=2.8611];
	layer0_expert3	[height=0.94444,
		label="Layer0 Expert 0_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 3",
		pos="775,83",
		width=2.8611];
	layer0_expert4	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer0 Expert 1_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 4",
		pos="999,83",
		width=2.8611];
	layer0_expert5	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer0 Expert 1_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 5",
		pos="1223,83",
		width=2.8611];
	layer0_expert6	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer0 Expert 1_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 6",
		pos="1447,83",
		width=2.8611];
	layer0_expert7	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer0 Expert 1_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 7",
		pos="1671,83",
		width=2.8611];
	layer0_expert8	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer0 Expert 2_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 8",
		pos="1895,83",
		width=2.8611];
	layer0_expert9	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer0 Expert 2_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 9",
		pos="2119,83",
		width=2.8611];
	layer0_expert10	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer0 Expert 2_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 10",
		pos="2343,83",
		width=2.8611];
	layer0_expert11	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer0 Expert 2_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 11",
		pos="2567,83",
		width=2.8611];
	layer0_expert12	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer0 Expert 3_0\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 12",
		pos="2791,83",
		width=2.8611];
	layer0_expert13	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer0 Expert 3_1\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 13",
		pos="3015,83",
		width=2.8611];
	layer0_expert14	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer0 Expert 3_2\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 14",
		pos="3239,83",
		width=2.8611];
	layer0_expert15	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer0 Expert 3_3\nInput: [~70, 1024, 1024]\nOutput: [~70, 1024, 2048]\nGPU: 15",
		pos="3463,83",
		width=2.8611];
	layer0_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer0 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="2231,239.48",
		shape=ellipse,
		width=3.8105];
	layer0_moe_all2all -> layer0_expert0	[pos="e,103,117.28 2124,263 1692.5,263 103,263 103,263 103,263 103,127.28 103,127.28"];
	layer0_moe_all2all -> layer0_expert1	[pos="e,327,117.18 2107.5,256 1685.6,256 327,256 327,256 327,256 327,127.18 327,127.18"];
	layer0_moe_all2all -> layer0_expert2	[pos="e,551,117.4 2098.1,249 1701.1,249 551,249 551,249 551,249 551,127.4 551,127.4"];
	layer0_moe_all2all -> layer0_expert3	[pos="e,775,117.11 2094,242 1731.6,242 775,242 775,242 775,242 775,127.11 775,127.11"];
	layer0_moe_all2all -> layer0_expert4	[pos="e,999,117.34 2094.2,236 1773.2,236 999,236 999,236 999,236 999,127.34 999,127.34"];
	layer0_moe_all2all -> layer0_expert5	[pos="e,1223,117.23 2099.2,229 1826,229 1223,229 1223,229 1223,229 1223,127.23 1223,127.23"];
	layer0_moe_all2all -> layer0_expert6	[pos="e,1447,117 2109.4,222 1889.1,222 1447,222 1447,222 1447,222 1447,127 1447,127"];
	layer0_moe_all2all -> layer0_expert7	[pos="e,1671,117.34 2127,215 1965,215 1671,215 1671,215 1671,215 1671,127.34 1671,127.34"];
	layer0_moe_all2all -> layer0_expert8	[pos="e,1915,117.16 2156.5,208 2063.1,208 1915,208 1915,208 1915,208 1915,127.16 1915,127.16"];
	layer0_moe_all2all -> layer0_expert9	[pos="e,2157.9,117.17 2157.9,207.43 2157.9,207.43 2157.9,127.17 2157.9,127.17"];
	layer0_moe_all2all -> layer0_expert10	[pos="e,2261.4,117.05 2261.4,202.79 2261.4,202.79 2261.4,127.05 2261.4,127.05"];
	layer0_moe_all2all -> layer0_expert11	[pos="e,2547,117.23 2324.5,212 2417.7,212 2547,212 2547,212 2547,212 2547,127.23 2547,127.23"];
	layer0_moe_all2all -> layer0_expert12	[pos="e,2783.3,117.25 2354.3,223 2517.1,223 2783.3,223 2783.3,223 2783.3,223 2783.3,127.25 2783.3,127.25"];
	layer0_moe_all2all -> layer0_expert13	[pos="e,3007.8,117.26 2366.7,234 2589.4,234 3007.8,234 3007.8,234 3007.8,234 3007.8,127.26 3007.8,127.26"];
	layer0_moe_all2all -> layer0_expert14	[pos="e,3235,117.14 2367.2,244 2641.6,244 3235,244 3235,244 3235,244 3235,127.14 3235,127.14"];
	layer0_moe_all2all -> layer0_expert15	[pos="e,3452,117.39 2355.9,255 2667.1,255 3452,255 3452,255 3452,255 3452,127.39 3452,127.39"];
	layer0_moe_agg	[height=2.3056,
		label="Layer0 MoE\nOutput Aggregation\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1024]\nGPU: 0,1,2,3",
		pos="3785,83",
		shape=parallelogram,
		width=5.5869];
	layer0_moe_all2all -> layer0_moe_agg	[pos="e,3661.1,156 2346.8,219.27 2346.8,194.56 2346.8,156 2346.8,156 2346.8,156 3651.1,156 3651.1,156"];
	layer0_moe_all2all -> layer0_moe_agg	[pos="e,3656.1,146 2325.5,212.19 2325.5,184.6 2325.5,146 2325.5,146 2325.5,146 3646.1,146 3646.1,146"];
	layer0_moe_all2all -> layer0_moe_agg	[pos="e,3651,136 2304.1,207.59 2304.1,177.18 2304.1,136 2304.1,136 2304.1,136 3641,136 3641,136"];
	layer0_moe_all2all -> layer0_moe_agg	[pos="e,3646,126 2282.7,204.5 2282.7,171.16 2282.7,126 2282.7,126 2282.7,126 3636,126 3636,126"];
	layer1_attn_qkv_gpu0	[height=1.1528,
		label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="3000,1150.3",
		width=2.875];
	layer1_attn_score_gpu0	[height=0.94444,
		label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0",
		pos="3228,1150.3",
		width=2.9444];
	layer1_attn_out_gpu0	[height=1.1528,
		label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="3448,1150.3",
		width=2.6667];
	layer1_moe_route_gpu0	[height=1.8889,
		label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0",
		pos="3751,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer1_attn_qkv_gpu1	[height=1.1528,
		label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="4061,1150.3",
		width=2.875];
	layer1_attn_score_gpu1	[height=0.94444,
		label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1",
		pos="4289,1150.3",
		width=2.9444];
	layer1_attn_out_gpu1	[height=1.1528,
		label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="4509,1150.3",
		width=2.6667];
	layer1_moe_route_gpu1	[height=1.8889,
		label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 1",
		pos="4812,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer1_attn_qkv_gpu2	[height=1.1528,
		label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="5122,1150.3",
		width=2.875];
	layer1_attn_score_gpu2	[height=0.94444,
		label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2",
		pos="5350,1150.3",
		width=2.9444];
	layer1_attn_out_gpu2	[height=1.1528,
		label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="5570,1150.3",
		width=2.6667];
	layer1_moe_route_gpu2	[height=1.8889,
		label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 2",
		pos="5873,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer1_attn_qkv_gpu3	[height=1.1528,
		label="Layer1 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="6183,1150.3",
		width=2.875];
	layer1_attn_score_gpu3	[height=0.94444,
		label="Layer1 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3",
		pos="6411,1150.3",
		width=2.9444];
	layer1_attn_out_gpu3	[height=1.1528,
		label="Layer1 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="6631,1150.3",
		width=2.6667];
	layer1_moe_route_gpu3	[height=1.8889,
		label="Layer1 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 3",
		pos="6934,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer1_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer1 Attention\nAll-Reduce Sum\nGPU: 0,1,2,3",
		pos="7238,1150.3",
		shape=ellipse,
		width=2.6909];
	layer1_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer1 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="7490,1150.3",
		shape=ellipse,
		width=3.8105];
	layer1_moe_agg	[height=1.4722,
		label="Layer1 MoE\nOutput Aggregation\nGPU: 0,1,2,3",
		pos="7810,1150.3",
		shape=parallelogram,
		width=4.579];
	layer2_attn_qkv_gpu0	[height=1.1528,
		label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="8096,1150.3",
		width=2.875];
	layer2_attn_score_gpu0	[height=0.94444,
		label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0",
		pos="8324,1150.3",
		width=2.9444];
	layer2_attn_out_gpu0	[height=1.1528,
		label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="8544,1150.3",
		width=2.6667];
	layer2_moe_route_gpu0	[height=1.8889,
		label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0",
		pos="8847,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer2_attn_qkv_gpu1	[height=1.1528,
		label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="9157,1150.3",
		width=2.875];
	layer2_attn_score_gpu1	[height=0.94444,
		label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1",
		pos="9385,1150.3",
		width=2.9444];
	layer2_attn_out_gpu1	[height=1.1528,
		label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="9605,1150.3",
		width=2.6667];
	layer2_moe_route_gpu1	[height=1.8889,
		label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 1",
		pos="9908,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer2_attn_qkv_gpu2	[height=1.1528,
		label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="10218,1150.3",
		width=2.875];
	layer2_attn_score_gpu2	[height=0.94444,
		label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2",
		pos="10446,1150.3",
		width=2.9444];
	layer2_attn_out_gpu2	[height=1.1528,
		label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="10666,1150.3",
		width=2.6667];
	layer2_moe_route_gpu2	[height=1.8889,
		label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 2",
		pos="10969,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer2_attn_qkv_gpu3	[height=1.1528,
		label="Layer2 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="11279,1150.3",
		width=2.875];
	layer2_attn_score_gpu3	[height=0.94444,
		label="Layer2 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3",
		pos="11507,1150.3",
		width=2.9444];
	layer2_attn_out_gpu3	[height=1.1528,
		label="Layer2 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="11727,1150.3",
		width=2.6667];
	layer2_moe_route_gpu3	[height=1.8889,
		label="Layer2 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 3",
		pos="12030,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer2_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer2 Attention\nAll-Reduce Sum\nGPU: 0,1,2,3",
		pos="12334,1150.3",
		shape=ellipse,
		width=2.6909];
	layer2_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer2 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="12586,1150.3",
		shape=ellipse,
		width=3.8105];
	layer2_moe_agg	[height=1.4722,
		label="Layer2 MoE\nOutput Aggregation\nGPU: 0,1,2,3",
		pos="12906,1150.3",
		shape=parallelogram,
		width=4.579];
	layer3_attn_qkv_gpu0	[height=1.1528,
		label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="13192,1150.3",
		width=2.875];
	layer3_attn_score_gpu0	[height=0.94444,
		label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 0",
		pos="13420,1150.3",
		width=2.9444];
	layer3_attn_out_gpu0	[height=1.1528,
		label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 0",
		pos="13640,1150.3",
		width=2.6667];
	layer3_moe_route_gpu0	[height=1.8889,
		label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 0",
		pos="13943,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer3_attn_qkv_gpu1	[height=1.1528,
		label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="14253,1150.3",
		width=2.875];
	layer3_attn_score_gpu1	[height=0.94444,
		label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 1",
		pos="14481,1150.3",
		width=2.9444];
	layer3_attn_out_gpu1	[height=1.1528,
		label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 1",
		pos="14701,1150.3",
		width=2.6667];
	layer3_moe_route_gpu1	[height=1.8889,
		label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 1",
		pos="15004,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer3_attn_qkv_gpu2	[height=1.1528,
		label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="15314,1150.3",
		width=2.875];
	layer3_attn_score_gpu2	[height=0.94444,
		label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 2",
		pos="15542,1150.3",
		width=2.9444];
	layer3_attn_out_gpu2	[height=1.1528,
		label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 2",
		pos="15762,1150.3",
		width=2.6667];
	layer3_moe_route_gpu2	[height=1.8889,
		label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 2",
		pos="16065,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer3_attn_qkv_gpu3	[height=1.1528,
		label="Layer3 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="16375,1150.3",
		width=2.875];
	layer3_attn_score_gpu3	[height=0.94444,
		label="Layer3 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 3",
		pos="16603,1150.3",
		width=2.9444];
	layer3_attn_out_gpu3	[height=1.1528,
		label="Layer3 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 3",
		pos="16823,1150.3",
		width=2.6667];
	layer3_moe_route_gpu3	[height=1.8889,
		label="Layer3 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 3",
		pos="17126,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer3_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer3 Attention\nAll-Reduce Sum\nGPU: 0,1,2,3",
		pos="17430,1150.3",
		shape=ellipse,
		width=2.6909];
	layer3_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer3 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="17682,1150.3",
		shape=ellipse,
		width=3.8105];
	layer3_moe_agg	[height=1.4722,
		label="Layer3 MoE\nOutput Aggregation\nGPU: 0,1,2,3",
		pos="18002,1150.3",
		shape=parallelogram,
		width=4.579];
	layer4_attn_qkv_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="18288,1150.3",
		width=2.875];
	layer4_attn_score_gpu4	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4",
		pos="18516,1150.3",
		width=2.9444];
	layer4_attn_out_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="18736,1150.3",
		width=2.6667];
	layer4_moe_route_gpu4	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4",
		pos="19039,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer4_attn_qkv_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="19349,1150.3",
		width=2.875];
	layer4_attn_score_gpu5	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5",
		pos="19577,1150.3",
		width=2.9444];
	layer4_attn_out_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="19797,1150.3",
		width=2.6667];
	layer4_moe_route_gpu5	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5",
		pos="20100,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer4_attn_qkv_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="20410,1150.3",
		width=2.875];
	layer4_attn_score_gpu6	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6",
		pos="20638,1150.3",
		width=2.9444];
	layer4_attn_out_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="20858,1150.3",
		width=2.6667];
	layer4_moe_route_gpu6	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6",
		pos="21161,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer4_attn_qkv_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="21471,1150.3",
		width=2.875];
	layer4_attn_score_gpu7	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer4 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7",
		pos="21699,1150.3",
		width=2.9444];
	layer4_attn_out_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer4 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="21919,1150.3",
		width=2.6667];
	layer4_moe_route_gpu7	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer4 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7",
		pos="22222,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer4_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer4 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7",
		pos="22526,1150.3",
		shape=ellipse,
		width=2.6909];
	layer4_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer4 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="22778,1150.3",
		shape=ellipse,
		width=3.8105];
	layer4_moe_agg	[fillcolor=lightgreen,
		height=1.4722,
		label="Layer4 MoE\nOutput Aggregation\nGPU: 4,5,6,7",
		pos="23098,1150.3",
		shape=parallelogram,
		width=4.579];
	layer5_attn_qkv_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="23384,1150.3",
		width=2.875];
	layer5_attn_score_gpu4	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4",
		pos="23612,1150.3",
		width=2.9444];
	layer5_attn_out_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="23832,1150.3",
		width=2.6667];
	layer5_moe_route_gpu4	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4",
		pos="24135,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer5_attn_qkv_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="24445,1150.3",
		width=2.875];
	layer5_attn_score_gpu5	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5",
		pos="24673,1150.3",
		width=2.9444];
	layer5_attn_out_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="24893,1150.3",
		width=2.6667];
	layer5_moe_route_gpu5	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5",
		pos="25196,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer5_attn_qkv_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="25506,1150.3",
		width=2.875];
	layer5_attn_score_gpu6	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6",
		pos="25734,1150.3",
		width=2.9444];
	layer5_attn_out_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="25954,1150.3",
		width=2.6667];
	layer5_moe_route_gpu6	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6",
		pos="26257,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer5_attn_qkv_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="26567,1150.3",
		width=2.875];
	layer5_attn_score_gpu7	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer5 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7",
		pos="26795,1150.3",
		width=2.9444];
	layer5_attn_out_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer5 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="27015,1150.3",
		width=2.6667];
	layer5_moe_route_gpu7	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer5 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7",
		pos="27318,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer5_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer5 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7",
		pos="27622,1150.3",
		shape=ellipse,
		width=2.6909];
	layer5_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer5 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="27874,1150.3",
		shape=ellipse,
		width=3.8105];
	layer5_moe_agg	[fillcolor=lightgreen,
		height=1.4722,
		label="Layer5 MoE\nOutput Aggregation\nGPU: 4,5,6,7",
		pos="28194,1150.3",
		shape=parallelogram,
		width=4.579];
	layer6_attn_qkv_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="28480,1150.3",
		width=2.875];
	layer6_attn_score_gpu4	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4",
		pos="28708,1150.3",
		width=2.9444];
	layer6_attn_out_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="28928,1150.3",
		width=2.6667];
	layer6_moe_route_gpu4	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4",
		pos="29231,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer6_attn_qkv_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="29541,1150.3",
		width=2.875];
	layer6_attn_score_gpu5	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5",
		pos="29769,1150.3",
		width=2.9444];
	layer6_attn_out_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="29989,1150.3",
		width=2.6667];
	layer6_moe_route_gpu5	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5",
		pos="30292,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer6_attn_qkv_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="30602,1150.3",
		width=2.875];
	layer6_attn_score_gpu6	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6",
		pos="30830,1150.3",
		width=2.9444];
	layer6_attn_out_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="31050,1150.3",
		width=2.6667];
	layer6_moe_route_gpu6	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6",
		pos="31353,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer6_attn_qkv_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="31663,1150.3",
		width=2.875];
	layer6_attn_score_gpu7	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer6 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7",
		pos="31891,1150.3",
		width=2.9444];
	layer6_attn_out_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer6 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="32111,1150.3",
		width=2.6667];
	layer6_moe_route_gpu7	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer6 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7",
		pos="32414,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer6_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer6 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7",
		pos="32718,1150.3",
		shape=ellipse,
		width=2.6909];
	layer6_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer6 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="32970,1150.3",
		shape=ellipse,
		width=3.8105];
	layer6_moe_agg	[fillcolor=lightgreen,
		height=1.4722,
		label="Layer6 MoE\nOutput Aggregation\nGPU: 4,5,6,7",
		pos="33290,1150.3",
		shape=parallelogram,
		width=4.579];
	layer7_attn_qkv_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="33576,1150.3",
		width=2.875];
	layer7_attn_score_gpu4	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 4",
		pos="33804,1150.3",
		width=2.9444];
	layer7_attn_out_gpu4	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 4",
		pos="34024,1150.3",
		width=2.6667];
	layer7_moe_route_gpu4	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 4",
		pos="34327,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer7_attn_qkv_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="34637,1150.3",
		width=2.875];
	layer7_attn_score_gpu5	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 5",
		pos="34865,1150.3",
		width=2.9444];
	layer7_attn_out_gpu5	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 5",
		pos="35085,1150.3",
		width=2.6667];
	layer7_moe_route_gpu5	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 5",
		pos="35388,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer7_attn_qkv_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="35698,1150.3",
		width=2.875];
	layer7_attn_score_gpu6	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 6",
		pos="35926,1150.3",
		width=2.9444];
	layer7_attn_out_gpu6	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 6",
		pos="36146,1150.3",
		width=2.6667];
	layer7_moe_route_gpu6	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 6",
		pos="36449,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer7_attn_qkv_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="36759,1150.3",
		width=2.875];
	layer7_attn_score_gpu7	[fillcolor=lightgreen,
		height=0.94444,
		label="Layer7 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 7",
		pos="36987,1150.3",
		width=2.9444];
	layer7_attn_out_gpu7	[fillcolor=lightgreen,
		height=1.1528,
		label="Layer7 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 7",
		pos="37207,1150.3",
		width=2.6667];
	layer7_moe_route_gpu7	[fillcolor=lightgreen,
		height=1.8889,
		label="Layer7 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 7",
		pos="37510,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer7_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer7 Attention\nAll-Reduce Sum\nGPU: 4,5,6,7",
		pos="37814,1150.3",
		shape=ellipse,
		width=2.6909];
	layer7_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer7 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="38066,1150.3",
		shape=ellipse,
		width=3.8105];
	layer7_moe_agg	[fillcolor=lightgreen,
		height=1.4722,
		label="Layer7 MoE\nOutput Aggregation\nGPU: 4,5,6,7",
		pos="38386,1150.3",
		shape=parallelogram,
		width=4.579];
	layer8_attn_qkv_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="38672,1150.3",
		width=2.875];
	layer8_attn_score_gpu8	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8",
		pos="38900,1150.3",
		width=2.9444];
	layer8_attn_out_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="39120,1150.3",
		width=2.6667];
	layer8_moe_route_gpu8	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8",
		pos="39423,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer8_attn_qkv_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="39733,1150.3",
		width=2.875];
	layer8_attn_score_gpu9	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9",
		pos="39961,1150.3",
		width=2.9444];
	layer8_attn_out_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="40181,1150.3",
		width=2.6667];
	layer8_moe_route_gpu9	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9",
		pos="40484,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer8_attn_qkv_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="40794,1150.3",
		width=2.875];
	layer8_attn_score_gpu10	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10",
		pos="41022,1150.3",
		width=2.9444];
	layer8_attn_out_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="41242,1150.3",
		width=2.6667];
	layer8_moe_route_gpu10	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10",
		pos="41545,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer8_attn_qkv_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="41855,1150.3",
		width=2.875];
	layer8_attn_score_gpu11	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer8 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11",
		pos="42083,1150.3",
		width=2.9444];
	layer8_attn_out_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer8 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="42303,1150.3",
		width=2.6667];
	layer8_moe_route_gpu11	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer8 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11",
		pos="42606,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer8_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer8 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11",
		pos="42910,1150.3",
		shape=ellipse,
		width=2.6909];
	layer8_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer8 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="43162,1150.3",
		shape=ellipse,
		width=3.8105];
	layer8_moe_agg	[fillcolor=lightyellow,
		height=1.4722,
		label="Layer8 MoE\nOutput Aggregation\nGPU: 8,9,10,11",
		pos="43482,1150.3",
		shape=parallelogram,
		width=4.579];
	layer9_attn_qkv_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="43768,1150.3",
		width=2.875];
	layer9_attn_score_gpu8	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8",
		pos="43996,1150.3",
		width=2.9444];
	layer9_attn_out_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="44216,1150.3",
		width=2.6667];
	layer9_moe_route_gpu8	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8",
		pos="44519,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer9_attn_qkv_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="44829,1150.3",
		width=2.875];
	layer9_attn_score_gpu9	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9",
		pos="45057,1150.3",
		width=2.9444];
	layer9_attn_out_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="45277,1150.3",
		width=2.6667];
	layer9_moe_route_gpu9	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9",
		pos="45580,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer9_attn_qkv_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="45890,1150.3",
		width=2.875];
	layer9_attn_score_gpu10	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10",
		pos="46118,1150.3",
		width=2.9444];
	layer9_attn_out_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="46338,1150.3",
		width=2.6667];
	layer9_moe_route_gpu10	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10",
		pos="46641,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer9_attn_qkv_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="46951,1150.3",
		width=2.875];
	layer9_attn_score_gpu11	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer9 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11",
		pos="47179,1150.3",
		width=2.9444];
	layer9_attn_out_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer9 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="47399,1150.3",
		width=2.6667];
	layer9_moe_route_gpu11	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer9 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11",
		pos="47702,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer9_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer9 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11",
		pos="48006,1150.3",
		shape=ellipse,
		width=2.6909];
	layer9_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer9 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="48258,1150.3",
		shape=ellipse,
		width=3.8105];
	layer9_moe_agg	[fillcolor=lightyellow,
		height=1.4722,
		label="Layer9 MoE\nOutput Aggregation\nGPU: 8,9,10,11",
		pos="48578,1150.3",
		shape=parallelogram,
		width=4.579];
	layer10_attn_qkv_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="48869,1150.3",
		width=3];
	layer10_attn_score_gpu8	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8",
		pos="49101,1150.3",
		width=2.9444];
	layer10_attn_out_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="49326,1150.3",
		width=2.7917];
	layer10_moe_route_gpu8	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8",
		pos="49633,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer10_attn_qkv_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="49948,1150.3",
		width=3];
	layer10_attn_score_gpu9	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9",
		pos="50180,1150.3",
		width=2.9444];
	layer10_attn_out_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="50405,1150.3",
		width=2.7917];
	layer10_moe_route_gpu9	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9",
		pos="50712,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer10_attn_qkv_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="51027,1150.3",
		width=3];
	layer10_attn_score_gpu10	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10",
		pos="51259,1150.3",
		width=2.9444];
	layer10_attn_out_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="51484,1150.3",
		width=2.7917];
	layer10_moe_route_gpu10	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10",
		pos="51791,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer10_attn_qkv_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="52106,1150.3",
		width=3];
	layer10_attn_score_gpu11	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer10 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11",
		pos="52338,1150.3",
		width=2.9444];
	layer10_attn_out_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer10 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="52563,1150.3",
		width=2.7917];
	layer10_moe_route_gpu11	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer10 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11",
		pos="52870,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer10_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer10 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11",
		pos="53180,1150.3",
		shape=ellipse,
		width=2.8677];
	layer10_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer10 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="53438,1150.3",
		shape=ellipse,
		width=3.8105];
	layer10_moe_agg	[fillcolor=lightyellow,
		height=1.4722,
		label="Layer10 MoE\nOutput Aggregation\nGPU: 8,9,10,11",
		pos="53758,1150.3",
		shape=parallelogram,
		width=4.579];
	layer11_attn_qkv_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="54049,1150.3",
		width=3];
	layer11_attn_score_gpu8	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 8",
		pos="54281,1150.3",
		width=2.9444];
	layer11_attn_out_gpu8	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 8",
		pos="54506,1150.3",
		width=2.7917];
	layer11_moe_route_gpu8	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 8",
		pos="54813,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer11_attn_qkv_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="55128,1150.3",
		width=3];
	layer11_attn_score_gpu9	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 9",
		pos="55360,1150.3",
		width=2.9444];
	layer11_attn_out_gpu9	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 9",
		pos="55585,1150.3",
		width=2.7917];
	layer11_moe_route_gpu9	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 9",
		pos="55892,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer11_attn_qkv_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="56207,1150.3",
		width=3];
	layer11_attn_score_gpu10	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 10",
		pos="56439,1150.3",
		width=2.9444];
	layer11_attn_out_gpu10	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 10",
		pos="56664,1150.3",
		width=2.7917];
	layer11_moe_route_gpu10	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 10",
		pos="56971,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer11_attn_qkv_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="57286,1150.3",
		width=3];
	layer11_attn_score_gpu11	[fillcolor=lightyellow,
		height=0.94444,
		label="Layer11 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 11",
		pos="57518,1150.3",
		width=2.9444];
	layer11_attn_out_gpu11	[fillcolor=lightyellow,
		height=1.1528,
		label="Layer11 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 11",
		pos="57743,1150.3",
		width=2.7917];
	layer11_moe_route_gpu11	[fillcolor=lightyellow,
		height=1.8889,
		label="Layer11 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 11",
		pos="58050,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer11_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer11 Attention\nAll-Reduce Sum\nGPU: 8,9,10,11",
		pos="58360,1150.3",
		shape=ellipse,
		width=2.8677];
	layer11_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer11 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="58618,1150.3",
		shape=ellipse,
		width=3.8105];
	layer11_moe_agg	[fillcolor=lightyellow,
		height=1.4722,
		label="Layer11 MoE\nOutput Aggregation\nGPU: 8,9,10,11",
		pos="58938,1150.3",
		shape=parallelogram,
		width=4.579];
	layer12_attn_qkv_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="59229,1150.3",
		width=3];
	layer12_attn_score_gpu12	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12",
		pos="59461,1150.3",
		width=2.9444];
	layer12_attn_out_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="59686,1150.3",
		width=2.7917];
	layer12_moe_route_gpu12	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12",
		pos="59993,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer12_attn_qkv_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="60308,1150.3",
		width=3];
	layer12_attn_score_gpu13	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13",
		pos="60540,1150.3",
		width=2.9444];
	layer12_attn_out_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="60765,1150.3",
		width=2.7917];
	layer12_moe_route_gpu13	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13",
		pos="61072,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer12_attn_qkv_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="61387,1150.3",
		width=3];
	layer12_attn_score_gpu14	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14",
		pos="61619,1150.3",
		width=2.9444];
	layer12_attn_out_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="61844,1150.3",
		width=2.7917];
	layer12_moe_route_gpu14	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14",
		pos="62151,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer12_attn_qkv_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="62466,1150.3",
		width=3];
	layer12_attn_score_gpu15	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer12 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15",
		pos="62698,1150.3",
		width=2.9444];
	layer12_attn_out_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer12 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="62923,1150.3",
		width=2.7917];
	layer12_moe_route_gpu15	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer12 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15",
		pos="63230,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer12_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer12 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15",
		pos="63540,1150.3",
		shape=ellipse,
		width=2.8677];
	layer12_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer12 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="63798,1150.3",
		shape=ellipse,
		width=3.8105];
	layer12_moe_agg	[fillcolor=lightcoral,
		height=1.4722,
		label="Layer12 MoE\nOutput Aggregation\nGPU: 12,13,14,15",
		pos="64118,1150.3",
		shape=parallelogram,
		width=4.579];
	layer13_attn_qkv_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="64409,1150.3",
		width=3];
	layer13_attn_score_gpu12	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12",
		pos="64641,1150.3",
		width=2.9444];
	layer13_attn_out_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="64866,1150.3",
		width=2.7917];
	layer13_moe_route_gpu12	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12",
		pos="65173,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer13_attn_qkv_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="65488,1150.3",
		width=3];
	layer13_attn_score_gpu13	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13",
		pos="65720,1150.3",
		width=2.9444];
	layer13_attn_out_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="65945,1150.3",
		width=2.7917];
	layer13_moe_route_gpu13	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13",
		pos="66252,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer13_attn_qkv_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="66567,1150.3",
		width=3];
	layer13_attn_score_gpu14	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14",
		pos="66799,1150.3",
		width=2.9444];
	layer13_attn_out_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="67024,1150.3",
		width=2.7917];
	layer13_moe_route_gpu14	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14",
		pos="67331,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer13_attn_qkv_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="67646,1150.3",
		width=3];
	layer13_attn_score_gpu15	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer13 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15",
		pos="67878,1150.3",
		width=2.9444];
	layer13_attn_out_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer13 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="68103,1150.3",
		width=2.7917];
	layer13_moe_route_gpu15	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer13 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15",
		pos="68410,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer13_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer13 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15",
		pos="68720,1150.3",
		shape=ellipse,
		width=2.8677];
	layer13_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer13 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="68978,1150.3",
		shape=ellipse,
		width=3.8105];
	layer13_moe_agg	[fillcolor=lightcoral,
		height=1.4722,
		label="Layer13 MoE\nOutput Aggregation\nGPU: 12,13,14,15",
		pos="69298,1150.3",
		shape=parallelogram,
		width=4.579];
	layer14_attn_qkv_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="69589,1150.3",
		width=3];
	layer14_attn_score_gpu12	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12",
		pos="69821,1150.3",
		width=2.9444];
	layer14_attn_out_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="70046,1150.3",
		width=2.7917];
	layer14_moe_route_gpu12	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12",
		pos="70353,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer14_attn_qkv_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="70668,1150.3",
		width=3];
	layer14_attn_score_gpu13	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13",
		pos="70900,1150.3",
		width=2.9444];
	layer14_attn_out_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="71125,1150.3",
		width=2.7917];
	layer14_moe_route_gpu13	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13",
		pos="71432,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer14_attn_qkv_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="71747,1150.3",
		width=3];
	layer14_attn_score_gpu14	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14",
		pos="71979,1150.3",
		width=2.9444];
	layer14_attn_out_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="72204,1150.3",
		width=2.7917];
	layer14_moe_route_gpu14	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14",
		pos="72511,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer14_attn_qkv_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="72826,1150.3",
		width=3];
	layer14_attn_score_gpu15	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer14 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15",
		pos="73058,1150.3",
		width=2.9444];
	layer14_attn_out_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer14 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="73283,1150.3",
		width=2.7917];
	layer14_moe_route_gpu15	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer14 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15",
		pos="73590,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer14_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer14 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15",
		pos="73900,1150.3",
		shape=ellipse,
		width=2.8677];
	layer14_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer14 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="74158,1150.3",
		shape=ellipse,
		width=3.8105];
	layer14_moe_agg	[fillcolor=lightcoral,
		height=1.4722,
		label="Layer14 MoE\nOutput Aggregation\nGPU: 12,13,14,15",
		pos="74478,1150.3",
		shape=parallelogram,
		width=4.579];
	layer15_attn_qkv_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="74769,1150.3",
		width=3];
	layer15_attn_score_gpu12	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 12",
		pos="75001,1150.3",
		width=2.9444];
	layer15_attn_out_gpu12	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 12",
		pos="75226,1150.3",
		width=2.7917];
	layer15_moe_route_gpu12	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 12",
		pos="75533,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer15_attn_qkv_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="75848,1150.3",
		width=3];
	layer15_attn_score_gpu13	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 13",
		pos="76080,1150.3",
		width=2.9444];
	layer15_attn_out_gpu13	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 13",
		pos="76305,1150.3",
		width=2.7917];
	layer15_moe_route_gpu13	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 13",
		pos="76612,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer15_attn_qkv_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="76927,1150.3",
		width=3];
	layer15_attn_score_gpu14	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 14",
		pos="77159,1150.3",
		width=2.9444];
	layer15_attn_out_gpu14	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 14",
		pos="77384,1150.3",
		width=2.7917];
	layer15_moe_route_gpu14	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 14",
		pos="77691,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer15_attn_qkv_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention QKV Proj\n(Column Parallel)\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="78006,1150.3",
		width=3];
	layer15_attn_score_gpu15	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer15 Attention Scores\nInput: [64, 16, 1024, 64]\nOutput: [64, 4, 1024, 1024]\nGPU: 15",
		pos="78238,1150.3",
		width=2.9444];
	layer15_attn_out_gpu15	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer15 Attention Output\n(Row Parallel)\nInput: [64, 1024, 256]\nOutput: [64, 1024, 256]\nGPU: 15",
		pos="78463,1150.3",
		width=2.7917];
	layer15_moe_route_gpu15	[fillcolor=lightcoral,
		height=1.8889,
		label="Layer15 MoE Routing\nInput: [64, 1024, 1024]\nOutput: [64, 1024, 1]\nGPU: 15",
		pos="78770,1150.3",
		shape=parallelogram,
		width=5.2414];
	layer15_attn_allreduce	[fillcolor=lightgray,
		height=1.041,
		label="Layer15 Attention\nAll-Reduce Sum\nGPU: 12,13,14,15",
		pos="79080,1150.3",
		shape=ellipse,
		width=2.8677];
	layer15_moe_all2all	[fillcolor=lightgray,
		height=1.041,
		label="Layer15 MoE\nAll-to-All Communication\nGPU: 0-15",
		pos="79338,1150.3",
		shape=ellipse,
		width=3.8105];
	layer15_moe_agg	[fillcolor=lightcoral,
		height=1.4722,
		label="Layer15 MoE\nOutput Aggregation\nGPU: 12,13,14,15",
		pos="79658,1150.3",
		shape=parallelogram,
		width=4.579];
	output_agg	[fillcolor=lightpink,
		height=2.3056,
		label="Output Aggregation\nAll-Reduce Sum\nInput: [128, 1024, 1024]\nOutput: [128, 1024, 1024]\nGPU: 12,13,14,15",
		pos="79658,963.33",
		shape=parallelogram,
		width=5.8461];
	layer15_moe_agg -> output_agg	[pos="e,79658,1046.6 79658,1097.2 79658,1097.2 79658,1056.6 79658,1056.6"];
	output	[fillcolor=white,
		height=1.041,
		label="Final Output\n[batch_size=128, seq_len=1024, hidden=1024]\nGPU: ALL",
		pos="79658,802.83",
		shape=ellipse,
		width=6.9925];
	output_agg -> output	[pos="e,79658,840.52 79658,880.26 79658,880.26 79658,850.52 79658,850.52"];
	layer0_moe_route_gpu0 -> layer0_moe_all2all	[pos="e,2151.4,270 2007,312.67 2007,294.71 2007,270 2007,270 2007,270 2141.4,270 2141.4,270"];
	layer0_moe_route_gpu1 -> layer0_moe_all2all	[pos="e,2157.9,271.22 2157.9,312.88 2157.9,312.88 2157.9,281.22 2157.9,281.22"];
	layer0_moe_route_gpu2 -> layer0_moe_all2all	[pos="e,2304.1,271.22 2304.1,312.88 2304.1,312.88 2304.1,281.22 2304.1,281.22"];
	layer0_moe_route_gpu3 -> layer0_moe_all2all	[pos="e,2327.9,266 2455,312.86 2455,293.64 2455,266 2455,266 2455,266 2337.9,266 2337.9,266"];
}
