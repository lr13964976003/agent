{
  "dag_files": [
    {
      "name": "moe_deployment_detailed",
      "dot_path": "../outputs/2025-12-05-10-26-38/moe_deployment_detailed.dot",
      "svg_path": "../outputs/2025-12-05-10-26-38/moe_deployment_detailed.svg",
      "description": "Detailed DAG showing all 16 layers with tensor parallelism (4-way), expert parallelism (16-way), pipeline parallelism (4-stage), and data parallelism (2-way) across 16 GPUs"
    },
    {
      "name": "moe_deployment_simplified", 
      "dot_path": "../outputs/2025-12-05-10-26-38/moe_deployment_simplified.dot",
      "svg_path": "../outputs/2025-12-05-10-26-38/moe_deployment_simplified.svg",
      "description": "Simplified DAG showing the main flow through 4 pipeline stages with key communication patterns"
    }
  ],
  "generation_timestamp": "2025-12-05-10-26-38",
  "model_config": {
    "model_type": "30B MoE Transformer",
    "layers": 16,
    "experts_per_layer": 64,
    "hidden_size": 1024,
    "attention_heads": 16,
    "batch_size": 128,
    "sequence_length": 1024
  },
  "parallel_config": {
    "tensor_parallelism": 4,
    "expert_parallelism": 16,
    "pipeline_parallelism": 4,
    "data_parallelism": 2,
    "total_gpus": 16,
    "experts_per_gpu": 4,
    "micro_batch_size": 8,
    "gradient_accumulation": 16
  },
  "key_features": [
    "GPU boundaries clearly defined with color coding",
    "All communication operations represented (All-Reduce, All-to-All)",
    "Attention components broken down into QKV projection, scores, and output",
    "MoE layers showing expert routing and aggregation",
    "Input/output dimensions specified for each node",
    "Rectangles for computation, ellipses for communication, parallelograms for routing",
    "Pipeline stages grouped with visual boundaries",
    "No cycles - pure DAG structure"
  ]
}