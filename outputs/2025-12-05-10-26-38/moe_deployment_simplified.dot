digraph {
	graph [bb="0,0,7908.4,1165.7",
		bgcolor=white,
		rankdir=TB,
		ranksep=1.0,
		splines=ortho
	];
	node [label="\N"];
	subgraph cluster_stage0 {
		graph [bb="24.403,306.35,3502.4,907.98",
			fillcolor=lightblue,
			label="Stage 0: GPUs 0-3\nLayers 0-3",
			lheight=0.42,
			lp="1763.4,888.98",
			lwidth=1.82,
			style="rounded,filled"
		];
		s0_l0_attn	[fillcolor=lightblue,
			height=1.3356,
			label="Layer 0 Attention\n4-way Tensor Parallel\nQKV→Scores→Output\nGPU: 0,1,2,3",
			pos="152.4,813.9",
			width=3.3391];
		s0_l0_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Attention All-Reduce\nGPU: 0,1,2,3",
			pos="155.4,656.34",
			shape=ellipse,
			width=3.2213];
		s0_l0_attn -> s0_l0_attn_comm	[pos="e,155.4,683.57 155.4,765.77 155.4,765.77 155.4,693.57 155.4,693.57"];
		s0_l0_moe	[fillcolor=lightblue,
			height=1.3356,
			label="Layer 0 MoE\n16-way Expert Parallel\n4 experts per GPU\nGPU: 0-15",
			pos="159.4,498.78",
			width=3.5159];
		s0_l0_attn_comm -> s0_l0_moe	[pos="e,155.4,547.12 155.4,629.25 155.4,629.25 155.4,557.12 155.4,557.12"];
		s0_l0_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="MoE All-to-All\nGPU: 0-15",
			pos="159.4,341.22",
			shape=ellipse,
			width=2.2588];
		s0_l0_moe -> s0_l0_moe_comm	[pos="e,159.4,368.45 159.4,450.65 159.4,450.65 159.4,378.45 159.4,378.45"];
		s0_l1_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 1 Attention\n4-way Tensor Parallel\nGPU: 0,1,2,3",
			pos="3111.4,813.9",
			width=3.3391];
		s0_l1_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 1 Attn All-Reduce\nGPU: 0,1,2,3",
			pos="2841.4,813.9",
			shape=ellipse,
			width=3.6534];
		s0_l1_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 1 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="2565.4,813.9",
			width=3.5159];
		s0_l1_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 1 MoE All-to-All\nGPU: 0-15",
			pos="2298.4,813.9",
			shape=ellipse,
			width=3.398];
		s0_l2_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 2 Attention\n4-way Tensor Parallel\nGPU: 0,1,2,3",
			pos="2037.4,813.9",
			width=3.3391];
		s0_l2_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 2 Attn All-Reduce\nGPU: 0,1,2,3",
			pos="1767.4,813.9",
			shape=ellipse,
			width=3.6534];
		s0_l2_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 2 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="1491.4,813.9",
			width=3.5159];
		s0_l2_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 2 MoE All-to-All\nGPU: 0-15",
			pos="1224.4,813.9",
			shape=ellipse,
			width=3.398];
		s0_l3_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 3 Attention\n4-way Tensor Parallel\nGPU: 0,1,2,3",
			pos="963.4,813.9",
			width=3.3391];
		s0_l3_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 3 Attn All-Reduce\nGPU: 0,1,2,3",
			pos="693.4,813.9",
			shape=ellipse,
			width=3.6534];
		s0_l3_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 3 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="417.4,813.9",
			width=3.5159];
		s0_l3_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 3 MoE All-to-All\nGPU: 0-15",
			pos="3372.4,813.9",
			shape=ellipse,
			width=3.398];
	}
	subgraph cluster_stage1 {
		graph [bb="3558.4,610.86,7851.4,739.81",
			fillcolor=lightgreen,
			label="Stage 1: GPUs 4-7\nLayers 4-7",
			lheight=0.42,
			lp="5704.9,720.81",
			lwidth=1.82,
			style="rounded,filled"
		];
		s1_l4_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 4 Attention\n4-way Tensor Parallel\nGPU: 4,5,6,7",
			pos="7723.4,656.34",
			width=3.3391];
		s1_l4_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 4 Attn All-Reduce\nGPU: 4,5,6,7",
			pos="7453.4,656.34",
			shape=ellipse,
			width=3.6534];
		s1_l4_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 4 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="7177.4,656.34",
			width=3.5159];
		s1_l4_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 4 MoE All-to-All\nGPU: 0-15",
			pos="6910.4,656.34",
			shape=ellipse,
			width=3.398];
		s1_l5_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 5 Attention\n4-way Tensor Parallel\nGPU: 4,5,6,7",
			pos="6649.4,656.34",
			width=3.3391];
		s1_l5_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 5 Attn All-Reduce\nGPU: 4,5,6,7",
			pos="6379.4,656.34",
			shape=ellipse,
			width=3.6534];
		s1_l5_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 5 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="6103.4,656.34",
			width=3.5159];
		s1_l5_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 5 MoE All-to-All\nGPU: 0-15",
			pos="5836.4,656.34",
			shape=ellipse,
			width=3.398];
		s1_l6_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 6 Attention\n4-way Tensor Parallel\nGPU: 4,5,6,7",
			pos="5575.4,656.34",
			width=3.3391];
		s1_l6_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 6 Attn All-Reduce\nGPU: 4,5,6,7",
			pos="5305.4,656.34",
			shape=ellipse,
			width=3.6534];
		s1_l6_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 6 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="5029.4,656.34",
			width=3.5159];
		s1_l6_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 6 MoE All-to-All\nGPU: 0-15",
			pos="4762.4,656.34",
			shape=ellipse,
			width=3.398];
		s1_l7_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 7 Attention\n4-way Tensor Parallel\nGPU: 4,5,6,7",
			pos="4501.4,656.34",
			width=3.3391];
		s1_l7_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 7 Attn All-Reduce\nGPU: 4,5,6,7",
			pos="4231.4,656.34",
			shape=ellipse,
			width=3.6534];
		s1_l7_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 7 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="3955.4,656.34",
			width=3.5159];
		s1_l7_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 7 MoE All-to-All\nGPU: 0-15",
			pos="3688.4,656.34",
			shape=ellipse,
			width=3.398];
	}
	subgraph cluster_stage2 {
		graph [bb="3534.4,453.3,7876.4,582.25",
			fillcolor=lightyellow,
			label="Stage 2: GPUs 8-11\nLayers 8-11",
			lheight=0.42,
			lp="5705.4,563.25",
			lwidth=1.94,
			style="rounded,filled"
		];
		s2_l8_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 8 Attention\n4-way Tensor Parallel\nGPU: 8,9,10,11",
			pos="7748.4,498.78",
			width=3.3391];
		s2_l8_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 8 Attn All-Reduce\nGPU: 8,9,10,11",
			pos="7478.4,498.78",
			shape=ellipse,
			width=3.6534];
		s2_l8_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 8 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="7202.4,498.78",
			width=3.5159];
		s2_l8_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 8 MoE All-to-All\nGPU: 0-15",
			pos="6935.4,498.78",
			shape=ellipse,
			width=3.398];
		s2_l9_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 9 Attention\n4-way Tensor Parallel\nGPU: 8,9,10,11",
			pos="6674.4,498.78",
			width=3.3391];
		s2_l9_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 9 Attn All-Reduce\nGPU: 8,9,10,11",
			pos="6404.4,498.78",
			shape=ellipse,
			width=3.6534];
		s2_l9_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 9 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="6128.4,498.78",
			width=3.5159];
		s2_l9_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 9 MoE All-to-All\nGPU: 0-15",
			pos="5861.4,498.78",
			shape=ellipse,
			width=3.398];
		s2_l10_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 10 Attention\n4-way Tensor Parallel\nGPU: 8,9,10,11",
			pos="5600.4,498.78",
			width=3.3391];
		s2_l10_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 10 Attn All-Reduce\nGPU: 8,9,10,11",
			pos="5324.4,498.78",
			shape=ellipse,
			width=3.8302];
		s2_l10_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 10 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="5042.4,498.78",
			width=3.5159];
		s2_l10_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 10 MoE All-to-All\nGPU: 0-15",
			pos="4769.4,498.78",
			shape=ellipse,
			width=3.5748];
		s2_l11_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 11 Attention\n4-way Tensor Parallel\nGPU: 8,9,10,11",
			pos="4502.4,498.78",
			width=3.3391];
		s2_l11_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 11 Attn All-Reduce\nGPU: 8,9,10,11",
			pos="4226.4,498.78",
			shape=ellipse,
			width=3.8302];
		s2_l11_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 11 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="3944.4,498.78",
			width=3.5159];
		s2_l11_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 11 MoE All-to-All\nGPU: 0-15",
			pos="3671.4,498.78",
			shape=ellipse,
			width=3.5748];
	}
	subgraph cluster_stage3 {
		graph [bb="3510.4,295.74,7900.4,424.69",
			fillcolor=lightcoral,
			label="Stage 3: GPUs 12-15\nLayers 12-15",
			lheight=0.42,
			lp="5705.4,405.69",
			lwidth=2.07,
			style="rounded,filled"
		];
		s3_l12_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 12 Attention\n4-way Tensor Parallel\nGPU: 12,13,14,15",
			pos="7772.4,341.22",
			width=3.3391];
		s3_l12_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 12 Attn All-Reduce\nGPU: 12,13,14,15",
			pos="7496.4,341.22",
			shape=ellipse,
			width=3.8302];
		s3_l12_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 12 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="7214.4,341.22",
			width=3.5159];
		s3_l12_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 12 MoE All-to-All\nGPU: 0-15",
			pos="6941.4,341.22",
			shape=ellipse,
			width=3.5748];
		s3_l13_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 13 Attention\n4-way Tensor Parallel\nGPU: 12,13,14,15",
			pos="6674.4,341.22",
			width=3.3391];
		s3_l13_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 13 Attn All-Reduce\nGPU: 12,13,14,15",
			pos="6398.4,341.22",
			shape=ellipse,
			width=3.8302];
		s3_l13_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 13 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="6116.4,341.22",
			width=3.5159];
		s3_l13_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 13 MoE All-to-All\nGPU: 0-15",
			pos="5843.4,341.22",
			shape=ellipse,
			width=3.5748];
		s3_l14_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 14 Attention\n4-way Tensor Parallel\nGPU: 12,13,14,15",
			pos="5576.4,341.22",
			width=3.3391];
		s3_l14_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 14 Attn All-Reduce\nGPU: 12,13,14,15",
			pos="5300.4,341.22",
			shape=ellipse,
			width=3.8302];
		s3_l14_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 14 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="5018.4,341.22",
			width=3.5159];
		s3_l14_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 14 MoE All-to-All\nGPU: 0-15",
			pos="4745.4,341.22",
			shape=ellipse,
			width=3.5748];
		s3_l15_attn	[fillcolor=lightblue,
			height=1.041,
			label="Layer 15 Attention\n4-way Tensor Parallel\nGPU: 12,13,14,15",
			pos="4478.4,341.22",
			width=3.3391];
		s3_l15_attn_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 15 Attn All-Reduce\nGPU: 12,13,14,15",
			pos="4202.4,341.22",
			shape=ellipse,
			width=3.8302];
		s3_l15_moe	[fillcolor=lightblue,
			height=1.041,
			label="Layer 15 MoE\n16-way Expert Parallel\nGPU: 0-15",
			pos="3920.4,341.22",
			width=3.5159];
		s3_l15_moe_comm	[fillcolor=lightgray,
			height=0.74639,
			label="Layer 15 MoE All-to-All\nGPU: 0-15",
			pos="3647.4,341.22",
			shape=ellipse,
			width=3.5748];
	}
	input	[fillcolor=white,
		height=0.74639,
		label="Input Batch\n[128, 1024, 1024]",
		pos="152.4,1138.8",
		shape=ellipse,
		width=2.8284];
	dp_split	[fillcolor=lightyellow,
		height=1.4722,
		label="Data Parallel Split\n→ 2 groups of 64\n[64, 1024, 1024]",
		pos="152.4,986.98",
		shape=parallelogram,
		width=4.2334];
	input -> dp_split	[pos="e,152.4,1040 152.4,1111.7 152.4,1111.7 152.4,1050 152.4,1050"];
	dp_split -> s0_l0_attn	[pos="e,152.4,862.06 152.4,933.64 152.4,933.64 152.4,872.06 152.4,872.06"];
	s0_l3_moe_comm -> s1_l4_attn	[pos="e,7723.4,693.86 3494.7,813 4200.8,813 7723.4,813 7723.4,813 7723.4,813 7723.4,703.86 7723.4,703.86"];
	s1_l7_moe_comm -> s2_l8_attn	[pos="e,7643.2,480.47 3805.4,648.19 3805.4,610.53 3805.4,455 3805.4,455 3805.4,455 7643.2,455 7643.2,455 7643.2,455 7643.2,470.47 7643.2,\
470.47"];
	s2_l11_moe_comm -> s3_l12_attn	[pos="e,7760.4,378.89 3785,485.9 3785,462.18 3785,414 3785,414 3785,414 7760.4,414 7760.4,414 7760.4,414 7760.4,388.89 7760.4,388.89"];
	output_agg	[fillcolor=lightyellow,
		height=1.4722,
		label="Output Aggregation\nAll-Reduce Sum\nGPU: 12,13,14,15",
		pos="3647.4,178.74",
		shape=parallelogram,
		width=4.579];
	s3_l15_moe_comm -> output_agg	[pos="e,3647.4,232.01 3647.4,314.02 3647.4,314.02 3647.4,242.01 3647.4,242.01"];
	output	[fillcolor=lightgreen,
		height=0.74639,
		label="Final Output\n[128, 1024, 1024]",
		pos="3647.4,26.87",
		shape=ellipse,
		width=2.8284];
	output_agg -> output	[pos="e,3647.4,54.147 3647.4,125.62 3647.4,125.62 3647.4,64.147 3647.4,64.147"];
}
