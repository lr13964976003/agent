digraph detailed_expert_mlp {
	rankdir=TB
	input [label="Expert Input\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=7168]\nOutput: [batch_size=4, seq_len=2048, hidden=7168]" fillcolor=lightblue shape=ellipse style=filled]
	ln2 [label="LayerNorm\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=7168]\nOutput: [batch_size=4, seq_len=2048, hidden=7168]" fillcolor=lightblue shape=rectangle style=filled]
	gate [label="Gating Network\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=7168]\nOutput: [batch_size=4, seq_len=2048, experts=64]" fillcolor=orange shape=parallelogram style=filled]
	fc1 [label="FC1 (Up-project)\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=7168]\nOutput: [batch_size=4, seq_len=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
	gelu [label="GELU Activation\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=2048]\nOutput: [batch_size=4, seq_len=2048, hidden=2048]" fillcolor=lightcoral shape=rectangle style=filled]
	fc2 [label="FC2 (Down-project)\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=2048]\nOutput: [batch_size=4, seq_len=2048, hidden=7168]" fillcolor=lightcoral shape=rectangle style=filled]
	agg [label="Expert Aggregation\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=7168]\nOutput: [batch_size=4, seq_len=2048, hidden=7168]" fillcolor=lightgrey shape=parallelogram style=filled]
	residual2 [label="Residual Add\nGPU:0\nInput: [batch_size=4, seq_len=2048, hidden=7168]\nOutput: [batch_size=4, seq_len=2048, hidden=7168]" fillcolor=lightgrey shape=rectangle style=filled]
	input -> ln2
	ln2 -> gate
	ln2 -> fc1
	fc1 -> gelu
	gelu -> fc2
	fc2 -> agg
	agg -> residual2
}
