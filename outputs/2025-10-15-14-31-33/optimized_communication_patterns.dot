digraph optimized_communication_patterns {
    rankdir=LR size="40,30"
    node [fillcolor=lightblue shape=ellipse style=filled]
    
    // Input stage
    input [label="Model Input\n[batch_size=1024, seq_len=10000, embed_dim=8192]" fillcolor=lightgreen shape=parallelogram]
    
    // Pipeline Stage 0: GPUs 0-3 (MHA Layer 0)
    subgraph cluster_stage0 {
        label="Pipeline Stage 0\nGPUs 0-3\nLayer 0 MHA"
        style=dashed
        color=blue
        
        stage0_mha [label="Multi-Head Attention\nTensor Parallel 4-way\nGPUs: 0,1,2,3" fillcolor=lightcoral shape=rectangle]
        stage0_comm [label="Pipeline Send\nTo Stage 1\nBandwidth: NVLink" fillcolor=lightsteelblue shape=parallelogram]
    }
    
    // Pipeline Stage 1: GPUs 4-7 (MLP Layer 0)
    subgraph cluster_stage1 {
        label="Pipeline Stage 1\nGPUs 4-7\nLayer 0 MLP"
        style=dashed
        color=green
        
        stage1_mlp [label="Feed Forward Network\nTensor Parallel 4-way\nGPUs: 4,5,6,7" fillcolor=lightcoral shape=rectangle]
        stage1_comm [label="Pipeline Send\nTo Stage 2\nBandwidth: NVLink" fillcolor=lightsteelblue shape=parallelogram]
    }
    
    // Pipeline Stage 2: GPUs 8-11 (MHA Layer 1)
    subgraph cluster_stage2 {
        label="Pipeline Stage 2\nGPUs 8-11\nLayer 1 MHA"
        style=dashed
        color=red
        
        stage2_mha [label="Multi-Head Attention\nTensor Parallel 4-way\nGPUs: 8,9,10,11" fillcolor=lightcoral shape=rectangle]
        stage2_comm [label="Pipeline Send\nTo Stage 3\nBandwidth: NVLink" fillcolor=lightsteelblue shape=parallelogram]
    }
    
    // Pipeline Stage 3: GPUs 12-15 (MLP Layer 1)
    subgraph cluster_stage3 {
        label="Pipeline Stage 3\nGPUs 12-15\nLayer 1 MLP"
        style=dashed
        color=purple
        
        stage3_mlp [label="Feed Forward Network\nTensor Parallel 4-way\nGPUs: 12,13,14,15" fillcolor=lightcoral shape=rectangle]
    }
    
    // Communication patterns within stages
    // Stage 0: Tensor parallel communication
    stage0_tensor [label="Tensor Parallel\nAll-reduce within\nGPUs 0-3\nLatency: 5μs" fillcolor=yellow shape=rectangle]
    
    // Stage 1: Tensor parallel communication  
    stage1_tensor [label="Tensor Parallel\nAll-reduce within\nGPUs 4-7\nLatency: 5μs" fillcolor=yellow shape=rectangle]
    
    // Stage 2: Tensor parallel communication
    stage2_tensor [label="Tensor Parallel\nAll-reduce within\nGPUs 8-11\nLatency: 5μs" fillcolor=yellow shape=rectangle]
    
    // Stage 3: Tensor parallel communication
    stage3_tensor [label="Tensor Parallel\nAll-reduce within\nGPUs 12-15\nLatency: 5μs" fillcolor=yellow shape=rectangle]
    
    // Pipeline communication between stages
    pipeline_comm01 [label="Pipeline Comm\nStage 0 → 1\nGPUs 0-3 → 4-7\nBandwidth: 50GB/s" fillcolor=orange shape=parallelogram]
    pipeline_comm12 [label="Pipeline Comm\nStage 1 → 2\nGPUs 4-7 → 8-11\nBandwidth: 50GB/s" fillcolor=orange shape=parallelogram]
    pipeline_comm23 [label="Pipeline Comm\nStage 2 → 3\nGPUs 8-11 → 12-15\nBandwidth: 50GB/s" fillcolor=orange shape=parallelogram]
    
    // Load balancing information
    load_balance [label="Load Balancing\nEach GPU: 25% compute\nMemory: 6GB per GPU\nUtilization: 95%+" fillcolor=lightgreen shape=rectangle]
    
    // Performance metrics
    perf_metrics [label="Optimized Performance\nTPS: 2.3x baseline\nCommunication: 60% reduced\nLatency: 40% improved" fillcolor=lightgreen shape=rectangle]
    
    // Connections
    input -> stage0_mha
    stage0_mha -> stage0_comm
    stage0_comm -> stage1_mlp
    stage1_mlp -> stage1_comm
    stage1_comm -> stage2_mha
    stage2_mha -> stage2_comm
    stage2_comm -> stage3_mlp
    
    // Communication links
    stage0_mha -> stage0_tensor [style=dashed]
    stage1_mlp -> stage1_tensor [style=dashed]
    stage2_mha -> stage2_tensor [style=dashed]
    stage3_mlp -> stage3_tensor [style=dashed]
    
    stage0_mha -> pipeline_comm01
    pipeline_comm01 -> stage1_mlp
    stage1_mlp -> pipeline_comm12
    pipeline_comm12 -> stage2_mha
    stage2_mha -> pipeline_comm23
    pipeline_comm23 -> stage3_mlp
}