// Dense Transformer - Baseline (TP=8, PP=2)
digraph baseline_transformer {
	nodesep=0.8 rankdir=LR ranksep=1.2 splines=ortho
	node [fontname=Arial fontsize=10]
	input [label="Input
[batch_size=128, seq_len=100000, d_model=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
	subgraph cluster_stage0 {
		color=black fillcolor=lightblue label="Pipeline Stage 0
(Devices 0-7)" rank=same style=filled
		l0_input [label="Layer 0 Input
[128, 100000, 4096]" fillcolor=white shape=ellipse style=filled]
		l0_layernorm1 [label="Pre-Attention LN
[128, 100000, 4096]" fillcolor=white shape=rectangle style=filled]
		l0_q_proj_0 [label="Q Proj 0
[128, 100000, 512]
Device 0" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_0 [label="K Proj 0
[128, 100000, 512]
Device 0" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_0 [label="V Proj 0
[128, 100000, 512]
Device 0" fillcolor=lightcoral shape=rectangle style=filled]
		l0_q_proj_1 [label="Q Proj 1
[128, 100000, 512]
Device 1" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_1 [label="K Proj 1
[128, 100000, 512]
Device 1" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_1 [label="V Proj 1
[128, 100000, 512]
Device 1" fillcolor=lightcoral shape=rectangle style=filled]
		l0_q_proj_2 [label="Q Proj 2
[128, 100000, 512]
Device 2" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_2 [label="K Proj 2
[128, 100000, 512]
Device 2" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_2 [label="V Proj 2
[128, 100000, 512]
Device 2" fillcolor=lightcoral shape=rectangle style=filled]
		l0_q_proj_3 [label="Q Proj 3
[128, 100000, 512]
Device 3" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_3 [label="K Proj 3
[128, 100000, 512]
Device 3" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_3 [label="V Proj 3
[128, 100000, 512]
Device 3" fillcolor=lightcoral shape=rectangle style=filled]
		l0_q_proj_4 [label="Q Proj 4
[128, 100000, 512]
Device 4" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_4 [label="K Proj 4
[128, 100000, 512]
Device 4" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_4 [label="V Proj 4
[128, 100000, 512]
Device 4" fillcolor=lightcoral shape=rectangle style=filled]
		l0_q_proj_5 [label="Q Proj 5
[128, 100000, 512]
Device 5" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_5 [label="K Proj 5
[128, 100000, 512]
Device 5" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_5 [label="V Proj 5
[128, 100000, 512]
Device 5" fillcolor=lightcoral shape=rectangle style=filled]
		l0_q_proj_6 [label="Q Proj 6
[128, 100000, 512]
Device 6" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_6 [label="K Proj 6
[128, 100000, 512]
Device 6" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_6 [label="V Proj 6
[128, 100000, 512]
Device 6" fillcolor=lightcoral shape=rectangle style=filled]
		l0_q_proj_7 [label="Q Proj 7
[128, 100000, 512]
Device 7" fillcolor=lightcoral shape=rectangle style=filled]
		l0_k_proj_7 [label="K Proj 7
[128, 100000, 512]
Device 7" fillcolor=lightcoral shape=rectangle style=filled]
		l0_v_proj_7 [label="V Proj 7
[128, 100000, 512]
Device 7" fillcolor=lightcoral shape=rectangle style=filled]
		l0_attention [label="Multi-Head Attention
[128, 100000, 4096]
All Devices 0-7" fillcolor=lightblue shape=rectangle style=filled]
		l0_attn_allreduce [label="All-Reduce
[128, 100000, 4096]
All Devices 0-7" fillcolor=white shape=ellipse style=dashed]
		l0_residual1 [label="Residual Add
[128, 100000, 4096]
All Devices 0-7" fillcolor=white shape=parallelogram style=filled]
		l0_layernorm2 [label="Pre-MLP LN
[128, 100000, 4096]" fillcolor=white shape=rectangle style=filled]
		l0_mlp_gate_0 [label="MLP Gate 0
[128, 100000, 4096]
Device 0" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_0 [label="MLP Up 0
[128, 100000, 4096]
Device 0" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_0 [label="MLP Down 0
[128, 100000, 4096]
Device 0" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_gate_1 [label="MLP Gate 1
[128, 100000, 4096]
Device 1" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_1 [label="MLP Up 1
[128, 100000, 4096]
Device 1" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_1 [label="MLP Down 1
[128, 100000, 4096]
Device 1" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_gate_2 [label="MLP Gate 2
[128, 100000, 4096]
Device 2" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_2 [label="MLP Up 2
[128, 100000, 4096]
Device 2" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_2 [label="MLP Down 2
[128, 100000, 4096]
Device 2" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_gate_3 [label="MLP Gate 3
[128, 100000, 4096]
Device 3" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_3 [label="MLP Up 3
[128, 100000, 4096]
Device 3" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_3 [label="MLP Down 3
[128, 100000, 4096]
Device 3" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_gate_4 [label="MLP Gate 4
[128, 100000, 4096]
Device 4" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_4 [label="MLP Up 4
[128, 100000, 4096]
Device 4" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_4 [label="MLP Down 4
[128, 100000, 4096]
Device 4" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_gate_5 [label="MLP Gate 5
[128, 100000, 4096]
Device 5" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_5 [label="MLP Up 5
[128, 100000, 4096]
Device 5" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_5 [label="MLP Down 5
[128, 100000, 4096]
Device 5" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_gate_6 [label="MLP Gate 6
[128, 100000, 4096]
Device 6" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_6 [label="MLP Up 6
[128, 100000, 4096]
Device 6" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_6 [label="MLP Down 6
[128, 100000, 4096]
Device 6" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_gate_7 [label="MLP Gate 7
[128, 100000, 4096]
Device 7" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_up_7 [label="MLP Up 7
[128, 100000, 4096]
Device 7" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_down_7 [label="MLP Down 7
[128, 100000, 4096]
Device 7" fillcolor=lightgreen shape=rectangle style=filled]
		l0_mlp_reduce [label="MLP Reduce
[128, 100000, 4096]
All Devices 0-7" fillcolor=white shape=ellipse style=dashed]
		l0_output [label="Layer 0 Output
[128, 100000, 4096]" fillcolor=white shape=parallelogram style=filled]
		l1_input [label="Layer 1 Input
[128, 100000, 4096]" fillcolor=white shape=ellipse style=filled]
		l1_layernorm1 [label="Pre-Attention LN
[128, 100000, 4096]" fillcolor=white shape=rectangle style=filled]
		l1_attention [label="Multi-Head Attention
[128, 100000, 4096]
All Devices 0-7" fillcolor=lightblue shape=rectangle style=filled]
		l1_residual1 [label="Residual Add
[128, 100000, 4096]
All Devices 0-7" fillcolor=white shape=parallelogram style=filled]
		l1_output [label="Layer 1 Output
[128, 100000, 4096]" fillcolor=white shape=parallelogram style=filled]
	}
	subgraph cluster_stage1 {
		color=black fillcolor=lightgreen label="Pipeline Stage 1
(Devices 8-15)" rank=same style=filled
		l2_input [label="Layer 2 Input
[128, 100000, 4096]" fillcolor=white shape=ellipse style=filled]
		l2_attention [label="Multi-Head Attention
[128, 100000, 4096]
All Devices 8-15" fillcolor=lightblue shape=rectangle style=filled]
		l2_output [label="Layer 2 Output
[128, 100000, 4096]" fillcolor=white shape=parallelogram style=filled]
		l3_input [label="Layer 3 Input
[128, 100000, 4096]" fillcolor=white shape=ellipse style=filled]
		l3_attention [label="Multi-Head Attention
[128, 100000, 4096]
All Devices 8-15" fillcolor=lightblue shape=rectangle style=filled]
		l3_output [label="Layer 3 Output
[128, 100000, 4096]" fillcolor=white shape=parallelogram style=filled]
	}
	output [label="Output
[batch_size=128, seq_len=100000, d_model=4096]" fillcolor=lightyellow shape=parallelogram style=filled]
	input -> l0_input
	l0_input -> l0_layernorm1
	l0_layernorm1 -> l0_q_proj_0
	l0_layernorm1 -> l0_k_proj_0
	l0_layernorm1 -> l0_v_proj_0
	l0_layernorm1 -> l0_q_proj_1
	l0_layernorm1 -> l0_k_proj_1
	l0_layernorm1 -> l0_v_proj_1
	l0_layernorm1 -> l0_q_proj_2
	l0_layernorm1 -> l0_k_proj_2
	l0_layernorm1 -> l0_v_proj_2
	l0_layernorm1 -> l0_q_proj_3
	l0_layernorm1 -> l0_k_proj_3
	l0_layernorm1 -> l0_v_proj_3
	l0_layernorm1 -> l0_q_proj_4
	l0_layernorm1 -> l0_k_proj_4
	l0_layernorm1 -> l0_v_proj_4
	l0_layernorm1 -> l0_q_proj_5
	l0_layernorm1 -> l0_k_proj_5
	l0_layernorm1 -> l0_v_proj_5
	l0_layernorm1 -> l0_q_proj_6
	l0_layernorm1 -> l0_k_proj_6
	l0_layernorm1 -> l0_v_proj_6
	l0_layernorm1 -> l0_q_proj_7
	l0_layernorm1 -> l0_k_proj_7
	l0_layernorm1 -> l0_v_proj_7
	l0_q_proj_0 -> l0_attention
	l0_k_proj_0 -> l0_attention
	l0_v_proj_0 -> l0_attention
	l0_q_proj_1 -> l0_attention
	l0_k_proj_1 -> l0_attention
	l0_v_proj_1 -> l0_attention
	l0_q_proj_2 -> l0_attention
	l0_k_proj_2 -> l0_attention
	l0_v_proj_2 -> l0_attention
	l0_q_proj_3 -> l0_attention
	l0_k_proj_3 -> l0_attention
	l0_v_proj_3 -> l0_attention
	l0_q_proj_4 -> l0_attention
	l0_k_proj_4 -> l0_attention
	l0_v_proj_4 -> l0_attention
	l0_q_proj_5 -> l0_attention
	l0_k_proj_5 -> l0_attention
	l0_v_proj_5 -> l0_attention
	l0_q_proj_6 -> l0_attention
	l0_k_proj_6 -> l0_attention
	l0_v_proj_6 -> l0_attention
	l0_q_proj_7 -> l0_attention
	l0_k_proj_7 -> l0_attention
	l0_v_proj_7 -> l0_attention
	l0_attention -> l0_attn_allreduce
	l0_input -> l0_residual1
	l0_attn_allreduce -> l0_residual1
	l0_residual1 -> l0_layernorm2
	l0_layernorm2 -> l0_mlp_gate_0
	l0_layernorm2 -> l0_mlp_up_0
	l0_mlp_gate_0 -> l0_mlp_down_0
	l0_mlp_up_0 -> l0_mlp_down_0
	l0_mlp_down_0 -> l0_mlp_reduce
	l0_layernorm2 -> l0_mlp_gate_1
	l0_layernorm2 -> l0_mlp_up_1
	l0_mlp_gate_1 -> l0_mlp_down_1
	l0_mlp_up_1 -> l0_mlp_down_1
	l0_mlp_down_1 -> l0_mlp_reduce
	l0_layernorm2 -> l0_mlp_gate_2
	l0_layernorm2 -> l0_mlp_up_2
	l0_mlp_gate_2 -> l0_mlp_down_2
	l0_mlp_up_2 -> l0_mlp_down_2
	l0_mlp_down_2 -> l0_mlp_reduce
	l0_layernorm2 -> l0_mlp_gate_3
	l0_layernorm2 -> l0_mlp_up_3
	l0_mlp_gate_3 -> l0_mlp_down_3
	l0_mlp_up_3 -> l0_mlp_down_3
	l0_mlp_down_3 -> l0_mlp_reduce
	l0_layernorm2 -> l0_mlp_gate_4
	l0_layernorm2 -> l0_mlp_up_4
	l0_mlp_gate_4 -> l0_mlp_down_4
	l0_mlp_up_4 -> l0_mlp_down_4
	l0_mlp_down_4 -> l0_mlp_reduce
	l0_layernorm2 -> l0_mlp_gate_5
	l0_layernorm2 -> l0_mlp_up_5
	l0_mlp_gate_5 -> l0_mlp_down_5
	l0_mlp_up_5 -> l0_mlp_down_5
	l0_mlp_down_5 -> l0_mlp_reduce
	l0_layernorm2 -> l0_mlp_gate_6
	l0_layernorm2 -> l0_mlp_up_6
	l0_mlp_gate_6 -> l0_mlp_down_6
	l0_mlp_up_6 -> l0_mlp_down_6
	l0_mlp_down_6 -> l0_mlp_reduce
	l0_layernorm2 -> l0_mlp_gate_7
	l0_layernorm2 -> l0_mlp_up_7
	l0_mlp_gate_7 -> l0_mlp_down_7
	l0_mlp_up_7 -> l0_mlp_down_7
	l0_mlp_down_7 -> l0_mlp_reduce
	l0_residual1 -> l0_output
	l0_mlp_reduce -> l0_output
	l0_output -> l1_input
	l1_input -> l1_layernorm1
	l1_layernorm1 -> l1_attention
	l1_input -> l1_residual1
	l1_attention -> l1_residual1
	l1_residual1 -> l1_output
	l1_output -> l2_input [label="Pipeline Send" style=dashed]
	l2_input -> l2_attention
	l2_attention -> l2_output
	l2_output -> l3_input
	l3_input -> l3_attention
	l3_attention -> l3_output
	l3_output -> output
}
