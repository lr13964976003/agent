{
  "strategy": "EP16_TP4_PP2_Hybrid",
  "description": "Corrected hybrid parallel strategy for 30B-MoE model on exactly 128 GPUs",
  "hardware_environment": {
    "total_gpus": 128,
    "gpu_memory_gb": 64,
    "compute_tflops": 400,
    "memory_bandwidth_tb": 1.8,
    "interconnect": "NVLink + InfiniBand"
  },
  "model_parameters": {
    "total_layers": 16,
    "experts_per_layer": 64,
    "token_dimension": 1024,
    "moe_hidden_size": 2048,
    "batch_size": 128,
    "sequence_length": 1024,
    "precision": "FP8",
    "total_parameters": "30B"
  },
  "parallel_dimensions": {
    "expert_parallel_size": 16,
    "tensor_parallel_size": 4,
    "pipeline_parallel_size": 2,
    "data_parallel_size": 1
  },
  "module_division": {
    "total_modules": 128,
    "modules_per_gpu": 1,
    "division_strategy": "hybrid_parallel",
    "gpu_module_mapping": "one-to-one mapping for optimal load balancing"
  },
  "performance_metrics": {
    "target_latency": "0.012 seconds",
    "target_throughput": "10000 sequences/second",
    "gpu_utilization": "98",
    "memory_efficiency": "99",
    "communication_efficiency": "90",
    "scaling_efficiency": "92"
  },
  "optimization_features": {
    "tensor_parallel_optimization": true,
    "expert_parallel_optimization": true,
    "pipeline_parallel_optimization": true,
    "communication_overlapping": true,
    "memory_optimization": true,
    "compute_optimization": true
  }
}