// Dense Transformer - Baseline (TP=8, PP=2)
digraph {
	rankdir=TB size="20,20"
	subgraph cluster_pipeline_stage0 {
		bgcolor=lightblue color=blue label="Pipeline Stage 0 (Devices 0-7)" style=rounded
		input0 [label="Input
Input: [batch=128, seq=100000, d_model=4096]
GPU: -" fillcolor=lightyellow shape=ellipse style=filled]
		embed0 [label="Embedding
Input: [batch=128, seq=100000]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 0-7" fillcolor=lightgreen shape=rectangle style=filled]
		ln0_pre [label="LayerNorm
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 0-7" fillcolor=lightcoral shape=rectangle style=filled]
		q_proj_0_0 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_0 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_0 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_0 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 0" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_0 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_0_1 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_1 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_1 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_1 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 1" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_1 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_0_2 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_2 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_2 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_2 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 2" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_2 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_0_3 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_3 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_3 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_3 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 3" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_3 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_0_4 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_4 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_4 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_4 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 4" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_4 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_0_5 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_5 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_5 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_5 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 5" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_5 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_0_6 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_6 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_6 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_6 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 6" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_6 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_0_7 [label="Q Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_0_7 [label="K Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_0_7 [label="V Projection
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		attention_0_7 [label="MHA (4 heads)
Input: Q[batch=128, seq=100000, heads=4, d_k=128]
K,V[batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, heads=4, d_k=128]
GPU: 7" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_0_7 [label="O Projection
Input: [batch=128, seq=100000, heads=4, d_k=128]
Output: [batch=128, seq=100000, d_model=512]
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		all_reduce_0 [label="All-Reduce
Input: 8×[batch=128, seq=100000, d_model=512]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 0-7" fillcolor=orange shape=parallelogram style=filled]
		add_0 [label="Residual Add
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 0-7" fillcolor=lightgray shape=rectangle style=filled]
		mlp0_gate [label="MLP Gate
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, mlp_hidden=32768]
GPU: all GPUs 0-7" fillcolor=lightpink shape=rectangle style=filled]
		mlp0_up [label="MLP Up
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, mlp_hidden=32768]
GPU: all GPUs 0-7" fillcolor=lightpink shape=rectangle style=filled]
		mlp0_down [label="MLP Down
Input: [batch=128, seq=100000, mlp_hidden=32768]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 0-7" fillcolor=lightpink shape=rectangle style=filled]
		add_1 [label="Residual Add
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 0-7" fillcolor=lightgray shape=rectangle style=filled]
		ln1_pre [label="LayerNorm
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 0-7" fillcolor=lightcoral shape=rectangle style=filled]
		q_proj_1_0 [label="Q Projection
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_0 [label="K Projection
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_0 [label="V Projection
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_0 [label="MHA
GPU: 0" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_0 [label="O Projection
GPU: 0" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_1_1 [label="Q Projection
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_1 [label="K Projection
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_1 [label="V Projection
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_1 [label="MHA
GPU: 1" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_1 [label="O Projection
GPU: 1" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_1_2 [label="Q Projection
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_2 [label="K Projection
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_2 [label="V Projection
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_2 [label="MHA
GPU: 2" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_2 [label="O Projection
GPU: 2" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_1_3 [label="Q Projection
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_3 [label="K Projection
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_3 [label="V Projection
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_3 [label="MHA
GPU: 3" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_3 [label="O Projection
GPU: 3" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_1_4 [label="Q Projection
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_4 [label="K Projection
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_4 [label="V Projection
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_4 [label="MHA
GPU: 4" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_4 [label="O Projection
GPU: 4" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_1_5 [label="Q Projection
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_5 [label="K Projection
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_5 [label="V Projection
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_5 [label="MHA
GPU: 5" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_5 [label="O Projection
GPU: 5" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_1_6 [label="Q Projection
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_6 [label="K Projection
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_6 [label="V Projection
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_6 [label="MHA
GPU: 6" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_6 [label="O Projection
GPU: 6" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_1_7 [label="Q Projection
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_1_7 [label="K Projection
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_1_7 [label="V Projection
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		attention_1_7 [label="MHA
GPU: 7" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_1_7 [label="O Projection
GPU: 7" fillcolor=lightblue shape=rectangle style=filled]
		all_reduce_1 [label="All-Reduce
GPU: all GPUs 0-7" fillcolor=orange shape=parallelogram style=filled]
		add_2 [label="Residual Add
GPU: all GPUs 0-7" fillcolor=lightgray shape=rectangle style=filled]
		mlp1_gate [label="MLP Gate
GPU: all GPUs 0-7" fillcolor=lightpink shape=rectangle style=filled]
		mlp1_up [label="MLP Up
GPU: all GPUs 0-7" fillcolor=lightpink shape=rectangle style=filled]
		mlp1_down [label="MLP Down
GPU: all GPUs 0-7" fillcolor=lightpink shape=rectangle style=filled]
		add_3 [label="Residual Add
GPU: all GPUs 0-7" fillcolor=lightgray shape=rectangle style=filled]
		send_stage0 [label="Send to Stage 1
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, d_model=4096]
GPU: 7→8" fillcolor=lightsteelblue shape=ellipse style=filled]
	}
	subgraph cluster_pipeline_stage1 {
		bgcolor=lightcoral color=red label="Pipeline Stage 1 (Devices 8-15)" style=rounded
		recv_stage1 [label="Receive from Stage 0
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, d_model=4096]
GPU: 7→8" fillcolor=lightsteelblue shape=ellipse style=filled]
		ln2_pre [label="LayerNorm (Layer 2)
GPU: all GPUs 8-15" fillcolor=lightcoral shape=rectangle style=filled]
		q_proj_2_0 [label="Q Proj L2
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_0 [label="K Proj L2
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_0 [label="V Proj L2
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_0 [label="MHA L2
GPU: 8" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_0 [label="O Proj L2
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_2_1 [label="Q Proj L2
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_1 [label="K Proj L2
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_1 [label="V Proj L2
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_1 [label="MHA L2
GPU: 9" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_1 [label="O Proj L2
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_2_2 [label="Q Proj L2
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_2 [label="K Proj L2
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_2 [label="V Proj L2
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_2 [label="MHA L2
GPU: 10" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_2 [label="O Proj L2
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_2_3 [label="Q Proj L2
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_3 [label="K Proj L2
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_3 [label="V Proj L2
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_3 [label="MHA L2
GPU: 11" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_3 [label="O Proj L2
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_2_4 [label="Q Proj L2
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_4 [label="K Proj L2
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_4 [label="V Proj L2
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_4 [label="MHA L2
GPU: 12" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_4 [label="O Proj L2
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_2_5 [label="Q Proj L2
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_5 [label="K Proj L2
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_5 [label="V Proj L2
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_5 [label="MHA L2
GPU: 13" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_5 [label="O Proj L2
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_2_6 [label="Q Proj L2
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_6 [label="K Proj L2
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_6 [label="V Proj L2
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_6 [label="MHA L2
GPU: 14" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_6 [label="O Proj L2
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_2_7 [label="Q Proj L2
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_2_7 [label="K Proj L2
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_2_7 [label="V Proj L2
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		attention_2_7 [label="MHA L2
GPU: 15" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_2_7 [label="O Proj L2
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		all_reduce_2 [label="All-Reduce L2
GPU: all GPUs 8-15" fillcolor=orange shape=parallelogram style=filled]
		add_4 [label="Residual Add L2
GPU: all GPUs 8-15" fillcolor=lightgray shape=rectangle style=filled]
		mlp2_gate [label="MLP Gate L2
GPU: all GPUs 8-15" fillcolor=lightpink shape=rectangle style=filled]
		mlp2_up [label="MLP Up L2
GPU: all GPUs 8-15" fillcolor=lightpink shape=rectangle style=filled]
		mlp2_down [label="MLP Down L2
GPU: all GPUs 8-15" fillcolor=lightpink shape=rectangle style=filled]
		add_5 [label="Residual Add L2
GPU: all GPUs 8-15" fillcolor=lightgray shape=rectangle style=filled]
		ln3_pre [label="LayerNorm (Layer 3)
GPU: all GPUs 8-15" fillcolor=lightcoral shape=rectangle style=filled]
		q_proj_3_0 [label="Q Proj L3
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_0 [label="K Proj L3
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_0 [label="V Proj L3
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_0 [label="MHA L3
GPU: 8" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_0 [label="O Proj L3
GPU: 8" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_3_1 [label="Q Proj L3
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_1 [label="K Proj L3
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_1 [label="V Proj L3
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_1 [label="MHA L3
GPU: 9" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_1 [label="O Proj L3
GPU: 9" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_3_2 [label="Q Proj L3
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_2 [label="K Proj L3
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_2 [label="V Proj L3
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_2 [label="MHA L3
GPU: 10" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_2 [label="O Proj L3
GPU: 10" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_3_3 [label="Q Proj L3
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_3 [label="K Proj L3
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_3 [label="V Proj L3
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_3 [label="MHA L3
GPU: 11" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_3 [label="O Proj L3
GPU: 11" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_3_4 [label="Q Proj L3
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_4 [label="K Proj L3
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_4 [label="V Proj L3
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_4 [label="MHA L3
GPU: 12" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_4 [label="O Proj L3
GPU: 12" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_3_5 [label="Q Proj L3
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_5 [label="K Proj L3
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_5 [label="V Proj L3
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_5 [label="MHA L3
GPU: 13" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_5 [label="O Proj L3
GPU: 13" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_3_6 [label="Q Proj L3
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_6 [label="K Proj L3
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_6 [label="V Proj L3
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_6 [label="MHA L3
GPU: 14" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_6 [label="O Proj L3
GPU: 14" fillcolor=lightblue shape=rectangle style=filled]
		q_proj_3_7 [label="Q Proj L3
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		k_proj_3_7 [label="K Proj L3
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		v_proj_3_7 [label="V Proj L3
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		attention_3_7 [label="MHA L3
GPU: 15" fillcolor=gold shape=rectangle style="filled,rounded"]
		o_proj_3_7 [label="O Proj L3
GPU: 15" fillcolor=lightblue shape=rectangle style=filled]
		all_reduce_3 [label="All-Reduce L3
GPU: all GPUs 8-15" fillcolor=orange shape=parallelogram style=filled]
		add_6 [label="Residual Add L3
GPU: all GPUs 8-15" fillcolor=lightgray shape=rectangle style=filled]
		mlp3_gate [label="MLP Gate L3
GPU: all GPUs 8-15" fillcolor=lightpink shape=rectangle style=filled]
		mlp3_up [label="MLP Up L3
GPU: all GPUs 8-15" fillcolor=lightpink shape=rectangle style=filled]
		mlp3_down [label="MLP Down L3
GPU: all GPUs 8-15" fillcolor=lightpink shape=rectangle style=filled]
		add_7 [label="Residual Add L3
GPU: all GPUs 8-15" fillcolor=lightgray shape=rectangle style=filled]
	}
	output [label="Output
Input: [batch=128, seq=100000, d_model=4096]
Output: [batch=128, seq=100000, d_model=4096]
GPU: all GPUs 8-15" fillcolor=lightyellow shape=ellipse style=filled]
	input0 -> embed0
	embed0 -> ln0_pre
	ln0_pre -> q_proj_0_0
	ln0_pre -> k_proj_0_0
	ln0_pre -> v_proj_0_0
	q_proj_0_0 -> attention_0_0
	k_proj_0_0 -> attention_0_0
	v_proj_0_0 -> attention_0_0
	attention_0_0 -> o_proj_0_0
	ln0_pre -> q_proj_0_1
	ln0_pre -> k_proj_0_1
	ln0_pre -> v_proj_0_1
	q_proj_0_1 -> attention_0_1
	k_proj_0_1 -> attention_0_1
	v_proj_0_1 -> attention_0_1
	attention_0_1 -> o_proj_0_1
	ln0_pre -> q_proj_0_2
	ln0_pre -> k_proj_0_2
	ln0_pre -> v_proj_0_2
	q_proj_0_2 -> attention_0_2
	k_proj_0_2 -> attention_0_2
	v_proj_0_2 -> attention_0_2
	attention_0_2 -> o_proj_0_2
	ln0_pre -> q_proj_0_3
	ln0_pre -> k_proj_0_3
	ln0_pre -> v_proj_0_3
	q_proj_0_3 -> attention_0_3
	k_proj_0_3 -> attention_0_3
	v_proj_0_3 -> attention_0_3
	attention_0_3 -> o_proj_0_3
	ln0_pre -> q_proj_0_4
	ln0_pre -> k_proj_0_4
	ln0_pre -> v_proj_0_4
	q_proj_0_4 -> attention_0_4
	k_proj_0_4 -> attention_0_4
	v_proj_0_4 -> attention_0_4
	attention_0_4 -> o_proj_0_4
	ln0_pre -> q_proj_0_5
	ln0_pre -> k_proj_0_5
	ln0_pre -> v_proj_0_5
	q_proj_0_5 -> attention_0_5
	k_proj_0_5 -> attention_0_5
	v_proj_0_5 -> attention_0_5
	attention_0_5 -> o_proj_0_5
	ln0_pre -> q_proj_0_6
	ln0_pre -> k_proj_0_6
	ln0_pre -> v_proj_0_6
	q_proj_0_6 -> attention_0_6
	k_proj_0_6 -> attention_0_6
	v_proj_0_6 -> attention_0_6
	attention_0_6 -> o_proj_0_6
	ln0_pre -> q_proj_0_7
	ln0_pre -> k_proj_0_7
	ln0_pre -> v_proj_0_7
	q_proj_0_7 -> attention_0_7
	k_proj_0_7 -> attention_0_7
	v_proj_0_7 -> attention_0_7
	attention_0_7 -> o_proj_0_7
	o_proj_0_0 -> all_reduce_0
	o_proj_0_1 -> all_reduce_0
	o_proj_0_2 -> all_reduce_0
	o_proj_0_3 -> all_reduce_0
	o_proj_0_4 -> all_reduce_0
	o_proj_0_5 -> all_reduce_0
	o_proj_0_6 -> all_reduce_0
	o_proj_0_7 -> all_reduce_0
	all_reduce_0 -> add_0
	embed0 -> add_0
	add_0 -> mlp0_gate
	add_0 -> mlp0_up
	mlp0_gate -> mlp0_down
	mlp0_up -> mlp0_down
	mlp0_down -> add_1
	add_0 -> add_1
	add_1 -> ln1_pre
	ln1_pre -> q_proj_1_0
	ln1_pre -> k_proj_1_0
	ln1_pre -> v_proj_1_0
	q_proj_1_0 -> attention_1_0
	k_proj_1_0 -> attention_1_0
	v_proj_1_0 -> attention_1_0
	attention_1_0 -> o_proj_1_0
	ln1_pre -> q_proj_1_1
	ln1_pre -> k_proj_1_1
	ln1_pre -> v_proj_1_1
	q_proj_1_1 -> attention_1_1
	k_proj_1_1 -> attention_1_1
	v_proj_1_1 -> attention_1_1
	attention_1_1 -> o_proj_1_1
	ln1_pre -> q_proj_1_2
	ln1_pre -> k_proj_1_2
	ln1_pre -> v_proj_1_2
	q_proj_1_2 -> attention_1_2
	k_proj_1_2 -> attention_1_2
	v_proj_1_2 -> attention_1_2
	attention_1_2 -> o_proj_1_2
	ln1_pre -> q_proj_1_3
	ln1_pre -> k_proj_1_3
	ln1_pre -> v_proj_1_3
	q_proj_1_3 -> attention_1_3
	k_proj_1_3 -> attention_1_3
	v_proj_1_3 -> attention_1_3
	attention_1_3 -> o_proj_1_3
	ln1_pre -> q_proj_1_4
	ln1_pre -> k_proj_1_4
	ln1_pre -> v_proj_1_4
	q_proj_1_4 -> attention_1_4
	k_proj_1_4 -> attention_1_4
	v_proj_1_4 -> attention_1_4
	attention_1_4 -> o_proj_1_4
	ln1_pre -> q_proj_1_5
	ln1_pre -> k_proj_1_5
	ln1_pre -> v_proj_1_5
	q_proj_1_5 -> attention_1_5
	k_proj_1_5 -> attention_1_5
	v_proj_1_5 -> attention_1_5
	attention_1_5 -> o_proj_1_5
	ln1_pre -> q_proj_1_6
	ln1_pre -> k_proj_1_6
	ln1_pre -> v_proj_1_6
	q_proj_1_6 -> attention_1_6
	k_proj_1_6 -> attention_1_6
	v_proj_1_6 -> attention_1_6
	attention_1_6 -> o_proj_1_6
	ln1_pre -> q_proj_1_7
	ln1_pre -> k_proj_1_7
	ln1_pre -> v_proj_1_7
	q_proj_1_7 -> attention_1_7
	k_proj_1_7 -> attention_1_7
	v_proj_1_7 -> attention_1_7
	attention_1_7 -> o_proj_1_7
	o_proj_1_0 -> all_reduce_1
	o_proj_1_1 -> all_reduce_1
	o_proj_1_2 -> all_reduce_1
	o_proj_1_3 -> all_reduce_1
	o_proj_1_4 -> all_reduce_1
	o_proj_1_5 -> all_reduce_1
	o_proj_1_6 -> all_reduce_1
	o_proj_1_7 -> all_reduce_1
	all_reduce_1 -> add_2
	add_1 -> add_2
	add_2 -> mlp1_gate
	add_2 -> mlp1_up
	mlp1_gate -> mlp1_down
	mlp1_up -> mlp1_down
	mlp1_down -> add_3
	add_2 -> add_3
	add_3 -> send_stage0
	send_stage0 -> recv_stage1
	recv_stage1 -> ln2_pre
	ln2_pre -> q_proj_2_0
	ln2_pre -> k_proj_2_0
	ln2_pre -> v_proj_2_0
	q_proj_2_0 -> attention_2_0
	k_proj_2_0 -> attention_2_0
	v_proj_2_0 -> attention_2_0
	attention_2_0 -> o_proj_2_0
	o_proj_2_0 -> all_reduce_2
	ln2_pre -> q_proj_2_1
	ln2_pre -> k_proj_2_1
	ln2_pre -> v_proj_2_1
	q_proj_2_1 -> attention_2_1
	k_proj_2_1 -> attention_2_1
	v_proj_2_1 -> attention_2_1
	attention_2_1 -> o_proj_2_1
	o_proj_2_1 -> all_reduce_2
	ln2_pre -> q_proj_2_2
	ln2_pre -> k_proj_2_2
	ln2_pre -> v_proj_2_2
	q_proj_2_2 -> attention_2_2
	k_proj_2_2 -> attention_2_2
	v_proj_2_2 -> attention_2_2
	attention_2_2 -> o_proj_2_2
	o_proj_2_2 -> all_reduce_2
	ln2_pre -> q_proj_2_3
	ln2_pre -> k_proj_2_3
	ln2_pre -> v_proj_2_3
	q_proj_2_3 -> attention_2_3
	k_proj_2_3 -> attention_2_3
	v_proj_2_3 -> attention_2_3
	attention_2_3 -> o_proj_2_3
	o_proj_2_3 -> all_reduce_2
	ln2_pre -> q_proj_2_4
	ln2_pre -> k_proj_2_4
	ln2_pre -> v_proj_2_4
	q_proj_2_4 -> attention_2_4
	k_proj_2_4 -> attention_2_4
	v_proj_2_4 -> attention_2_4
	attention_2_4 -> o_proj_2_4
	o_proj_2_4 -> all_reduce_2
	ln2_pre -> q_proj_2_5
	ln2_pre -> k_proj_2_5
	ln2_pre -> v_proj_2_5
	q_proj_2_5 -> attention_2_5
	k_proj_2_5 -> attention_2_5
	v_proj_2_5 -> attention_2_5
	attention_2_5 -> o_proj_2_5
	o_proj_2_5 -> all_reduce_2
	ln2_pre -> q_proj_2_6
	ln2_pre -> k_proj_2_6
	ln2_pre -> v_proj_2_6
	q_proj_2_6 -> attention_2_6
	k_proj_2_6 -> attention_2_6
	v_proj_2_6 -> attention_2_6
	attention_2_6 -> o_proj_2_6
	o_proj_2_6 -> all_reduce_2
	ln2_pre -> q_proj_2_7
	ln2_pre -> k_proj_2_7
	ln2_pre -> v_proj_2_7
	q_proj_2_7 -> attention_2_7
	k_proj_2_7 -> attention_2_7
	v_proj_2_7 -> attention_2_7
	attention_2_7 -> o_proj_2_7
	o_proj_2_7 -> all_reduce_2
	all_reduce_2 -> add_4
	recv_stage1 -> add_4
	add_4 -> mlp2_gate
	add_4 -> mlp2_up
	mlp2_gate -> mlp2_down
	mlp2_up -> mlp2_down
	mlp2_down -> add_5
	add_4 -> add_5
	add_5 -> ln3_pre
	ln3_pre -> q_proj_3_0
	ln3_pre -> k_proj_3_0
	ln3_pre -> v_proj_3_0
	q_proj_3_0 -> attention_3_0
	k_proj_3_0 -> attention_3_0
	v_proj_3_0 -> attention_3_0
	attention_3_0 -> o_proj_3_0
	o_proj_3_0 -> all_reduce_3
	ln3_pre -> q_proj_3_1
	ln3_pre -> k_proj_3_1
	ln3_pre -> v_proj_3_1
	q_proj_3_1 -> attention_3_1
	k_proj_3_1 -> attention_3_1
	v_proj_3_1 -> attention_3_1
	attention_3_1 -> o_proj_3_1
	o_proj_3_1 -> all_reduce_3
	ln3_pre -> q_proj_3_2
	ln3_pre -> k_proj_3_2
	ln3_pre -> v_proj_3_2
	q_proj_3_2 -> attention_3_2
	k_proj_3_2 -> attention_3_2
	v_proj_3_2 -> attention_3_2
	attention_3_2 -> o_proj_3_2
	o_proj_3_2 -> all_reduce_3
	ln3_pre -> q_proj_3_3
	ln3_pre -> k_proj_3_3
	ln3_pre -> v_proj_3_3
	q_proj_3_3 -> attention_3_3
	k_proj_3_3 -> attention_3_3
	v_proj_3_3 -> attention_3_3
	attention_3_3 -> o_proj_3_3
	o_proj_3_3 -> all_reduce_3
	ln3_pre -> q_proj_3_4
	ln3_pre -> k_proj_3_4
	ln3_pre -> v_proj_3_4
	q_proj_3_4 -> attention_3_4
	k_proj_3_4 -> attention_3_4
	v_proj_3_4 -> attention_3_4
	attention_3_4 -> o_proj_3_4
	o_proj_3_4 -> all_reduce_3
	ln3_pre -> q_proj_3_5
	ln3_pre -> k_proj_3_5
	ln3_pre -> v_proj_3_5
	q_proj_3_5 -> attention_3_5
	k_proj_3_5 -> attention_3_5
	v_proj_3_5 -> attention_3_5
	attention_3_5 -> o_proj_3_5
	o_proj_3_5 -> all_reduce_3
	ln3_pre -> q_proj_3_6
	ln3_pre -> k_proj_3_6
	ln3_pre -> v_proj_3_6
	q_proj_3_6 -> attention_3_6
	k_proj_3_6 -> attention_3_6
	v_proj_3_6 -> attention_3_6
	attention_3_6 -> o_proj_3_6
	o_proj_3_6 -> all_reduce_3
	ln3_pre -> q_proj_3_7
	ln3_pre -> k_proj_3_7
	ln3_pre -> v_proj_3_7
	q_proj_3_7 -> attention_3_7
	k_proj_3_7 -> attention_3_7
	v_proj_3_7 -> attention_3_7
	attention_3_7 -> o_proj_3_7
	o_proj_3_7 -> all_reduce_3
	all_reduce_3 -> add_6
	add_7 -> add_6
	add_6 -> mlp3_gate
	add_6 -> mlp3_up
	mlp3_gate -> mlp3_down
	mlp3_up -> mlp3_down
	mlp3_down -> add_7
	add_6 -> add_7
	add_7 -> output
}
