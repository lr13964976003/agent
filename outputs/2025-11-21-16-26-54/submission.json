{
  "submission_paths": {
    "baseline_dag_dot": "../outputs/2025-11-21-16-26-54/baseline_tp_pp_dag.dot",
    "baseline_dag_svg": "../outputs/2025-11-21-16-26-54/baseline_tp_pp_dag.svg",
    "baseline_dag_python": "../outputs/2025-11-21-16-26-54/baseline_tp_pp_dag.py",
    "ring_attention_sp_dag_dot": "../outputs/2025-11-21-16-26-54/ring_attention_sp_dag.dot",
    "ring_attention_sp_dag_svg": "../outputs/2025-11-21-16-26-54/ring_attention_sp_dag.svg",
    "ring_attention_sp_dag_python": "../outputs/2025-11-21-16-26-54/ring_attention_sp_dag.py"
  },
  "analysis_summary": {
    "total_gpus": 16,
    "baseline_strategy": "TP=8, PP=2",
    "ring_attention_strategy": "SP=16 with Ring Attention",
    "sequence_partitioning": {
      "baseline": "No sequence parallelism (full 100k tokens per device)",
      "ring_attention": "Sequence split to 6250 tokens per device (100k/16)"
    },
    "gpu_load_balancing": {
      "baseline": "Pipeline stage 0: devices 0-7, Pipeline stage 1: devices 8-15",
      "ring_attention": "Even distribution across all 16 devices"
    },
    "communication_patterns": {
      "baseline": "All-reduce within TP groups, send/recv between PP stages",
      "ring_attention": "Ring communication (send/recv) for 16 stages per attention layer"
    }
  }
}