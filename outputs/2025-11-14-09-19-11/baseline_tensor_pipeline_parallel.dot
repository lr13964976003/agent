digraph baseline_tensor_pipeline_parallel {
    rankdir=TB;
    node [shape=box, fontname="Arial"];
    
    // Global input
    input [label="Input\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: all GPUs", shape=ellipse];
    
    // Pipeline Stage 0: Devices 0-7, Layers 0-1
    subgraph cluster_pipeline_stage_0 {
        label="Pipeline Stage 0\nDevices: 0-7\nLayers: 0,1";
        style=dashed;
        
        // Layer 0
        subgraph cluster_layer_0 {
            label="Layer 0 (Pipeline Stage 0)";
            style=dashed;
            
            // Device 0-7 operations for Layer 0
            layer0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7", shape=box];
            
            // QKV Projection (Column Parallel)
            layer0_q_proj [label="Q Projection\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, head_dim=128]\nGPU: 0-7 (TP: column split)", shape=box];
            layer0_k_proj [label="K Projection\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, head_dim=128]\nGPU: 0-7 (TP: column split)", shape=box];
            layer0_v_proj [label="V Projection\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, heads=32, head_dim=128]\nGPU: 0-7 (TP: column split)", shape=box];
            
            // All-gather for KV
            layer0_allgather_k [label="AllGather K\nInput: [batch_size=128, seq_len=100000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=100000, heads=32, head_dim=128]\nGPU: 0-7 (TP communication)", shape=ellipse];
            layer0_allgather_v [label="AllGather V\nInput: [batch_size=128, seq_len=100000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=100000, heads=32, head_dim=128]\nGPU: 0-7 (TP communication)", shape=ellipse];
            
            // Attention computation
            layer0_attention [label="Multi-Head Attention\nInput: [batch_size=128, seq_len=100000, heads=32, head_dim=128]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7", shape=box];
            
            // Output projection (Row Parallel)
            layer0_out_proj [label="Output Projection\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7 (TP: row split)", shape=box];
            
            // All-reduce for output
            layer0_allreduce [label="AllReduce\nInput: [batch_size=128, seq_len=100000, hidden=512]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7 (TP communication)", shape=ellipse];
            
            // Residual connection
            layer0_residual [label="Residual Add\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7", shape=box];
            
            // LayerNorm 2
            layer0_norm2 [label="LayerNorm\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7", shape=box];
            
            // MLP (Column/Row Parallel)
            layer0_mlp_fc1 [label="MLP FC1\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, mlp_hidden=4096]\nGPU: 0-7 (TP: column split)", shape=box];
            layer0_mlp_fc2 [label="MLP FC2\nInput: [batch_size=128, seq_len=100000, mlp_hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=512]\nGPU: 0-7 (TP: row split)", shape=box];
            layer0_mlp_allreduce [label="AllReduce\nInput: [batch_size=128, seq_len=100000, hidden=512]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7 (TP communication)", shape=ellipse];
            layer0_mlp_residual [label="MLP Residual Add\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7", shape=box];
        }
        
        // Layer 1 (similar structure to Layer 0)
        subgraph cluster_layer_1 {
            label="Layer 1 (Pipeline Stage 0)";
            style=dashed;
            
            layer1_norm [label="LayerNorm\nGPU: 0-7", shape=box];
            layer1_q_proj [label="Q Projection\nGPU: 0-7", shape=box];
            layer1_k_proj [label="K Projection\nGPU: 0-7", shape=box];
            layer1_v_proj [label="V Projection\nGPU: 0-7", shape=box];
            layer1_allgather_k [label="AllGather K\nGPU: 0-7", shape=ellipse];
            layer1_allgather_v [label="AllGather V\nGPU: 0-7", shape=ellipse];
            layer1_attention [label="Multi-Head Attention\nGPU: 0-7", shape=box];
            layer1_out_proj [label="Output Projection\nGPU: 0-7", shape=box];
            layer1_allreduce [label="AllReduce\nGPU: 0-7", shape=ellipse];
            layer1_residual [label="Residual Add\nGPU: 0-7", shape=box];
            layer1_norm2 [label="LayerNorm\nGPU: 0-7", shape=box];
            layer1_mlp_fc1 [label="MLP FC1\nGPU: 0-7", shape=box];
            layer1_mlp_fc2 [label="MLP FC2\nGPU: 0-7", shape=box];
            layer1_mlp_allreduce [label="AllReduce\nGPU: 0-7", shape=ellipse];
            layer1_mlp_residual [label="MLP Residual Add\nGPU: 0-7", shape=box];
        }
    }
    
    // Pipeline Stage 1: Devices 8-15, Layers 2-3
    subgraph cluster_pipeline_stage_1 {
        label="Pipeline Stage 1\nDevices: 8-15\nLayers: 2,3";
        style=dashed;
        
        // Pipeline communication
        pipeline_send_0_1 [label="Send\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 0-7 â†’ 8-15", shape=parallelogram];
        pipeline_recv_1_0 [label="Receive\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: 8-15", shape=parallelogram];
        
        // Layer 2
        subgraph cluster_layer_2 {
            label="Layer 2 (Pipeline Stage 1)";
            style=dashed;
            
            layer2_norm [label="LayerNorm\nGPU: 8-15", shape=box];
            layer2_q_proj [label="Q Projection\nGPU: 8-15", shape=box];
            layer2_k_proj [label="K Projection\nGPU: 8-15", shape=box];
            layer2_v_proj [label="V Projection\nGPU: 8-15", shape=box];
            layer2_allgather_k [label="AllGather K\nGPU: 8-15", shape=ellipse];
            layer2_allgather_v [label="AllGather V\nGPU: 8-15", shape=ellipse];
            layer2_attention [label="Multi-Head Attention\nGPU: 8-15", shape=box];
            layer2_out_proj [label="Output Projection\nGPU: 8-15", shape=box];
            layer2_allreduce [label="AllReduce\nGPU: 8-15", shape=ellipse];
            layer2_residual [label="Residual Add\nGPU: 8-15", shape=box];
            layer2_norm2 [label="LayerNorm\nGPU: 8-15", shape=box];
            layer2_mlp_fc1 [label="MLP FC1\nGPU: 8-15", shape=box];
            layer2_mlp_fc2 [label="MLP FC2\nGPU: 8-15", shape=box];
            layer2_mlp_allreduce [label="AllReduce\nGPU: 8-15", shape=ellipse];
            layer2_mlp_residual [label="MLP Residual Add\nGPU: 8-15", shape=box];
        }
        
        // Layer 3
        subgraph cluster_layer_3 {
            label="Layer 3 (Pipeline Stage 1)";
            style=dashed;
            
            layer3_norm [label="LayerNorm\nGPU: 8-15", shape=box];
            layer3_q_proj [label="Q Projection\nGPU: 8-15", shape=box];
            layer3_k_proj [label="K Projection\nGPU: 8-15", shape=box];
            layer3_v_proj [label="V Projection\nGPU: 8-15", shape=box];
            layer3_allgather_k [label="AllGather K\nGPU: 8-15", shape=ellipse];
            layer3_allgather_v [label="AllGather V\nGPU: 8-15", shape=ellipse];
            layer3_attention [label="Multi-Head Attention\nGPU: 8-15", shape=box];
            layer3_out_proj [label="Output Projection\nGPU: 8-15", shape=box];
            layer3_allreduce [label="AllReduce\nGPU: 8-15", shape=ellipse];
            layer3_residual [label="Residual Add\nGPU: 8-15", shape=box];
            layer3_norm2 [label="LayerNorm\nGPU: 8-15", shape=box];
            layer3_mlp_fc1 [label="MLP FC1\nGPU: 8-15", shape=box];
            layer3_mlp_fc2 [label="MLP FC2\nGPU: 8-15", shape=box];
            layer3_mlp_allreduce [label="AllReduce\nGPU: 8-15", shape=ellipse];
            layer3_mlp_residual [label="MLP Residual Add\nGPU: 8-15", shape=box];
        }
    }
    
    // Global output
    output [label="Output\nInput: [batch_size=128, seq_len=100000, hidden=4096]\nOutput: [batch_size=128, seq_len=100000, hidden=4096]\nGPU: all GPUs", shape=ellipse];
    
    // Connections
    input -> layer0_norm;
    
    // Layer 0 connections
    layer0_norm -> layer0_q_proj;
    layer0_norm -> layer0_k_proj;
    layer0_norm -> layer0_v_proj;
    layer0_k_proj -> layer0_allgather_k;
    layer0_v_proj -> layer0_allgather_v;
    layer0_q_proj -> layer0_attention;
    layer0_allgather_k -> layer0_attention;
    layer0_allgather_v -> layer0_attention;
    layer0_attention -> layer0_out_proj;
    layer0_out_proj -> layer0_allreduce;
    layer0_allreduce -> layer0_residual;
    input -> layer0_residual [style=dashed];
    layer0_residual -> layer0_norm2;
    layer0_norm2 -> layer0_mlp_fc1;
    layer0_mlp_fc1 -> layer0_mlp_fc2;
    layer0_mlp_fc2 -> layer0_mlp_allreduce;
    layer0_mlp_allreduce -> layer0_mlp_residual;
    layer0_residual -> layer0_mlp_residual [style=dashed];
    
    // Layer 1 connections (simplified)
    layer0_mlp_residual -> layer1_norm;
    layer1_norm -> layer1_q_proj;
    layer1_norm -> layer1_k_proj;
    layer1_norm -> layer1_v_proj;
    layer1_k_proj -> layer1_allgather_k;
    layer1_v_proj -> layer1_allgather_v;
    layer1_q_proj -> layer1_attention;
    layer1_allgather_k -> layer1_attention;
    layer1_allgather_v -> layer1_attention;
    layer1_attention -> layer1_out_proj;
    layer1_out_proj -> layer1_allreduce;
    layer1_allreduce -> layer1_residual;
    layer0_mlp_residual -> layer1_residual [style=dashed];
    layer1_residual -> layer1_norm2;
    layer1_norm2 -> layer1_mlp_fc1;
    layer1_mlp_fc1 -> layer1_mlp_fc2;
    layer1_mlp_fc2 -> layer1_mlp_allreduce;
    layer1_mlp_allreduce -> layer1_mlp_residual;
    layer1_residual -> layer1_mlp_residual [style=dashed];
    
    // Pipeline communication
    layer1_mlp_residual -> pipeline_send_0_1;
    pipeline_send_0_1 -> pipeline_recv_1_0;
    
    // Layer 2 connections (simplified)
    pipeline_recv_1_0 -> layer2_norm;
    layer2_norm -> layer2_q_proj;
    layer2_norm -> layer2_k_proj;
    layer2_norm -> layer2_v_proj;
    layer2_k_proj -> layer2_allgather_k;
    layer2_v_proj -> layer2_allgather_v;
    layer2_q_proj -> layer2_attention;
    layer2_allgather_k -> layer2_attention;
    layer2_allgather_v -> layer2_attention;
    layer2_attention -> layer2_out_proj;
    layer2_out_proj -> layer2_allreduce;
    layer2_allreduce -> layer2_residual;
    pipeline_recv_1_0 -> layer2_residual [style=dashed];
    layer2_residual -> layer2_norm2;
    layer2_norm2 -> layer2_mlp_fc1;
    layer2_mlp_fc1 -> layer2_mlp_fc2;
    layer2_mlp_fc2 -> layer2_mlp_allreduce;
    layer2_mlp_allreduce -> layer2_mlp_residual;
    layer2_residual -> layer2_mlp_residual [style=dashed];
    
    // Layer 3 connections (simplified)
    layer2_mlp_residual -> layer3_norm;
    layer3_norm -> layer3_q_proj;
    layer3_norm -> layer3_k_proj;
    layer3_norm -> layer3_v_proj;
    layer3_k_proj -> layer3_allgather_k;
    layer3_v_proj -> layer3_allgather_v;
    layer3_q_proj -> layer3_attention;
    layer3_allgather_k -> layer3_attention;
    layer3_allgather_v -> layer3_attention;
    layer3_attention -> layer3_out_proj;
    layer3_out_proj -> layer3_allreduce;
    layer3_allreduce -> layer3_residual;
    layer2_mlp_residual -> layer3_residual [style=dashed];
    layer3_residual -> layer3_norm2;
    layer3_norm2 -> layer3_mlp_fc1;
    layer3_mlp_fc1 -> layer3_mlp_fc2;
    layer3_mlp_fc2 -> layer3_mlp_allreduce;
    layer3_mlp_allreduce -> layer3_mlp_residual;
    layer3_residual -> layer3_mlp_residual [style=dashed];
    
    layer3_mlp_residual -> output;
}
