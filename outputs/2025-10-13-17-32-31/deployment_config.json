{
  "model_name": "Transformer_TensorParallel_4GPU",
  "num_gpus": 4,
  "parallel_strategy": "tensor_parallelism",
  "model_config": {
    "hidden_size": 4096,
    "num_heads": 32,
    "head_dim": 128,
    "ffn_hidden_size": 16384,
    "num_layers": 2,
    "vocab_size": 32000,
    "max_seq_len": 2048,
    "batch_size": 8
  },
  "gpu_assignment": {
    "gpu_0": ["attention_qkv_0", "attention_dense_0", "mlp_gate_0", "mlp_up_0", "mlp_down_0"],
    "gpu_1": ["attention_qkv_1", "attention_dense_1", "mlp_gate_1", "mlp_up_1", "mlp_down_1"],
    "gpu_2": ["attention_qkv_2", "attention_dense_2", "mlp_gate_2", "mlp_up_2", "mlp_down_2"],
    "gpu_3": ["attention_qkv_3", "attention_dense_3", "mlp_gate_3", "mlp_up_3", "mlp_down_3"]
  },
  "partitioning": {
    "attention_qkv": {
      "type": "column_parallel",
      "split_dim": 1,
      "parts": 4
    },
    "attention_dense": {
      "type": "row_parallel",
      "split_dim": 0,
      "parts": 4
    },
    "mlp_gate": {
      "type": "column_parallel",
      "split_dim": 0,
      "parts": 4
    },
    "mlp_up": {
      "type": "column_parallel",
      "split_dim": 0,
      "parts": 4
    },
    "mlp_down": {
      "type": "row_parallel",
      "split_dim": 1,
      "parts": 4
    }
  }
}