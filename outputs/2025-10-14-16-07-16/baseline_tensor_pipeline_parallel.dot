digraph baseline_tensor_pipeline {
    rankdir=TB;
    node [shape=box, style=filled, fillcolor=lightblue];
    
    // Input node
    input [label="Input\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs", shape=ellipse, fillcolor=lightgreen];
    
    // Layer 0 - MHA (Devices 0-7)
    subgraph cluster_layer0_mha {
        label="Layer 0 - Multi-Head Attention (Tensor Parallel)";
        style=dashed;
        
        // QKV Linear transformations
        q_linear_0 [label="Q Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 0", shape=rectangle];
        k_linear_0 [label="K Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 1", shape=rectangle];
        v_linear_0 [label="V Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 2", shape=rectangle];
        
        q_linear_1 [label="Q Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 3", shape=rectangle];
        k_linear_1 [label="K Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 4", shape=rectangle];
        v_linear_1 [label="V Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 5", shape=rectangle];
        
        q_linear_2 [label="Q Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 6", shape=rectangle];
        k_linear_2 [label="K Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 7", shape=rectangle];
        v_linear_2 [label="V Linear\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 0", shape=rectangle];
        
        // Attention computation
        attn_0 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 0", shape=rectangle];
        attn_1 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 1", shape=rectangle];
        attn_2 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 2", shape=rectangle];
        attn_3 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 3", shape=rectangle];
        attn_4 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 4", shape=rectangle];
        attn_5 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 5", shape=rectangle];
        attn_6 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 6", shape=rectangle];
        attn_7 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 7", shape=rectangle];
        
        // Output linear transformations
        out_linear_0 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 0", shape=rectangle];
        out_linear_1 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 1", shape=rectangle];
        out_linear_2 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 2", shape=rectangle];
        out_linear_3 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 3", shape=rectangle];
        out_linear_4 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 4", shape=rectangle];
        out_linear_5 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 5", shape=rectangle];
        out_linear_6 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 6", shape=rectangle];
        out_linear_7 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 7", shape=rectangle];
        
        // All-reduce for attention output
        attn_allreduce [label="All-Reduce\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 0-7", shape=ellipse, fillcolor=yellow];
        
        // Residual add and layer norm
        attn_residual [label="Residual Add + LayerNorm\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 0-7", shape=parallelogram, fillcolor=lightcoral];
    }
    
    // Layer 0 - MLP (Devices 0-7)
    subgraph cluster_layer0_mlp {
        label="Layer 0 - MLP (Tensor Parallel)";
        style=dashed;
        
        // First linear (column parallel)
        mlp_linear1_0 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 0", shape=rectangle];
        mlp_linear1_1 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 1", shape=rectangle];
        mlp_linear1_2 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 2", shape=rectangle];
        mlp_linear1_3 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 3", shape=rectangle];
        mlp_linear1_4 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 4", shape=rectangle];
        mlp_linear1_5 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 5", shape=rectangle];
        mlp_linear1_6 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 6", shape=rectangle];
        mlp_linear1_7 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 7", shape=rectangle];
        
        // GELU activation
        gelu_0 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 0", shape=rectangle];
        gelu_1 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 1", shape=rectangle];
        gelu_2 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 2", shape=rectangle];
        gelu_3 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 3", shape=rectangle];
        gelu_4 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 4", shape=rectangle];
        gelu_5 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 5", shape=rectangle];
        gelu_6 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 6", shape=rectangle];
        gelu_7 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 4096]\nGPU: 7", shape=rectangle];
        
        // Second linear (row parallel)
        mlp_linear2_0 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 0", shape=rectangle];
        mlp_linear2_1 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 1", shape=rectangle];
        mlp_linear2_2 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 2", shape=rectangle];
        mlp_linear2_3 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 3", shape=rectangle];
        mlp_linear2_4 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 4", shape=rectangle];
        mlp_linear2_5 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 5", shape=rectangle];
        mlp_linear2_6 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 6", shape=rectangle];
        mlp_linear2_7 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 4096]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 7", shape=rectangle];
        
        // All-reduce for MLP output
        mlp_allreduce [label="All-Reduce\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 0-7", shape=ellipse, fillcolor=yellow];
        
        // Residual add and layer norm
        mlp_residual [label="Residual Add + LayerNorm\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 0-7", shape=parallelogram, fillcolor=lightcoral];
    }
    
    // Pipeline communication
    pipeline_send [label="Pipeline Send\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: 0-7 → 8-15", shape=ellipse, fillcolor=orange];
    
    // Layer 1 - MHA (Devices 8-15)
    subgraph cluster_layer1_mha {
        label="Layer 1 - Multi-Head Attention (Tensor Parallel)";
        style=dashed;
        
        // Similar to layer 0 but on devices 8-15
        q_linear_8 [label="Q Linear\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 8", shape=rectangle];
        k_linear_8 [label="K Linear\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 9", shape=rectangle];
        v_linear_8 [label="V Linear\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 10", shape=rectangle];
        
        // ... (similar for devices 11-15)
        attn_8 [label="Attention\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 8", shape=rectangle];
        out_linear_8 [label="Output Linear\nInput: [batch_size=1024, seq_len=10000, 1024]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 8", shape=rectangle];
        
        attn_allreduce_1 [label="All-Reduce\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 8-15", shape=ellipse, fillcolor=yellow];
        attn_residual_1 [label="Residual Add + LayerNorm\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 8-15", shape=parallelogram, fillcolor=lightcoral];
    }
    
    // Layer 1 - MLP (Devices 8-15)
    subgraph cluster_layer1_mlp {
        label="Layer 1 - MLP (Tensor Parallel)";
        style=dashed;
        
        mlp_linear1_8 [label="MLP Linear 1\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 2048]\nGPU: 8", shape=rectangle];
        gelu_8 [label="GELU Activation\nInput: [batch_size=1024, seq_len=10000, 2048]\nOutput: [batch_size=1024, seq_len=10000, 2048]\nGPU: 8", shape=rectangle];
        mlp_linear2_8 [label="MLP Linear 2\nInput: [batch_size=1024, seq_len=10000, 2048]\nOutput: [batch_size=1024, seq_len=10000, 1024]\nGPU: 8", shape=rectangle];
        
        mlp_allreduce_1 [label="All-Reduce\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 8-15", shape=ellipse, fillcolor=yellow];
        mlp_residual_1 [label="Residual Add + LayerNorm\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs 8-15", shape=parallelogram, fillcolor=lightcoral];
    }
    
    // Output node
    output [label="Output\nInput: [batch_size=1024, seq_len=10000, 8192]\nOutput: [batch_size=1024, seq_len=10000, 8192]\nGPU: all GPUs", shape=ellipse, fillcolor=lightgreen];
    
    // Connections for Layer 0 MHA
    input -> q_linear_0 [label="broadcast"];
    input -> k_linear_0 [label="broadcast"];
    input -> v_linear_0 [label="broadcast"];
    input -> q_linear_1 [label="broadcast"];
    input -> k_linear_1 [label="broadcast"];
    input -> v_linear_1 [label="broadcast"];
    input -> q_linear_2 [label="broadcast"];
    input -> k_linear_2 [label="broadcast"];
    input -> v_linear_2 [label="broadcast"];
    
    q_linear_0 -> attn_0;
    k_linear_0 -> attn_0;
    v_linear_0 -> attn_0;
    q_linear_1 -> attn_1;
    k_linear_1 -> attn_1;
    v_linear_1 -> attn_1;
    q_linear_2 -> attn_2;
    k_linear_2 -> attn_2;
    v_linear_2 -> attn_2;
    
    attn_0 -> out_linear_0;
    attn_1 -> out_linear_1;
    attn_2 -> out_linear_2;
    
    out_linear_0 -> attn_allreduce;
    out_linear_1 -> attn_allreduce;
    out_linear_2 -> attn_allreduce;
    out_linear_3 -> attn_allreduce;
    out_linear_4 -> attn_allreduce;
    out_linear_5 -> attn_allreduce;
    out_linear_6 -> attn_allreduce;
    out_linear_7 -> attn_allreduce;
    
    attn_allreduce -> attn_residual;
    input -> attn_residual [style=dashed, label="residual"];
    
    // Connections for Layer 0 MLP
    attn_residual -> mlp_linear1_0 [label="broadcast"];
    attn_residual -> mlp_linear1_1 [label="broadcast"];
    attn_residual -> mlp_linear1_2 [label="broadcast"];
    attn_residual -> mlp_linear1_3 [label="broadcast"];
    attn_residual -> mlp_linear1_4 [label="broadcast"];
    attn_residual -> mlp_linear1_5 [label="broadcast"];
    attn_residual -> mlp_linear1_6 [label="broadcast"];
    attn_residual -> mlp_linear1_7 [label="broadcast"];
    
    mlp_linear1_0 -> gelu_0;
    mlp_linear1_1 -> gelu_1;
    mlp_linear1_2 -> gelu_2;
    mlp_linear1_3 -> gelu_3;
    mlp_linear1_4 -> gelu_4;
    mlp_linear1_5 -> gelu_5;
    mlp_linear1_6 -> gelu_6;
    mlp_linear1_7 -> gelu_7;
    
    gelu_0 -> mlp_linear2_0;
    gelu_1 -> mlp_linear2_1;
    gelu_2 -> mlp_linear2_2;
    gelu_3 -> mlp_linear2_3;
    gelu_4 -> mlp_linear2_4;
    gelu_5 -> mlp_linear2_5;
    gelu_6 -> mlp_linear2_6;
    gelu_7 -> mlp_linear2_7;
    
    mlp_linear2_0 -> mlp_allreduce;
    mlp_linear2_1 -> mlp_allreduce;
    mlp_linear2_2 -> mlp_allreduce;
    mlp_linear2_3 -> mlp_allreduce;
    mlp_linear2_4 -> mlp_allreduce;
    mlp_linear2_5 -> mlp_allreduce;
    mlp_linear2_6 -> mlp_allreduce;
    mlp_linear2_7 -> mlp_allreduce;
    
    mlp_allreduce -> mlp_residual;
    attn_residual -> mlp_residual [style=dashed, label="residual"];
    
    // Pipeline communication
    mlp_residual -> pipeline_send;
    
    // Simplified Layer 1 connections (similar structure)
    pipeline_send -> q_linear_8 [label="pipeline receive"];
    pipeline_send -> k_linear_8 [label="pipeline receive"];
    pipeline_send -> v_linear_8 [label="pipeline receive"];
    
    q_linear_8 -> attn_8;
    k_linear_8 -> attn_8;
    v_linear_8 -> attn_8;
    attn_8 -> out_linear_8;
    out_linear_8 -> attn_allreduce_1;
    attn_allreduce_1 -> attn_residual_1;
    pipeline_send -> attn_residual_1 [style=dashed, label="residual"];
    
    attn_residual_1 -> mlp_linear1_8;
    mlp_linear1_8 -> gelu_8;
    gelu_8 -> mlp_linear2_8;
    mlp_linear2_8 -> mlp_allreduce_1;
    mlp_allreduce_1 -> mlp_residual_1;
    attn_residual_1 -> mlp_residual_1 [style=dashed, label="residual"];
    
    mlp_residual_1 -> output;
}