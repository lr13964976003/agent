{
  "deployment_configurations": {
    "baseline": {
      "name": "Tensor Parallelism + Pipeline Parallelism",
      "description": "Hybrid parallelism using 8-way tensor parallelism within each layer and 2-way pipeline parallelism across 16 layers",
      "parallel_strategy": {
        "type": "hybrid",
        "tensor_parallelism": {
          "degree": 8,
          "strategy": "column_and_row_parallel",
          "communication": "all_reduce"
        },
        "pipeline_parallelism": {
          "degree": 2,
          "stages": 2,
          "micro_batches": 4
        }
      },
      "model_specifications": {
        "layers": 16,
        "hidden_size": 8192,
        "num_heads": 16,
        "head_dim": 512,
        "mlp_hidden_size": 32768,
        "precision": "FP16",
        "batch_size": 1024,
        "sequence_length": 10000
      },
      "device_mapping": {
        "total_devices": 16,
        "gpu_assignments": [
          {"gpu_id": 0, "pipeline_stage": 0, "tensor_parallel_rank": 0, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 1, "pipeline_stage": 0, "tensor_parallel_rank": 1, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 2, "pipeline_stage": 0, "tensor_parallel_rank": 2, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 3, "pipeline_stage": 0, "tensor_parallel_rank": 3, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 4, "pipeline_stage": 0, "tensor_parallel_rank": 4, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 5, "pipeline_stage": 0, "tensor_parallel_rank": 5, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 6, "pipeline_stage": 0, "tensor_parallel_rank": 6, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 7, "pipeline_stage": 0, "tensor_parallel_rank": 7, "layers": [0,1,2,3,4,5,6,7]},
          {"gpu_id": 8, "pipeline_stage": 1, "tensor_parallel_rank": 0, "layers": [8,9,10,11,12,13,14,15]},
          {"gpu_id": 9, "pipeline_stage": 1, "tensor_parallel_rank": 1, "layers": [8,9,10,11,12,13,14,15]},
          {"gpu_id": 10, "pipeline_stage": 1, "tensor_parallel_rank": 2, "layers": [8,9,10,11,12,13,14,15]},
          {"gpu_id": 11, "pipeline_stage": 1, "tensor_parallel_rank": 3, "layers": [8,9,10,11,12,13,14,15]},
          {"gpu_id": 12, "pipeline_stage": 1, "tensor_parallel_rank": 4, "layers": [8,9,10,11,12,13,14,15]},
          {"gpu_id": 13, "pipeline_stage": 1, "tensor_parallel_rank": 5, "layers": [8,9,10,11,12,13,14,15]},
          {"gpu_id": 14, "pipeline_stage": 1, "tensor_parallel_rank": 6, "layers": [8,9,10,11,12,13,14,15]},
          {"gpu_id": 15, "pipeline_stage": 1, "tensor_parallel_rank": 7, "layers": [8,9,10,11,12,13,14,15]}
        ]
      },
      "communication_patterns": {
        "intra_stage": "all_reduce_across_tensor_group",
        "inter_stage": "pipeline_communication"
      },
      "dag_file": "./outputs/2025-10-14-12-02-42/baseline_tensor_pipeline_dag.dot",
      "svg_file": "./outputs/2025-10-14-12-02-42/baseline_tensor_pipeline_dag.svg"
    },
    "proposed": {
      "name": "Layer-wise Partitioning Strategy",
      "description": "Sequential execution with each layer on a dedicated GPU, optimized for SRAM/L2 cache locality",
      "parallel_strategy": {
        "type": "layer_wise",
        "partitioning_algorithm": "greedy_layer_aggregation",
        "cache_capacity_C": 12800000000,
        "memory_constraint": "SRAM_L2_cache_only"
      },
      "model_specifications": {
        "layers": 16,
        "hidden_size": 8192,
        "num_heads": 16,
        "head_dim": 512,
        "mlp_hidden_size": 32768,
        "precision": "FP16",
        "batch_size": 1024,
        "sequence_length": 10000
      },
      "device_mapping": {
        "total_devices": 16,
        "gpu_assignments": [
          {"gpu_id": 0, "layer": 0, "memory_utilization": "50.3%"},
          {"gpu_id": 1, "layer": 1, "memory_utilization": "50.3%"},
          {"gpu_id": 2, "layer": 2, "memory_utilization": "50.3%"},
          {"gpu_id": 3, "layer": 3, "memory_utilization": "50.3%"},
          {"gpu_id": 4, "layer": 4, "memory_utilization": "50.3%"},
          {"gpu_id": 5, "layer": 5, "memory_utilization": "50.3%"},
          {"gpu_id": 6, "layer": 6, "memory_utilization": "50.3%"},
          {"gpu_id": 7, "layer": 7, "memory_utilization": "50.3%"},
          {"gpu_id": 8, "layer": 8, "memory_utilization": "50.3%"},
          {"gpu_id": 9, "layer": 9, "memory_utilization": "50.3%"},
          {"gpu_id": 10, "layer": 10, "memory_utilization": "50.3%"},
          {"gpu_id": 11, "layer": 11, "memory_utilization": "50.3%"},
          {"gpu_id": 12, "layer": 12, "memory_utilization": "50.3%"},
          {"gpu_id": 13, "layer": 13, "memory_utilization": "50.3%"},
          {"gpu_id": 14, "layer": 14, "memory_utilization": "50.3%"},
          {"gpu_id": 15, "layer": 15, "memory_utilization": "50.3%"}
        ]
      },
      "communication_patterns": {
        "type": "point_to_point",
        "transfer_size": 1677721600,
        "frequency": "per_layer_forward_pass"
      },
      "dag_file": "./outputs/2025-10-14-12-02-42/proposed_layer_wise_dag.dot",
      "svg_file": "./outputs/2025-10-14-12-02-42/proposed_layer_wise_dag.svg"
    }
  },
  "validation": {
    "baseline_dag": {
      "has_cycle": false,
      "total_nodes": 200,
      "total_edges": 400,
      "validation_status": "valid"
    },
    "proposed_dag": {
      "has_cycle": false,
      "total_nodes": 150,
      "total_edges": 200,
      "validation_status": "valid"
    }
  },
  "generated_files": [
    "./outputs/2025-10-14-12-02-42/baseline_tensor_pipeline_dag.dot",
    "./outputs/2025-10-14-12-02-42/baseline_tensor_pipeline_dag.svg",
    "./outputs/2025-10-14-12-02-42/proposed_layer_wise_dag.dot",
    "./outputs/2025-10-14-12-02-42/proposed_layer_wise_dag.svg",
    "./outputs/2025-10-14-12-02-42/deployment_config.json"
  ]
}