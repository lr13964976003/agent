digraph baseline_tensor_pipeline {
    rankdir=TB;
    node [shape=rectangle];
    
    // Input node
    Input [shape=ellipse, label="Input\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
    
    // Pipeline Stage 0 (Layers 0-7)
    subgraph cluster_stage0 {
        label="Pipeline Stage 0 (Layers 0-7)\nGPUs 0-7";
        style=dashed;
        
        // Layer 0
        Layer0_QKV_Split [shape=parallelogram, label="Split\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: all GPUs"];
        
        Layer0_Q_Linear0 [label="Q Linear 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 0"];
        Layer0_Q_Linear1 [label="Q Linear 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 1"];
        Layer0_Q_Linear2 [label="Q Linear 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 2"];
        Layer0_Q_Linear3 [label="Q Linear 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 3"];
        Layer0_Q_Linear4 [label="Q Linear 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 4"];
        Layer0_Q_Linear5 [label="Q Linear 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 5"];
        Layer0_Q_Linear6 [label="Q Linear 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 6"];
        Layer0_Q_Linear7 [label="Q Linear 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 7"];
        
        Layer0_K_Linear0 [label="K Linear 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 0"];
        Layer0_K_Linear1 [label="K Linear 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 1"];
        Layer0_K_Linear2 [label="K Linear 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 2"];
        Layer0_K_Linear3 [label="K Linear 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 3"];
        Layer0_K_Linear4 [label="K Linear 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 4"];
        Layer0_K_Linear5 [label="K Linear 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 5"];
        Layer0_K_Linear6 [label="K Linear 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 6"];
        Layer0_K_Linear7 [label="K Linear 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 7"];
        
        Layer0_V_Linear0 [label="V Linear 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 0"];
        Layer0_V_Linear1 [label="V Linear 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 1"];
        Layer0_V_Linear2 [label="V Linear 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 2"];
        Layer0_V_Linear3 [label="V Linear 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 3"];
        Layer0_V_Linear4 [label="V Linear 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 4"];
        Layer0_V_Linear5 [label="V Linear 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 5"];
        Layer0_V_Linear6 [label="V Linear 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 6"];
        Layer0_V_Linear7 [label="V Linear 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 7"];
        
        Layer0_QKV_AllGather [shape=parallelogram, label="All-Gather\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]×8\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
        
        // Attention computation
        Layer0_Attention_Score [label="Attention Score\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]\nGPU: all GPUs"];
        Layer0_Attention_Softmax [label="Attention Softmax\nInput: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]\nOutput: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]\nGPU: all GPUs"];
        Layer0_Attention_Output [label="Attention Output\nInput: [batch_size=1024, num_heads=16, seq_len=10000, seq_len=10000]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
        
        // MLP
        Layer0_MLP_Linear1_Split [shape=parallelogram, label="Split\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: all GPUs"];
        
        Layer0_MLP_Linear1_0 [label="MLP Linear1 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 0"];
        Layer0_MLP_Linear1_1 [label="MLP Linear1 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 1"];
        Layer0_MLP_Linear1_2 [label="MLP Linear1 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 2"];
        Layer0_MLP_Linear1_3 [label="MLP Linear1 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 3"];
        Layer0_MLP_Linear1_4 [label="MLP Linear1 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 4"];
        Layer0_MLP_Linear1_5 [label="MLP Linear1 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 5"];
        Layer0_MLP_Linear1_6 [label="MLP Linear1 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 6"];
        Layer0_MLP_Linear1_7 [label="MLP Linear1 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 7"];
        
        Layer0_MLP_GELU_0 [label="GELU 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 0"];
        Layer0_MLP_GELU_1 [label="GELU 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 1"];
        Layer0_MLP_GELU_2 [label="GELU 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 2"];
        Layer0_MLP_GELU_3 [label="GELU 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 3"];
        Layer0_MLP_GELU_4 [label="GELU 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 4"];
        Layer0_MLP_GELU_5 [label="GELU 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 5"];
        Layer0_MLP_GELU_6 [label="GELU 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 6"];
        Layer0_MLP_GELU_7 [label="GELU 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nGPU: 7"];
        
        Layer0_MLP_Linear2_0 [label="MLP Linear2 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 0"];
        Layer0_MLP_Linear2_1 [label="MLP Linear2 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 1"];
        Layer0_MLP_Linear2_2 [label="MLP Linear2 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 2"];
        Layer0_MLP_Linear2_3 [label="MLP Linear2 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 3"];
        Layer0_MLP_Linear2_4 [label="MLP Linear2 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 4"];
        Layer0_MLP_Linear2_5 [label="MLP Linear2 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 5"];
        Layer0_MLP_Linear2_6 [label="MLP Linear2 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 6"];
        Layer0_MLP_Linear2_7 [label="MLP Linear2 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=4096]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 7"];
        
        Layer0_MLP_AllReduce [shape=parallelogram, label="All-Reduce Sum\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]×8\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
        
        Layer0_Add [label="Residual Add\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]×2\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
        
        // Layer 1 (similar structure)
        Layer1_QKV_Split [shape=parallelogram, label="Split\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: all GPUs"];
        
        Layer1_Q_Linear0 [label="Q Linear 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 0"];
        Layer1_Q_Linear1 [label="Q Linear 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 1"];
        Layer1_Q_Linear2 [label="Q Linear 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 2"];
        Layer1_Q_Linear3 [label="Q Linear 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 3"];
        Layer1_Q_Linear4 [label="Q Linear 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 4"];
        Layer1_Q_Linear5 [label="Q Linear 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 5"];
        Layer1_Q_Linear6 [label="Q Linear 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 6"];
        Layer1_Q_Linear7 [label="Q Linear 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 7"];
        
        // ... [similar structure for Layer1 K, V, Attention, MLP] ...
        
        Layer1_MLP_AllReduce [shape=parallelogram, label="All-Reduce Sum\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]×8\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
        
        Layer1_Add [label="Residual Add\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]×2\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
    }
    
    // Pipeline Stage 1 (Layers 8-15)
    subgraph cluster_stage1 {
        label="Pipeline Stage 1 (Layers 8-15)\nGPUs 8-15";
        style=dashed;
        
        // Layer 8 (similar to Layer 0)
        Layer8_QKV_Split [shape=parallelogram, label="Split\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: all GPUs"];
        
        Layer8_Q_Linear0 [label="Q Linear 0\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 8"];
        Layer8_Q_Linear1 [label="Q Linear 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 9"];
        Layer8_Q_Linear2 [label="Q Linear 2\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 10"];
        Layer8_Q_Linear3 [label="Q Linear 3\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 11"];
        Layer8_Q_Linear4 [label="Q Linear 4\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 12"];
        Layer8_Q_Linear5 [label="Q Linear 5\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 13"];
        Layer8_Q_Linear6 [label="Q Linear 6\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 14"];
        Layer8_Q_Linear7 [label="Q Linear 7\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=1024]\nGPU: 15"];
        
        // ... [similar structure for Layer8 K, V, Attention, MLP] ...
        
        Layer15_MLP_AllReduce [shape=parallelogram, label="All-Reduce Sum\nInput: [batch_size=1024, seq_len=10000, hidden_size=1024]×8\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
        
        Layer15_Add [label="Residual Add\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]×2\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
    }
    
    // Output node
    Output [shape=ellipse, label="Output\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: all GPUs"];
    
    // Pipeline communication
    Pipeline_Comm_Stage0_1 [shape=ellipse, style=dashed, label="Pipeline Communication\nStage 0 → Stage 1\nInput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nOutput: [batch_size=1024, seq_len=10000, hidden_size=8192]\nGPU: 7→8"];
    
    // Connections for Layer 0
    Input -> Layer0_QKV_Split;
    Layer0_QKV_Split -> Layer0_Q_Linear0;
    Layer0_QKV_Split -> Layer0_Q_Linear1;
    Layer0_QKV_Split -> Layer0_Q_Linear2;
    Layer0_QKV_Split -> Layer0_Q_Linear3;
    Layer0_QKV_Split -> Layer0_Q_Linear4;
    Layer0_QKV_Split -> Layer0_Q_Linear5;
    Layer0_QKV_Split -> Layer0_Q_Linear6;
    Layer0_QKV_Split -> Layer0_Q_Linear7;
    
    Layer0_QKV_Split -> Layer0_K_Linear0;
    Layer0_QKV_Split -> Layer0_K_Linear1;
    Layer0_QKV_Split -> Layer0_K_Linear2;
    Layer0_QKV_Split -> Layer0_K_Linear3;
    Layer0_QKV_Split -> Layer0_K_Linear4;
    Layer0_QKV_Split -> Layer0_K_Linear5;
    Layer0_QKV_Split -> Layer0_K_Linear6;
    Layer0_QKV_Split -> Layer0_K_Linear7;
    
    Layer0_QKV_Split -> Layer0_V_Linear0;
    Layer0_QKV_Split -> Layer0_V_Linear1;
    Layer0_QKV_Split -> Layer0_V_Linear2;
    Layer0_QKV_Split -> Layer0_V_Linear3;
    Layer0_QKV_Split -> Layer0_V_Linear4;
    Layer0_QKV_Split -> Layer0_V_Linear5;
    Layer0_QKV_Split -> Layer0_V_Linear6;
    Layer0_QKV_Split -> Layer0_V_Linear7;
    
    Layer0_Q_Linear0 -> Layer0_QKV_AllGather;
    Layer0_Q_Linear1 -> Layer0_QKV_AllGather;
    Layer0_Q_Linear2 -> Layer0_QKV_AllGather;
    Layer0_Q_Linear3 -> Layer0_QKV_AllGather;
    Layer0_Q_Linear4 -> Layer0_QKV_AllGather;
    Layer0_Q_Linear5 -> Layer0_QKV_AllGather;
    Layer0_Q_Linear6 -> Layer0_QKV_AllGather;
    Layer0_Q_Linear7 -> Layer0_QKV_AllGather;
    
    Layer0_K_Linear0 -> Layer0_QKV_AllGather;
    Layer0_K_Linear1 -> Layer0_QKV_AllGather;
    Layer0_K_Linear2 -> Layer0_QKV_AllGather;
    Layer0_K_Linear3 -> Layer0_QKV_AllGather;
    Layer0_K_Linear4 -> Layer0_QKV_AllGather;
    Layer0_K_Linear5 -> Layer0_QKV_AllGather;
    Layer0_K_Linear6 -> Layer0_QKV_AllGather;
    Layer0_K_Linear7 -> Layer0_QKV_AllGather;
    
    Layer0_V_Linear0 -> Layer0_QKV_AllGather;
    Layer0_V_Linear1 -> Layer0_QKV_AllGather;
    Layer0_V_Linear2 -> Layer0_QKV_AllGather;
    Layer0_V_Linear3 -> Layer0_QKV_AllGather;
    Layer0_V_Linear4 -> Layer0_QKV_AllGather;
    Layer0_V_Linear5 -> Layer0_QKV_AllGather;
    Layer0_V_Linear6 -> Layer0_QKV_AllGather;
    Layer0_V_Linear7 -> Layer0_QKV_AllGather;
    
    Layer0_QKV_AllGather -> Layer0_Attention_Score;
    Layer0_Attention_Score -> Layer0_Attention_Softmax;
    Layer0_Attention_Softmax -> Layer0_Attention_Output;
    Layer0_Attention_Output -> Layer0_MLP_Linear1_Split;
    
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_0;
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_1;
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_2;
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_3;
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_4;
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_5;
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_6;
    Layer0_MLP_Linear1_Split -> Layer0_MLP_Linear1_7;
    
    Layer0_MLP_Linear1_0 -> Layer0_MLP_GELU_0;
    Layer0_MLP_Linear1_1 -> Layer0_MLP_GELU_1;
    Layer0_MLP_Linear1_2 -> Layer0_MLP_GELU_2;
    Layer0_MLP_Linear1_3 -> Layer0_MLP_GELU_3;
    Layer0_MLP_Linear1_4 -> Layer0_MLP_GELU_4;
    Layer0_MLP_Linear1_5 -> Layer0_MLP_GELU_5;
    Layer0_MLP_Linear1_6 -> Layer0_MLP_GELU_6;
    Layer0_MLP_Linear1_7 -> Layer0_MLP_GELU_7;
    
    Layer0_MLP_GELU_0 -> Layer0_MLP_Linear2_0;
    Layer0_MLP_GELU_1 -> Layer0_MLP_Linear2_1;
    Layer0_MLP_GELU_2 -> Layer0_MLP_Linear2_2;
    Layer0_MLP_GELU_3 -> Layer0_MLP_Linear2_3;
    Layer0_MLP_GELU_4 -> Layer0_MLP_Linear2_4;
    Layer0_MLP_GELU_5 -> Layer0_MLP_Linear2_5;
    Layer0_MLP_GELU_6 -> Layer0_MLP_Linear2_6;
    Layer0_MLP_GELU_7 -> Layer0_MLP_Linear2_7;
    
    Layer0_MLP_Linear2_0 -> Layer0_MLP_AllReduce;
    Layer0_MLP_Linear2_1 -> Layer0_MLP_AllReduce;
    Layer0_MLP_Linear2_2 -> Layer0_MLP_AllReduce;
    Layer0_MLP_Linear2_3 -> Layer0_MLP_AllReduce;
    Layer0_MLP_Linear2_4 -> Layer0_MLP_AllReduce;
    Layer0_MLP_Linear2_5 -> Layer0_MLP_AllReduce;
    Layer0_MLP_Linear2_6 -> Layer0_MLP_AllReduce;
    Layer0_MLP_Linear2_7 -> Layer0_MLP_AllReduce;
    
    Layer0_MLP_AllReduce -> Layer0_Add;
    Layer0_Add -> Layer1_QKV_Split;
    
    // Simplified connections for remaining layers (showing only key nodes)
    Layer1_Add -> Pipeline_Comm_Stage0_1;
    Pipeline_Comm_Stage0_1 -> Layer8_QKV_Split;
    Layer15_Add -> Output;
}