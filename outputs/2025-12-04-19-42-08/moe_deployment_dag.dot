
digraph MoE_Deployment_EP8_TP4_PP4 {
    rankdir=TB;
    bgcolor=white;
    node [shape=box, style=filled, fillcolor=lightblue];
    edge [color=black, arrowhead=normal];
    
    // Graph attributes
    graph [fontname="Arial", fontsize=12, ranksep=1.2, nodesep=0.8];
    node [fontname="Arial", fontsize=10];
    edge [fontname="Arial", fontsize=9];
    
    // Define node shapes
    node [shape=box]; // Computation nodes (default)
    
    // Input node
    input [shape=ellipse, label="Input\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", fillcolor=lightgreen];
    
    // EP Groups - 8 groups, each with 16 GPUs
    
    // EP Group 0 - GPUs [0-15]
    subgraph cluster_ep0 {
        label="EP Group 0\nGPUs [0-15]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [0-3]
        subgraph cluster_pp0_0 {
            label="PP Stage 0\nGPUs [0-3]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu0_layer0 [label="Attention L0 GPU0\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu0_layer0 [label="Gate L0 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu0_layer0_exp0 [label="Expert 0 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer0_exp1 [label="Expert 1 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer0_exp2 [label="Expert 2 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer0_exp3 [label="Expert 3 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer0_exp4 [label="Expert 4 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer0_exp5 [label="Expert 5 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer0_exp6 [label="Expert 6 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer0_exp7 [label="Expert 7 L0 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu0_layer0 [label="MLP L0 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu0_layer1 [label="Attention L1 GPU0\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu0_layer1 [label="Gate L1 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu0_layer1_exp0 [label="Expert 0 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer1_exp1 [label="Expert 1 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer1_exp2 [label="Expert 2 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer1_exp3 [label="Expert 3 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer1_exp4 [label="Expert 4 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer1_exp5 [label="Expert 5 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer1_exp6 [label="Expert 6 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer1_exp7 [label="Expert 7 L1 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu0_layer1 [label="MLP L1 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu0_layer2 [label="Attention L2 GPU0\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu0_layer2 [label="Gate L2 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu0_layer2_exp0 [label="Expert 0 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer2_exp1 [label="Expert 1 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer2_exp2 [label="Expert 2 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer2_exp3 [label="Expert 3 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer2_exp4 [label="Expert 4 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer2_exp5 [label="Expert 5 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer2_exp6 [label="Expert 6 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer2_exp7 [label="Expert 7 L2 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu0_layer2 [label="MLP L2 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu0_layer3 [label="Attention L3 GPU0\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu0_layer3 [label="Gate L3 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu0_layer3_exp0 [label="Expert 0 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer3_exp1 [label="Expert 1 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer3_exp2 [label="Expert 2 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer3_exp3 [label="Expert 3 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer3_exp4 [label="Expert 4 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer3_exp5 [label="Expert 5 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer3_exp6 [label="Expert 6 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu0_layer3_exp7 [label="Expert 7 L3 GPU0\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu0_layer3 [label="MLP L3 GPU0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu1_layer0 [label="Attention L0 GPU1\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu1_layer0 [label="Gate L0 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu1_layer0_exp0 [label="Expert 0 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer0_exp1 [label="Expert 1 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer0_exp2 [label="Expert 2 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer0_exp3 [label="Expert 3 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer0_exp4 [label="Expert 4 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer0_exp5 [label="Expert 5 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer0_exp6 [label="Expert 6 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer0_exp7 [label="Expert 7 L0 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu1_layer0 [label="MLP L0 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu1_layer1 [label="Attention L1 GPU1\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu1_layer1 [label="Gate L1 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu1_layer1_exp0 [label="Expert 0 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer1_exp1 [label="Expert 1 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer1_exp2 [label="Expert 2 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer1_exp3 [label="Expert 3 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer1_exp4 [label="Expert 4 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer1_exp5 [label="Expert 5 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer1_exp6 [label="Expert 6 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer1_exp7 [label="Expert 7 L1 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu1_layer1 [label="MLP L1 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu1_layer2 [label="Attention L2 GPU1\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu1_layer2 [label="Gate L2 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu1_layer2_exp0 [label="Expert 0 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer2_exp1 [label="Expert 1 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer2_exp2 [label="Expert 2 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer2_exp3 [label="Expert 3 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer2_exp4 [label="Expert 4 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer2_exp5 [label="Expert 5 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer2_exp6 [label="Expert 6 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer2_exp7 [label="Expert 7 L2 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu1_layer2 [label="MLP L2 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu1_layer3 [label="Attention L3 GPU1\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu1_layer3 [label="Gate L3 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu1_layer3_exp0 [label="Expert 0 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer3_exp1 [label="Expert 1 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer3_exp2 [label="Expert 2 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer3_exp3 [label="Expert 3 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer3_exp4 [label="Expert 4 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer3_exp5 [label="Expert 5 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer3_exp6 [label="Expert 6 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu1_layer3_exp7 [label="Expert 7 L3 GPU1\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu1_layer3 [label="MLP L3 GPU1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu2_layer0 [label="Attention L0 GPU2\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu2_layer0 [label="Gate L0 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu2_layer0_exp0 [label="Expert 0 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer0_exp1 [label="Expert 1 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer0_exp2 [label="Expert 2 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer0_exp3 [label="Expert 3 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer0_exp4 [label="Expert 4 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer0_exp5 [label="Expert 5 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer0_exp6 [label="Expert 6 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer0_exp7 [label="Expert 7 L0 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu2_layer0 [label="MLP L0 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu2_layer1 [label="Attention L1 GPU2\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu2_layer1 [label="Gate L1 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu2_layer1_exp0 [label="Expert 0 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer1_exp1 [label="Expert 1 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer1_exp2 [label="Expert 2 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer1_exp3 [label="Expert 3 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer1_exp4 [label="Expert 4 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer1_exp5 [label="Expert 5 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer1_exp6 [label="Expert 6 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer1_exp7 [label="Expert 7 L1 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu2_layer1 [label="MLP L1 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu2_layer2 [label="Attention L2 GPU2\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu2_layer2 [label="Gate L2 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu2_layer2_exp0 [label="Expert 0 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer2_exp1 [label="Expert 1 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer2_exp2 [label="Expert 2 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer2_exp3 [label="Expert 3 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer2_exp4 [label="Expert 4 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer2_exp5 [label="Expert 5 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer2_exp6 [label="Expert 6 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer2_exp7 [label="Expert 7 L2 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu2_layer2 [label="MLP L2 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu2_layer3 [label="Attention L3 GPU2\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu2_layer3 [label="Gate L3 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu2_layer3_exp0 [label="Expert 0 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer3_exp1 [label="Expert 1 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer3_exp2 [label="Expert 2 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer3_exp3 [label="Expert 3 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer3_exp4 [label="Expert 4 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer3_exp5 [label="Expert 5 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer3_exp6 [label="Expert 6 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu2_layer3_exp7 [label="Expert 7 L3 GPU2\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu2_layer3 [label="MLP L3 GPU2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu3_layer0 [label="Attention L0 GPU3\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu3_layer0 [label="Gate L0 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu3_layer0_exp0 [label="Expert 0 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer0_exp1 [label="Expert 1 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer0_exp2 [label="Expert 2 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer0_exp3 [label="Expert 3 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer0_exp4 [label="Expert 4 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer0_exp5 [label="Expert 5 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer0_exp6 [label="Expert 6 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer0_exp7 [label="Expert 7 L0 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu3_layer0 [label="MLP L0 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu3_layer1 [label="Attention L1 GPU3\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu3_layer1 [label="Gate L1 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu3_layer1_exp0 [label="Expert 0 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer1_exp1 [label="Expert 1 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer1_exp2 [label="Expert 2 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer1_exp3 [label="Expert 3 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer1_exp4 [label="Expert 4 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer1_exp5 [label="Expert 5 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer1_exp6 [label="Expert 6 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer1_exp7 [label="Expert 7 L1 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu3_layer1 [label="MLP L1 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu3_layer2 [label="Attention L2 GPU3\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu3_layer2 [label="Gate L2 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu3_layer2_exp0 [label="Expert 0 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer2_exp1 [label="Expert 1 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer2_exp2 [label="Expert 2 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer2_exp3 [label="Expert 3 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer2_exp4 [label="Expert 4 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer2_exp5 [label="Expert 5 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer2_exp6 [label="Expert 6 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer2_exp7 [label="Expert 7 L2 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu3_layer2 [label="MLP L2 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu3_layer3 [label="Attention L3 GPU3\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu3_layer3 [label="Gate L3 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu3_layer3_exp0 [label="Expert 0 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer3_exp1 [label="Expert 1 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer3_exp2 [label="Expert 2 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer3_exp3 [label="Expert 3 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer3_exp4 [label="Expert 4 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer3_exp5 [label="Expert 5 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer3_exp6 [label="Expert 6 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu3_layer3_exp7 [label="Expert 7 L3 GPU3\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu3_layer3 [label="MLP L3 GPU3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [4-7]
        subgraph cluster_pp0_1 {
            label="PP Stage 1\nGPUs [4-7]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu4_layer4 [label="Attention L4 GPU4\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu4_layer4 [label="Gate L4 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu4_layer4_exp0 [label="Expert 0 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer4_exp1 [label="Expert 1 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer4_exp2 [label="Expert 2 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer4_exp3 [label="Expert 3 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer4_exp4 [label="Expert 4 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer4_exp5 [label="Expert 5 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer4_exp6 [label="Expert 6 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer4_exp7 [label="Expert 7 L4 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu4_layer4 [label="MLP L4 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu4_layer5 [label="Attention L5 GPU4\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu4_layer5 [label="Gate L5 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu4_layer5_exp0 [label="Expert 0 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer5_exp1 [label="Expert 1 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer5_exp2 [label="Expert 2 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer5_exp3 [label="Expert 3 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer5_exp4 [label="Expert 4 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer5_exp5 [label="Expert 5 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer5_exp6 [label="Expert 6 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer5_exp7 [label="Expert 7 L5 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu4_layer5 [label="MLP L5 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu4_layer6 [label="Attention L6 GPU4\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu4_layer6 [label="Gate L6 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu4_layer6_exp0 [label="Expert 0 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer6_exp1 [label="Expert 1 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer6_exp2 [label="Expert 2 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer6_exp3 [label="Expert 3 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer6_exp4 [label="Expert 4 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer6_exp5 [label="Expert 5 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer6_exp6 [label="Expert 6 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer6_exp7 [label="Expert 7 L6 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu4_layer6 [label="MLP L6 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu4_layer7 [label="Attention L7 GPU4\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu4_layer7 [label="Gate L7 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu4_layer7_exp0 [label="Expert 0 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer7_exp1 [label="Expert 1 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer7_exp2 [label="Expert 2 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer7_exp3 [label="Expert 3 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer7_exp4 [label="Expert 4 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer7_exp5 [label="Expert 5 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer7_exp6 [label="Expert 6 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu4_layer7_exp7 [label="Expert 7 L7 GPU4\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu4_layer7 [label="MLP L7 GPU4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu5_layer4 [label="Attention L4 GPU5\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu5_layer4 [label="Gate L4 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu5_layer4_exp0 [label="Expert 0 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer4_exp1 [label="Expert 1 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer4_exp2 [label="Expert 2 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer4_exp3 [label="Expert 3 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer4_exp4 [label="Expert 4 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer4_exp5 [label="Expert 5 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer4_exp6 [label="Expert 6 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer4_exp7 [label="Expert 7 L4 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu5_layer4 [label="MLP L4 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu5_layer5 [label="Attention L5 GPU5\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu5_layer5 [label="Gate L5 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu5_layer5_exp0 [label="Expert 0 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer5_exp1 [label="Expert 1 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer5_exp2 [label="Expert 2 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer5_exp3 [label="Expert 3 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer5_exp4 [label="Expert 4 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer5_exp5 [label="Expert 5 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer5_exp6 [label="Expert 6 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer5_exp7 [label="Expert 7 L5 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu5_layer5 [label="MLP L5 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu5_layer6 [label="Attention L6 GPU5\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu5_layer6 [label="Gate L6 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu5_layer6_exp0 [label="Expert 0 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer6_exp1 [label="Expert 1 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer6_exp2 [label="Expert 2 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer6_exp3 [label="Expert 3 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer6_exp4 [label="Expert 4 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer6_exp5 [label="Expert 5 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer6_exp6 [label="Expert 6 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer6_exp7 [label="Expert 7 L6 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu5_layer6 [label="MLP L6 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu5_layer7 [label="Attention L7 GPU5\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu5_layer7 [label="Gate L7 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu5_layer7_exp0 [label="Expert 0 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer7_exp1 [label="Expert 1 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer7_exp2 [label="Expert 2 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer7_exp3 [label="Expert 3 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer7_exp4 [label="Expert 4 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer7_exp5 [label="Expert 5 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer7_exp6 [label="Expert 6 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu5_layer7_exp7 [label="Expert 7 L7 GPU5\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu5_layer7 [label="MLP L7 GPU5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu6_layer4 [label="Attention L4 GPU6\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu6_layer4 [label="Gate L4 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu6_layer4_exp0 [label="Expert 0 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer4_exp1 [label="Expert 1 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer4_exp2 [label="Expert 2 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer4_exp3 [label="Expert 3 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer4_exp4 [label="Expert 4 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer4_exp5 [label="Expert 5 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer4_exp6 [label="Expert 6 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer4_exp7 [label="Expert 7 L4 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu6_layer4 [label="MLP L4 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu6_layer5 [label="Attention L5 GPU6\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu6_layer5 [label="Gate L5 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu6_layer5_exp0 [label="Expert 0 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer5_exp1 [label="Expert 1 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer5_exp2 [label="Expert 2 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer5_exp3 [label="Expert 3 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer5_exp4 [label="Expert 4 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer5_exp5 [label="Expert 5 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer5_exp6 [label="Expert 6 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer5_exp7 [label="Expert 7 L5 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu6_layer5 [label="MLP L5 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu6_layer6 [label="Attention L6 GPU6\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu6_layer6 [label="Gate L6 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu6_layer6_exp0 [label="Expert 0 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer6_exp1 [label="Expert 1 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer6_exp2 [label="Expert 2 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer6_exp3 [label="Expert 3 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer6_exp4 [label="Expert 4 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer6_exp5 [label="Expert 5 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer6_exp6 [label="Expert 6 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer6_exp7 [label="Expert 7 L6 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu6_layer6 [label="MLP L6 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu6_layer7 [label="Attention L7 GPU6\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu6_layer7 [label="Gate L7 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu6_layer7_exp0 [label="Expert 0 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer7_exp1 [label="Expert 1 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer7_exp2 [label="Expert 2 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer7_exp3 [label="Expert 3 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer7_exp4 [label="Expert 4 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer7_exp5 [label="Expert 5 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer7_exp6 [label="Expert 6 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu6_layer7_exp7 [label="Expert 7 L7 GPU6\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu6_layer7 [label="MLP L7 GPU6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu7_layer4 [label="Attention L4 GPU7\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu7_layer4 [label="Gate L4 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu7_layer4_exp0 [label="Expert 0 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer4_exp1 [label="Expert 1 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer4_exp2 [label="Expert 2 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer4_exp3 [label="Expert 3 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer4_exp4 [label="Expert 4 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer4_exp5 [label="Expert 5 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer4_exp6 [label="Expert 6 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer4_exp7 [label="Expert 7 L4 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu7_layer4 [label="MLP L4 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu7_layer5 [label="Attention L5 GPU7\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu7_layer5 [label="Gate L5 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu7_layer5_exp0 [label="Expert 0 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer5_exp1 [label="Expert 1 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer5_exp2 [label="Expert 2 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer5_exp3 [label="Expert 3 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer5_exp4 [label="Expert 4 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer5_exp5 [label="Expert 5 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer5_exp6 [label="Expert 6 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer5_exp7 [label="Expert 7 L5 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu7_layer5 [label="MLP L5 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu7_layer6 [label="Attention L6 GPU7\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu7_layer6 [label="Gate L6 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu7_layer6_exp0 [label="Expert 0 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer6_exp1 [label="Expert 1 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer6_exp2 [label="Expert 2 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer6_exp3 [label="Expert 3 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer6_exp4 [label="Expert 4 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer6_exp5 [label="Expert 5 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer6_exp6 [label="Expert 6 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer6_exp7 [label="Expert 7 L6 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu7_layer6 [label="MLP L6 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu7_layer7 [label="Attention L7 GPU7\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu7_layer7 [label="Gate L7 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu7_layer7_exp0 [label="Expert 0 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer7_exp1 [label="Expert 1 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer7_exp2 [label="Expert 2 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer7_exp3 [label="Expert 3 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer7_exp4 [label="Expert 4 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer7_exp5 [label="Expert 5 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer7_exp6 [label="Expert 6 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu7_layer7_exp7 [label="Expert 7 L7 GPU7\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu7_layer7 [label="MLP L7 GPU7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [8-11]
        subgraph cluster_pp0_2 {
            label="PP Stage 2\nGPUs [8-11]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu8_layer8 [label="Attention L8 GPU8\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu8_layer8 [label="Gate L8 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu8_layer8_exp0 [label="Expert 0 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer8_exp1 [label="Expert 1 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer8_exp2 [label="Expert 2 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer8_exp3 [label="Expert 3 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer8_exp4 [label="Expert 4 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer8_exp5 [label="Expert 5 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer8_exp6 [label="Expert 6 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer8_exp7 [label="Expert 7 L8 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu8_layer8 [label="MLP L8 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu8_layer9 [label="Attention L9 GPU8\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu8_layer9 [label="Gate L9 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu8_layer9_exp0 [label="Expert 0 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer9_exp1 [label="Expert 1 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer9_exp2 [label="Expert 2 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer9_exp3 [label="Expert 3 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer9_exp4 [label="Expert 4 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer9_exp5 [label="Expert 5 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer9_exp6 [label="Expert 6 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer9_exp7 [label="Expert 7 L9 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu8_layer9 [label="MLP L9 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu8_layer10 [label="Attention L10 GPU8\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu8_layer10 [label="Gate L10 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu8_layer10_exp0 [label="Expert 0 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer10_exp1 [label="Expert 1 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer10_exp2 [label="Expert 2 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer10_exp3 [label="Expert 3 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer10_exp4 [label="Expert 4 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer10_exp5 [label="Expert 5 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer10_exp6 [label="Expert 6 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer10_exp7 [label="Expert 7 L10 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu8_layer10 [label="MLP L10 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu8_layer11 [label="Attention L11 GPU8\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu8_layer11 [label="Gate L11 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu8_layer11_exp0 [label="Expert 0 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer11_exp1 [label="Expert 1 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer11_exp2 [label="Expert 2 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer11_exp3 [label="Expert 3 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer11_exp4 [label="Expert 4 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer11_exp5 [label="Expert 5 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer11_exp6 [label="Expert 6 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu8_layer11_exp7 [label="Expert 7 L11 GPU8\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu8_layer11 [label="MLP L11 GPU8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu9_layer8 [label="Attention L8 GPU9\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu9_layer8 [label="Gate L8 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu9_layer8_exp0 [label="Expert 0 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer8_exp1 [label="Expert 1 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer8_exp2 [label="Expert 2 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer8_exp3 [label="Expert 3 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer8_exp4 [label="Expert 4 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer8_exp5 [label="Expert 5 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer8_exp6 [label="Expert 6 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer8_exp7 [label="Expert 7 L8 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu9_layer8 [label="MLP L8 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu9_layer9 [label="Attention L9 GPU9\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu9_layer9 [label="Gate L9 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu9_layer9_exp0 [label="Expert 0 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer9_exp1 [label="Expert 1 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer9_exp2 [label="Expert 2 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer9_exp3 [label="Expert 3 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer9_exp4 [label="Expert 4 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer9_exp5 [label="Expert 5 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer9_exp6 [label="Expert 6 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer9_exp7 [label="Expert 7 L9 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu9_layer9 [label="MLP L9 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu9_layer10 [label="Attention L10 GPU9\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu9_layer10 [label="Gate L10 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu9_layer10_exp0 [label="Expert 0 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer10_exp1 [label="Expert 1 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer10_exp2 [label="Expert 2 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer10_exp3 [label="Expert 3 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer10_exp4 [label="Expert 4 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer10_exp5 [label="Expert 5 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer10_exp6 [label="Expert 6 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer10_exp7 [label="Expert 7 L10 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu9_layer10 [label="MLP L10 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu9_layer11 [label="Attention L11 GPU9\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu9_layer11 [label="Gate L11 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu9_layer11_exp0 [label="Expert 0 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer11_exp1 [label="Expert 1 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer11_exp2 [label="Expert 2 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer11_exp3 [label="Expert 3 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer11_exp4 [label="Expert 4 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer11_exp5 [label="Expert 5 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer11_exp6 [label="Expert 6 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu9_layer11_exp7 [label="Expert 7 L11 GPU9\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu9_layer11 [label="MLP L11 GPU9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu10_layer8 [label="Attention L8 GPU10\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu10_layer8 [label="Gate L8 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu10_layer8_exp0 [label="Expert 0 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer8_exp1 [label="Expert 1 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer8_exp2 [label="Expert 2 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer8_exp3 [label="Expert 3 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer8_exp4 [label="Expert 4 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer8_exp5 [label="Expert 5 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer8_exp6 [label="Expert 6 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer8_exp7 [label="Expert 7 L8 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu10_layer8 [label="MLP L8 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu10_layer9 [label="Attention L9 GPU10\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu10_layer9 [label="Gate L9 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu10_layer9_exp0 [label="Expert 0 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer9_exp1 [label="Expert 1 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer9_exp2 [label="Expert 2 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer9_exp3 [label="Expert 3 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer9_exp4 [label="Expert 4 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer9_exp5 [label="Expert 5 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer9_exp6 [label="Expert 6 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer9_exp7 [label="Expert 7 L9 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu10_layer9 [label="MLP L9 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu10_layer10 [label="Attention L10 GPU10\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu10_layer10 [label="Gate L10 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu10_layer10_exp0 [label="Expert 0 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer10_exp1 [label="Expert 1 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer10_exp2 [label="Expert 2 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer10_exp3 [label="Expert 3 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer10_exp4 [label="Expert 4 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer10_exp5 [label="Expert 5 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer10_exp6 [label="Expert 6 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer10_exp7 [label="Expert 7 L10 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu10_layer10 [label="MLP L10 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu10_layer11 [label="Attention L11 GPU10\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu10_layer11 [label="Gate L11 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu10_layer11_exp0 [label="Expert 0 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer11_exp1 [label="Expert 1 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer11_exp2 [label="Expert 2 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer11_exp3 [label="Expert 3 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer11_exp4 [label="Expert 4 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer11_exp5 [label="Expert 5 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer11_exp6 [label="Expert 6 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu10_layer11_exp7 [label="Expert 7 L11 GPU10\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu10_layer11 [label="MLP L11 GPU10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu11_layer8 [label="Attention L8 GPU11\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu11_layer8 [label="Gate L8 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu11_layer8_exp0 [label="Expert 0 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer8_exp1 [label="Expert 1 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer8_exp2 [label="Expert 2 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer8_exp3 [label="Expert 3 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer8_exp4 [label="Expert 4 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer8_exp5 [label="Expert 5 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer8_exp6 [label="Expert 6 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer8_exp7 [label="Expert 7 L8 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu11_layer8 [label="MLP L8 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu11_layer9 [label="Attention L9 GPU11\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu11_layer9 [label="Gate L9 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu11_layer9_exp0 [label="Expert 0 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer9_exp1 [label="Expert 1 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer9_exp2 [label="Expert 2 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer9_exp3 [label="Expert 3 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer9_exp4 [label="Expert 4 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer9_exp5 [label="Expert 5 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer9_exp6 [label="Expert 6 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer9_exp7 [label="Expert 7 L9 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu11_layer9 [label="MLP L9 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu11_layer10 [label="Attention L10 GPU11\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu11_layer10 [label="Gate L10 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu11_layer10_exp0 [label="Expert 0 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer10_exp1 [label="Expert 1 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer10_exp2 [label="Expert 2 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer10_exp3 [label="Expert 3 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer10_exp4 [label="Expert 4 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer10_exp5 [label="Expert 5 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer10_exp6 [label="Expert 6 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer10_exp7 [label="Expert 7 L10 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu11_layer10 [label="MLP L10 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu11_layer11 [label="Attention L11 GPU11\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu11_layer11 [label="Gate L11 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu11_layer11_exp0 [label="Expert 0 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer11_exp1 [label="Expert 1 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer11_exp2 [label="Expert 2 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer11_exp3 [label="Expert 3 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer11_exp4 [label="Expert 4 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer11_exp5 [label="Expert 5 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer11_exp6 [label="Expert 6 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu11_layer11_exp7 [label="Expert 7 L11 GPU11\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu11_layer11 [label="MLP L11 GPU11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [12-15]
        subgraph cluster_pp0_3 {
            label="PP Stage 3\nGPUs [12-15]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu12_layer12 [label="Attention L12 GPU12\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu12_layer12 [label="Gate L12 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu12_layer12_exp0 [label="Expert 0 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer12_exp1 [label="Expert 1 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer12_exp2 [label="Expert 2 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer12_exp3 [label="Expert 3 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer12_exp4 [label="Expert 4 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer12_exp5 [label="Expert 5 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer12_exp6 [label="Expert 6 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer12_exp7 [label="Expert 7 L12 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu12_layer12 [label="MLP L12 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu12_layer13 [label="Attention L13 GPU12\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu12_layer13 [label="Gate L13 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu12_layer13_exp0 [label="Expert 0 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer13_exp1 [label="Expert 1 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer13_exp2 [label="Expert 2 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer13_exp3 [label="Expert 3 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer13_exp4 [label="Expert 4 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer13_exp5 [label="Expert 5 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer13_exp6 [label="Expert 6 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer13_exp7 [label="Expert 7 L13 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu12_layer13 [label="MLP L13 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu12_layer14 [label="Attention L14 GPU12\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu12_layer14 [label="Gate L14 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu12_layer14_exp0 [label="Expert 0 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer14_exp1 [label="Expert 1 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer14_exp2 [label="Expert 2 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer14_exp3 [label="Expert 3 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer14_exp4 [label="Expert 4 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer14_exp5 [label="Expert 5 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer14_exp6 [label="Expert 6 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer14_exp7 [label="Expert 7 L14 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu12_layer14 [label="MLP L14 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu12_layer15 [label="Attention L15 GPU12\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu12_layer15 [label="Gate L15 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu12_layer15_exp0 [label="Expert 0 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer15_exp1 [label="Expert 1 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer15_exp2 [label="Expert 2 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer15_exp3 [label="Expert 3 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer15_exp4 [label="Expert 4 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer15_exp5 [label="Expert 5 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer15_exp6 [label="Expert 6 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu12_layer15_exp7 [label="Expert 7 L15 GPU12\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu12_layer15 [label="MLP L15 GPU12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu13_layer12 [label="Attention L12 GPU13\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu13_layer12 [label="Gate L12 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu13_layer12_exp0 [label="Expert 0 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer12_exp1 [label="Expert 1 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer12_exp2 [label="Expert 2 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer12_exp3 [label="Expert 3 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer12_exp4 [label="Expert 4 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer12_exp5 [label="Expert 5 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer12_exp6 [label="Expert 6 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer12_exp7 [label="Expert 7 L12 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu13_layer12 [label="MLP L12 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu13_layer13 [label="Attention L13 GPU13\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu13_layer13 [label="Gate L13 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu13_layer13_exp0 [label="Expert 0 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer13_exp1 [label="Expert 1 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer13_exp2 [label="Expert 2 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer13_exp3 [label="Expert 3 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer13_exp4 [label="Expert 4 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer13_exp5 [label="Expert 5 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer13_exp6 [label="Expert 6 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer13_exp7 [label="Expert 7 L13 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu13_layer13 [label="MLP L13 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu13_layer14 [label="Attention L14 GPU13\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu13_layer14 [label="Gate L14 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu13_layer14_exp0 [label="Expert 0 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer14_exp1 [label="Expert 1 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer14_exp2 [label="Expert 2 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer14_exp3 [label="Expert 3 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer14_exp4 [label="Expert 4 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer14_exp5 [label="Expert 5 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer14_exp6 [label="Expert 6 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer14_exp7 [label="Expert 7 L14 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu13_layer14 [label="MLP L14 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu13_layer15 [label="Attention L15 GPU13\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu13_layer15 [label="Gate L15 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu13_layer15_exp0 [label="Expert 0 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer15_exp1 [label="Expert 1 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer15_exp2 [label="Expert 2 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer15_exp3 [label="Expert 3 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer15_exp4 [label="Expert 4 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer15_exp5 [label="Expert 5 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer15_exp6 [label="Expert 6 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu13_layer15_exp7 [label="Expert 7 L15 GPU13\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu13_layer15 [label="MLP L15 GPU13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu14_layer12 [label="Attention L12 GPU14\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu14_layer12 [label="Gate L12 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu14_layer12_exp0 [label="Expert 0 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer12_exp1 [label="Expert 1 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer12_exp2 [label="Expert 2 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer12_exp3 [label="Expert 3 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer12_exp4 [label="Expert 4 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer12_exp5 [label="Expert 5 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer12_exp6 [label="Expert 6 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer12_exp7 [label="Expert 7 L12 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu14_layer12 [label="MLP L12 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu14_layer13 [label="Attention L13 GPU14\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu14_layer13 [label="Gate L13 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu14_layer13_exp0 [label="Expert 0 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer13_exp1 [label="Expert 1 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer13_exp2 [label="Expert 2 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer13_exp3 [label="Expert 3 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer13_exp4 [label="Expert 4 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer13_exp5 [label="Expert 5 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer13_exp6 [label="Expert 6 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer13_exp7 [label="Expert 7 L13 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu14_layer13 [label="MLP L13 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu14_layer14 [label="Attention L14 GPU14\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu14_layer14 [label="Gate L14 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu14_layer14_exp0 [label="Expert 0 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer14_exp1 [label="Expert 1 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer14_exp2 [label="Expert 2 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer14_exp3 [label="Expert 3 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer14_exp4 [label="Expert 4 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer14_exp5 [label="Expert 5 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer14_exp6 [label="Expert 6 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer14_exp7 [label="Expert 7 L14 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu14_layer14 [label="MLP L14 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu14_layer15 [label="Attention L15 GPU14\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu14_layer15 [label="Gate L15 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu14_layer15_exp0 [label="Expert 0 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer15_exp1 [label="Expert 1 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer15_exp2 [label="Expert 2 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer15_exp3 [label="Expert 3 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer15_exp4 [label="Expert 4 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer15_exp5 [label="Expert 5 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer15_exp6 [label="Expert 6 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu14_layer15_exp7 [label="Expert 7 L15 GPU14\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu14_layer15 [label="MLP L15 GPU14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu15_layer12 [label="Attention L12 GPU15\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu15_layer12 [label="Gate L12 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu15_layer12_exp0 [label="Expert 0 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer12_exp1 [label="Expert 1 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer12_exp2 [label="Expert 2 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer12_exp3 [label="Expert 3 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer12_exp4 [label="Expert 4 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer12_exp5 [label="Expert 5 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer12_exp6 [label="Expert 6 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer12_exp7 [label="Expert 7 L12 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu15_layer12 [label="MLP L12 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu15_layer13 [label="Attention L13 GPU15\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu15_layer13 [label="Gate L13 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu15_layer13_exp0 [label="Expert 0 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer13_exp1 [label="Expert 1 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer13_exp2 [label="Expert 2 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer13_exp3 [label="Expert 3 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer13_exp4 [label="Expert 4 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer13_exp5 [label="Expert 5 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer13_exp6 [label="Expert 6 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer13_exp7 [label="Expert 7 L13 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu15_layer13 [label="MLP L13 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu15_layer14 [label="Attention L14 GPU15\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu15_layer14 [label="Gate L14 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu15_layer14_exp0 [label="Expert 0 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer14_exp1 [label="Expert 1 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer14_exp2 [label="Expert 2 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer14_exp3 [label="Expert 3 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer14_exp4 [label="Expert 4 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer14_exp5 [label="Expert 5 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer14_exp6 [label="Expert 6 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer14_exp7 [label="Expert 7 L14 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu15_layer14 [label="MLP L14 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu15_layer15 [label="Attention L15 GPU15\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu15_layer15 [label="Gate L15 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu15_layer15_exp0 [label="Expert 0 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer15_exp1 [label="Expert 1 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer15_exp2 [label="Expert 2 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer15_exp3 [label="Expert 3 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer15_exp4 [label="Expert 4 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer15_exp5 [label="Expert 5 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer15_exp6 [label="Expert 6 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu15_layer15_exp7 [label="Expert 7 L15 GPU15\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu15_layer15 [label="MLP L15 GPU15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // EP Group 1 - GPUs [16-31]
    subgraph cluster_ep1 {
        label="EP Group 1\nGPUs [16-31]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [16-19]
        subgraph cluster_pp1_0 {
            label="PP Stage 0\nGPUs [16-19]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu16_layer0 [label="Attention L0 GPU16\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu16_layer0 [label="Gate L0 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu16_layer0_exp0 [label="Expert 0 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer0_exp1 [label="Expert 1 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer0_exp2 [label="Expert 2 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer0_exp3 [label="Expert 3 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer0_exp4 [label="Expert 4 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer0_exp5 [label="Expert 5 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer0_exp6 [label="Expert 6 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer0_exp7 [label="Expert 7 L0 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu16_layer0 [label="MLP L0 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu16_layer1 [label="Attention L1 GPU16\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu16_layer1 [label="Gate L1 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu16_layer1_exp0 [label="Expert 0 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer1_exp1 [label="Expert 1 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer1_exp2 [label="Expert 2 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer1_exp3 [label="Expert 3 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer1_exp4 [label="Expert 4 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer1_exp5 [label="Expert 5 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer1_exp6 [label="Expert 6 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer1_exp7 [label="Expert 7 L1 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu16_layer1 [label="MLP L1 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu16_layer2 [label="Attention L2 GPU16\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu16_layer2 [label="Gate L2 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu16_layer2_exp0 [label="Expert 0 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer2_exp1 [label="Expert 1 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer2_exp2 [label="Expert 2 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer2_exp3 [label="Expert 3 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer2_exp4 [label="Expert 4 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer2_exp5 [label="Expert 5 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer2_exp6 [label="Expert 6 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer2_exp7 [label="Expert 7 L2 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu16_layer2 [label="MLP L2 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu16_layer3 [label="Attention L3 GPU16\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu16_layer3 [label="Gate L3 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu16_layer3_exp0 [label="Expert 0 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer3_exp1 [label="Expert 1 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer3_exp2 [label="Expert 2 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer3_exp3 [label="Expert 3 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer3_exp4 [label="Expert 4 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer3_exp5 [label="Expert 5 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer3_exp6 [label="Expert 6 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu16_layer3_exp7 [label="Expert 7 L3 GPU16\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu16_layer3 [label="MLP L3 GPU16\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu17_layer0 [label="Attention L0 GPU17\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu17_layer0 [label="Gate L0 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu17_layer0_exp0 [label="Expert 0 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer0_exp1 [label="Expert 1 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer0_exp2 [label="Expert 2 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer0_exp3 [label="Expert 3 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer0_exp4 [label="Expert 4 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer0_exp5 [label="Expert 5 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer0_exp6 [label="Expert 6 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer0_exp7 [label="Expert 7 L0 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu17_layer0 [label="MLP L0 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu17_layer1 [label="Attention L1 GPU17\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu17_layer1 [label="Gate L1 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu17_layer1_exp0 [label="Expert 0 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer1_exp1 [label="Expert 1 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer1_exp2 [label="Expert 2 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer1_exp3 [label="Expert 3 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer1_exp4 [label="Expert 4 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer1_exp5 [label="Expert 5 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer1_exp6 [label="Expert 6 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer1_exp7 [label="Expert 7 L1 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu17_layer1 [label="MLP L1 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu17_layer2 [label="Attention L2 GPU17\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu17_layer2 [label="Gate L2 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu17_layer2_exp0 [label="Expert 0 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer2_exp1 [label="Expert 1 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer2_exp2 [label="Expert 2 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer2_exp3 [label="Expert 3 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer2_exp4 [label="Expert 4 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer2_exp5 [label="Expert 5 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer2_exp6 [label="Expert 6 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer2_exp7 [label="Expert 7 L2 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu17_layer2 [label="MLP L2 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu17_layer3 [label="Attention L3 GPU17\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu17_layer3 [label="Gate L3 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu17_layer3_exp0 [label="Expert 0 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer3_exp1 [label="Expert 1 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer3_exp2 [label="Expert 2 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer3_exp3 [label="Expert 3 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer3_exp4 [label="Expert 4 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer3_exp5 [label="Expert 5 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer3_exp6 [label="Expert 6 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu17_layer3_exp7 [label="Expert 7 L3 GPU17\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu17_layer3 [label="MLP L3 GPU17\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu18_layer0 [label="Attention L0 GPU18\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu18_layer0 [label="Gate L0 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu18_layer0_exp0 [label="Expert 0 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer0_exp1 [label="Expert 1 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer0_exp2 [label="Expert 2 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer0_exp3 [label="Expert 3 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer0_exp4 [label="Expert 4 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer0_exp5 [label="Expert 5 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer0_exp6 [label="Expert 6 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer0_exp7 [label="Expert 7 L0 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu18_layer0 [label="MLP L0 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu18_layer1 [label="Attention L1 GPU18\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu18_layer1 [label="Gate L1 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu18_layer1_exp0 [label="Expert 0 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer1_exp1 [label="Expert 1 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer1_exp2 [label="Expert 2 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer1_exp3 [label="Expert 3 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer1_exp4 [label="Expert 4 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer1_exp5 [label="Expert 5 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer1_exp6 [label="Expert 6 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer1_exp7 [label="Expert 7 L1 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu18_layer1 [label="MLP L1 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu18_layer2 [label="Attention L2 GPU18\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu18_layer2 [label="Gate L2 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu18_layer2_exp0 [label="Expert 0 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer2_exp1 [label="Expert 1 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer2_exp2 [label="Expert 2 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer2_exp3 [label="Expert 3 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer2_exp4 [label="Expert 4 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer2_exp5 [label="Expert 5 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer2_exp6 [label="Expert 6 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer2_exp7 [label="Expert 7 L2 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu18_layer2 [label="MLP L2 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu18_layer3 [label="Attention L3 GPU18\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu18_layer3 [label="Gate L3 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu18_layer3_exp0 [label="Expert 0 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer3_exp1 [label="Expert 1 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer3_exp2 [label="Expert 2 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer3_exp3 [label="Expert 3 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer3_exp4 [label="Expert 4 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer3_exp5 [label="Expert 5 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer3_exp6 [label="Expert 6 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu18_layer3_exp7 [label="Expert 7 L3 GPU18\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu18_layer3 [label="MLP L3 GPU18\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu19_layer0 [label="Attention L0 GPU19\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu19_layer0 [label="Gate L0 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu19_layer0_exp0 [label="Expert 0 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer0_exp1 [label="Expert 1 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer0_exp2 [label="Expert 2 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer0_exp3 [label="Expert 3 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer0_exp4 [label="Expert 4 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer0_exp5 [label="Expert 5 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer0_exp6 [label="Expert 6 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer0_exp7 [label="Expert 7 L0 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu19_layer0 [label="MLP L0 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu19_layer1 [label="Attention L1 GPU19\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu19_layer1 [label="Gate L1 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu19_layer1_exp0 [label="Expert 0 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer1_exp1 [label="Expert 1 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer1_exp2 [label="Expert 2 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer1_exp3 [label="Expert 3 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer1_exp4 [label="Expert 4 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer1_exp5 [label="Expert 5 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer1_exp6 [label="Expert 6 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer1_exp7 [label="Expert 7 L1 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu19_layer1 [label="MLP L1 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu19_layer2 [label="Attention L2 GPU19\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu19_layer2 [label="Gate L2 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu19_layer2_exp0 [label="Expert 0 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer2_exp1 [label="Expert 1 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer2_exp2 [label="Expert 2 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer2_exp3 [label="Expert 3 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer2_exp4 [label="Expert 4 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer2_exp5 [label="Expert 5 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer2_exp6 [label="Expert 6 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer2_exp7 [label="Expert 7 L2 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu19_layer2 [label="MLP L2 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu19_layer3 [label="Attention L3 GPU19\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu19_layer3 [label="Gate L3 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu19_layer3_exp0 [label="Expert 0 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer3_exp1 [label="Expert 1 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer3_exp2 [label="Expert 2 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer3_exp3 [label="Expert 3 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer3_exp4 [label="Expert 4 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer3_exp5 [label="Expert 5 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer3_exp6 [label="Expert 6 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu19_layer3_exp7 [label="Expert 7 L3 GPU19\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu19_layer3 [label="MLP L3 GPU19\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [20-23]
        subgraph cluster_pp1_1 {
            label="PP Stage 1\nGPUs [20-23]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu20_layer4 [label="Attention L4 GPU20\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu20_layer4 [label="Gate L4 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu20_layer4_exp0 [label="Expert 0 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer4_exp1 [label="Expert 1 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer4_exp2 [label="Expert 2 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer4_exp3 [label="Expert 3 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer4_exp4 [label="Expert 4 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer4_exp5 [label="Expert 5 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer4_exp6 [label="Expert 6 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer4_exp7 [label="Expert 7 L4 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu20_layer4 [label="MLP L4 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu20_layer5 [label="Attention L5 GPU20\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu20_layer5 [label="Gate L5 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu20_layer5_exp0 [label="Expert 0 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer5_exp1 [label="Expert 1 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer5_exp2 [label="Expert 2 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer5_exp3 [label="Expert 3 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer5_exp4 [label="Expert 4 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer5_exp5 [label="Expert 5 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer5_exp6 [label="Expert 6 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer5_exp7 [label="Expert 7 L5 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu20_layer5 [label="MLP L5 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu20_layer6 [label="Attention L6 GPU20\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu20_layer6 [label="Gate L6 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu20_layer6_exp0 [label="Expert 0 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer6_exp1 [label="Expert 1 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer6_exp2 [label="Expert 2 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer6_exp3 [label="Expert 3 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer6_exp4 [label="Expert 4 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer6_exp5 [label="Expert 5 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer6_exp6 [label="Expert 6 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer6_exp7 [label="Expert 7 L6 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu20_layer6 [label="MLP L6 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu20_layer7 [label="Attention L7 GPU20\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu20_layer7 [label="Gate L7 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu20_layer7_exp0 [label="Expert 0 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer7_exp1 [label="Expert 1 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer7_exp2 [label="Expert 2 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer7_exp3 [label="Expert 3 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer7_exp4 [label="Expert 4 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer7_exp5 [label="Expert 5 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer7_exp6 [label="Expert 6 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu20_layer7_exp7 [label="Expert 7 L7 GPU20\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu20_layer7 [label="MLP L7 GPU20\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu21_layer4 [label="Attention L4 GPU21\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu21_layer4 [label="Gate L4 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu21_layer4_exp0 [label="Expert 0 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer4_exp1 [label="Expert 1 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer4_exp2 [label="Expert 2 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer4_exp3 [label="Expert 3 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer4_exp4 [label="Expert 4 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer4_exp5 [label="Expert 5 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer4_exp6 [label="Expert 6 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer4_exp7 [label="Expert 7 L4 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu21_layer4 [label="MLP L4 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu21_layer5 [label="Attention L5 GPU21\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu21_layer5 [label="Gate L5 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu21_layer5_exp0 [label="Expert 0 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer5_exp1 [label="Expert 1 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer5_exp2 [label="Expert 2 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer5_exp3 [label="Expert 3 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer5_exp4 [label="Expert 4 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer5_exp5 [label="Expert 5 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer5_exp6 [label="Expert 6 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer5_exp7 [label="Expert 7 L5 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu21_layer5 [label="MLP L5 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu21_layer6 [label="Attention L6 GPU21\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu21_layer6 [label="Gate L6 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu21_layer6_exp0 [label="Expert 0 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer6_exp1 [label="Expert 1 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer6_exp2 [label="Expert 2 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer6_exp3 [label="Expert 3 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer6_exp4 [label="Expert 4 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer6_exp5 [label="Expert 5 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer6_exp6 [label="Expert 6 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer6_exp7 [label="Expert 7 L6 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu21_layer6 [label="MLP L6 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu21_layer7 [label="Attention L7 GPU21\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu21_layer7 [label="Gate L7 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu21_layer7_exp0 [label="Expert 0 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer7_exp1 [label="Expert 1 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer7_exp2 [label="Expert 2 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer7_exp3 [label="Expert 3 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer7_exp4 [label="Expert 4 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer7_exp5 [label="Expert 5 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer7_exp6 [label="Expert 6 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu21_layer7_exp7 [label="Expert 7 L7 GPU21\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu21_layer7 [label="MLP L7 GPU21\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu22_layer4 [label="Attention L4 GPU22\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu22_layer4 [label="Gate L4 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu22_layer4_exp0 [label="Expert 0 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer4_exp1 [label="Expert 1 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer4_exp2 [label="Expert 2 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer4_exp3 [label="Expert 3 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer4_exp4 [label="Expert 4 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer4_exp5 [label="Expert 5 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer4_exp6 [label="Expert 6 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer4_exp7 [label="Expert 7 L4 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu22_layer4 [label="MLP L4 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu22_layer5 [label="Attention L5 GPU22\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu22_layer5 [label="Gate L5 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu22_layer5_exp0 [label="Expert 0 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer5_exp1 [label="Expert 1 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer5_exp2 [label="Expert 2 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer5_exp3 [label="Expert 3 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer5_exp4 [label="Expert 4 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer5_exp5 [label="Expert 5 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer5_exp6 [label="Expert 6 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer5_exp7 [label="Expert 7 L5 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu22_layer5 [label="MLP L5 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu22_layer6 [label="Attention L6 GPU22\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu22_layer6 [label="Gate L6 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu22_layer6_exp0 [label="Expert 0 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer6_exp1 [label="Expert 1 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer6_exp2 [label="Expert 2 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer6_exp3 [label="Expert 3 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer6_exp4 [label="Expert 4 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer6_exp5 [label="Expert 5 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer6_exp6 [label="Expert 6 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer6_exp7 [label="Expert 7 L6 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu22_layer6 [label="MLP L6 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu22_layer7 [label="Attention L7 GPU22\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu22_layer7 [label="Gate L7 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu22_layer7_exp0 [label="Expert 0 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer7_exp1 [label="Expert 1 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer7_exp2 [label="Expert 2 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer7_exp3 [label="Expert 3 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer7_exp4 [label="Expert 4 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer7_exp5 [label="Expert 5 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer7_exp6 [label="Expert 6 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu22_layer7_exp7 [label="Expert 7 L7 GPU22\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu22_layer7 [label="MLP L7 GPU22\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu23_layer4 [label="Attention L4 GPU23\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu23_layer4 [label="Gate L4 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu23_layer4_exp0 [label="Expert 0 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer4_exp1 [label="Expert 1 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer4_exp2 [label="Expert 2 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer4_exp3 [label="Expert 3 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer4_exp4 [label="Expert 4 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer4_exp5 [label="Expert 5 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer4_exp6 [label="Expert 6 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer4_exp7 [label="Expert 7 L4 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu23_layer4 [label="MLP L4 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu23_layer5 [label="Attention L5 GPU23\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu23_layer5 [label="Gate L5 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu23_layer5_exp0 [label="Expert 0 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer5_exp1 [label="Expert 1 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer5_exp2 [label="Expert 2 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer5_exp3 [label="Expert 3 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer5_exp4 [label="Expert 4 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer5_exp5 [label="Expert 5 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer5_exp6 [label="Expert 6 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer5_exp7 [label="Expert 7 L5 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu23_layer5 [label="MLP L5 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu23_layer6 [label="Attention L6 GPU23\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu23_layer6 [label="Gate L6 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu23_layer6_exp0 [label="Expert 0 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer6_exp1 [label="Expert 1 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer6_exp2 [label="Expert 2 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer6_exp3 [label="Expert 3 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer6_exp4 [label="Expert 4 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer6_exp5 [label="Expert 5 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer6_exp6 [label="Expert 6 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer6_exp7 [label="Expert 7 L6 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu23_layer6 [label="MLP L6 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu23_layer7 [label="Attention L7 GPU23\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu23_layer7 [label="Gate L7 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu23_layer7_exp0 [label="Expert 0 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer7_exp1 [label="Expert 1 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer7_exp2 [label="Expert 2 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer7_exp3 [label="Expert 3 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer7_exp4 [label="Expert 4 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer7_exp5 [label="Expert 5 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer7_exp6 [label="Expert 6 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu23_layer7_exp7 [label="Expert 7 L7 GPU23\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu23_layer7 [label="MLP L7 GPU23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [24-27]
        subgraph cluster_pp1_2 {
            label="PP Stage 2\nGPUs [24-27]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu24_layer8 [label="Attention L8 GPU24\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu24_layer8 [label="Gate L8 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu24_layer8_exp0 [label="Expert 0 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer8_exp1 [label="Expert 1 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer8_exp2 [label="Expert 2 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer8_exp3 [label="Expert 3 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer8_exp4 [label="Expert 4 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer8_exp5 [label="Expert 5 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer8_exp6 [label="Expert 6 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer8_exp7 [label="Expert 7 L8 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu24_layer8 [label="MLP L8 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu24_layer9 [label="Attention L9 GPU24\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu24_layer9 [label="Gate L9 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu24_layer9_exp0 [label="Expert 0 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer9_exp1 [label="Expert 1 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer9_exp2 [label="Expert 2 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer9_exp3 [label="Expert 3 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer9_exp4 [label="Expert 4 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer9_exp5 [label="Expert 5 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer9_exp6 [label="Expert 6 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer9_exp7 [label="Expert 7 L9 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu24_layer9 [label="MLP L9 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu24_layer10 [label="Attention L10 GPU24\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu24_layer10 [label="Gate L10 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu24_layer10_exp0 [label="Expert 0 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer10_exp1 [label="Expert 1 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer10_exp2 [label="Expert 2 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer10_exp3 [label="Expert 3 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer10_exp4 [label="Expert 4 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer10_exp5 [label="Expert 5 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer10_exp6 [label="Expert 6 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer10_exp7 [label="Expert 7 L10 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu24_layer10 [label="MLP L10 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu24_layer11 [label="Attention L11 GPU24\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu24_layer11 [label="Gate L11 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu24_layer11_exp0 [label="Expert 0 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer11_exp1 [label="Expert 1 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer11_exp2 [label="Expert 2 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer11_exp3 [label="Expert 3 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer11_exp4 [label="Expert 4 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer11_exp5 [label="Expert 5 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer11_exp6 [label="Expert 6 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu24_layer11_exp7 [label="Expert 7 L11 GPU24\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu24_layer11 [label="MLP L11 GPU24\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu25_layer8 [label="Attention L8 GPU25\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu25_layer8 [label="Gate L8 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu25_layer8_exp0 [label="Expert 0 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer8_exp1 [label="Expert 1 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer8_exp2 [label="Expert 2 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer8_exp3 [label="Expert 3 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer8_exp4 [label="Expert 4 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer8_exp5 [label="Expert 5 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer8_exp6 [label="Expert 6 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer8_exp7 [label="Expert 7 L8 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu25_layer8 [label="MLP L8 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu25_layer9 [label="Attention L9 GPU25\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu25_layer9 [label="Gate L9 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu25_layer9_exp0 [label="Expert 0 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer9_exp1 [label="Expert 1 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer9_exp2 [label="Expert 2 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer9_exp3 [label="Expert 3 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer9_exp4 [label="Expert 4 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer9_exp5 [label="Expert 5 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer9_exp6 [label="Expert 6 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer9_exp7 [label="Expert 7 L9 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu25_layer9 [label="MLP L9 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu25_layer10 [label="Attention L10 GPU25\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu25_layer10 [label="Gate L10 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu25_layer10_exp0 [label="Expert 0 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer10_exp1 [label="Expert 1 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer10_exp2 [label="Expert 2 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer10_exp3 [label="Expert 3 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer10_exp4 [label="Expert 4 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer10_exp5 [label="Expert 5 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer10_exp6 [label="Expert 6 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer10_exp7 [label="Expert 7 L10 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu25_layer10 [label="MLP L10 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu25_layer11 [label="Attention L11 GPU25\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu25_layer11 [label="Gate L11 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu25_layer11_exp0 [label="Expert 0 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer11_exp1 [label="Expert 1 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer11_exp2 [label="Expert 2 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer11_exp3 [label="Expert 3 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer11_exp4 [label="Expert 4 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer11_exp5 [label="Expert 5 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer11_exp6 [label="Expert 6 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu25_layer11_exp7 [label="Expert 7 L11 GPU25\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu25_layer11 [label="MLP L11 GPU25\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu26_layer8 [label="Attention L8 GPU26\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu26_layer8 [label="Gate L8 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu26_layer8_exp0 [label="Expert 0 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer8_exp1 [label="Expert 1 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer8_exp2 [label="Expert 2 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer8_exp3 [label="Expert 3 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer8_exp4 [label="Expert 4 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer8_exp5 [label="Expert 5 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer8_exp6 [label="Expert 6 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer8_exp7 [label="Expert 7 L8 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu26_layer8 [label="MLP L8 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu26_layer9 [label="Attention L9 GPU26\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu26_layer9 [label="Gate L9 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu26_layer9_exp0 [label="Expert 0 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer9_exp1 [label="Expert 1 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer9_exp2 [label="Expert 2 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer9_exp3 [label="Expert 3 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer9_exp4 [label="Expert 4 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer9_exp5 [label="Expert 5 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer9_exp6 [label="Expert 6 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer9_exp7 [label="Expert 7 L9 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu26_layer9 [label="MLP L9 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu26_layer10 [label="Attention L10 GPU26\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu26_layer10 [label="Gate L10 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu26_layer10_exp0 [label="Expert 0 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer10_exp1 [label="Expert 1 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer10_exp2 [label="Expert 2 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer10_exp3 [label="Expert 3 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer10_exp4 [label="Expert 4 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer10_exp5 [label="Expert 5 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer10_exp6 [label="Expert 6 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer10_exp7 [label="Expert 7 L10 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu26_layer10 [label="MLP L10 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu26_layer11 [label="Attention L11 GPU26\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu26_layer11 [label="Gate L11 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu26_layer11_exp0 [label="Expert 0 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer11_exp1 [label="Expert 1 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer11_exp2 [label="Expert 2 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer11_exp3 [label="Expert 3 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer11_exp4 [label="Expert 4 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer11_exp5 [label="Expert 5 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer11_exp6 [label="Expert 6 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu26_layer11_exp7 [label="Expert 7 L11 GPU26\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu26_layer11 [label="MLP L11 GPU26\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu27_layer8 [label="Attention L8 GPU27\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu27_layer8 [label="Gate L8 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu27_layer8_exp0 [label="Expert 0 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer8_exp1 [label="Expert 1 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer8_exp2 [label="Expert 2 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer8_exp3 [label="Expert 3 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer8_exp4 [label="Expert 4 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer8_exp5 [label="Expert 5 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer8_exp6 [label="Expert 6 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer8_exp7 [label="Expert 7 L8 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu27_layer8 [label="MLP L8 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu27_layer9 [label="Attention L9 GPU27\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu27_layer9 [label="Gate L9 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu27_layer9_exp0 [label="Expert 0 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer9_exp1 [label="Expert 1 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer9_exp2 [label="Expert 2 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer9_exp3 [label="Expert 3 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer9_exp4 [label="Expert 4 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer9_exp5 [label="Expert 5 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer9_exp6 [label="Expert 6 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer9_exp7 [label="Expert 7 L9 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu27_layer9 [label="MLP L9 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu27_layer10 [label="Attention L10 GPU27\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu27_layer10 [label="Gate L10 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu27_layer10_exp0 [label="Expert 0 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer10_exp1 [label="Expert 1 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer10_exp2 [label="Expert 2 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer10_exp3 [label="Expert 3 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer10_exp4 [label="Expert 4 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer10_exp5 [label="Expert 5 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer10_exp6 [label="Expert 6 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer10_exp7 [label="Expert 7 L10 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu27_layer10 [label="MLP L10 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu27_layer11 [label="Attention L11 GPU27\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu27_layer11 [label="Gate L11 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu27_layer11_exp0 [label="Expert 0 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer11_exp1 [label="Expert 1 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer11_exp2 [label="Expert 2 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer11_exp3 [label="Expert 3 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer11_exp4 [label="Expert 4 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer11_exp5 [label="Expert 5 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer11_exp6 [label="Expert 6 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu27_layer11_exp7 [label="Expert 7 L11 GPU27\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu27_layer11 [label="MLP L11 GPU27\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [28-31]
        subgraph cluster_pp1_3 {
            label="PP Stage 3\nGPUs [28-31]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu28_layer12 [label="Attention L12 GPU28\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu28_layer12 [label="Gate L12 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu28_layer12_exp0 [label="Expert 0 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer12_exp1 [label="Expert 1 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer12_exp2 [label="Expert 2 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer12_exp3 [label="Expert 3 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer12_exp4 [label="Expert 4 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer12_exp5 [label="Expert 5 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer12_exp6 [label="Expert 6 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer12_exp7 [label="Expert 7 L12 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu28_layer12 [label="MLP L12 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu28_layer13 [label="Attention L13 GPU28\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu28_layer13 [label="Gate L13 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu28_layer13_exp0 [label="Expert 0 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer13_exp1 [label="Expert 1 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer13_exp2 [label="Expert 2 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer13_exp3 [label="Expert 3 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer13_exp4 [label="Expert 4 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer13_exp5 [label="Expert 5 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer13_exp6 [label="Expert 6 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer13_exp7 [label="Expert 7 L13 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu28_layer13 [label="MLP L13 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu28_layer14 [label="Attention L14 GPU28\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu28_layer14 [label="Gate L14 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu28_layer14_exp0 [label="Expert 0 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer14_exp1 [label="Expert 1 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer14_exp2 [label="Expert 2 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer14_exp3 [label="Expert 3 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer14_exp4 [label="Expert 4 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer14_exp5 [label="Expert 5 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer14_exp6 [label="Expert 6 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer14_exp7 [label="Expert 7 L14 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu28_layer14 [label="MLP L14 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu28_layer15 [label="Attention L15 GPU28\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu28_layer15 [label="Gate L15 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu28_layer15_exp0 [label="Expert 0 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer15_exp1 [label="Expert 1 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer15_exp2 [label="Expert 2 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer15_exp3 [label="Expert 3 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer15_exp4 [label="Expert 4 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer15_exp5 [label="Expert 5 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer15_exp6 [label="Expert 6 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu28_layer15_exp7 [label="Expert 7 L15 GPU28\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu28_layer15 [label="MLP L15 GPU28\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu29_layer12 [label="Attention L12 GPU29\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu29_layer12 [label="Gate L12 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu29_layer12_exp0 [label="Expert 0 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer12_exp1 [label="Expert 1 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer12_exp2 [label="Expert 2 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer12_exp3 [label="Expert 3 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer12_exp4 [label="Expert 4 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer12_exp5 [label="Expert 5 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer12_exp6 [label="Expert 6 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer12_exp7 [label="Expert 7 L12 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu29_layer12 [label="MLP L12 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu29_layer13 [label="Attention L13 GPU29\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu29_layer13 [label="Gate L13 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu29_layer13_exp0 [label="Expert 0 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer13_exp1 [label="Expert 1 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer13_exp2 [label="Expert 2 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer13_exp3 [label="Expert 3 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer13_exp4 [label="Expert 4 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer13_exp5 [label="Expert 5 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer13_exp6 [label="Expert 6 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer13_exp7 [label="Expert 7 L13 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu29_layer13 [label="MLP L13 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu29_layer14 [label="Attention L14 GPU29\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu29_layer14 [label="Gate L14 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu29_layer14_exp0 [label="Expert 0 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer14_exp1 [label="Expert 1 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer14_exp2 [label="Expert 2 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer14_exp3 [label="Expert 3 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer14_exp4 [label="Expert 4 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer14_exp5 [label="Expert 5 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer14_exp6 [label="Expert 6 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer14_exp7 [label="Expert 7 L14 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu29_layer14 [label="MLP L14 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu29_layer15 [label="Attention L15 GPU29\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu29_layer15 [label="Gate L15 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu29_layer15_exp0 [label="Expert 0 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer15_exp1 [label="Expert 1 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer15_exp2 [label="Expert 2 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer15_exp3 [label="Expert 3 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer15_exp4 [label="Expert 4 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer15_exp5 [label="Expert 5 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer15_exp6 [label="Expert 6 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu29_layer15_exp7 [label="Expert 7 L15 GPU29\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu29_layer15 [label="MLP L15 GPU29\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu30_layer12 [label="Attention L12 GPU30\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu30_layer12 [label="Gate L12 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu30_layer12_exp0 [label="Expert 0 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer12_exp1 [label="Expert 1 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer12_exp2 [label="Expert 2 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer12_exp3 [label="Expert 3 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer12_exp4 [label="Expert 4 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer12_exp5 [label="Expert 5 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer12_exp6 [label="Expert 6 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer12_exp7 [label="Expert 7 L12 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu30_layer12 [label="MLP L12 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu30_layer13 [label="Attention L13 GPU30\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu30_layer13 [label="Gate L13 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu30_layer13_exp0 [label="Expert 0 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer13_exp1 [label="Expert 1 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer13_exp2 [label="Expert 2 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer13_exp3 [label="Expert 3 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer13_exp4 [label="Expert 4 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer13_exp5 [label="Expert 5 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer13_exp6 [label="Expert 6 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer13_exp7 [label="Expert 7 L13 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu30_layer13 [label="MLP L13 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu30_layer14 [label="Attention L14 GPU30\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu30_layer14 [label="Gate L14 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu30_layer14_exp0 [label="Expert 0 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer14_exp1 [label="Expert 1 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer14_exp2 [label="Expert 2 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer14_exp3 [label="Expert 3 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer14_exp4 [label="Expert 4 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer14_exp5 [label="Expert 5 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer14_exp6 [label="Expert 6 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer14_exp7 [label="Expert 7 L14 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu30_layer14 [label="MLP L14 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu30_layer15 [label="Attention L15 GPU30\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu30_layer15 [label="Gate L15 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu30_layer15_exp0 [label="Expert 0 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer15_exp1 [label="Expert 1 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer15_exp2 [label="Expert 2 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer15_exp3 [label="Expert 3 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer15_exp4 [label="Expert 4 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer15_exp5 [label="Expert 5 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer15_exp6 [label="Expert 6 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu30_layer15_exp7 [label="Expert 7 L15 GPU30\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu30_layer15 [label="MLP L15 GPU30\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu31_layer12 [label="Attention L12 GPU31\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu31_layer12 [label="Gate L12 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu31_layer12_exp0 [label="Expert 0 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer12_exp1 [label="Expert 1 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer12_exp2 [label="Expert 2 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer12_exp3 [label="Expert 3 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer12_exp4 [label="Expert 4 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer12_exp5 [label="Expert 5 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer12_exp6 [label="Expert 6 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer12_exp7 [label="Expert 7 L12 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu31_layer12 [label="MLP L12 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu31_layer13 [label="Attention L13 GPU31\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu31_layer13 [label="Gate L13 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu31_layer13_exp0 [label="Expert 0 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer13_exp1 [label="Expert 1 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer13_exp2 [label="Expert 2 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer13_exp3 [label="Expert 3 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer13_exp4 [label="Expert 4 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer13_exp5 [label="Expert 5 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer13_exp6 [label="Expert 6 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer13_exp7 [label="Expert 7 L13 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu31_layer13 [label="MLP L13 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu31_layer14 [label="Attention L14 GPU31\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu31_layer14 [label="Gate L14 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu31_layer14_exp0 [label="Expert 0 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer14_exp1 [label="Expert 1 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer14_exp2 [label="Expert 2 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer14_exp3 [label="Expert 3 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer14_exp4 [label="Expert 4 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer14_exp5 [label="Expert 5 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer14_exp6 [label="Expert 6 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer14_exp7 [label="Expert 7 L14 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu31_layer14 [label="MLP L14 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu31_layer15 [label="Attention L15 GPU31\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu31_layer15 [label="Gate L15 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu31_layer15_exp0 [label="Expert 0 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer15_exp1 [label="Expert 1 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer15_exp2 [label="Expert 2 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer15_exp3 [label="Expert 3 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer15_exp4 [label="Expert 4 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer15_exp5 [label="Expert 5 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer15_exp6 [label="Expert 6 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu31_layer15_exp7 [label="Expert 7 L15 GPU31\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu31_layer15 [label="MLP L15 GPU31\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // EP Group 2 - GPUs [32-47]
    subgraph cluster_ep2 {
        label="EP Group 2\nGPUs [32-47]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [32-35]
        subgraph cluster_pp2_0 {
            label="PP Stage 0\nGPUs [32-35]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu32_layer0 [label="Attention L0 GPU32\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu32_layer0 [label="Gate L0 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu32_layer0_exp0 [label="Expert 0 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer0_exp1 [label="Expert 1 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer0_exp2 [label="Expert 2 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer0_exp3 [label="Expert 3 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer0_exp4 [label="Expert 4 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer0_exp5 [label="Expert 5 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer0_exp6 [label="Expert 6 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer0_exp7 [label="Expert 7 L0 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu32_layer0 [label="MLP L0 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu32_layer1 [label="Attention L1 GPU32\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu32_layer1 [label="Gate L1 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu32_layer1_exp0 [label="Expert 0 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer1_exp1 [label="Expert 1 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer1_exp2 [label="Expert 2 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer1_exp3 [label="Expert 3 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer1_exp4 [label="Expert 4 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer1_exp5 [label="Expert 5 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer1_exp6 [label="Expert 6 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer1_exp7 [label="Expert 7 L1 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu32_layer1 [label="MLP L1 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu32_layer2 [label="Attention L2 GPU32\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu32_layer2 [label="Gate L2 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu32_layer2_exp0 [label="Expert 0 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer2_exp1 [label="Expert 1 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer2_exp2 [label="Expert 2 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer2_exp3 [label="Expert 3 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer2_exp4 [label="Expert 4 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer2_exp5 [label="Expert 5 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer2_exp6 [label="Expert 6 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer2_exp7 [label="Expert 7 L2 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu32_layer2 [label="MLP L2 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu32_layer3 [label="Attention L3 GPU32\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu32_layer3 [label="Gate L3 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu32_layer3_exp0 [label="Expert 0 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer3_exp1 [label="Expert 1 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer3_exp2 [label="Expert 2 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer3_exp3 [label="Expert 3 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer3_exp4 [label="Expert 4 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer3_exp5 [label="Expert 5 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer3_exp6 [label="Expert 6 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu32_layer3_exp7 [label="Expert 7 L3 GPU32\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu32_layer3 [label="MLP L3 GPU32\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu33_layer0 [label="Attention L0 GPU33\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu33_layer0 [label="Gate L0 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu33_layer0_exp0 [label="Expert 0 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer0_exp1 [label="Expert 1 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer0_exp2 [label="Expert 2 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer0_exp3 [label="Expert 3 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer0_exp4 [label="Expert 4 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer0_exp5 [label="Expert 5 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer0_exp6 [label="Expert 6 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer0_exp7 [label="Expert 7 L0 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu33_layer0 [label="MLP L0 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu33_layer1 [label="Attention L1 GPU33\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu33_layer1 [label="Gate L1 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu33_layer1_exp0 [label="Expert 0 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer1_exp1 [label="Expert 1 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer1_exp2 [label="Expert 2 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer1_exp3 [label="Expert 3 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer1_exp4 [label="Expert 4 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer1_exp5 [label="Expert 5 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer1_exp6 [label="Expert 6 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer1_exp7 [label="Expert 7 L1 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu33_layer1 [label="MLP L1 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu33_layer2 [label="Attention L2 GPU33\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu33_layer2 [label="Gate L2 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu33_layer2_exp0 [label="Expert 0 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer2_exp1 [label="Expert 1 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer2_exp2 [label="Expert 2 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer2_exp3 [label="Expert 3 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer2_exp4 [label="Expert 4 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer2_exp5 [label="Expert 5 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer2_exp6 [label="Expert 6 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer2_exp7 [label="Expert 7 L2 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu33_layer2 [label="MLP L2 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu33_layer3 [label="Attention L3 GPU33\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu33_layer3 [label="Gate L3 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu33_layer3_exp0 [label="Expert 0 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer3_exp1 [label="Expert 1 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer3_exp2 [label="Expert 2 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer3_exp3 [label="Expert 3 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer3_exp4 [label="Expert 4 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer3_exp5 [label="Expert 5 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer3_exp6 [label="Expert 6 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu33_layer3_exp7 [label="Expert 7 L3 GPU33\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu33_layer3 [label="MLP L3 GPU33\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu34_layer0 [label="Attention L0 GPU34\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu34_layer0 [label="Gate L0 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu34_layer0_exp0 [label="Expert 0 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer0_exp1 [label="Expert 1 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer0_exp2 [label="Expert 2 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer0_exp3 [label="Expert 3 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer0_exp4 [label="Expert 4 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer0_exp5 [label="Expert 5 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer0_exp6 [label="Expert 6 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer0_exp7 [label="Expert 7 L0 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu34_layer0 [label="MLP L0 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu34_layer1 [label="Attention L1 GPU34\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu34_layer1 [label="Gate L1 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu34_layer1_exp0 [label="Expert 0 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer1_exp1 [label="Expert 1 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer1_exp2 [label="Expert 2 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer1_exp3 [label="Expert 3 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer1_exp4 [label="Expert 4 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer1_exp5 [label="Expert 5 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer1_exp6 [label="Expert 6 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer1_exp7 [label="Expert 7 L1 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu34_layer1 [label="MLP L1 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu34_layer2 [label="Attention L2 GPU34\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu34_layer2 [label="Gate L2 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu34_layer2_exp0 [label="Expert 0 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer2_exp1 [label="Expert 1 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer2_exp2 [label="Expert 2 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer2_exp3 [label="Expert 3 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer2_exp4 [label="Expert 4 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer2_exp5 [label="Expert 5 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer2_exp6 [label="Expert 6 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer2_exp7 [label="Expert 7 L2 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu34_layer2 [label="MLP L2 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu34_layer3 [label="Attention L3 GPU34\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu34_layer3 [label="Gate L3 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu34_layer3_exp0 [label="Expert 0 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer3_exp1 [label="Expert 1 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer3_exp2 [label="Expert 2 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer3_exp3 [label="Expert 3 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer3_exp4 [label="Expert 4 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer3_exp5 [label="Expert 5 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer3_exp6 [label="Expert 6 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu34_layer3_exp7 [label="Expert 7 L3 GPU34\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu34_layer3 [label="MLP L3 GPU34\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu35_layer0 [label="Attention L0 GPU35\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu35_layer0 [label="Gate L0 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu35_layer0_exp0 [label="Expert 0 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer0_exp1 [label="Expert 1 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer0_exp2 [label="Expert 2 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer0_exp3 [label="Expert 3 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer0_exp4 [label="Expert 4 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer0_exp5 [label="Expert 5 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer0_exp6 [label="Expert 6 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer0_exp7 [label="Expert 7 L0 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu35_layer0 [label="MLP L0 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu35_layer1 [label="Attention L1 GPU35\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu35_layer1 [label="Gate L1 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu35_layer1_exp0 [label="Expert 0 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer1_exp1 [label="Expert 1 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer1_exp2 [label="Expert 2 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer1_exp3 [label="Expert 3 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer1_exp4 [label="Expert 4 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer1_exp5 [label="Expert 5 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer1_exp6 [label="Expert 6 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer1_exp7 [label="Expert 7 L1 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu35_layer1 [label="MLP L1 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu35_layer2 [label="Attention L2 GPU35\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu35_layer2 [label="Gate L2 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu35_layer2_exp0 [label="Expert 0 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer2_exp1 [label="Expert 1 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer2_exp2 [label="Expert 2 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer2_exp3 [label="Expert 3 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer2_exp4 [label="Expert 4 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer2_exp5 [label="Expert 5 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer2_exp6 [label="Expert 6 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer2_exp7 [label="Expert 7 L2 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu35_layer2 [label="MLP L2 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu35_layer3 [label="Attention L3 GPU35\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu35_layer3 [label="Gate L3 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu35_layer3_exp0 [label="Expert 0 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer3_exp1 [label="Expert 1 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer3_exp2 [label="Expert 2 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer3_exp3 [label="Expert 3 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer3_exp4 [label="Expert 4 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer3_exp5 [label="Expert 5 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer3_exp6 [label="Expert 6 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu35_layer3_exp7 [label="Expert 7 L3 GPU35\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu35_layer3 [label="MLP L3 GPU35\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [36-39]
        subgraph cluster_pp2_1 {
            label="PP Stage 1\nGPUs [36-39]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu36_layer4 [label="Attention L4 GPU36\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu36_layer4 [label="Gate L4 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu36_layer4_exp0 [label="Expert 0 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer4_exp1 [label="Expert 1 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer4_exp2 [label="Expert 2 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer4_exp3 [label="Expert 3 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer4_exp4 [label="Expert 4 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer4_exp5 [label="Expert 5 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer4_exp6 [label="Expert 6 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer4_exp7 [label="Expert 7 L4 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu36_layer4 [label="MLP L4 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu36_layer5 [label="Attention L5 GPU36\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu36_layer5 [label="Gate L5 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu36_layer5_exp0 [label="Expert 0 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer5_exp1 [label="Expert 1 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer5_exp2 [label="Expert 2 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer5_exp3 [label="Expert 3 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer5_exp4 [label="Expert 4 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer5_exp5 [label="Expert 5 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer5_exp6 [label="Expert 6 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer5_exp7 [label="Expert 7 L5 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu36_layer5 [label="MLP L5 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu36_layer6 [label="Attention L6 GPU36\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu36_layer6 [label="Gate L6 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu36_layer6_exp0 [label="Expert 0 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer6_exp1 [label="Expert 1 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer6_exp2 [label="Expert 2 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer6_exp3 [label="Expert 3 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer6_exp4 [label="Expert 4 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer6_exp5 [label="Expert 5 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer6_exp6 [label="Expert 6 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer6_exp7 [label="Expert 7 L6 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu36_layer6 [label="MLP L6 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu36_layer7 [label="Attention L7 GPU36\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu36_layer7 [label="Gate L7 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu36_layer7_exp0 [label="Expert 0 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer7_exp1 [label="Expert 1 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer7_exp2 [label="Expert 2 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer7_exp3 [label="Expert 3 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer7_exp4 [label="Expert 4 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer7_exp5 [label="Expert 5 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer7_exp6 [label="Expert 6 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu36_layer7_exp7 [label="Expert 7 L7 GPU36\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu36_layer7 [label="MLP L7 GPU36\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu37_layer4 [label="Attention L4 GPU37\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu37_layer4 [label="Gate L4 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu37_layer4_exp0 [label="Expert 0 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer4_exp1 [label="Expert 1 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer4_exp2 [label="Expert 2 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer4_exp3 [label="Expert 3 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer4_exp4 [label="Expert 4 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer4_exp5 [label="Expert 5 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer4_exp6 [label="Expert 6 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer4_exp7 [label="Expert 7 L4 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu37_layer4 [label="MLP L4 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu37_layer5 [label="Attention L5 GPU37\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu37_layer5 [label="Gate L5 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu37_layer5_exp0 [label="Expert 0 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer5_exp1 [label="Expert 1 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer5_exp2 [label="Expert 2 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer5_exp3 [label="Expert 3 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer5_exp4 [label="Expert 4 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer5_exp5 [label="Expert 5 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer5_exp6 [label="Expert 6 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer5_exp7 [label="Expert 7 L5 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu37_layer5 [label="MLP L5 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu37_layer6 [label="Attention L6 GPU37\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu37_layer6 [label="Gate L6 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu37_layer6_exp0 [label="Expert 0 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer6_exp1 [label="Expert 1 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer6_exp2 [label="Expert 2 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer6_exp3 [label="Expert 3 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer6_exp4 [label="Expert 4 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer6_exp5 [label="Expert 5 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer6_exp6 [label="Expert 6 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer6_exp7 [label="Expert 7 L6 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu37_layer6 [label="MLP L6 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu37_layer7 [label="Attention L7 GPU37\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu37_layer7 [label="Gate L7 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu37_layer7_exp0 [label="Expert 0 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer7_exp1 [label="Expert 1 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer7_exp2 [label="Expert 2 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer7_exp3 [label="Expert 3 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer7_exp4 [label="Expert 4 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer7_exp5 [label="Expert 5 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer7_exp6 [label="Expert 6 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu37_layer7_exp7 [label="Expert 7 L7 GPU37\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu37_layer7 [label="MLP L7 GPU37\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu38_layer4 [label="Attention L4 GPU38\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu38_layer4 [label="Gate L4 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu38_layer4_exp0 [label="Expert 0 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer4_exp1 [label="Expert 1 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer4_exp2 [label="Expert 2 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer4_exp3 [label="Expert 3 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer4_exp4 [label="Expert 4 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer4_exp5 [label="Expert 5 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer4_exp6 [label="Expert 6 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer4_exp7 [label="Expert 7 L4 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu38_layer4 [label="MLP L4 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu38_layer5 [label="Attention L5 GPU38\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu38_layer5 [label="Gate L5 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu38_layer5_exp0 [label="Expert 0 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer5_exp1 [label="Expert 1 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer5_exp2 [label="Expert 2 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer5_exp3 [label="Expert 3 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer5_exp4 [label="Expert 4 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer5_exp5 [label="Expert 5 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer5_exp6 [label="Expert 6 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer5_exp7 [label="Expert 7 L5 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu38_layer5 [label="MLP L5 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu38_layer6 [label="Attention L6 GPU38\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu38_layer6 [label="Gate L6 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu38_layer6_exp0 [label="Expert 0 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer6_exp1 [label="Expert 1 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer6_exp2 [label="Expert 2 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer6_exp3 [label="Expert 3 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer6_exp4 [label="Expert 4 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer6_exp5 [label="Expert 5 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer6_exp6 [label="Expert 6 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer6_exp7 [label="Expert 7 L6 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu38_layer6 [label="MLP L6 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu38_layer7 [label="Attention L7 GPU38\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu38_layer7 [label="Gate L7 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu38_layer7_exp0 [label="Expert 0 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer7_exp1 [label="Expert 1 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer7_exp2 [label="Expert 2 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer7_exp3 [label="Expert 3 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer7_exp4 [label="Expert 4 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer7_exp5 [label="Expert 5 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer7_exp6 [label="Expert 6 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu38_layer7_exp7 [label="Expert 7 L7 GPU38\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu38_layer7 [label="MLP L7 GPU38\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu39_layer4 [label="Attention L4 GPU39\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu39_layer4 [label="Gate L4 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu39_layer4_exp0 [label="Expert 0 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer4_exp1 [label="Expert 1 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer4_exp2 [label="Expert 2 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer4_exp3 [label="Expert 3 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer4_exp4 [label="Expert 4 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer4_exp5 [label="Expert 5 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer4_exp6 [label="Expert 6 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer4_exp7 [label="Expert 7 L4 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu39_layer4 [label="MLP L4 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu39_layer5 [label="Attention L5 GPU39\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu39_layer5 [label="Gate L5 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu39_layer5_exp0 [label="Expert 0 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer5_exp1 [label="Expert 1 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer5_exp2 [label="Expert 2 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer5_exp3 [label="Expert 3 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer5_exp4 [label="Expert 4 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer5_exp5 [label="Expert 5 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer5_exp6 [label="Expert 6 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer5_exp7 [label="Expert 7 L5 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu39_layer5 [label="MLP L5 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu39_layer6 [label="Attention L6 GPU39\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu39_layer6 [label="Gate L6 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu39_layer6_exp0 [label="Expert 0 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer6_exp1 [label="Expert 1 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer6_exp2 [label="Expert 2 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer6_exp3 [label="Expert 3 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer6_exp4 [label="Expert 4 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer6_exp5 [label="Expert 5 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer6_exp6 [label="Expert 6 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer6_exp7 [label="Expert 7 L6 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu39_layer6 [label="MLP L6 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu39_layer7 [label="Attention L7 GPU39\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu39_layer7 [label="Gate L7 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu39_layer7_exp0 [label="Expert 0 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer7_exp1 [label="Expert 1 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer7_exp2 [label="Expert 2 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer7_exp3 [label="Expert 3 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer7_exp4 [label="Expert 4 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer7_exp5 [label="Expert 5 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer7_exp6 [label="Expert 6 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu39_layer7_exp7 [label="Expert 7 L7 GPU39\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu39_layer7 [label="MLP L7 GPU39\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [40-43]
        subgraph cluster_pp2_2 {
            label="PP Stage 2\nGPUs [40-43]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu40_layer8 [label="Attention L8 GPU40\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu40_layer8 [label="Gate L8 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu40_layer8_exp0 [label="Expert 0 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer8_exp1 [label="Expert 1 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer8_exp2 [label="Expert 2 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer8_exp3 [label="Expert 3 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer8_exp4 [label="Expert 4 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer8_exp5 [label="Expert 5 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer8_exp6 [label="Expert 6 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer8_exp7 [label="Expert 7 L8 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu40_layer8 [label="MLP L8 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu40_layer9 [label="Attention L9 GPU40\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu40_layer9 [label="Gate L9 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu40_layer9_exp0 [label="Expert 0 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer9_exp1 [label="Expert 1 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer9_exp2 [label="Expert 2 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer9_exp3 [label="Expert 3 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer9_exp4 [label="Expert 4 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer9_exp5 [label="Expert 5 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer9_exp6 [label="Expert 6 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer9_exp7 [label="Expert 7 L9 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu40_layer9 [label="MLP L9 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu40_layer10 [label="Attention L10 GPU40\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu40_layer10 [label="Gate L10 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu40_layer10_exp0 [label="Expert 0 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer10_exp1 [label="Expert 1 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer10_exp2 [label="Expert 2 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer10_exp3 [label="Expert 3 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer10_exp4 [label="Expert 4 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer10_exp5 [label="Expert 5 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer10_exp6 [label="Expert 6 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer10_exp7 [label="Expert 7 L10 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu40_layer10 [label="MLP L10 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu40_layer11 [label="Attention L11 GPU40\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu40_layer11 [label="Gate L11 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu40_layer11_exp0 [label="Expert 0 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer11_exp1 [label="Expert 1 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer11_exp2 [label="Expert 2 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer11_exp3 [label="Expert 3 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer11_exp4 [label="Expert 4 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer11_exp5 [label="Expert 5 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer11_exp6 [label="Expert 6 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu40_layer11_exp7 [label="Expert 7 L11 GPU40\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu40_layer11 [label="MLP L11 GPU40\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu41_layer8 [label="Attention L8 GPU41\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu41_layer8 [label="Gate L8 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu41_layer8_exp0 [label="Expert 0 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer8_exp1 [label="Expert 1 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer8_exp2 [label="Expert 2 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer8_exp3 [label="Expert 3 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer8_exp4 [label="Expert 4 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer8_exp5 [label="Expert 5 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer8_exp6 [label="Expert 6 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer8_exp7 [label="Expert 7 L8 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu41_layer8 [label="MLP L8 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu41_layer9 [label="Attention L9 GPU41\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu41_layer9 [label="Gate L9 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu41_layer9_exp0 [label="Expert 0 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer9_exp1 [label="Expert 1 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer9_exp2 [label="Expert 2 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer9_exp3 [label="Expert 3 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer9_exp4 [label="Expert 4 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer9_exp5 [label="Expert 5 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer9_exp6 [label="Expert 6 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer9_exp7 [label="Expert 7 L9 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu41_layer9 [label="MLP L9 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu41_layer10 [label="Attention L10 GPU41\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu41_layer10 [label="Gate L10 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu41_layer10_exp0 [label="Expert 0 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer10_exp1 [label="Expert 1 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer10_exp2 [label="Expert 2 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer10_exp3 [label="Expert 3 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer10_exp4 [label="Expert 4 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer10_exp5 [label="Expert 5 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer10_exp6 [label="Expert 6 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer10_exp7 [label="Expert 7 L10 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu41_layer10 [label="MLP L10 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu41_layer11 [label="Attention L11 GPU41\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu41_layer11 [label="Gate L11 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu41_layer11_exp0 [label="Expert 0 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer11_exp1 [label="Expert 1 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer11_exp2 [label="Expert 2 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer11_exp3 [label="Expert 3 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer11_exp4 [label="Expert 4 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer11_exp5 [label="Expert 5 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer11_exp6 [label="Expert 6 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu41_layer11_exp7 [label="Expert 7 L11 GPU41\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu41_layer11 [label="MLP L11 GPU41\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu42_layer8 [label="Attention L8 GPU42\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu42_layer8 [label="Gate L8 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu42_layer8_exp0 [label="Expert 0 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer8_exp1 [label="Expert 1 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer8_exp2 [label="Expert 2 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer8_exp3 [label="Expert 3 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer8_exp4 [label="Expert 4 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer8_exp5 [label="Expert 5 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer8_exp6 [label="Expert 6 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer8_exp7 [label="Expert 7 L8 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu42_layer8 [label="MLP L8 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu42_layer9 [label="Attention L9 GPU42\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu42_layer9 [label="Gate L9 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu42_layer9_exp0 [label="Expert 0 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer9_exp1 [label="Expert 1 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer9_exp2 [label="Expert 2 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer9_exp3 [label="Expert 3 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer9_exp4 [label="Expert 4 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer9_exp5 [label="Expert 5 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer9_exp6 [label="Expert 6 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer9_exp7 [label="Expert 7 L9 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu42_layer9 [label="MLP L9 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu42_layer10 [label="Attention L10 GPU42\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu42_layer10 [label="Gate L10 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu42_layer10_exp0 [label="Expert 0 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer10_exp1 [label="Expert 1 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer10_exp2 [label="Expert 2 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer10_exp3 [label="Expert 3 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer10_exp4 [label="Expert 4 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer10_exp5 [label="Expert 5 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer10_exp6 [label="Expert 6 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer10_exp7 [label="Expert 7 L10 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu42_layer10 [label="MLP L10 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu42_layer11 [label="Attention L11 GPU42\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu42_layer11 [label="Gate L11 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu42_layer11_exp0 [label="Expert 0 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer11_exp1 [label="Expert 1 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer11_exp2 [label="Expert 2 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer11_exp3 [label="Expert 3 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer11_exp4 [label="Expert 4 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer11_exp5 [label="Expert 5 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer11_exp6 [label="Expert 6 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu42_layer11_exp7 [label="Expert 7 L11 GPU42\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu42_layer11 [label="MLP L11 GPU42\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu43_layer8 [label="Attention L8 GPU43\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu43_layer8 [label="Gate L8 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu43_layer8_exp0 [label="Expert 0 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer8_exp1 [label="Expert 1 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer8_exp2 [label="Expert 2 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer8_exp3 [label="Expert 3 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer8_exp4 [label="Expert 4 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer8_exp5 [label="Expert 5 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer8_exp6 [label="Expert 6 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer8_exp7 [label="Expert 7 L8 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu43_layer8 [label="MLP L8 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu43_layer9 [label="Attention L9 GPU43\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu43_layer9 [label="Gate L9 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu43_layer9_exp0 [label="Expert 0 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer9_exp1 [label="Expert 1 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer9_exp2 [label="Expert 2 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer9_exp3 [label="Expert 3 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer9_exp4 [label="Expert 4 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer9_exp5 [label="Expert 5 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer9_exp6 [label="Expert 6 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer9_exp7 [label="Expert 7 L9 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu43_layer9 [label="MLP L9 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu43_layer10 [label="Attention L10 GPU43\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu43_layer10 [label="Gate L10 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu43_layer10_exp0 [label="Expert 0 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer10_exp1 [label="Expert 1 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer10_exp2 [label="Expert 2 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer10_exp3 [label="Expert 3 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer10_exp4 [label="Expert 4 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer10_exp5 [label="Expert 5 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer10_exp6 [label="Expert 6 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer10_exp7 [label="Expert 7 L10 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu43_layer10 [label="MLP L10 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu43_layer11 [label="Attention L11 GPU43\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu43_layer11 [label="Gate L11 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu43_layer11_exp0 [label="Expert 0 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer11_exp1 [label="Expert 1 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer11_exp2 [label="Expert 2 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer11_exp3 [label="Expert 3 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer11_exp4 [label="Expert 4 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer11_exp5 [label="Expert 5 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer11_exp6 [label="Expert 6 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu43_layer11_exp7 [label="Expert 7 L11 GPU43\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu43_layer11 [label="MLP L11 GPU43\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [44-47]
        subgraph cluster_pp2_3 {
            label="PP Stage 3\nGPUs [44-47]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu44_layer12 [label="Attention L12 GPU44\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu44_layer12 [label="Gate L12 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu44_layer12_exp0 [label="Expert 0 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer12_exp1 [label="Expert 1 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer12_exp2 [label="Expert 2 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer12_exp3 [label="Expert 3 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer12_exp4 [label="Expert 4 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer12_exp5 [label="Expert 5 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer12_exp6 [label="Expert 6 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer12_exp7 [label="Expert 7 L12 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu44_layer12 [label="MLP L12 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu44_layer13 [label="Attention L13 GPU44\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu44_layer13 [label="Gate L13 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu44_layer13_exp0 [label="Expert 0 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer13_exp1 [label="Expert 1 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer13_exp2 [label="Expert 2 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer13_exp3 [label="Expert 3 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer13_exp4 [label="Expert 4 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer13_exp5 [label="Expert 5 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer13_exp6 [label="Expert 6 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer13_exp7 [label="Expert 7 L13 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu44_layer13 [label="MLP L13 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu44_layer14 [label="Attention L14 GPU44\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu44_layer14 [label="Gate L14 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu44_layer14_exp0 [label="Expert 0 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer14_exp1 [label="Expert 1 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer14_exp2 [label="Expert 2 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer14_exp3 [label="Expert 3 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer14_exp4 [label="Expert 4 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer14_exp5 [label="Expert 5 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer14_exp6 [label="Expert 6 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer14_exp7 [label="Expert 7 L14 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu44_layer14 [label="MLP L14 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu44_layer15 [label="Attention L15 GPU44\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu44_layer15 [label="Gate L15 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu44_layer15_exp0 [label="Expert 0 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer15_exp1 [label="Expert 1 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer15_exp2 [label="Expert 2 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer15_exp3 [label="Expert 3 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer15_exp4 [label="Expert 4 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer15_exp5 [label="Expert 5 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer15_exp6 [label="Expert 6 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu44_layer15_exp7 [label="Expert 7 L15 GPU44\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu44_layer15 [label="MLP L15 GPU44\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu45_layer12 [label="Attention L12 GPU45\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu45_layer12 [label="Gate L12 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu45_layer12_exp0 [label="Expert 0 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer12_exp1 [label="Expert 1 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer12_exp2 [label="Expert 2 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer12_exp3 [label="Expert 3 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer12_exp4 [label="Expert 4 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer12_exp5 [label="Expert 5 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer12_exp6 [label="Expert 6 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer12_exp7 [label="Expert 7 L12 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu45_layer12 [label="MLP L12 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu45_layer13 [label="Attention L13 GPU45\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu45_layer13 [label="Gate L13 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu45_layer13_exp0 [label="Expert 0 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer13_exp1 [label="Expert 1 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer13_exp2 [label="Expert 2 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer13_exp3 [label="Expert 3 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer13_exp4 [label="Expert 4 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer13_exp5 [label="Expert 5 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer13_exp6 [label="Expert 6 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer13_exp7 [label="Expert 7 L13 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu45_layer13 [label="MLP L13 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu45_layer14 [label="Attention L14 GPU45\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu45_layer14 [label="Gate L14 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu45_layer14_exp0 [label="Expert 0 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer14_exp1 [label="Expert 1 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer14_exp2 [label="Expert 2 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer14_exp3 [label="Expert 3 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer14_exp4 [label="Expert 4 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer14_exp5 [label="Expert 5 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer14_exp6 [label="Expert 6 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer14_exp7 [label="Expert 7 L14 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu45_layer14 [label="MLP L14 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu45_layer15 [label="Attention L15 GPU45\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu45_layer15 [label="Gate L15 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu45_layer15_exp0 [label="Expert 0 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer15_exp1 [label="Expert 1 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer15_exp2 [label="Expert 2 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer15_exp3 [label="Expert 3 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer15_exp4 [label="Expert 4 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer15_exp5 [label="Expert 5 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer15_exp6 [label="Expert 6 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu45_layer15_exp7 [label="Expert 7 L15 GPU45\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu45_layer15 [label="MLP L15 GPU45\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu46_layer12 [label="Attention L12 GPU46\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu46_layer12 [label="Gate L12 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu46_layer12_exp0 [label="Expert 0 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer12_exp1 [label="Expert 1 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer12_exp2 [label="Expert 2 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer12_exp3 [label="Expert 3 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer12_exp4 [label="Expert 4 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer12_exp5 [label="Expert 5 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer12_exp6 [label="Expert 6 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer12_exp7 [label="Expert 7 L12 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu46_layer12 [label="MLP L12 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu46_layer13 [label="Attention L13 GPU46\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu46_layer13 [label="Gate L13 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu46_layer13_exp0 [label="Expert 0 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer13_exp1 [label="Expert 1 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer13_exp2 [label="Expert 2 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer13_exp3 [label="Expert 3 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer13_exp4 [label="Expert 4 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer13_exp5 [label="Expert 5 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer13_exp6 [label="Expert 6 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer13_exp7 [label="Expert 7 L13 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu46_layer13 [label="MLP L13 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu46_layer14 [label="Attention L14 GPU46\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu46_layer14 [label="Gate L14 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu46_layer14_exp0 [label="Expert 0 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer14_exp1 [label="Expert 1 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer14_exp2 [label="Expert 2 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer14_exp3 [label="Expert 3 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer14_exp4 [label="Expert 4 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer14_exp5 [label="Expert 5 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer14_exp6 [label="Expert 6 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer14_exp7 [label="Expert 7 L14 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu46_layer14 [label="MLP L14 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu46_layer15 [label="Attention L15 GPU46\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu46_layer15 [label="Gate L15 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu46_layer15_exp0 [label="Expert 0 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer15_exp1 [label="Expert 1 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer15_exp2 [label="Expert 2 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer15_exp3 [label="Expert 3 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer15_exp4 [label="Expert 4 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer15_exp5 [label="Expert 5 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer15_exp6 [label="Expert 6 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu46_layer15_exp7 [label="Expert 7 L15 GPU46\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu46_layer15 [label="MLP L15 GPU46\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu47_layer12 [label="Attention L12 GPU47\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu47_layer12 [label="Gate L12 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu47_layer12_exp0 [label="Expert 0 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer12_exp1 [label="Expert 1 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer12_exp2 [label="Expert 2 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer12_exp3 [label="Expert 3 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer12_exp4 [label="Expert 4 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer12_exp5 [label="Expert 5 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer12_exp6 [label="Expert 6 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer12_exp7 [label="Expert 7 L12 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu47_layer12 [label="MLP L12 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu47_layer13 [label="Attention L13 GPU47\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu47_layer13 [label="Gate L13 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu47_layer13_exp0 [label="Expert 0 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer13_exp1 [label="Expert 1 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer13_exp2 [label="Expert 2 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer13_exp3 [label="Expert 3 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer13_exp4 [label="Expert 4 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer13_exp5 [label="Expert 5 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer13_exp6 [label="Expert 6 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer13_exp7 [label="Expert 7 L13 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu47_layer13 [label="MLP L13 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu47_layer14 [label="Attention L14 GPU47\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu47_layer14 [label="Gate L14 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu47_layer14_exp0 [label="Expert 0 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer14_exp1 [label="Expert 1 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer14_exp2 [label="Expert 2 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer14_exp3 [label="Expert 3 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer14_exp4 [label="Expert 4 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer14_exp5 [label="Expert 5 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer14_exp6 [label="Expert 6 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer14_exp7 [label="Expert 7 L14 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu47_layer14 [label="MLP L14 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu47_layer15 [label="Attention L15 GPU47\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu47_layer15 [label="Gate L15 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu47_layer15_exp0 [label="Expert 0 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer15_exp1 [label="Expert 1 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer15_exp2 [label="Expert 2 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer15_exp3 [label="Expert 3 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer15_exp4 [label="Expert 4 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer15_exp5 [label="Expert 5 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer15_exp6 [label="Expert 6 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu47_layer15_exp7 [label="Expert 7 L15 GPU47\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu47_layer15 [label="MLP L15 GPU47\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // EP Group 3 - GPUs [48-63]
    subgraph cluster_ep3 {
        label="EP Group 3\nGPUs [48-63]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [48-51]
        subgraph cluster_pp3_0 {
            label="PP Stage 0\nGPUs [48-51]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu48_layer0 [label="Attention L0 GPU48\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu48_layer0 [label="Gate L0 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu48_layer0_exp0 [label="Expert 0 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer0_exp1 [label="Expert 1 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer0_exp2 [label="Expert 2 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer0_exp3 [label="Expert 3 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer0_exp4 [label="Expert 4 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer0_exp5 [label="Expert 5 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer0_exp6 [label="Expert 6 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer0_exp7 [label="Expert 7 L0 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu48_layer0 [label="MLP L0 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu48_layer1 [label="Attention L1 GPU48\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu48_layer1 [label="Gate L1 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu48_layer1_exp0 [label="Expert 0 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer1_exp1 [label="Expert 1 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer1_exp2 [label="Expert 2 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer1_exp3 [label="Expert 3 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer1_exp4 [label="Expert 4 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer1_exp5 [label="Expert 5 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer1_exp6 [label="Expert 6 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer1_exp7 [label="Expert 7 L1 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu48_layer1 [label="MLP L1 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu48_layer2 [label="Attention L2 GPU48\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu48_layer2 [label="Gate L2 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu48_layer2_exp0 [label="Expert 0 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer2_exp1 [label="Expert 1 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer2_exp2 [label="Expert 2 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer2_exp3 [label="Expert 3 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer2_exp4 [label="Expert 4 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer2_exp5 [label="Expert 5 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer2_exp6 [label="Expert 6 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer2_exp7 [label="Expert 7 L2 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu48_layer2 [label="MLP L2 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu48_layer3 [label="Attention L3 GPU48\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu48_layer3 [label="Gate L3 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu48_layer3_exp0 [label="Expert 0 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer3_exp1 [label="Expert 1 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer3_exp2 [label="Expert 2 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer3_exp3 [label="Expert 3 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer3_exp4 [label="Expert 4 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer3_exp5 [label="Expert 5 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer3_exp6 [label="Expert 6 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu48_layer3_exp7 [label="Expert 7 L3 GPU48\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu48_layer3 [label="MLP L3 GPU48\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu49_layer0 [label="Attention L0 GPU49\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu49_layer0 [label="Gate L0 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu49_layer0_exp0 [label="Expert 0 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer0_exp1 [label="Expert 1 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer0_exp2 [label="Expert 2 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer0_exp3 [label="Expert 3 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer0_exp4 [label="Expert 4 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer0_exp5 [label="Expert 5 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer0_exp6 [label="Expert 6 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer0_exp7 [label="Expert 7 L0 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu49_layer0 [label="MLP L0 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu49_layer1 [label="Attention L1 GPU49\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu49_layer1 [label="Gate L1 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu49_layer1_exp0 [label="Expert 0 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer1_exp1 [label="Expert 1 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer1_exp2 [label="Expert 2 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer1_exp3 [label="Expert 3 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer1_exp4 [label="Expert 4 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer1_exp5 [label="Expert 5 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer1_exp6 [label="Expert 6 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer1_exp7 [label="Expert 7 L1 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu49_layer1 [label="MLP L1 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu49_layer2 [label="Attention L2 GPU49\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu49_layer2 [label="Gate L2 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu49_layer2_exp0 [label="Expert 0 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer2_exp1 [label="Expert 1 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer2_exp2 [label="Expert 2 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer2_exp3 [label="Expert 3 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer2_exp4 [label="Expert 4 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer2_exp5 [label="Expert 5 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer2_exp6 [label="Expert 6 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer2_exp7 [label="Expert 7 L2 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu49_layer2 [label="MLP L2 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu49_layer3 [label="Attention L3 GPU49\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu49_layer3 [label="Gate L3 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu49_layer3_exp0 [label="Expert 0 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer3_exp1 [label="Expert 1 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer3_exp2 [label="Expert 2 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer3_exp3 [label="Expert 3 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer3_exp4 [label="Expert 4 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer3_exp5 [label="Expert 5 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer3_exp6 [label="Expert 6 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu49_layer3_exp7 [label="Expert 7 L3 GPU49\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu49_layer3 [label="MLP L3 GPU49\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu50_layer0 [label="Attention L0 GPU50\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu50_layer0 [label="Gate L0 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu50_layer0_exp0 [label="Expert 0 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer0_exp1 [label="Expert 1 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer0_exp2 [label="Expert 2 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer0_exp3 [label="Expert 3 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer0_exp4 [label="Expert 4 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer0_exp5 [label="Expert 5 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer0_exp6 [label="Expert 6 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer0_exp7 [label="Expert 7 L0 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu50_layer0 [label="MLP L0 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu50_layer1 [label="Attention L1 GPU50\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu50_layer1 [label="Gate L1 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu50_layer1_exp0 [label="Expert 0 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer1_exp1 [label="Expert 1 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer1_exp2 [label="Expert 2 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer1_exp3 [label="Expert 3 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer1_exp4 [label="Expert 4 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer1_exp5 [label="Expert 5 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer1_exp6 [label="Expert 6 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer1_exp7 [label="Expert 7 L1 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu50_layer1 [label="MLP L1 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu50_layer2 [label="Attention L2 GPU50\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu50_layer2 [label="Gate L2 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu50_layer2_exp0 [label="Expert 0 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer2_exp1 [label="Expert 1 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer2_exp2 [label="Expert 2 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer2_exp3 [label="Expert 3 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer2_exp4 [label="Expert 4 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer2_exp5 [label="Expert 5 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer2_exp6 [label="Expert 6 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer2_exp7 [label="Expert 7 L2 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu50_layer2 [label="MLP L2 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu50_layer3 [label="Attention L3 GPU50\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu50_layer3 [label="Gate L3 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu50_layer3_exp0 [label="Expert 0 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer3_exp1 [label="Expert 1 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer3_exp2 [label="Expert 2 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer3_exp3 [label="Expert 3 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer3_exp4 [label="Expert 4 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer3_exp5 [label="Expert 5 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer3_exp6 [label="Expert 6 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu50_layer3_exp7 [label="Expert 7 L3 GPU50\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu50_layer3 [label="MLP L3 GPU50\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu51_layer0 [label="Attention L0 GPU51\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu51_layer0 [label="Gate L0 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu51_layer0_exp0 [label="Expert 0 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer0_exp1 [label="Expert 1 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer0_exp2 [label="Expert 2 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer0_exp3 [label="Expert 3 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer0_exp4 [label="Expert 4 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer0_exp5 [label="Expert 5 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer0_exp6 [label="Expert 6 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer0_exp7 [label="Expert 7 L0 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu51_layer0 [label="MLP L0 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu51_layer1 [label="Attention L1 GPU51\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu51_layer1 [label="Gate L1 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu51_layer1_exp0 [label="Expert 0 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer1_exp1 [label="Expert 1 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer1_exp2 [label="Expert 2 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer1_exp3 [label="Expert 3 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer1_exp4 [label="Expert 4 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer1_exp5 [label="Expert 5 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer1_exp6 [label="Expert 6 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer1_exp7 [label="Expert 7 L1 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu51_layer1 [label="MLP L1 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu51_layer2 [label="Attention L2 GPU51\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu51_layer2 [label="Gate L2 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu51_layer2_exp0 [label="Expert 0 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer2_exp1 [label="Expert 1 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer2_exp2 [label="Expert 2 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer2_exp3 [label="Expert 3 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer2_exp4 [label="Expert 4 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer2_exp5 [label="Expert 5 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer2_exp6 [label="Expert 6 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer2_exp7 [label="Expert 7 L2 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu51_layer2 [label="MLP L2 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu51_layer3 [label="Attention L3 GPU51\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu51_layer3 [label="Gate L3 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu51_layer3_exp0 [label="Expert 0 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer3_exp1 [label="Expert 1 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer3_exp2 [label="Expert 2 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer3_exp3 [label="Expert 3 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer3_exp4 [label="Expert 4 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer3_exp5 [label="Expert 5 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer3_exp6 [label="Expert 6 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu51_layer3_exp7 [label="Expert 7 L3 GPU51\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu51_layer3 [label="MLP L3 GPU51\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [52-55]
        subgraph cluster_pp3_1 {
            label="PP Stage 1\nGPUs [52-55]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu52_layer4 [label="Attention L4 GPU52\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu52_layer4 [label="Gate L4 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu52_layer4_exp0 [label="Expert 0 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer4_exp1 [label="Expert 1 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer4_exp2 [label="Expert 2 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer4_exp3 [label="Expert 3 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer4_exp4 [label="Expert 4 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer4_exp5 [label="Expert 5 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer4_exp6 [label="Expert 6 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer4_exp7 [label="Expert 7 L4 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu52_layer4 [label="MLP L4 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu52_layer5 [label="Attention L5 GPU52\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu52_layer5 [label="Gate L5 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu52_layer5_exp0 [label="Expert 0 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer5_exp1 [label="Expert 1 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer5_exp2 [label="Expert 2 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer5_exp3 [label="Expert 3 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer5_exp4 [label="Expert 4 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer5_exp5 [label="Expert 5 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer5_exp6 [label="Expert 6 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer5_exp7 [label="Expert 7 L5 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu52_layer5 [label="MLP L5 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu52_layer6 [label="Attention L6 GPU52\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu52_layer6 [label="Gate L6 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu52_layer6_exp0 [label="Expert 0 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer6_exp1 [label="Expert 1 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer6_exp2 [label="Expert 2 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer6_exp3 [label="Expert 3 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer6_exp4 [label="Expert 4 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer6_exp5 [label="Expert 5 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer6_exp6 [label="Expert 6 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer6_exp7 [label="Expert 7 L6 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu52_layer6 [label="MLP L6 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu52_layer7 [label="Attention L7 GPU52\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu52_layer7 [label="Gate L7 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu52_layer7_exp0 [label="Expert 0 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer7_exp1 [label="Expert 1 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer7_exp2 [label="Expert 2 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer7_exp3 [label="Expert 3 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer7_exp4 [label="Expert 4 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer7_exp5 [label="Expert 5 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer7_exp6 [label="Expert 6 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu52_layer7_exp7 [label="Expert 7 L7 GPU52\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu52_layer7 [label="MLP L7 GPU52\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu53_layer4 [label="Attention L4 GPU53\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu53_layer4 [label="Gate L4 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu53_layer4_exp0 [label="Expert 0 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer4_exp1 [label="Expert 1 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer4_exp2 [label="Expert 2 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer4_exp3 [label="Expert 3 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer4_exp4 [label="Expert 4 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer4_exp5 [label="Expert 5 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer4_exp6 [label="Expert 6 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer4_exp7 [label="Expert 7 L4 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu53_layer4 [label="MLP L4 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu53_layer5 [label="Attention L5 GPU53\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu53_layer5 [label="Gate L5 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu53_layer5_exp0 [label="Expert 0 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer5_exp1 [label="Expert 1 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer5_exp2 [label="Expert 2 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer5_exp3 [label="Expert 3 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer5_exp4 [label="Expert 4 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer5_exp5 [label="Expert 5 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer5_exp6 [label="Expert 6 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer5_exp7 [label="Expert 7 L5 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu53_layer5 [label="MLP L5 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu53_layer6 [label="Attention L6 GPU53\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu53_layer6 [label="Gate L6 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu53_layer6_exp0 [label="Expert 0 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer6_exp1 [label="Expert 1 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer6_exp2 [label="Expert 2 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer6_exp3 [label="Expert 3 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer6_exp4 [label="Expert 4 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer6_exp5 [label="Expert 5 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer6_exp6 [label="Expert 6 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer6_exp7 [label="Expert 7 L6 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu53_layer6 [label="MLP L6 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu53_layer7 [label="Attention L7 GPU53\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu53_layer7 [label="Gate L7 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu53_layer7_exp0 [label="Expert 0 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer7_exp1 [label="Expert 1 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer7_exp2 [label="Expert 2 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer7_exp3 [label="Expert 3 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer7_exp4 [label="Expert 4 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer7_exp5 [label="Expert 5 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer7_exp6 [label="Expert 6 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu53_layer7_exp7 [label="Expert 7 L7 GPU53\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu53_layer7 [label="MLP L7 GPU53\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu54_layer4 [label="Attention L4 GPU54\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu54_layer4 [label="Gate L4 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu54_layer4_exp0 [label="Expert 0 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer4_exp1 [label="Expert 1 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer4_exp2 [label="Expert 2 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer4_exp3 [label="Expert 3 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer4_exp4 [label="Expert 4 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer4_exp5 [label="Expert 5 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer4_exp6 [label="Expert 6 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer4_exp7 [label="Expert 7 L4 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu54_layer4 [label="MLP L4 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu54_layer5 [label="Attention L5 GPU54\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu54_layer5 [label="Gate L5 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu54_layer5_exp0 [label="Expert 0 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer5_exp1 [label="Expert 1 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer5_exp2 [label="Expert 2 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer5_exp3 [label="Expert 3 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer5_exp4 [label="Expert 4 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer5_exp5 [label="Expert 5 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer5_exp6 [label="Expert 6 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer5_exp7 [label="Expert 7 L5 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu54_layer5 [label="MLP L5 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu54_layer6 [label="Attention L6 GPU54\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu54_layer6 [label="Gate L6 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu54_layer6_exp0 [label="Expert 0 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer6_exp1 [label="Expert 1 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer6_exp2 [label="Expert 2 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer6_exp3 [label="Expert 3 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer6_exp4 [label="Expert 4 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer6_exp5 [label="Expert 5 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer6_exp6 [label="Expert 6 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer6_exp7 [label="Expert 7 L6 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu54_layer6 [label="MLP L6 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu54_layer7 [label="Attention L7 GPU54\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu54_layer7 [label="Gate L7 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu54_layer7_exp0 [label="Expert 0 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer7_exp1 [label="Expert 1 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer7_exp2 [label="Expert 2 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer7_exp3 [label="Expert 3 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer7_exp4 [label="Expert 4 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer7_exp5 [label="Expert 5 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer7_exp6 [label="Expert 6 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu54_layer7_exp7 [label="Expert 7 L7 GPU54\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu54_layer7 [label="MLP L7 GPU54\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu55_layer4 [label="Attention L4 GPU55\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu55_layer4 [label="Gate L4 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu55_layer4_exp0 [label="Expert 0 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer4_exp1 [label="Expert 1 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer4_exp2 [label="Expert 2 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer4_exp3 [label="Expert 3 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer4_exp4 [label="Expert 4 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer4_exp5 [label="Expert 5 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer4_exp6 [label="Expert 6 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer4_exp7 [label="Expert 7 L4 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu55_layer4 [label="MLP L4 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu55_layer5 [label="Attention L5 GPU55\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu55_layer5 [label="Gate L5 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu55_layer5_exp0 [label="Expert 0 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer5_exp1 [label="Expert 1 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer5_exp2 [label="Expert 2 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer5_exp3 [label="Expert 3 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer5_exp4 [label="Expert 4 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer5_exp5 [label="Expert 5 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer5_exp6 [label="Expert 6 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer5_exp7 [label="Expert 7 L5 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu55_layer5 [label="MLP L5 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu55_layer6 [label="Attention L6 GPU55\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu55_layer6 [label="Gate L6 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu55_layer6_exp0 [label="Expert 0 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer6_exp1 [label="Expert 1 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer6_exp2 [label="Expert 2 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer6_exp3 [label="Expert 3 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer6_exp4 [label="Expert 4 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer6_exp5 [label="Expert 5 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer6_exp6 [label="Expert 6 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer6_exp7 [label="Expert 7 L6 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu55_layer6 [label="MLP L6 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu55_layer7 [label="Attention L7 GPU55\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu55_layer7 [label="Gate L7 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu55_layer7_exp0 [label="Expert 0 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer7_exp1 [label="Expert 1 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer7_exp2 [label="Expert 2 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer7_exp3 [label="Expert 3 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer7_exp4 [label="Expert 4 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer7_exp5 [label="Expert 5 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer7_exp6 [label="Expert 6 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu55_layer7_exp7 [label="Expert 7 L7 GPU55\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu55_layer7 [label="MLP L7 GPU55\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [56-59]
        subgraph cluster_pp3_2 {
            label="PP Stage 2\nGPUs [56-59]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu56_layer8 [label="Attention L8 GPU56\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu56_layer8 [label="Gate L8 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu56_layer8_exp0 [label="Expert 0 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer8_exp1 [label="Expert 1 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer8_exp2 [label="Expert 2 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer8_exp3 [label="Expert 3 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer8_exp4 [label="Expert 4 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer8_exp5 [label="Expert 5 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer8_exp6 [label="Expert 6 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer8_exp7 [label="Expert 7 L8 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu56_layer8 [label="MLP L8 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu56_layer9 [label="Attention L9 GPU56\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu56_layer9 [label="Gate L9 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu56_layer9_exp0 [label="Expert 0 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer9_exp1 [label="Expert 1 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer9_exp2 [label="Expert 2 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer9_exp3 [label="Expert 3 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer9_exp4 [label="Expert 4 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer9_exp5 [label="Expert 5 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer9_exp6 [label="Expert 6 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer9_exp7 [label="Expert 7 L9 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu56_layer9 [label="MLP L9 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu56_layer10 [label="Attention L10 GPU56\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu56_layer10 [label="Gate L10 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu56_layer10_exp0 [label="Expert 0 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer10_exp1 [label="Expert 1 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer10_exp2 [label="Expert 2 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer10_exp3 [label="Expert 3 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer10_exp4 [label="Expert 4 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer10_exp5 [label="Expert 5 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer10_exp6 [label="Expert 6 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer10_exp7 [label="Expert 7 L10 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu56_layer10 [label="MLP L10 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu56_layer11 [label="Attention L11 GPU56\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu56_layer11 [label="Gate L11 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu56_layer11_exp0 [label="Expert 0 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer11_exp1 [label="Expert 1 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer11_exp2 [label="Expert 2 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer11_exp3 [label="Expert 3 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer11_exp4 [label="Expert 4 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer11_exp5 [label="Expert 5 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer11_exp6 [label="Expert 6 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu56_layer11_exp7 [label="Expert 7 L11 GPU56\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu56_layer11 [label="MLP L11 GPU56\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu57_layer8 [label="Attention L8 GPU57\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu57_layer8 [label="Gate L8 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu57_layer8_exp0 [label="Expert 0 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer8_exp1 [label="Expert 1 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer8_exp2 [label="Expert 2 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer8_exp3 [label="Expert 3 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer8_exp4 [label="Expert 4 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer8_exp5 [label="Expert 5 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer8_exp6 [label="Expert 6 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer8_exp7 [label="Expert 7 L8 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu57_layer8 [label="MLP L8 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu57_layer9 [label="Attention L9 GPU57\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu57_layer9 [label="Gate L9 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu57_layer9_exp0 [label="Expert 0 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer9_exp1 [label="Expert 1 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer9_exp2 [label="Expert 2 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer9_exp3 [label="Expert 3 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer9_exp4 [label="Expert 4 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer9_exp5 [label="Expert 5 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer9_exp6 [label="Expert 6 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer9_exp7 [label="Expert 7 L9 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu57_layer9 [label="MLP L9 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu57_layer10 [label="Attention L10 GPU57\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu57_layer10 [label="Gate L10 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu57_layer10_exp0 [label="Expert 0 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer10_exp1 [label="Expert 1 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer10_exp2 [label="Expert 2 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer10_exp3 [label="Expert 3 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer10_exp4 [label="Expert 4 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer10_exp5 [label="Expert 5 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer10_exp6 [label="Expert 6 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer10_exp7 [label="Expert 7 L10 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu57_layer10 [label="MLP L10 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu57_layer11 [label="Attention L11 GPU57\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu57_layer11 [label="Gate L11 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu57_layer11_exp0 [label="Expert 0 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer11_exp1 [label="Expert 1 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer11_exp2 [label="Expert 2 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer11_exp3 [label="Expert 3 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer11_exp4 [label="Expert 4 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer11_exp5 [label="Expert 5 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer11_exp6 [label="Expert 6 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu57_layer11_exp7 [label="Expert 7 L11 GPU57\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu57_layer11 [label="MLP L11 GPU57\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu58_layer8 [label="Attention L8 GPU58\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu58_layer8 [label="Gate L8 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu58_layer8_exp0 [label="Expert 0 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer8_exp1 [label="Expert 1 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer8_exp2 [label="Expert 2 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer8_exp3 [label="Expert 3 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer8_exp4 [label="Expert 4 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer8_exp5 [label="Expert 5 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer8_exp6 [label="Expert 6 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer8_exp7 [label="Expert 7 L8 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu58_layer8 [label="MLP L8 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu58_layer9 [label="Attention L9 GPU58\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu58_layer9 [label="Gate L9 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu58_layer9_exp0 [label="Expert 0 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer9_exp1 [label="Expert 1 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer9_exp2 [label="Expert 2 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer9_exp3 [label="Expert 3 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer9_exp4 [label="Expert 4 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer9_exp5 [label="Expert 5 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer9_exp6 [label="Expert 6 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer9_exp7 [label="Expert 7 L9 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu58_layer9 [label="MLP L9 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu58_layer10 [label="Attention L10 GPU58\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu58_layer10 [label="Gate L10 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu58_layer10_exp0 [label="Expert 0 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer10_exp1 [label="Expert 1 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer10_exp2 [label="Expert 2 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer10_exp3 [label="Expert 3 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer10_exp4 [label="Expert 4 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer10_exp5 [label="Expert 5 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer10_exp6 [label="Expert 6 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer10_exp7 [label="Expert 7 L10 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu58_layer10 [label="MLP L10 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu58_layer11 [label="Attention L11 GPU58\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu58_layer11 [label="Gate L11 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu58_layer11_exp0 [label="Expert 0 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer11_exp1 [label="Expert 1 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer11_exp2 [label="Expert 2 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer11_exp3 [label="Expert 3 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer11_exp4 [label="Expert 4 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer11_exp5 [label="Expert 5 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer11_exp6 [label="Expert 6 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu58_layer11_exp7 [label="Expert 7 L11 GPU58\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu58_layer11 [label="MLP L11 GPU58\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu59_layer8 [label="Attention L8 GPU59\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu59_layer8 [label="Gate L8 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu59_layer8_exp0 [label="Expert 0 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer8_exp1 [label="Expert 1 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer8_exp2 [label="Expert 2 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer8_exp3 [label="Expert 3 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer8_exp4 [label="Expert 4 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer8_exp5 [label="Expert 5 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer8_exp6 [label="Expert 6 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer8_exp7 [label="Expert 7 L8 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu59_layer8 [label="MLP L8 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu59_layer9 [label="Attention L9 GPU59\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu59_layer9 [label="Gate L9 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu59_layer9_exp0 [label="Expert 0 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer9_exp1 [label="Expert 1 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer9_exp2 [label="Expert 2 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer9_exp3 [label="Expert 3 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer9_exp4 [label="Expert 4 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer9_exp5 [label="Expert 5 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer9_exp6 [label="Expert 6 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer9_exp7 [label="Expert 7 L9 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu59_layer9 [label="MLP L9 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu59_layer10 [label="Attention L10 GPU59\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu59_layer10 [label="Gate L10 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu59_layer10_exp0 [label="Expert 0 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer10_exp1 [label="Expert 1 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer10_exp2 [label="Expert 2 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer10_exp3 [label="Expert 3 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer10_exp4 [label="Expert 4 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer10_exp5 [label="Expert 5 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer10_exp6 [label="Expert 6 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer10_exp7 [label="Expert 7 L10 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu59_layer10 [label="MLP L10 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu59_layer11 [label="Attention L11 GPU59\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu59_layer11 [label="Gate L11 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu59_layer11_exp0 [label="Expert 0 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer11_exp1 [label="Expert 1 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer11_exp2 [label="Expert 2 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer11_exp3 [label="Expert 3 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer11_exp4 [label="Expert 4 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer11_exp5 [label="Expert 5 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer11_exp6 [label="Expert 6 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu59_layer11_exp7 [label="Expert 7 L11 GPU59\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu59_layer11 [label="MLP L11 GPU59\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [60-63]
        subgraph cluster_pp3_3 {
            label="PP Stage 3\nGPUs [60-63]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu60_layer12 [label="Attention L12 GPU60\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu60_layer12 [label="Gate L12 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu60_layer12_exp0 [label="Expert 0 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer12_exp1 [label="Expert 1 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer12_exp2 [label="Expert 2 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer12_exp3 [label="Expert 3 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer12_exp4 [label="Expert 4 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer12_exp5 [label="Expert 5 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer12_exp6 [label="Expert 6 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer12_exp7 [label="Expert 7 L12 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu60_layer12 [label="MLP L12 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu60_layer13 [label="Attention L13 GPU60\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu60_layer13 [label="Gate L13 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu60_layer13_exp0 [label="Expert 0 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer13_exp1 [label="Expert 1 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer13_exp2 [label="Expert 2 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer13_exp3 [label="Expert 3 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer13_exp4 [label="Expert 4 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer13_exp5 [label="Expert 5 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer13_exp6 [label="Expert 6 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer13_exp7 [label="Expert 7 L13 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu60_layer13 [label="MLP L13 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu60_layer14 [label="Attention L14 GPU60\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu60_layer14 [label="Gate L14 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu60_layer14_exp0 [label="Expert 0 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer14_exp1 [label="Expert 1 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer14_exp2 [label="Expert 2 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer14_exp3 [label="Expert 3 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer14_exp4 [label="Expert 4 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer14_exp5 [label="Expert 5 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer14_exp6 [label="Expert 6 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer14_exp7 [label="Expert 7 L14 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu60_layer14 [label="MLP L14 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu60_layer15 [label="Attention L15 GPU60\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu60_layer15 [label="Gate L15 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu60_layer15_exp0 [label="Expert 0 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer15_exp1 [label="Expert 1 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer15_exp2 [label="Expert 2 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer15_exp3 [label="Expert 3 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer15_exp4 [label="Expert 4 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer15_exp5 [label="Expert 5 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer15_exp6 [label="Expert 6 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu60_layer15_exp7 [label="Expert 7 L15 GPU60\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu60_layer15 [label="MLP L15 GPU60\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu61_layer12 [label="Attention L12 GPU61\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu61_layer12 [label="Gate L12 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu61_layer12_exp0 [label="Expert 0 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer12_exp1 [label="Expert 1 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer12_exp2 [label="Expert 2 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer12_exp3 [label="Expert 3 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer12_exp4 [label="Expert 4 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer12_exp5 [label="Expert 5 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer12_exp6 [label="Expert 6 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer12_exp7 [label="Expert 7 L12 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu61_layer12 [label="MLP L12 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu61_layer13 [label="Attention L13 GPU61\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu61_layer13 [label="Gate L13 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu61_layer13_exp0 [label="Expert 0 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer13_exp1 [label="Expert 1 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer13_exp2 [label="Expert 2 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer13_exp3 [label="Expert 3 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer13_exp4 [label="Expert 4 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer13_exp5 [label="Expert 5 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer13_exp6 [label="Expert 6 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer13_exp7 [label="Expert 7 L13 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu61_layer13 [label="MLP L13 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu61_layer14 [label="Attention L14 GPU61\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu61_layer14 [label="Gate L14 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu61_layer14_exp0 [label="Expert 0 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer14_exp1 [label="Expert 1 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer14_exp2 [label="Expert 2 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer14_exp3 [label="Expert 3 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer14_exp4 [label="Expert 4 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer14_exp5 [label="Expert 5 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer14_exp6 [label="Expert 6 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer14_exp7 [label="Expert 7 L14 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu61_layer14 [label="MLP L14 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu61_layer15 [label="Attention L15 GPU61\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu61_layer15 [label="Gate L15 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu61_layer15_exp0 [label="Expert 0 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer15_exp1 [label="Expert 1 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer15_exp2 [label="Expert 2 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer15_exp3 [label="Expert 3 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer15_exp4 [label="Expert 4 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer15_exp5 [label="Expert 5 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer15_exp6 [label="Expert 6 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu61_layer15_exp7 [label="Expert 7 L15 GPU61\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu61_layer15 [label="MLP L15 GPU61\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu62_layer12 [label="Attention L12 GPU62\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu62_layer12 [label="Gate L12 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu62_layer12_exp0 [label="Expert 0 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer12_exp1 [label="Expert 1 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer12_exp2 [label="Expert 2 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer12_exp3 [label="Expert 3 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer12_exp4 [label="Expert 4 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer12_exp5 [label="Expert 5 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer12_exp6 [label="Expert 6 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer12_exp7 [label="Expert 7 L12 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu62_layer12 [label="MLP L12 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu62_layer13 [label="Attention L13 GPU62\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu62_layer13 [label="Gate L13 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu62_layer13_exp0 [label="Expert 0 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer13_exp1 [label="Expert 1 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer13_exp2 [label="Expert 2 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer13_exp3 [label="Expert 3 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer13_exp4 [label="Expert 4 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer13_exp5 [label="Expert 5 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer13_exp6 [label="Expert 6 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer13_exp7 [label="Expert 7 L13 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu62_layer13 [label="MLP L13 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu62_layer14 [label="Attention L14 GPU62\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu62_layer14 [label="Gate L14 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu62_layer14_exp0 [label="Expert 0 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer14_exp1 [label="Expert 1 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer14_exp2 [label="Expert 2 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer14_exp3 [label="Expert 3 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer14_exp4 [label="Expert 4 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer14_exp5 [label="Expert 5 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer14_exp6 [label="Expert 6 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer14_exp7 [label="Expert 7 L14 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu62_layer14 [label="MLP L14 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu62_layer15 [label="Attention L15 GPU62\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu62_layer15 [label="Gate L15 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu62_layer15_exp0 [label="Expert 0 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer15_exp1 [label="Expert 1 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer15_exp2 [label="Expert 2 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer15_exp3 [label="Expert 3 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer15_exp4 [label="Expert 4 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer15_exp5 [label="Expert 5 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer15_exp6 [label="Expert 6 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu62_layer15_exp7 [label="Expert 7 L15 GPU62\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu62_layer15 [label="MLP L15 GPU62\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu63_layer12 [label="Attention L12 GPU63\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu63_layer12 [label="Gate L12 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu63_layer12_exp0 [label="Expert 0 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer12_exp1 [label="Expert 1 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer12_exp2 [label="Expert 2 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer12_exp3 [label="Expert 3 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer12_exp4 [label="Expert 4 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer12_exp5 [label="Expert 5 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer12_exp6 [label="Expert 6 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer12_exp7 [label="Expert 7 L12 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu63_layer12 [label="MLP L12 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu63_layer13 [label="Attention L13 GPU63\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu63_layer13 [label="Gate L13 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu63_layer13_exp0 [label="Expert 0 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer13_exp1 [label="Expert 1 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer13_exp2 [label="Expert 2 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer13_exp3 [label="Expert 3 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer13_exp4 [label="Expert 4 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer13_exp5 [label="Expert 5 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer13_exp6 [label="Expert 6 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer13_exp7 [label="Expert 7 L13 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu63_layer13 [label="MLP L13 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu63_layer14 [label="Attention L14 GPU63\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu63_layer14 [label="Gate L14 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu63_layer14_exp0 [label="Expert 0 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer14_exp1 [label="Expert 1 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer14_exp2 [label="Expert 2 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer14_exp3 [label="Expert 3 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer14_exp4 [label="Expert 4 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer14_exp5 [label="Expert 5 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer14_exp6 [label="Expert 6 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer14_exp7 [label="Expert 7 L14 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu63_layer14 [label="MLP L14 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu63_layer15 [label="Attention L15 GPU63\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu63_layer15 [label="Gate L15 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu63_layer15_exp0 [label="Expert 0 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer15_exp1 [label="Expert 1 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer15_exp2 [label="Expert 2 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer15_exp3 [label="Expert 3 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer15_exp4 [label="Expert 4 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer15_exp5 [label="Expert 5 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer15_exp6 [label="Expert 6 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu63_layer15_exp7 [label="Expert 7 L15 GPU63\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu63_layer15 [label="MLP L15 GPU63\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // EP Group 4 - GPUs [64-79]
    subgraph cluster_ep4 {
        label="EP Group 4\nGPUs [64-79]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [64-67]
        subgraph cluster_pp4_0 {
            label="PP Stage 0\nGPUs [64-67]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu64_layer0 [label="Attention L0 GPU64\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu64_layer0 [label="Gate L0 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu64_layer0_exp0 [label="Expert 0 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer0_exp1 [label="Expert 1 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer0_exp2 [label="Expert 2 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer0_exp3 [label="Expert 3 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer0_exp4 [label="Expert 4 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer0_exp5 [label="Expert 5 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer0_exp6 [label="Expert 6 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer0_exp7 [label="Expert 7 L0 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu64_layer0 [label="MLP L0 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu64_layer1 [label="Attention L1 GPU64\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu64_layer1 [label="Gate L1 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu64_layer1_exp0 [label="Expert 0 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer1_exp1 [label="Expert 1 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer1_exp2 [label="Expert 2 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer1_exp3 [label="Expert 3 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer1_exp4 [label="Expert 4 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer1_exp5 [label="Expert 5 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer1_exp6 [label="Expert 6 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer1_exp7 [label="Expert 7 L1 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu64_layer1 [label="MLP L1 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu64_layer2 [label="Attention L2 GPU64\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu64_layer2 [label="Gate L2 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu64_layer2_exp0 [label="Expert 0 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer2_exp1 [label="Expert 1 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer2_exp2 [label="Expert 2 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer2_exp3 [label="Expert 3 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer2_exp4 [label="Expert 4 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer2_exp5 [label="Expert 5 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer2_exp6 [label="Expert 6 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer2_exp7 [label="Expert 7 L2 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu64_layer2 [label="MLP L2 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu64_layer3 [label="Attention L3 GPU64\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu64_layer3 [label="Gate L3 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu64_layer3_exp0 [label="Expert 0 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer3_exp1 [label="Expert 1 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer3_exp2 [label="Expert 2 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer3_exp3 [label="Expert 3 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer3_exp4 [label="Expert 4 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer3_exp5 [label="Expert 5 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer3_exp6 [label="Expert 6 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu64_layer3_exp7 [label="Expert 7 L3 GPU64\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu64_layer3 [label="MLP L3 GPU64\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu65_layer0 [label="Attention L0 GPU65\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu65_layer0 [label="Gate L0 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu65_layer0_exp0 [label="Expert 0 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer0_exp1 [label="Expert 1 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer0_exp2 [label="Expert 2 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer0_exp3 [label="Expert 3 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer0_exp4 [label="Expert 4 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer0_exp5 [label="Expert 5 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer0_exp6 [label="Expert 6 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer0_exp7 [label="Expert 7 L0 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu65_layer0 [label="MLP L0 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu65_layer1 [label="Attention L1 GPU65\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu65_layer1 [label="Gate L1 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu65_layer1_exp0 [label="Expert 0 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer1_exp1 [label="Expert 1 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer1_exp2 [label="Expert 2 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer1_exp3 [label="Expert 3 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer1_exp4 [label="Expert 4 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer1_exp5 [label="Expert 5 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer1_exp6 [label="Expert 6 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer1_exp7 [label="Expert 7 L1 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu65_layer1 [label="MLP L1 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu65_layer2 [label="Attention L2 GPU65\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu65_layer2 [label="Gate L2 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu65_layer2_exp0 [label="Expert 0 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer2_exp1 [label="Expert 1 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer2_exp2 [label="Expert 2 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer2_exp3 [label="Expert 3 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer2_exp4 [label="Expert 4 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer2_exp5 [label="Expert 5 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer2_exp6 [label="Expert 6 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer2_exp7 [label="Expert 7 L2 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu65_layer2 [label="MLP L2 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu65_layer3 [label="Attention L3 GPU65\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu65_layer3 [label="Gate L3 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu65_layer3_exp0 [label="Expert 0 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer3_exp1 [label="Expert 1 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer3_exp2 [label="Expert 2 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer3_exp3 [label="Expert 3 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer3_exp4 [label="Expert 4 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer3_exp5 [label="Expert 5 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer3_exp6 [label="Expert 6 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu65_layer3_exp7 [label="Expert 7 L3 GPU65\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu65_layer3 [label="MLP L3 GPU65\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu66_layer0 [label="Attention L0 GPU66\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu66_layer0 [label="Gate L0 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu66_layer0_exp0 [label="Expert 0 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer0_exp1 [label="Expert 1 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer0_exp2 [label="Expert 2 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer0_exp3 [label="Expert 3 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer0_exp4 [label="Expert 4 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer0_exp5 [label="Expert 5 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer0_exp6 [label="Expert 6 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer0_exp7 [label="Expert 7 L0 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu66_layer0 [label="MLP L0 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu66_layer1 [label="Attention L1 GPU66\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu66_layer1 [label="Gate L1 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu66_layer1_exp0 [label="Expert 0 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer1_exp1 [label="Expert 1 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer1_exp2 [label="Expert 2 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer1_exp3 [label="Expert 3 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer1_exp4 [label="Expert 4 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer1_exp5 [label="Expert 5 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer1_exp6 [label="Expert 6 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer1_exp7 [label="Expert 7 L1 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu66_layer1 [label="MLP L1 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu66_layer2 [label="Attention L2 GPU66\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu66_layer2 [label="Gate L2 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu66_layer2_exp0 [label="Expert 0 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer2_exp1 [label="Expert 1 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer2_exp2 [label="Expert 2 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer2_exp3 [label="Expert 3 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer2_exp4 [label="Expert 4 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer2_exp5 [label="Expert 5 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer2_exp6 [label="Expert 6 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer2_exp7 [label="Expert 7 L2 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu66_layer2 [label="MLP L2 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu66_layer3 [label="Attention L3 GPU66\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu66_layer3 [label="Gate L3 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu66_layer3_exp0 [label="Expert 0 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer3_exp1 [label="Expert 1 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer3_exp2 [label="Expert 2 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer3_exp3 [label="Expert 3 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer3_exp4 [label="Expert 4 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer3_exp5 [label="Expert 5 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer3_exp6 [label="Expert 6 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu66_layer3_exp7 [label="Expert 7 L3 GPU66\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu66_layer3 [label="MLP L3 GPU66\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu67_layer0 [label="Attention L0 GPU67\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu67_layer0 [label="Gate L0 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu67_layer0_exp0 [label="Expert 0 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer0_exp1 [label="Expert 1 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer0_exp2 [label="Expert 2 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer0_exp3 [label="Expert 3 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer0_exp4 [label="Expert 4 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer0_exp5 [label="Expert 5 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer0_exp6 [label="Expert 6 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer0_exp7 [label="Expert 7 L0 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu67_layer0 [label="MLP L0 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu67_layer1 [label="Attention L1 GPU67\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu67_layer1 [label="Gate L1 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu67_layer1_exp0 [label="Expert 0 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer1_exp1 [label="Expert 1 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer1_exp2 [label="Expert 2 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer1_exp3 [label="Expert 3 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer1_exp4 [label="Expert 4 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer1_exp5 [label="Expert 5 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer1_exp6 [label="Expert 6 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer1_exp7 [label="Expert 7 L1 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu67_layer1 [label="MLP L1 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu67_layer2 [label="Attention L2 GPU67\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu67_layer2 [label="Gate L2 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu67_layer2_exp0 [label="Expert 0 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer2_exp1 [label="Expert 1 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer2_exp2 [label="Expert 2 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer2_exp3 [label="Expert 3 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer2_exp4 [label="Expert 4 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer2_exp5 [label="Expert 5 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer2_exp6 [label="Expert 6 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer2_exp7 [label="Expert 7 L2 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu67_layer2 [label="MLP L2 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu67_layer3 [label="Attention L3 GPU67\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu67_layer3 [label="Gate L3 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu67_layer3_exp0 [label="Expert 0 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer3_exp1 [label="Expert 1 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer3_exp2 [label="Expert 2 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer3_exp3 [label="Expert 3 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer3_exp4 [label="Expert 4 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer3_exp5 [label="Expert 5 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer3_exp6 [label="Expert 6 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu67_layer3_exp7 [label="Expert 7 L3 GPU67\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu67_layer3 [label="MLP L3 GPU67\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [68-71]
        subgraph cluster_pp4_1 {
            label="PP Stage 1\nGPUs [68-71]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu68_layer4 [label="Attention L4 GPU68\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu68_layer4 [label="Gate L4 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu68_layer4_exp0 [label="Expert 0 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer4_exp1 [label="Expert 1 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer4_exp2 [label="Expert 2 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer4_exp3 [label="Expert 3 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer4_exp4 [label="Expert 4 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer4_exp5 [label="Expert 5 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer4_exp6 [label="Expert 6 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer4_exp7 [label="Expert 7 L4 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu68_layer4 [label="MLP L4 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu68_layer5 [label="Attention L5 GPU68\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu68_layer5 [label="Gate L5 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu68_layer5_exp0 [label="Expert 0 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer5_exp1 [label="Expert 1 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer5_exp2 [label="Expert 2 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer5_exp3 [label="Expert 3 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer5_exp4 [label="Expert 4 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer5_exp5 [label="Expert 5 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer5_exp6 [label="Expert 6 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer5_exp7 [label="Expert 7 L5 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu68_layer5 [label="MLP L5 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu68_layer6 [label="Attention L6 GPU68\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu68_layer6 [label="Gate L6 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu68_layer6_exp0 [label="Expert 0 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer6_exp1 [label="Expert 1 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer6_exp2 [label="Expert 2 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer6_exp3 [label="Expert 3 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer6_exp4 [label="Expert 4 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer6_exp5 [label="Expert 5 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer6_exp6 [label="Expert 6 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer6_exp7 [label="Expert 7 L6 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu68_layer6 [label="MLP L6 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu68_layer7 [label="Attention L7 GPU68\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu68_layer7 [label="Gate L7 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu68_layer7_exp0 [label="Expert 0 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer7_exp1 [label="Expert 1 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer7_exp2 [label="Expert 2 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer7_exp3 [label="Expert 3 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer7_exp4 [label="Expert 4 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer7_exp5 [label="Expert 5 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer7_exp6 [label="Expert 6 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu68_layer7_exp7 [label="Expert 7 L7 GPU68\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu68_layer7 [label="MLP L7 GPU68\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu69_layer4 [label="Attention L4 GPU69\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu69_layer4 [label="Gate L4 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu69_layer4_exp0 [label="Expert 0 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer4_exp1 [label="Expert 1 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer4_exp2 [label="Expert 2 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer4_exp3 [label="Expert 3 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer4_exp4 [label="Expert 4 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer4_exp5 [label="Expert 5 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer4_exp6 [label="Expert 6 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer4_exp7 [label="Expert 7 L4 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu69_layer4 [label="MLP L4 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu69_layer5 [label="Attention L5 GPU69\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu69_layer5 [label="Gate L5 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu69_layer5_exp0 [label="Expert 0 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer5_exp1 [label="Expert 1 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer5_exp2 [label="Expert 2 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer5_exp3 [label="Expert 3 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer5_exp4 [label="Expert 4 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer5_exp5 [label="Expert 5 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer5_exp6 [label="Expert 6 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer5_exp7 [label="Expert 7 L5 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu69_layer5 [label="MLP L5 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu69_layer6 [label="Attention L6 GPU69\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu69_layer6 [label="Gate L6 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu69_layer6_exp0 [label="Expert 0 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer6_exp1 [label="Expert 1 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer6_exp2 [label="Expert 2 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer6_exp3 [label="Expert 3 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer6_exp4 [label="Expert 4 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer6_exp5 [label="Expert 5 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer6_exp6 [label="Expert 6 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer6_exp7 [label="Expert 7 L6 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu69_layer6 [label="MLP L6 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu69_layer7 [label="Attention L7 GPU69\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu69_layer7 [label="Gate L7 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu69_layer7_exp0 [label="Expert 0 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer7_exp1 [label="Expert 1 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer7_exp2 [label="Expert 2 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer7_exp3 [label="Expert 3 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer7_exp4 [label="Expert 4 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer7_exp5 [label="Expert 5 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer7_exp6 [label="Expert 6 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu69_layer7_exp7 [label="Expert 7 L7 GPU69\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu69_layer7 [label="MLP L7 GPU69\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu70_layer4 [label="Attention L4 GPU70\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu70_layer4 [label="Gate L4 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu70_layer4_exp0 [label="Expert 0 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer4_exp1 [label="Expert 1 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer4_exp2 [label="Expert 2 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer4_exp3 [label="Expert 3 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer4_exp4 [label="Expert 4 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer4_exp5 [label="Expert 5 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer4_exp6 [label="Expert 6 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer4_exp7 [label="Expert 7 L4 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu70_layer4 [label="MLP L4 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu70_layer5 [label="Attention L5 GPU70\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu70_layer5 [label="Gate L5 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu70_layer5_exp0 [label="Expert 0 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer5_exp1 [label="Expert 1 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer5_exp2 [label="Expert 2 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer5_exp3 [label="Expert 3 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer5_exp4 [label="Expert 4 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer5_exp5 [label="Expert 5 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer5_exp6 [label="Expert 6 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer5_exp7 [label="Expert 7 L5 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu70_layer5 [label="MLP L5 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu70_layer6 [label="Attention L6 GPU70\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu70_layer6 [label="Gate L6 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu70_layer6_exp0 [label="Expert 0 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer6_exp1 [label="Expert 1 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer6_exp2 [label="Expert 2 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer6_exp3 [label="Expert 3 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer6_exp4 [label="Expert 4 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer6_exp5 [label="Expert 5 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer6_exp6 [label="Expert 6 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer6_exp7 [label="Expert 7 L6 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu70_layer6 [label="MLP L6 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu70_layer7 [label="Attention L7 GPU70\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu70_layer7 [label="Gate L7 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu70_layer7_exp0 [label="Expert 0 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer7_exp1 [label="Expert 1 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer7_exp2 [label="Expert 2 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer7_exp3 [label="Expert 3 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer7_exp4 [label="Expert 4 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer7_exp5 [label="Expert 5 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer7_exp6 [label="Expert 6 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu70_layer7_exp7 [label="Expert 7 L7 GPU70\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu70_layer7 [label="MLP L7 GPU70\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu71_layer4 [label="Attention L4 GPU71\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu71_layer4 [label="Gate L4 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu71_layer4_exp0 [label="Expert 0 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer4_exp1 [label="Expert 1 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer4_exp2 [label="Expert 2 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer4_exp3 [label="Expert 3 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer4_exp4 [label="Expert 4 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer4_exp5 [label="Expert 5 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer4_exp6 [label="Expert 6 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer4_exp7 [label="Expert 7 L4 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu71_layer4 [label="MLP L4 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu71_layer5 [label="Attention L5 GPU71\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu71_layer5 [label="Gate L5 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu71_layer5_exp0 [label="Expert 0 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer5_exp1 [label="Expert 1 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer5_exp2 [label="Expert 2 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer5_exp3 [label="Expert 3 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer5_exp4 [label="Expert 4 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer5_exp5 [label="Expert 5 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer5_exp6 [label="Expert 6 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer5_exp7 [label="Expert 7 L5 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu71_layer5 [label="MLP L5 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu71_layer6 [label="Attention L6 GPU71\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu71_layer6 [label="Gate L6 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu71_layer6_exp0 [label="Expert 0 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer6_exp1 [label="Expert 1 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer6_exp2 [label="Expert 2 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer6_exp3 [label="Expert 3 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer6_exp4 [label="Expert 4 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer6_exp5 [label="Expert 5 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer6_exp6 [label="Expert 6 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer6_exp7 [label="Expert 7 L6 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu71_layer6 [label="MLP L6 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu71_layer7 [label="Attention L7 GPU71\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu71_layer7 [label="Gate L7 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu71_layer7_exp0 [label="Expert 0 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer7_exp1 [label="Expert 1 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer7_exp2 [label="Expert 2 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer7_exp3 [label="Expert 3 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer7_exp4 [label="Expert 4 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer7_exp5 [label="Expert 5 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer7_exp6 [label="Expert 6 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu71_layer7_exp7 [label="Expert 7 L7 GPU71\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu71_layer7 [label="MLP L7 GPU71\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [72-75]
        subgraph cluster_pp4_2 {
            label="PP Stage 2\nGPUs [72-75]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu72_layer8 [label="Attention L8 GPU72\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu72_layer8 [label="Gate L8 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu72_layer8_exp0 [label="Expert 0 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer8_exp1 [label="Expert 1 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer8_exp2 [label="Expert 2 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer8_exp3 [label="Expert 3 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer8_exp4 [label="Expert 4 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer8_exp5 [label="Expert 5 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer8_exp6 [label="Expert 6 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer8_exp7 [label="Expert 7 L8 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu72_layer8 [label="MLP L8 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu72_layer9 [label="Attention L9 GPU72\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu72_layer9 [label="Gate L9 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu72_layer9_exp0 [label="Expert 0 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer9_exp1 [label="Expert 1 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer9_exp2 [label="Expert 2 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer9_exp3 [label="Expert 3 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer9_exp4 [label="Expert 4 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer9_exp5 [label="Expert 5 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer9_exp6 [label="Expert 6 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer9_exp7 [label="Expert 7 L9 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu72_layer9 [label="MLP L9 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu72_layer10 [label="Attention L10 GPU72\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu72_layer10 [label="Gate L10 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu72_layer10_exp0 [label="Expert 0 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer10_exp1 [label="Expert 1 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer10_exp2 [label="Expert 2 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer10_exp3 [label="Expert 3 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer10_exp4 [label="Expert 4 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer10_exp5 [label="Expert 5 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer10_exp6 [label="Expert 6 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer10_exp7 [label="Expert 7 L10 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu72_layer10 [label="MLP L10 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu72_layer11 [label="Attention L11 GPU72\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu72_layer11 [label="Gate L11 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu72_layer11_exp0 [label="Expert 0 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer11_exp1 [label="Expert 1 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer11_exp2 [label="Expert 2 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer11_exp3 [label="Expert 3 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer11_exp4 [label="Expert 4 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer11_exp5 [label="Expert 5 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer11_exp6 [label="Expert 6 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu72_layer11_exp7 [label="Expert 7 L11 GPU72\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu72_layer11 [label="MLP L11 GPU72\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu73_layer8 [label="Attention L8 GPU73\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu73_layer8 [label="Gate L8 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu73_layer8_exp0 [label="Expert 0 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer8_exp1 [label="Expert 1 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer8_exp2 [label="Expert 2 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer8_exp3 [label="Expert 3 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer8_exp4 [label="Expert 4 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer8_exp5 [label="Expert 5 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer8_exp6 [label="Expert 6 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer8_exp7 [label="Expert 7 L8 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu73_layer8 [label="MLP L8 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu73_layer9 [label="Attention L9 GPU73\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu73_layer9 [label="Gate L9 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu73_layer9_exp0 [label="Expert 0 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer9_exp1 [label="Expert 1 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer9_exp2 [label="Expert 2 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer9_exp3 [label="Expert 3 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer9_exp4 [label="Expert 4 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer9_exp5 [label="Expert 5 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer9_exp6 [label="Expert 6 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer9_exp7 [label="Expert 7 L9 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu73_layer9 [label="MLP L9 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu73_layer10 [label="Attention L10 GPU73\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu73_layer10 [label="Gate L10 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu73_layer10_exp0 [label="Expert 0 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer10_exp1 [label="Expert 1 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer10_exp2 [label="Expert 2 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer10_exp3 [label="Expert 3 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer10_exp4 [label="Expert 4 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer10_exp5 [label="Expert 5 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer10_exp6 [label="Expert 6 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer10_exp7 [label="Expert 7 L10 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu73_layer10 [label="MLP L10 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu73_layer11 [label="Attention L11 GPU73\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu73_layer11 [label="Gate L11 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu73_layer11_exp0 [label="Expert 0 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer11_exp1 [label="Expert 1 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer11_exp2 [label="Expert 2 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer11_exp3 [label="Expert 3 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer11_exp4 [label="Expert 4 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer11_exp5 [label="Expert 5 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer11_exp6 [label="Expert 6 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu73_layer11_exp7 [label="Expert 7 L11 GPU73\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu73_layer11 [label="MLP L11 GPU73\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu74_layer8 [label="Attention L8 GPU74\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu74_layer8 [label="Gate L8 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu74_layer8_exp0 [label="Expert 0 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer8_exp1 [label="Expert 1 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer8_exp2 [label="Expert 2 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer8_exp3 [label="Expert 3 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer8_exp4 [label="Expert 4 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer8_exp5 [label="Expert 5 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer8_exp6 [label="Expert 6 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer8_exp7 [label="Expert 7 L8 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu74_layer8 [label="MLP L8 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu74_layer9 [label="Attention L9 GPU74\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu74_layer9 [label="Gate L9 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu74_layer9_exp0 [label="Expert 0 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer9_exp1 [label="Expert 1 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer9_exp2 [label="Expert 2 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer9_exp3 [label="Expert 3 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer9_exp4 [label="Expert 4 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer9_exp5 [label="Expert 5 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer9_exp6 [label="Expert 6 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer9_exp7 [label="Expert 7 L9 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu74_layer9 [label="MLP L9 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu74_layer10 [label="Attention L10 GPU74\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu74_layer10 [label="Gate L10 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu74_layer10_exp0 [label="Expert 0 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer10_exp1 [label="Expert 1 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer10_exp2 [label="Expert 2 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer10_exp3 [label="Expert 3 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer10_exp4 [label="Expert 4 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer10_exp5 [label="Expert 5 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer10_exp6 [label="Expert 6 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer10_exp7 [label="Expert 7 L10 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu74_layer10 [label="MLP L10 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu74_layer11 [label="Attention L11 GPU74\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu74_layer11 [label="Gate L11 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu74_layer11_exp0 [label="Expert 0 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer11_exp1 [label="Expert 1 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer11_exp2 [label="Expert 2 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer11_exp3 [label="Expert 3 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer11_exp4 [label="Expert 4 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer11_exp5 [label="Expert 5 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer11_exp6 [label="Expert 6 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu74_layer11_exp7 [label="Expert 7 L11 GPU74\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu74_layer11 [label="MLP L11 GPU74\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu75_layer8 [label="Attention L8 GPU75\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu75_layer8 [label="Gate L8 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu75_layer8_exp0 [label="Expert 0 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer8_exp1 [label="Expert 1 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer8_exp2 [label="Expert 2 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer8_exp3 [label="Expert 3 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer8_exp4 [label="Expert 4 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer8_exp5 [label="Expert 5 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer8_exp6 [label="Expert 6 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer8_exp7 [label="Expert 7 L8 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu75_layer8 [label="MLP L8 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu75_layer9 [label="Attention L9 GPU75\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu75_layer9 [label="Gate L9 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu75_layer9_exp0 [label="Expert 0 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer9_exp1 [label="Expert 1 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer9_exp2 [label="Expert 2 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer9_exp3 [label="Expert 3 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer9_exp4 [label="Expert 4 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer9_exp5 [label="Expert 5 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer9_exp6 [label="Expert 6 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer9_exp7 [label="Expert 7 L9 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu75_layer9 [label="MLP L9 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu75_layer10 [label="Attention L10 GPU75\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu75_layer10 [label="Gate L10 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu75_layer10_exp0 [label="Expert 0 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer10_exp1 [label="Expert 1 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer10_exp2 [label="Expert 2 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer10_exp3 [label="Expert 3 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer10_exp4 [label="Expert 4 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer10_exp5 [label="Expert 5 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer10_exp6 [label="Expert 6 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer10_exp7 [label="Expert 7 L10 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu75_layer10 [label="MLP L10 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu75_layer11 [label="Attention L11 GPU75\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu75_layer11 [label="Gate L11 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu75_layer11_exp0 [label="Expert 0 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer11_exp1 [label="Expert 1 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer11_exp2 [label="Expert 2 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer11_exp3 [label="Expert 3 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer11_exp4 [label="Expert 4 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer11_exp5 [label="Expert 5 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer11_exp6 [label="Expert 6 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu75_layer11_exp7 [label="Expert 7 L11 GPU75\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu75_layer11 [label="MLP L11 GPU75\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [76-79]
        subgraph cluster_pp4_3 {
            label="PP Stage 3\nGPUs [76-79]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu76_layer12 [label="Attention L12 GPU76\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu76_layer12 [label="Gate L12 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu76_layer12_exp0 [label="Expert 0 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer12_exp1 [label="Expert 1 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer12_exp2 [label="Expert 2 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer12_exp3 [label="Expert 3 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer12_exp4 [label="Expert 4 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer12_exp5 [label="Expert 5 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer12_exp6 [label="Expert 6 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer12_exp7 [label="Expert 7 L12 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu76_layer12 [label="MLP L12 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu76_layer13 [label="Attention L13 GPU76\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu76_layer13 [label="Gate L13 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu76_layer13_exp0 [label="Expert 0 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer13_exp1 [label="Expert 1 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer13_exp2 [label="Expert 2 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer13_exp3 [label="Expert 3 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer13_exp4 [label="Expert 4 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer13_exp5 [label="Expert 5 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer13_exp6 [label="Expert 6 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer13_exp7 [label="Expert 7 L13 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu76_layer13 [label="MLP L13 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu76_layer14 [label="Attention L14 GPU76\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu76_layer14 [label="Gate L14 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu76_layer14_exp0 [label="Expert 0 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer14_exp1 [label="Expert 1 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer14_exp2 [label="Expert 2 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer14_exp3 [label="Expert 3 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer14_exp4 [label="Expert 4 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer14_exp5 [label="Expert 5 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer14_exp6 [label="Expert 6 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer14_exp7 [label="Expert 7 L14 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu76_layer14 [label="MLP L14 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu76_layer15 [label="Attention L15 GPU76\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu76_layer15 [label="Gate L15 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu76_layer15_exp0 [label="Expert 0 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer15_exp1 [label="Expert 1 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer15_exp2 [label="Expert 2 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer15_exp3 [label="Expert 3 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer15_exp4 [label="Expert 4 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer15_exp5 [label="Expert 5 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer15_exp6 [label="Expert 6 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu76_layer15_exp7 [label="Expert 7 L15 GPU76\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu76_layer15 [label="MLP L15 GPU76\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu77_layer12 [label="Attention L12 GPU77\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu77_layer12 [label="Gate L12 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu77_layer12_exp0 [label="Expert 0 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer12_exp1 [label="Expert 1 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer12_exp2 [label="Expert 2 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer12_exp3 [label="Expert 3 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer12_exp4 [label="Expert 4 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer12_exp5 [label="Expert 5 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer12_exp6 [label="Expert 6 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer12_exp7 [label="Expert 7 L12 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu77_layer12 [label="MLP L12 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu77_layer13 [label="Attention L13 GPU77\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu77_layer13 [label="Gate L13 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu77_layer13_exp0 [label="Expert 0 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer13_exp1 [label="Expert 1 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer13_exp2 [label="Expert 2 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer13_exp3 [label="Expert 3 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer13_exp4 [label="Expert 4 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer13_exp5 [label="Expert 5 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer13_exp6 [label="Expert 6 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer13_exp7 [label="Expert 7 L13 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu77_layer13 [label="MLP L13 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu77_layer14 [label="Attention L14 GPU77\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu77_layer14 [label="Gate L14 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu77_layer14_exp0 [label="Expert 0 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer14_exp1 [label="Expert 1 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer14_exp2 [label="Expert 2 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer14_exp3 [label="Expert 3 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer14_exp4 [label="Expert 4 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer14_exp5 [label="Expert 5 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer14_exp6 [label="Expert 6 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer14_exp7 [label="Expert 7 L14 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu77_layer14 [label="MLP L14 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu77_layer15 [label="Attention L15 GPU77\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu77_layer15 [label="Gate L15 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu77_layer15_exp0 [label="Expert 0 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer15_exp1 [label="Expert 1 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer15_exp2 [label="Expert 2 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer15_exp3 [label="Expert 3 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer15_exp4 [label="Expert 4 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer15_exp5 [label="Expert 5 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer15_exp6 [label="Expert 6 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu77_layer15_exp7 [label="Expert 7 L15 GPU77\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu77_layer15 [label="MLP L15 GPU77\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu78_layer12 [label="Attention L12 GPU78\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu78_layer12 [label="Gate L12 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu78_layer12_exp0 [label="Expert 0 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer12_exp1 [label="Expert 1 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer12_exp2 [label="Expert 2 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer12_exp3 [label="Expert 3 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer12_exp4 [label="Expert 4 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer12_exp5 [label="Expert 5 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer12_exp6 [label="Expert 6 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer12_exp7 [label="Expert 7 L12 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu78_layer12 [label="MLP L12 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu78_layer13 [label="Attention L13 GPU78\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu78_layer13 [label="Gate L13 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu78_layer13_exp0 [label="Expert 0 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer13_exp1 [label="Expert 1 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer13_exp2 [label="Expert 2 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer13_exp3 [label="Expert 3 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer13_exp4 [label="Expert 4 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer13_exp5 [label="Expert 5 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer13_exp6 [label="Expert 6 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer13_exp7 [label="Expert 7 L13 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu78_layer13 [label="MLP L13 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu78_layer14 [label="Attention L14 GPU78\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu78_layer14 [label="Gate L14 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu78_layer14_exp0 [label="Expert 0 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer14_exp1 [label="Expert 1 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer14_exp2 [label="Expert 2 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer14_exp3 [label="Expert 3 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer14_exp4 [label="Expert 4 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer14_exp5 [label="Expert 5 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer14_exp6 [label="Expert 6 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer14_exp7 [label="Expert 7 L14 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu78_layer14 [label="MLP L14 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu78_layer15 [label="Attention L15 GPU78\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu78_layer15 [label="Gate L15 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu78_layer15_exp0 [label="Expert 0 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer15_exp1 [label="Expert 1 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer15_exp2 [label="Expert 2 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer15_exp3 [label="Expert 3 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer15_exp4 [label="Expert 4 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer15_exp5 [label="Expert 5 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer15_exp6 [label="Expert 6 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu78_layer15_exp7 [label="Expert 7 L15 GPU78\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu78_layer15 [label="MLP L15 GPU78\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu79_layer12 [label="Attention L12 GPU79\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu79_layer12 [label="Gate L12 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu79_layer12_exp0 [label="Expert 0 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer12_exp1 [label="Expert 1 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer12_exp2 [label="Expert 2 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer12_exp3 [label="Expert 3 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer12_exp4 [label="Expert 4 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer12_exp5 [label="Expert 5 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer12_exp6 [label="Expert 6 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer12_exp7 [label="Expert 7 L12 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu79_layer12 [label="MLP L12 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu79_layer13 [label="Attention L13 GPU79\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu79_layer13 [label="Gate L13 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu79_layer13_exp0 [label="Expert 0 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer13_exp1 [label="Expert 1 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer13_exp2 [label="Expert 2 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer13_exp3 [label="Expert 3 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer13_exp4 [label="Expert 4 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer13_exp5 [label="Expert 5 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer13_exp6 [label="Expert 6 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer13_exp7 [label="Expert 7 L13 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu79_layer13 [label="MLP L13 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu79_layer14 [label="Attention L14 GPU79\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu79_layer14 [label="Gate L14 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu79_layer14_exp0 [label="Expert 0 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer14_exp1 [label="Expert 1 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer14_exp2 [label="Expert 2 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer14_exp3 [label="Expert 3 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer14_exp4 [label="Expert 4 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer14_exp5 [label="Expert 5 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer14_exp6 [label="Expert 6 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer14_exp7 [label="Expert 7 L14 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu79_layer14 [label="MLP L14 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu79_layer15 [label="Attention L15 GPU79\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu79_layer15 [label="Gate L15 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu79_layer15_exp0 [label="Expert 0 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer15_exp1 [label="Expert 1 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer15_exp2 [label="Expert 2 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer15_exp3 [label="Expert 3 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer15_exp4 [label="Expert 4 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer15_exp5 [label="Expert 5 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer15_exp6 [label="Expert 6 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu79_layer15_exp7 [label="Expert 7 L15 GPU79\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu79_layer15 [label="MLP L15 GPU79\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // EP Group 5 - GPUs [80-95]
    subgraph cluster_ep5 {
        label="EP Group 5\nGPUs [80-95]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [80-83]
        subgraph cluster_pp5_0 {
            label="PP Stage 0\nGPUs [80-83]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu80_layer0 [label="Attention L0 GPU80\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu80_layer0 [label="Gate L0 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu80_layer0_exp0 [label="Expert 0 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer0_exp1 [label="Expert 1 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer0_exp2 [label="Expert 2 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer0_exp3 [label="Expert 3 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer0_exp4 [label="Expert 4 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer0_exp5 [label="Expert 5 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer0_exp6 [label="Expert 6 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer0_exp7 [label="Expert 7 L0 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu80_layer0 [label="MLP L0 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu80_layer1 [label="Attention L1 GPU80\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu80_layer1 [label="Gate L1 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu80_layer1_exp0 [label="Expert 0 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer1_exp1 [label="Expert 1 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer1_exp2 [label="Expert 2 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer1_exp3 [label="Expert 3 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer1_exp4 [label="Expert 4 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer1_exp5 [label="Expert 5 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer1_exp6 [label="Expert 6 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer1_exp7 [label="Expert 7 L1 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu80_layer1 [label="MLP L1 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu80_layer2 [label="Attention L2 GPU80\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu80_layer2 [label="Gate L2 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu80_layer2_exp0 [label="Expert 0 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer2_exp1 [label="Expert 1 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer2_exp2 [label="Expert 2 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer2_exp3 [label="Expert 3 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer2_exp4 [label="Expert 4 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer2_exp5 [label="Expert 5 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer2_exp6 [label="Expert 6 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer2_exp7 [label="Expert 7 L2 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu80_layer2 [label="MLP L2 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu80_layer3 [label="Attention L3 GPU80\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu80_layer3 [label="Gate L3 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu80_layer3_exp0 [label="Expert 0 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer3_exp1 [label="Expert 1 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer3_exp2 [label="Expert 2 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer3_exp3 [label="Expert 3 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer3_exp4 [label="Expert 4 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer3_exp5 [label="Expert 5 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer3_exp6 [label="Expert 6 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu80_layer3_exp7 [label="Expert 7 L3 GPU80\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu80_layer3 [label="MLP L3 GPU80\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu81_layer0 [label="Attention L0 GPU81\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu81_layer0 [label="Gate L0 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu81_layer0_exp0 [label="Expert 0 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer0_exp1 [label="Expert 1 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer0_exp2 [label="Expert 2 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer0_exp3 [label="Expert 3 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer0_exp4 [label="Expert 4 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer0_exp5 [label="Expert 5 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer0_exp6 [label="Expert 6 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer0_exp7 [label="Expert 7 L0 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu81_layer0 [label="MLP L0 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu81_layer1 [label="Attention L1 GPU81\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu81_layer1 [label="Gate L1 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu81_layer1_exp0 [label="Expert 0 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer1_exp1 [label="Expert 1 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer1_exp2 [label="Expert 2 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer1_exp3 [label="Expert 3 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer1_exp4 [label="Expert 4 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer1_exp5 [label="Expert 5 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer1_exp6 [label="Expert 6 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer1_exp7 [label="Expert 7 L1 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu81_layer1 [label="MLP L1 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu81_layer2 [label="Attention L2 GPU81\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu81_layer2 [label="Gate L2 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu81_layer2_exp0 [label="Expert 0 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer2_exp1 [label="Expert 1 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer2_exp2 [label="Expert 2 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer2_exp3 [label="Expert 3 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer2_exp4 [label="Expert 4 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer2_exp5 [label="Expert 5 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer2_exp6 [label="Expert 6 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer2_exp7 [label="Expert 7 L2 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu81_layer2 [label="MLP L2 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu81_layer3 [label="Attention L3 GPU81\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu81_layer3 [label="Gate L3 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu81_layer3_exp0 [label="Expert 0 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer3_exp1 [label="Expert 1 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer3_exp2 [label="Expert 2 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer3_exp3 [label="Expert 3 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer3_exp4 [label="Expert 4 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer3_exp5 [label="Expert 5 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer3_exp6 [label="Expert 6 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu81_layer3_exp7 [label="Expert 7 L3 GPU81\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu81_layer3 [label="MLP L3 GPU81\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu82_layer0 [label="Attention L0 GPU82\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu82_layer0 [label="Gate L0 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu82_layer0_exp0 [label="Expert 0 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer0_exp1 [label="Expert 1 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer0_exp2 [label="Expert 2 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer0_exp3 [label="Expert 3 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer0_exp4 [label="Expert 4 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer0_exp5 [label="Expert 5 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer0_exp6 [label="Expert 6 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer0_exp7 [label="Expert 7 L0 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu82_layer0 [label="MLP L0 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu82_layer1 [label="Attention L1 GPU82\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu82_layer1 [label="Gate L1 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu82_layer1_exp0 [label="Expert 0 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer1_exp1 [label="Expert 1 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer1_exp2 [label="Expert 2 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer1_exp3 [label="Expert 3 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer1_exp4 [label="Expert 4 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer1_exp5 [label="Expert 5 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer1_exp6 [label="Expert 6 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer1_exp7 [label="Expert 7 L1 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu82_layer1 [label="MLP L1 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu82_layer2 [label="Attention L2 GPU82\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu82_layer2 [label="Gate L2 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu82_layer2_exp0 [label="Expert 0 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer2_exp1 [label="Expert 1 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer2_exp2 [label="Expert 2 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer2_exp3 [label="Expert 3 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer2_exp4 [label="Expert 4 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer2_exp5 [label="Expert 5 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer2_exp6 [label="Expert 6 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer2_exp7 [label="Expert 7 L2 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu82_layer2 [label="MLP L2 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu82_layer3 [label="Attention L3 GPU82\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu82_layer3 [label="Gate L3 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu82_layer3_exp0 [label="Expert 0 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer3_exp1 [label="Expert 1 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer3_exp2 [label="Expert 2 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer3_exp3 [label="Expert 3 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer3_exp4 [label="Expert 4 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer3_exp5 [label="Expert 5 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer3_exp6 [label="Expert 6 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu82_layer3_exp7 [label="Expert 7 L3 GPU82\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu82_layer3 [label="MLP L3 GPU82\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu83_layer0 [label="Attention L0 GPU83\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu83_layer0 [label="Gate L0 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu83_layer0_exp0 [label="Expert 0 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer0_exp1 [label="Expert 1 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer0_exp2 [label="Expert 2 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer0_exp3 [label="Expert 3 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer0_exp4 [label="Expert 4 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer0_exp5 [label="Expert 5 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer0_exp6 [label="Expert 6 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer0_exp7 [label="Expert 7 L0 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu83_layer0 [label="MLP L0 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu83_layer1 [label="Attention L1 GPU83\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu83_layer1 [label="Gate L1 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu83_layer1_exp0 [label="Expert 0 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer1_exp1 [label="Expert 1 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer1_exp2 [label="Expert 2 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer1_exp3 [label="Expert 3 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer1_exp4 [label="Expert 4 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer1_exp5 [label="Expert 5 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer1_exp6 [label="Expert 6 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer1_exp7 [label="Expert 7 L1 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu83_layer1 [label="MLP L1 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu83_layer2 [label="Attention L2 GPU83\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu83_layer2 [label="Gate L2 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu83_layer2_exp0 [label="Expert 0 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer2_exp1 [label="Expert 1 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer2_exp2 [label="Expert 2 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer2_exp3 [label="Expert 3 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer2_exp4 [label="Expert 4 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer2_exp5 [label="Expert 5 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer2_exp6 [label="Expert 6 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer2_exp7 [label="Expert 7 L2 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu83_layer2 [label="MLP L2 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu83_layer3 [label="Attention L3 GPU83\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu83_layer3 [label="Gate L3 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu83_layer3_exp0 [label="Expert 0 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer3_exp1 [label="Expert 1 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer3_exp2 [label="Expert 2 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer3_exp3 [label="Expert 3 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer3_exp4 [label="Expert 4 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer3_exp5 [label="Expert 5 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer3_exp6 [label="Expert 6 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu83_layer3_exp7 [label="Expert 7 L3 GPU83\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu83_layer3 [label="MLP L3 GPU83\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [84-87]
        subgraph cluster_pp5_1 {
            label="PP Stage 1\nGPUs [84-87]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu84_layer4 [label="Attention L4 GPU84\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu84_layer4 [label="Gate L4 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu84_layer4_exp0 [label="Expert 0 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer4_exp1 [label="Expert 1 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer4_exp2 [label="Expert 2 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer4_exp3 [label="Expert 3 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer4_exp4 [label="Expert 4 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer4_exp5 [label="Expert 5 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer4_exp6 [label="Expert 6 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer4_exp7 [label="Expert 7 L4 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu84_layer4 [label="MLP L4 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu84_layer5 [label="Attention L5 GPU84\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu84_layer5 [label="Gate L5 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu84_layer5_exp0 [label="Expert 0 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer5_exp1 [label="Expert 1 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer5_exp2 [label="Expert 2 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer5_exp3 [label="Expert 3 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer5_exp4 [label="Expert 4 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer5_exp5 [label="Expert 5 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer5_exp6 [label="Expert 6 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer5_exp7 [label="Expert 7 L5 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu84_layer5 [label="MLP L5 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu84_layer6 [label="Attention L6 GPU84\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu84_layer6 [label="Gate L6 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu84_layer6_exp0 [label="Expert 0 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer6_exp1 [label="Expert 1 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer6_exp2 [label="Expert 2 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer6_exp3 [label="Expert 3 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer6_exp4 [label="Expert 4 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer6_exp5 [label="Expert 5 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer6_exp6 [label="Expert 6 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer6_exp7 [label="Expert 7 L6 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu84_layer6 [label="MLP L6 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu84_layer7 [label="Attention L7 GPU84\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu84_layer7 [label="Gate L7 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu84_layer7_exp0 [label="Expert 0 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer7_exp1 [label="Expert 1 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer7_exp2 [label="Expert 2 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer7_exp3 [label="Expert 3 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer7_exp4 [label="Expert 4 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer7_exp5 [label="Expert 5 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer7_exp6 [label="Expert 6 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu84_layer7_exp7 [label="Expert 7 L7 GPU84\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu84_layer7 [label="MLP L7 GPU84\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu85_layer4 [label="Attention L4 GPU85\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu85_layer4 [label="Gate L4 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu85_layer4_exp0 [label="Expert 0 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer4_exp1 [label="Expert 1 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer4_exp2 [label="Expert 2 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer4_exp3 [label="Expert 3 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer4_exp4 [label="Expert 4 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer4_exp5 [label="Expert 5 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer4_exp6 [label="Expert 6 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer4_exp7 [label="Expert 7 L4 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu85_layer4 [label="MLP L4 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu85_layer5 [label="Attention L5 GPU85\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu85_layer5 [label="Gate L5 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu85_layer5_exp0 [label="Expert 0 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer5_exp1 [label="Expert 1 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer5_exp2 [label="Expert 2 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer5_exp3 [label="Expert 3 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer5_exp4 [label="Expert 4 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer5_exp5 [label="Expert 5 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer5_exp6 [label="Expert 6 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer5_exp7 [label="Expert 7 L5 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu85_layer5 [label="MLP L5 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu85_layer6 [label="Attention L6 GPU85\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu85_layer6 [label="Gate L6 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu85_layer6_exp0 [label="Expert 0 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer6_exp1 [label="Expert 1 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer6_exp2 [label="Expert 2 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer6_exp3 [label="Expert 3 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer6_exp4 [label="Expert 4 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer6_exp5 [label="Expert 5 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer6_exp6 [label="Expert 6 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer6_exp7 [label="Expert 7 L6 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu85_layer6 [label="MLP L6 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu85_layer7 [label="Attention L7 GPU85\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu85_layer7 [label="Gate L7 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu85_layer7_exp0 [label="Expert 0 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer7_exp1 [label="Expert 1 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer7_exp2 [label="Expert 2 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer7_exp3 [label="Expert 3 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer7_exp4 [label="Expert 4 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer7_exp5 [label="Expert 5 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer7_exp6 [label="Expert 6 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu85_layer7_exp7 [label="Expert 7 L7 GPU85\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu85_layer7 [label="MLP L7 GPU85\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu86_layer4 [label="Attention L4 GPU86\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu86_layer4 [label="Gate L4 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu86_layer4_exp0 [label="Expert 0 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer4_exp1 [label="Expert 1 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer4_exp2 [label="Expert 2 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer4_exp3 [label="Expert 3 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer4_exp4 [label="Expert 4 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer4_exp5 [label="Expert 5 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer4_exp6 [label="Expert 6 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer4_exp7 [label="Expert 7 L4 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu86_layer4 [label="MLP L4 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu86_layer5 [label="Attention L5 GPU86\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu86_layer5 [label="Gate L5 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu86_layer5_exp0 [label="Expert 0 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer5_exp1 [label="Expert 1 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer5_exp2 [label="Expert 2 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer5_exp3 [label="Expert 3 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer5_exp4 [label="Expert 4 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer5_exp5 [label="Expert 5 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer5_exp6 [label="Expert 6 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer5_exp7 [label="Expert 7 L5 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu86_layer5 [label="MLP L5 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu86_layer6 [label="Attention L6 GPU86\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu86_layer6 [label="Gate L6 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu86_layer6_exp0 [label="Expert 0 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer6_exp1 [label="Expert 1 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer6_exp2 [label="Expert 2 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer6_exp3 [label="Expert 3 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer6_exp4 [label="Expert 4 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer6_exp5 [label="Expert 5 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer6_exp6 [label="Expert 6 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer6_exp7 [label="Expert 7 L6 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu86_layer6 [label="MLP L6 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu86_layer7 [label="Attention L7 GPU86\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu86_layer7 [label="Gate L7 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu86_layer7_exp0 [label="Expert 0 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer7_exp1 [label="Expert 1 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer7_exp2 [label="Expert 2 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer7_exp3 [label="Expert 3 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer7_exp4 [label="Expert 4 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer7_exp5 [label="Expert 5 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer7_exp6 [label="Expert 6 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu86_layer7_exp7 [label="Expert 7 L7 GPU86\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu86_layer7 [label="MLP L7 GPU86\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu87_layer4 [label="Attention L4 GPU87\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu87_layer4 [label="Gate L4 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu87_layer4_exp0 [label="Expert 0 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer4_exp1 [label="Expert 1 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer4_exp2 [label="Expert 2 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer4_exp3 [label="Expert 3 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer4_exp4 [label="Expert 4 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer4_exp5 [label="Expert 5 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer4_exp6 [label="Expert 6 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer4_exp7 [label="Expert 7 L4 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu87_layer4 [label="MLP L4 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu87_layer5 [label="Attention L5 GPU87\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu87_layer5 [label="Gate L5 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu87_layer5_exp0 [label="Expert 0 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer5_exp1 [label="Expert 1 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer5_exp2 [label="Expert 2 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer5_exp3 [label="Expert 3 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer5_exp4 [label="Expert 4 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer5_exp5 [label="Expert 5 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer5_exp6 [label="Expert 6 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer5_exp7 [label="Expert 7 L5 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu87_layer5 [label="MLP L5 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu87_layer6 [label="Attention L6 GPU87\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu87_layer6 [label="Gate L6 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu87_layer6_exp0 [label="Expert 0 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer6_exp1 [label="Expert 1 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer6_exp2 [label="Expert 2 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer6_exp3 [label="Expert 3 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer6_exp4 [label="Expert 4 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer6_exp5 [label="Expert 5 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer6_exp6 [label="Expert 6 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer6_exp7 [label="Expert 7 L6 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu87_layer6 [label="MLP L6 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu87_layer7 [label="Attention L7 GPU87\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu87_layer7 [label="Gate L7 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu87_layer7_exp0 [label="Expert 0 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer7_exp1 [label="Expert 1 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer7_exp2 [label="Expert 2 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer7_exp3 [label="Expert 3 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer7_exp4 [label="Expert 4 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer7_exp5 [label="Expert 5 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer7_exp6 [label="Expert 6 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu87_layer7_exp7 [label="Expert 7 L7 GPU87\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu87_layer7 [label="MLP L7 GPU87\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [88-91]
        subgraph cluster_pp5_2 {
            label="PP Stage 2\nGPUs [88-91]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu88_layer8 [label="Attention L8 GPU88\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu88_layer8 [label="Gate L8 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu88_layer8_exp0 [label="Expert 0 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer8_exp1 [label="Expert 1 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer8_exp2 [label="Expert 2 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer8_exp3 [label="Expert 3 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer8_exp4 [label="Expert 4 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer8_exp5 [label="Expert 5 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer8_exp6 [label="Expert 6 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer8_exp7 [label="Expert 7 L8 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu88_layer8 [label="MLP L8 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu88_layer9 [label="Attention L9 GPU88\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu88_layer9 [label="Gate L9 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu88_layer9_exp0 [label="Expert 0 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer9_exp1 [label="Expert 1 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer9_exp2 [label="Expert 2 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer9_exp3 [label="Expert 3 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer9_exp4 [label="Expert 4 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer9_exp5 [label="Expert 5 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer9_exp6 [label="Expert 6 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer9_exp7 [label="Expert 7 L9 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu88_layer9 [label="MLP L9 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu88_layer10 [label="Attention L10 GPU88\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu88_layer10 [label="Gate L10 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu88_layer10_exp0 [label="Expert 0 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer10_exp1 [label="Expert 1 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer10_exp2 [label="Expert 2 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer10_exp3 [label="Expert 3 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer10_exp4 [label="Expert 4 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer10_exp5 [label="Expert 5 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer10_exp6 [label="Expert 6 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer10_exp7 [label="Expert 7 L10 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu88_layer10 [label="MLP L10 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu88_layer11 [label="Attention L11 GPU88\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu88_layer11 [label="Gate L11 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu88_layer11_exp0 [label="Expert 0 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer11_exp1 [label="Expert 1 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer11_exp2 [label="Expert 2 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer11_exp3 [label="Expert 3 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer11_exp4 [label="Expert 4 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer11_exp5 [label="Expert 5 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer11_exp6 [label="Expert 6 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu88_layer11_exp7 [label="Expert 7 L11 GPU88\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu88_layer11 [label="MLP L11 GPU88\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu89_layer8 [label="Attention L8 GPU89\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu89_layer8 [label="Gate L8 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu89_layer8_exp0 [label="Expert 0 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer8_exp1 [label="Expert 1 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer8_exp2 [label="Expert 2 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer8_exp3 [label="Expert 3 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer8_exp4 [label="Expert 4 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer8_exp5 [label="Expert 5 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer8_exp6 [label="Expert 6 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer8_exp7 [label="Expert 7 L8 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu89_layer8 [label="MLP L8 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu89_layer9 [label="Attention L9 GPU89\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu89_layer9 [label="Gate L9 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu89_layer9_exp0 [label="Expert 0 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer9_exp1 [label="Expert 1 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer9_exp2 [label="Expert 2 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer9_exp3 [label="Expert 3 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer9_exp4 [label="Expert 4 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer9_exp5 [label="Expert 5 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer9_exp6 [label="Expert 6 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer9_exp7 [label="Expert 7 L9 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu89_layer9 [label="MLP L9 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu89_layer10 [label="Attention L10 GPU89\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu89_layer10 [label="Gate L10 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu89_layer10_exp0 [label="Expert 0 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer10_exp1 [label="Expert 1 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer10_exp2 [label="Expert 2 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer10_exp3 [label="Expert 3 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer10_exp4 [label="Expert 4 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer10_exp5 [label="Expert 5 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer10_exp6 [label="Expert 6 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer10_exp7 [label="Expert 7 L10 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu89_layer10 [label="MLP L10 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu89_layer11 [label="Attention L11 GPU89\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu89_layer11 [label="Gate L11 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu89_layer11_exp0 [label="Expert 0 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer11_exp1 [label="Expert 1 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer11_exp2 [label="Expert 2 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer11_exp3 [label="Expert 3 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer11_exp4 [label="Expert 4 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer11_exp5 [label="Expert 5 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer11_exp6 [label="Expert 6 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu89_layer11_exp7 [label="Expert 7 L11 GPU89\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu89_layer11 [label="MLP L11 GPU89\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu90_layer8 [label="Attention L8 GPU90\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu90_layer8 [label="Gate L8 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu90_layer8_exp0 [label="Expert 0 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer8_exp1 [label="Expert 1 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer8_exp2 [label="Expert 2 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer8_exp3 [label="Expert 3 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer8_exp4 [label="Expert 4 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer8_exp5 [label="Expert 5 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer8_exp6 [label="Expert 6 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer8_exp7 [label="Expert 7 L8 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu90_layer8 [label="MLP L8 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu90_layer9 [label="Attention L9 GPU90\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu90_layer9 [label="Gate L9 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu90_layer9_exp0 [label="Expert 0 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer9_exp1 [label="Expert 1 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer9_exp2 [label="Expert 2 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer9_exp3 [label="Expert 3 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer9_exp4 [label="Expert 4 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer9_exp5 [label="Expert 5 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer9_exp6 [label="Expert 6 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer9_exp7 [label="Expert 7 L9 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu90_layer9 [label="MLP L9 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu90_layer10 [label="Attention L10 GPU90\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu90_layer10 [label="Gate L10 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu90_layer10_exp0 [label="Expert 0 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer10_exp1 [label="Expert 1 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer10_exp2 [label="Expert 2 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer10_exp3 [label="Expert 3 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer10_exp4 [label="Expert 4 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer10_exp5 [label="Expert 5 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer10_exp6 [label="Expert 6 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer10_exp7 [label="Expert 7 L10 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu90_layer10 [label="MLP L10 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu90_layer11 [label="Attention L11 GPU90\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu90_layer11 [label="Gate L11 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu90_layer11_exp0 [label="Expert 0 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer11_exp1 [label="Expert 1 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer11_exp2 [label="Expert 2 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer11_exp3 [label="Expert 3 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer11_exp4 [label="Expert 4 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer11_exp5 [label="Expert 5 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer11_exp6 [label="Expert 6 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu90_layer11_exp7 [label="Expert 7 L11 GPU90\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu90_layer11 [label="MLP L11 GPU90\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu91_layer8 [label="Attention L8 GPU91\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu91_layer8 [label="Gate L8 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu91_layer8_exp0 [label="Expert 0 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer8_exp1 [label="Expert 1 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer8_exp2 [label="Expert 2 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer8_exp3 [label="Expert 3 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer8_exp4 [label="Expert 4 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer8_exp5 [label="Expert 5 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer8_exp6 [label="Expert 6 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer8_exp7 [label="Expert 7 L8 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu91_layer8 [label="MLP L8 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu91_layer9 [label="Attention L9 GPU91\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu91_layer9 [label="Gate L9 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu91_layer9_exp0 [label="Expert 0 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer9_exp1 [label="Expert 1 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer9_exp2 [label="Expert 2 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer9_exp3 [label="Expert 3 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer9_exp4 [label="Expert 4 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer9_exp5 [label="Expert 5 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer9_exp6 [label="Expert 6 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer9_exp7 [label="Expert 7 L9 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu91_layer9 [label="MLP L9 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu91_layer10 [label="Attention L10 GPU91\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu91_layer10 [label="Gate L10 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu91_layer10_exp0 [label="Expert 0 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer10_exp1 [label="Expert 1 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer10_exp2 [label="Expert 2 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer10_exp3 [label="Expert 3 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer10_exp4 [label="Expert 4 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer10_exp5 [label="Expert 5 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer10_exp6 [label="Expert 6 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer10_exp7 [label="Expert 7 L10 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu91_layer10 [label="MLP L10 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu91_layer11 [label="Attention L11 GPU91\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu91_layer11 [label="Gate L11 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu91_layer11_exp0 [label="Expert 0 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer11_exp1 [label="Expert 1 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer11_exp2 [label="Expert 2 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer11_exp3 [label="Expert 3 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer11_exp4 [label="Expert 4 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer11_exp5 [label="Expert 5 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer11_exp6 [label="Expert 6 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu91_layer11_exp7 [label="Expert 7 L11 GPU91\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu91_layer11 [label="MLP L11 GPU91\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [92-95]
        subgraph cluster_pp5_3 {
            label="PP Stage 3\nGPUs [92-95]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu92_layer12 [label="Attention L12 GPU92\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu92_layer12 [label="Gate L12 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu92_layer12_exp0 [label="Expert 0 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer12_exp1 [label="Expert 1 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer12_exp2 [label="Expert 2 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer12_exp3 [label="Expert 3 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer12_exp4 [label="Expert 4 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer12_exp5 [label="Expert 5 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer12_exp6 [label="Expert 6 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer12_exp7 [label="Expert 7 L12 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu92_layer12 [label="MLP L12 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu92_layer13 [label="Attention L13 GPU92\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu92_layer13 [label="Gate L13 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu92_layer13_exp0 [label="Expert 0 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer13_exp1 [label="Expert 1 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer13_exp2 [label="Expert 2 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer13_exp3 [label="Expert 3 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer13_exp4 [label="Expert 4 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer13_exp5 [label="Expert 5 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer13_exp6 [label="Expert 6 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer13_exp7 [label="Expert 7 L13 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu92_layer13 [label="MLP L13 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu92_layer14 [label="Attention L14 GPU92\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu92_layer14 [label="Gate L14 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu92_layer14_exp0 [label="Expert 0 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer14_exp1 [label="Expert 1 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer14_exp2 [label="Expert 2 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer14_exp3 [label="Expert 3 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer14_exp4 [label="Expert 4 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer14_exp5 [label="Expert 5 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer14_exp6 [label="Expert 6 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer14_exp7 [label="Expert 7 L14 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu92_layer14 [label="MLP L14 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu92_layer15 [label="Attention L15 GPU92\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu92_layer15 [label="Gate L15 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu92_layer15_exp0 [label="Expert 0 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer15_exp1 [label="Expert 1 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer15_exp2 [label="Expert 2 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer15_exp3 [label="Expert 3 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer15_exp4 [label="Expert 4 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer15_exp5 [label="Expert 5 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer15_exp6 [label="Expert 6 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu92_layer15_exp7 [label="Expert 7 L15 GPU92\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu92_layer15 [label="MLP L15 GPU92\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu93_layer12 [label="Attention L12 GPU93\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu93_layer12 [label="Gate L12 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu93_layer12_exp0 [label="Expert 0 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer12_exp1 [label="Expert 1 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer12_exp2 [label="Expert 2 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer12_exp3 [label="Expert 3 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer12_exp4 [label="Expert 4 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer12_exp5 [label="Expert 5 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer12_exp6 [label="Expert 6 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer12_exp7 [label="Expert 7 L12 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu93_layer12 [label="MLP L12 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu93_layer13 [label="Attention L13 GPU93\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu93_layer13 [label="Gate L13 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu93_layer13_exp0 [label="Expert 0 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer13_exp1 [label="Expert 1 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer13_exp2 [label="Expert 2 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer13_exp3 [label="Expert 3 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer13_exp4 [label="Expert 4 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer13_exp5 [label="Expert 5 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer13_exp6 [label="Expert 6 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer13_exp7 [label="Expert 7 L13 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu93_layer13 [label="MLP L13 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu93_layer14 [label="Attention L14 GPU93\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu93_layer14 [label="Gate L14 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu93_layer14_exp0 [label="Expert 0 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer14_exp1 [label="Expert 1 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer14_exp2 [label="Expert 2 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer14_exp3 [label="Expert 3 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer14_exp4 [label="Expert 4 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer14_exp5 [label="Expert 5 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer14_exp6 [label="Expert 6 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer14_exp7 [label="Expert 7 L14 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu93_layer14 [label="MLP L14 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu93_layer15 [label="Attention L15 GPU93\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu93_layer15 [label="Gate L15 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu93_layer15_exp0 [label="Expert 0 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer15_exp1 [label="Expert 1 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer15_exp2 [label="Expert 2 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer15_exp3 [label="Expert 3 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer15_exp4 [label="Expert 4 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer15_exp5 [label="Expert 5 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer15_exp6 [label="Expert 6 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu93_layer15_exp7 [label="Expert 7 L15 GPU93\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu93_layer15 [label="MLP L15 GPU93\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu94_layer12 [label="Attention L12 GPU94\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu94_layer12 [label="Gate L12 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu94_layer12_exp0 [label="Expert 0 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer12_exp1 [label="Expert 1 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer12_exp2 [label="Expert 2 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer12_exp3 [label="Expert 3 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer12_exp4 [label="Expert 4 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer12_exp5 [label="Expert 5 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer12_exp6 [label="Expert 6 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer12_exp7 [label="Expert 7 L12 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu94_layer12 [label="MLP L12 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu94_layer13 [label="Attention L13 GPU94\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu94_layer13 [label="Gate L13 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu94_layer13_exp0 [label="Expert 0 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer13_exp1 [label="Expert 1 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer13_exp2 [label="Expert 2 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer13_exp3 [label="Expert 3 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer13_exp4 [label="Expert 4 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer13_exp5 [label="Expert 5 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer13_exp6 [label="Expert 6 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer13_exp7 [label="Expert 7 L13 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu94_layer13 [label="MLP L13 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu94_layer14 [label="Attention L14 GPU94\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu94_layer14 [label="Gate L14 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu94_layer14_exp0 [label="Expert 0 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer14_exp1 [label="Expert 1 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer14_exp2 [label="Expert 2 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer14_exp3 [label="Expert 3 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer14_exp4 [label="Expert 4 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer14_exp5 [label="Expert 5 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer14_exp6 [label="Expert 6 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer14_exp7 [label="Expert 7 L14 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu94_layer14 [label="MLP L14 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu94_layer15 [label="Attention L15 GPU94\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu94_layer15 [label="Gate L15 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu94_layer15_exp0 [label="Expert 0 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer15_exp1 [label="Expert 1 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer15_exp2 [label="Expert 2 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer15_exp3 [label="Expert 3 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer15_exp4 [label="Expert 4 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer15_exp5 [label="Expert 5 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer15_exp6 [label="Expert 6 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu94_layer15_exp7 [label="Expert 7 L15 GPU94\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu94_layer15 [label="MLP L15 GPU94\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu95_layer12 [label="Attention L12 GPU95\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu95_layer12 [label="Gate L12 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu95_layer12_exp0 [label="Expert 0 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer12_exp1 [label="Expert 1 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer12_exp2 [label="Expert 2 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer12_exp3 [label="Expert 3 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer12_exp4 [label="Expert 4 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer12_exp5 [label="Expert 5 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer12_exp6 [label="Expert 6 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer12_exp7 [label="Expert 7 L12 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu95_layer12 [label="MLP L12 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu95_layer13 [label="Attention L13 GPU95\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu95_layer13 [label="Gate L13 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu95_layer13_exp0 [label="Expert 0 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer13_exp1 [label="Expert 1 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer13_exp2 [label="Expert 2 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer13_exp3 [label="Expert 3 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer13_exp4 [label="Expert 4 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer13_exp5 [label="Expert 5 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer13_exp6 [label="Expert 6 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer13_exp7 [label="Expert 7 L13 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu95_layer13 [label="MLP L13 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu95_layer14 [label="Attention L14 GPU95\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu95_layer14 [label="Gate L14 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu95_layer14_exp0 [label="Expert 0 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer14_exp1 [label="Expert 1 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer14_exp2 [label="Expert 2 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer14_exp3 [label="Expert 3 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer14_exp4 [label="Expert 4 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer14_exp5 [label="Expert 5 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer14_exp6 [label="Expert 6 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer14_exp7 [label="Expert 7 L14 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu95_layer14 [label="MLP L14 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu95_layer15 [label="Attention L15 GPU95\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu95_layer15 [label="Gate L15 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu95_layer15_exp0 [label="Expert 0 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer15_exp1 [label="Expert 1 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer15_exp2 [label="Expert 2 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer15_exp3 [label="Expert 3 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer15_exp4 [label="Expert 4 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer15_exp5 [label="Expert 5 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer15_exp6 [label="Expert 6 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu95_layer15_exp7 [label="Expert 7 L15 GPU95\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu95_layer15 [label="MLP L15 GPU95\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // EP Group 6 - GPUs [96-111]
    subgraph cluster_ep6 {
        label="EP Group 6\nGPUs [96-111]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [96-99]
        subgraph cluster_pp6_0 {
            label="PP Stage 0\nGPUs [96-99]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu96_layer0 [label="Attention L0 GPU96\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu96_layer0 [label="Gate L0 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu96_layer0_exp0 [label="Expert 0 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer0_exp1 [label="Expert 1 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer0_exp2 [label="Expert 2 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer0_exp3 [label="Expert 3 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer0_exp4 [label="Expert 4 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer0_exp5 [label="Expert 5 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer0_exp6 [label="Expert 6 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer0_exp7 [label="Expert 7 L0 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu96_layer0 [label="MLP L0 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu96_layer1 [label="Attention L1 GPU96\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu96_layer1 [label="Gate L1 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu96_layer1_exp0 [label="Expert 0 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer1_exp1 [label="Expert 1 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer1_exp2 [label="Expert 2 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer1_exp3 [label="Expert 3 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer1_exp4 [label="Expert 4 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer1_exp5 [label="Expert 5 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer1_exp6 [label="Expert 6 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer1_exp7 [label="Expert 7 L1 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu96_layer1 [label="MLP L1 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu96_layer2 [label="Attention L2 GPU96\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu96_layer2 [label="Gate L2 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu96_layer2_exp0 [label="Expert 0 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer2_exp1 [label="Expert 1 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer2_exp2 [label="Expert 2 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer2_exp3 [label="Expert 3 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer2_exp4 [label="Expert 4 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer2_exp5 [label="Expert 5 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer2_exp6 [label="Expert 6 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer2_exp7 [label="Expert 7 L2 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu96_layer2 [label="MLP L2 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu96_layer3 [label="Attention L3 GPU96\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu96_layer3 [label="Gate L3 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu96_layer3_exp0 [label="Expert 0 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer3_exp1 [label="Expert 1 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer3_exp2 [label="Expert 2 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer3_exp3 [label="Expert 3 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer3_exp4 [label="Expert 4 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer3_exp5 [label="Expert 5 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer3_exp6 [label="Expert 6 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu96_layer3_exp7 [label="Expert 7 L3 GPU96\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu96_layer3 [label="MLP L3 GPU96\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu97_layer0 [label="Attention L0 GPU97\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu97_layer0 [label="Gate L0 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu97_layer0_exp0 [label="Expert 0 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer0_exp1 [label="Expert 1 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer0_exp2 [label="Expert 2 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer0_exp3 [label="Expert 3 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer0_exp4 [label="Expert 4 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer0_exp5 [label="Expert 5 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer0_exp6 [label="Expert 6 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer0_exp7 [label="Expert 7 L0 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu97_layer0 [label="MLP L0 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu97_layer1 [label="Attention L1 GPU97\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu97_layer1 [label="Gate L1 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu97_layer1_exp0 [label="Expert 0 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer1_exp1 [label="Expert 1 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer1_exp2 [label="Expert 2 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer1_exp3 [label="Expert 3 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer1_exp4 [label="Expert 4 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer1_exp5 [label="Expert 5 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer1_exp6 [label="Expert 6 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer1_exp7 [label="Expert 7 L1 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu97_layer1 [label="MLP L1 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu97_layer2 [label="Attention L2 GPU97\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu97_layer2 [label="Gate L2 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu97_layer2_exp0 [label="Expert 0 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer2_exp1 [label="Expert 1 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer2_exp2 [label="Expert 2 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer2_exp3 [label="Expert 3 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer2_exp4 [label="Expert 4 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer2_exp5 [label="Expert 5 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer2_exp6 [label="Expert 6 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer2_exp7 [label="Expert 7 L2 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu97_layer2 [label="MLP L2 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu97_layer3 [label="Attention L3 GPU97\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu97_layer3 [label="Gate L3 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu97_layer3_exp0 [label="Expert 0 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer3_exp1 [label="Expert 1 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer3_exp2 [label="Expert 2 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer3_exp3 [label="Expert 3 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer3_exp4 [label="Expert 4 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer3_exp5 [label="Expert 5 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer3_exp6 [label="Expert 6 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu97_layer3_exp7 [label="Expert 7 L3 GPU97\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu97_layer3 [label="MLP L3 GPU97\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu98_layer0 [label="Attention L0 GPU98\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu98_layer0 [label="Gate L0 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu98_layer0_exp0 [label="Expert 0 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer0_exp1 [label="Expert 1 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer0_exp2 [label="Expert 2 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer0_exp3 [label="Expert 3 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer0_exp4 [label="Expert 4 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer0_exp5 [label="Expert 5 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer0_exp6 [label="Expert 6 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer0_exp7 [label="Expert 7 L0 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu98_layer0 [label="MLP L0 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu98_layer1 [label="Attention L1 GPU98\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu98_layer1 [label="Gate L1 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu98_layer1_exp0 [label="Expert 0 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer1_exp1 [label="Expert 1 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer1_exp2 [label="Expert 2 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer1_exp3 [label="Expert 3 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer1_exp4 [label="Expert 4 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer1_exp5 [label="Expert 5 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer1_exp6 [label="Expert 6 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer1_exp7 [label="Expert 7 L1 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu98_layer1 [label="MLP L1 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu98_layer2 [label="Attention L2 GPU98\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu98_layer2 [label="Gate L2 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu98_layer2_exp0 [label="Expert 0 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer2_exp1 [label="Expert 1 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer2_exp2 [label="Expert 2 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer2_exp3 [label="Expert 3 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer2_exp4 [label="Expert 4 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer2_exp5 [label="Expert 5 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer2_exp6 [label="Expert 6 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer2_exp7 [label="Expert 7 L2 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu98_layer2 [label="MLP L2 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu98_layer3 [label="Attention L3 GPU98\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu98_layer3 [label="Gate L3 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu98_layer3_exp0 [label="Expert 0 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer3_exp1 [label="Expert 1 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer3_exp2 [label="Expert 2 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer3_exp3 [label="Expert 3 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer3_exp4 [label="Expert 4 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer3_exp5 [label="Expert 5 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer3_exp6 [label="Expert 6 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu98_layer3_exp7 [label="Expert 7 L3 GPU98\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu98_layer3 [label="MLP L3 GPU98\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu99_layer0 [label="Attention L0 GPU99\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu99_layer0 [label="Gate L0 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu99_layer0_exp0 [label="Expert 0 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer0_exp1 [label="Expert 1 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer0_exp2 [label="Expert 2 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer0_exp3 [label="Expert 3 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer0_exp4 [label="Expert 4 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer0_exp5 [label="Expert 5 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer0_exp6 [label="Expert 6 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer0_exp7 [label="Expert 7 L0 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu99_layer0 [label="MLP L0 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu99_layer1 [label="Attention L1 GPU99\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu99_layer1 [label="Gate L1 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu99_layer1_exp0 [label="Expert 0 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer1_exp1 [label="Expert 1 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer1_exp2 [label="Expert 2 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer1_exp3 [label="Expert 3 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer1_exp4 [label="Expert 4 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer1_exp5 [label="Expert 5 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer1_exp6 [label="Expert 6 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer1_exp7 [label="Expert 7 L1 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu99_layer1 [label="MLP L1 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu99_layer2 [label="Attention L2 GPU99\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu99_layer2 [label="Gate L2 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu99_layer2_exp0 [label="Expert 0 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer2_exp1 [label="Expert 1 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer2_exp2 [label="Expert 2 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer2_exp3 [label="Expert 3 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer2_exp4 [label="Expert 4 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer2_exp5 [label="Expert 5 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer2_exp6 [label="Expert 6 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer2_exp7 [label="Expert 7 L2 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu99_layer2 [label="MLP L2 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu99_layer3 [label="Attention L3 GPU99\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu99_layer3 [label="Gate L3 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu99_layer3_exp0 [label="Expert 0 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer3_exp1 [label="Expert 1 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer3_exp2 [label="Expert 2 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer3_exp3 [label="Expert 3 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer3_exp4 [label="Expert 4 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer3_exp5 [label="Expert 5 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer3_exp6 [label="Expert 6 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu99_layer3_exp7 [label="Expert 7 L3 GPU99\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu99_layer3 [label="MLP L3 GPU99\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [100-103]
        subgraph cluster_pp6_1 {
            label="PP Stage 1\nGPUs [100-103]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu100_layer4 [label="Attention L4 GPU100\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu100_layer4 [label="Gate L4 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu100_layer4_exp0 [label="Expert 0 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer4_exp1 [label="Expert 1 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer4_exp2 [label="Expert 2 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer4_exp3 [label="Expert 3 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer4_exp4 [label="Expert 4 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer4_exp5 [label="Expert 5 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer4_exp6 [label="Expert 6 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer4_exp7 [label="Expert 7 L4 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu100_layer4 [label="MLP L4 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu100_layer5 [label="Attention L5 GPU100\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu100_layer5 [label="Gate L5 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu100_layer5_exp0 [label="Expert 0 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer5_exp1 [label="Expert 1 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer5_exp2 [label="Expert 2 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer5_exp3 [label="Expert 3 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer5_exp4 [label="Expert 4 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer5_exp5 [label="Expert 5 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer5_exp6 [label="Expert 6 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer5_exp7 [label="Expert 7 L5 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu100_layer5 [label="MLP L5 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu100_layer6 [label="Attention L6 GPU100\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu100_layer6 [label="Gate L6 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu100_layer6_exp0 [label="Expert 0 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer6_exp1 [label="Expert 1 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer6_exp2 [label="Expert 2 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer6_exp3 [label="Expert 3 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer6_exp4 [label="Expert 4 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer6_exp5 [label="Expert 5 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer6_exp6 [label="Expert 6 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer6_exp7 [label="Expert 7 L6 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu100_layer6 [label="MLP L6 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu100_layer7 [label="Attention L7 GPU100\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu100_layer7 [label="Gate L7 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu100_layer7_exp0 [label="Expert 0 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer7_exp1 [label="Expert 1 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer7_exp2 [label="Expert 2 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer7_exp3 [label="Expert 3 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer7_exp4 [label="Expert 4 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer7_exp5 [label="Expert 5 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer7_exp6 [label="Expert 6 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu100_layer7_exp7 [label="Expert 7 L7 GPU100\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu100_layer7 [label="MLP L7 GPU100\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu101_layer4 [label="Attention L4 GPU101\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu101_layer4 [label="Gate L4 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu101_layer4_exp0 [label="Expert 0 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer4_exp1 [label="Expert 1 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer4_exp2 [label="Expert 2 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer4_exp3 [label="Expert 3 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer4_exp4 [label="Expert 4 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer4_exp5 [label="Expert 5 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer4_exp6 [label="Expert 6 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer4_exp7 [label="Expert 7 L4 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu101_layer4 [label="MLP L4 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu101_layer5 [label="Attention L5 GPU101\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu101_layer5 [label="Gate L5 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu101_layer5_exp0 [label="Expert 0 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer5_exp1 [label="Expert 1 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer5_exp2 [label="Expert 2 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer5_exp3 [label="Expert 3 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer5_exp4 [label="Expert 4 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer5_exp5 [label="Expert 5 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer5_exp6 [label="Expert 6 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer5_exp7 [label="Expert 7 L5 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu101_layer5 [label="MLP L5 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu101_layer6 [label="Attention L6 GPU101\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu101_layer6 [label="Gate L6 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu101_layer6_exp0 [label="Expert 0 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer6_exp1 [label="Expert 1 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer6_exp2 [label="Expert 2 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer6_exp3 [label="Expert 3 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer6_exp4 [label="Expert 4 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer6_exp5 [label="Expert 5 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer6_exp6 [label="Expert 6 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer6_exp7 [label="Expert 7 L6 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu101_layer6 [label="MLP L6 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu101_layer7 [label="Attention L7 GPU101\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu101_layer7 [label="Gate L7 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu101_layer7_exp0 [label="Expert 0 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer7_exp1 [label="Expert 1 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer7_exp2 [label="Expert 2 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer7_exp3 [label="Expert 3 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer7_exp4 [label="Expert 4 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer7_exp5 [label="Expert 5 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer7_exp6 [label="Expert 6 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu101_layer7_exp7 [label="Expert 7 L7 GPU101\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu101_layer7 [label="MLP L7 GPU101\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu102_layer4 [label="Attention L4 GPU102\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu102_layer4 [label="Gate L4 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu102_layer4_exp0 [label="Expert 0 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer4_exp1 [label="Expert 1 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer4_exp2 [label="Expert 2 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer4_exp3 [label="Expert 3 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer4_exp4 [label="Expert 4 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer4_exp5 [label="Expert 5 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer4_exp6 [label="Expert 6 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer4_exp7 [label="Expert 7 L4 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu102_layer4 [label="MLP L4 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu102_layer5 [label="Attention L5 GPU102\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu102_layer5 [label="Gate L5 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu102_layer5_exp0 [label="Expert 0 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer5_exp1 [label="Expert 1 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer5_exp2 [label="Expert 2 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer5_exp3 [label="Expert 3 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer5_exp4 [label="Expert 4 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer5_exp5 [label="Expert 5 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer5_exp6 [label="Expert 6 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer5_exp7 [label="Expert 7 L5 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu102_layer5 [label="MLP L5 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu102_layer6 [label="Attention L6 GPU102\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu102_layer6 [label="Gate L6 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu102_layer6_exp0 [label="Expert 0 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer6_exp1 [label="Expert 1 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer6_exp2 [label="Expert 2 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer6_exp3 [label="Expert 3 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer6_exp4 [label="Expert 4 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer6_exp5 [label="Expert 5 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer6_exp6 [label="Expert 6 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer6_exp7 [label="Expert 7 L6 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu102_layer6 [label="MLP L6 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu102_layer7 [label="Attention L7 GPU102\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu102_layer7 [label="Gate L7 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu102_layer7_exp0 [label="Expert 0 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer7_exp1 [label="Expert 1 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer7_exp2 [label="Expert 2 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer7_exp3 [label="Expert 3 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer7_exp4 [label="Expert 4 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer7_exp5 [label="Expert 5 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer7_exp6 [label="Expert 6 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu102_layer7_exp7 [label="Expert 7 L7 GPU102\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu102_layer7 [label="MLP L7 GPU102\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu103_layer4 [label="Attention L4 GPU103\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu103_layer4 [label="Gate L4 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu103_layer4_exp0 [label="Expert 0 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer4_exp1 [label="Expert 1 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer4_exp2 [label="Expert 2 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer4_exp3 [label="Expert 3 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer4_exp4 [label="Expert 4 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer4_exp5 [label="Expert 5 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer4_exp6 [label="Expert 6 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer4_exp7 [label="Expert 7 L4 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu103_layer4 [label="MLP L4 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu103_layer5 [label="Attention L5 GPU103\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu103_layer5 [label="Gate L5 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu103_layer5_exp0 [label="Expert 0 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer5_exp1 [label="Expert 1 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer5_exp2 [label="Expert 2 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer5_exp3 [label="Expert 3 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer5_exp4 [label="Expert 4 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer5_exp5 [label="Expert 5 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer5_exp6 [label="Expert 6 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer5_exp7 [label="Expert 7 L5 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu103_layer5 [label="MLP L5 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu103_layer6 [label="Attention L6 GPU103\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu103_layer6 [label="Gate L6 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu103_layer6_exp0 [label="Expert 0 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer6_exp1 [label="Expert 1 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer6_exp2 [label="Expert 2 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer6_exp3 [label="Expert 3 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer6_exp4 [label="Expert 4 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer6_exp5 [label="Expert 5 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer6_exp6 [label="Expert 6 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer6_exp7 [label="Expert 7 L6 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu103_layer6 [label="MLP L6 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu103_layer7 [label="Attention L7 GPU103\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu103_layer7 [label="Gate L7 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu103_layer7_exp0 [label="Expert 0 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer7_exp1 [label="Expert 1 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer7_exp2 [label="Expert 2 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer7_exp3 [label="Expert 3 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer7_exp4 [label="Expert 4 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer7_exp5 [label="Expert 5 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer7_exp6 [label="Expert 6 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu103_layer7_exp7 [label="Expert 7 L7 GPU103\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu103_layer7 [label="MLP L7 GPU103\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [104-107]
        subgraph cluster_pp6_2 {
            label="PP Stage 2\nGPUs [104-107]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu104_layer8 [label="Attention L8 GPU104\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu104_layer8 [label="Gate L8 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu104_layer8_exp0 [label="Expert 0 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer8_exp1 [label="Expert 1 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer8_exp2 [label="Expert 2 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer8_exp3 [label="Expert 3 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer8_exp4 [label="Expert 4 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer8_exp5 [label="Expert 5 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer8_exp6 [label="Expert 6 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer8_exp7 [label="Expert 7 L8 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu104_layer8 [label="MLP L8 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu104_layer9 [label="Attention L9 GPU104\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu104_layer9 [label="Gate L9 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu104_layer9_exp0 [label="Expert 0 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer9_exp1 [label="Expert 1 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer9_exp2 [label="Expert 2 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer9_exp3 [label="Expert 3 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer9_exp4 [label="Expert 4 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer9_exp5 [label="Expert 5 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer9_exp6 [label="Expert 6 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer9_exp7 [label="Expert 7 L9 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu104_layer9 [label="MLP L9 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu104_layer10 [label="Attention L10 GPU104\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu104_layer10 [label="Gate L10 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu104_layer10_exp0 [label="Expert 0 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer10_exp1 [label="Expert 1 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer10_exp2 [label="Expert 2 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer10_exp3 [label="Expert 3 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer10_exp4 [label="Expert 4 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer10_exp5 [label="Expert 5 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer10_exp6 [label="Expert 6 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer10_exp7 [label="Expert 7 L10 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu104_layer10 [label="MLP L10 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu104_layer11 [label="Attention L11 GPU104\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu104_layer11 [label="Gate L11 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu104_layer11_exp0 [label="Expert 0 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer11_exp1 [label="Expert 1 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer11_exp2 [label="Expert 2 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer11_exp3 [label="Expert 3 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer11_exp4 [label="Expert 4 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer11_exp5 [label="Expert 5 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer11_exp6 [label="Expert 6 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu104_layer11_exp7 [label="Expert 7 L11 GPU104\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu104_layer11 [label="MLP L11 GPU104\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu105_layer8 [label="Attention L8 GPU105\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu105_layer8 [label="Gate L8 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu105_layer8_exp0 [label="Expert 0 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer8_exp1 [label="Expert 1 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer8_exp2 [label="Expert 2 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer8_exp3 [label="Expert 3 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer8_exp4 [label="Expert 4 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer8_exp5 [label="Expert 5 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer8_exp6 [label="Expert 6 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer8_exp7 [label="Expert 7 L8 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu105_layer8 [label="MLP L8 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu105_layer9 [label="Attention L9 GPU105\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu105_layer9 [label="Gate L9 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu105_layer9_exp0 [label="Expert 0 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer9_exp1 [label="Expert 1 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer9_exp2 [label="Expert 2 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer9_exp3 [label="Expert 3 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer9_exp4 [label="Expert 4 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer9_exp5 [label="Expert 5 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer9_exp6 [label="Expert 6 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer9_exp7 [label="Expert 7 L9 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu105_layer9 [label="MLP L9 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu105_layer10 [label="Attention L10 GPU105\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu105_layer10 [label="Gate L10 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu105_layer10_exp0 [label="Expert 0 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer10_exp1 [label="Expert 1 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer10_exp2 [label="Expert 2 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer10_exp3 [label="Expert 3 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer10_exp4 [label="Expert 4 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer10_exp5 [label="Expert 5 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer10_exp6 [label="Expert 6 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer10_exp7 [label="Expert 7 L10 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu105_layer10 [label="MLP L10 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu105_layer11 [label="Attention L11 GPU105\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu105_layer11 [label="Gate L11 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu105_layer11_exp0 [label="Expert 0 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer11_exp1 [label="Expert 1 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer11_exp2 [label="Expert 2 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer11_exp3 [label="Expert 3 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer11_exp4 [label="Expert 4 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer11_exp5 [label="Expert 5 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer11_exp6 [label="Expert 6 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu105_layer11_exp7 [label="Expert 7 L11 GPU105\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu105_layer11 [label="MLP L11 GPU105\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu106_layer8 [label="Attention L8 GPU106\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu106_layer8 [label="Gate L8 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu106_layer8_exp0 [label="Expert 0 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer8_exp1 [label="Expert 1 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer8_exp2 [label="Expert 2 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer8_exp3 [label="Expert 3 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer8_exp4 [label="Expert 4 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer8_exp5 [label="Expert 5 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer8_exp6 [label="Expert 6 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer8_exp7 [label="Expert 7 L8 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu106_layer8 [label="MLP L8 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu106_layer9 [label="Attention L9 GPU106\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu106_layer9 [label="Gate L9 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu106_layer9_exp0 [label="Expert 0 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer9_exp1 [label="Expert 1 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer9_exp2 [label="Expert 2 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer9_exp3 [label="Expert 3 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer9_exp4 [label="Expert 4 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer9_exp5 [label="Expert 5 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer9_exp6 [label="Expert 6 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer9_exp7 [label="Expert 7 L9 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu106_layer9 [label="MLP L9 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu106_layer10 [label="Attention L10 GPU106\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu106_layer10 [label="Gate L10 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu106_layer10_exp0 [label="Expert 0 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer10_exp1 [label="Expert 1 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer10_exp2 [label="Expert 2 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer10_exp3 [label="Expert 3 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer10_exp4 [label="Expert 4 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer10_exp5 [label="Expert 5 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer10_exp6 [label="Expert 6 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer10_exp7 [label="Expert 7 L10 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu106_layer10 [label="MLP L10 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu106_layer11 [label="Attention L11 GPU106\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu106_layer11 [label="Gate L11 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu106_layer11_exp0 [label="Expert 0 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer11_exp1 [label="Expert 1 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer11_exp2 [label="Expert 2 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer11_exp3 [label="Expert 3 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer11_exp4 [label="Expert 4 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer11_exp5 [label="Expert 5 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer11_exp6 [label="Expert 6 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu106_layer11_exp7 [label="Expert 7 L11 GPU106\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu106_layer11 [label="MLP L11 GPU106\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu107_layer8 [label="Attention L8 GPU107\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu107_layer8 [label="Gate L8 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu107_layer8_exp0 [label="Expert 0 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer8_exp1 [label="Expert 1 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer8_exp2 [label="Expert 2 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer8_exp3 [label="Expert 3 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer8_exp4 [label="Expert 4 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer8_exp5 [label="Expert 5 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer8_exp6 [label="Expert 6 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer8_exp7 [label="Expert 7 L8 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu107_layer8 [label="MLP L8 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu107_layer9 [label="Attention L9 GPU107\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu107_layer9 [label="Gate L9 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu107_layer9_exp0 [label="Expert 0 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer9_exp1 [label="Expert 1 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer9_exp2 [label="Expert 2 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer9_exp3 [label="Expert 3 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer9_exp4 [label="Expert 4 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer9_exp5 [label="Expert 5 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer9_exp6 [label="Expert 6 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer9_exp7 [label="Expert 7 L9 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu107_layer9 [label="MLP L9 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu107_layer10 [label="Attention L10 GPU107\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu107_layer10 [label="Gate L10 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu107_layer10_exp0 [label="Expert 0 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer10_exp1 [label="Expert 1 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer10_exp2 [label="Expert 2 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer10_exp3 [label="Expert 3 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer10_exp4 [label="Expert 4 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer10_exp5 [label="Expert 5 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer10_exp6 [label="Expert 6 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer10_exp7 [label="Expert 7 L10 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu107_layer10 [label="MLP L10 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu107_layer11 [label="Attention L11 GPU107\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu107_layer11 [label="Gate L11 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu107_layer11_exp0 [label="Expert 0 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer11_exp1 [label="Expert 1 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer11_exp2 [label="Expert 2 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer11_exp3 [label="Expert 3 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer11_exp4 [label="Expert 4 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer11_exp5 [label="Expert 5 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer11_exp6 [label="Expert 6 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu107_layer11_exp7 [label="Expert 7 L11 GPU107\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu107_layer11 [label="MLP L11 GPU107\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [108-111]
        subgraph cluster_pp6_3 {
            label="PP Stage 3\nGPUs [108-111]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu108_layer12 [label="Attention L12 GPU108\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu108_layer12 [label="Gate L12 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu108_layer12_exp0 [label="Expert 0 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer12_exp1 [label="Expert 1 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer12_exp2 [label="Expert 2 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer12_exp3 [label="Expert 3 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer12_exp4 [label="Expert 4 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer12_exp5 [label="Expert 5 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer12_exp6 [label="Expert 6 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer12_exp7 [label="Expert 7 L12 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu108_layer12 [label="MLP L12 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu108_layer13 [label="Attention L13 GPU108\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu108_layer13 [label="Gate L13 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu108_layer13_exp0 [label="Expert 0 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer13_exp1 [label="Expert 1 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer13_exp2 [label="Expert 2 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer13_exp3 [label="Expert 3 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer13_exp4 [label="Expert 4 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer13_exp5 [label="Expert 5 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer13_exp6 [label="Expert 6 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer13_exp7 [label="Expert 7 L13 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu108_layer13 [label="MLP L13 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu108_layer14 [label="Attention L14 GPU108\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu108_layer14 [label="Gate L14 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu108_layer14_exp0 [label="Expert 0 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer14_exp1 [label="Expert 1 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer14_exp2 [label="Expert 2 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer14_exp3 [label="Expert 3 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer14_exp4 [label="Expert 4 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer14_exp5 [label="Expert 5 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer14_exp6 [label="Expert 6 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer14_exp7 [label="Expert 7 L14 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu108_layer14 [label="MLP L14 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu108_layer15 [label="Attention L15 GPU108\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu108_layer15 [label="Gate L15 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu108_layer15_exp0 [label="Expert 0 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer15_exp1 [label="Expert 1 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer15_exp2 [label="Expert 2 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer15_exp3 [label="Expert 3 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer15_exp4 [label="Expert 4 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer15_exp5 [label="Expert 5 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer15_exp6 [label="Expert 6 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu108_layer15_exp7 [label="Expert 7 L15 GPU108\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu108_layer15 [label="MLP L15 GPU108\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu109_layer12 [label="Attention L12 GPU109\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu109_layer12 [label="Gate L12 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu109_layer12_exp0 [label="Expert 0 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer12_exp1 [label="Expert 1 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer12_exp2 [label="Expert 2 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer12_exp3 [label="Expert 3 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer12_exp4 [label="Expert 4 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer12_exp5 [label="Expert 5 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer12_exp6 [label="Expert 6 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer12_exp7 [label="Expert 7 L12 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu109_layer12 [label="MLP L12 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu109_layer13 [label="Attention L13 GPU109\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu109_layer13 [label="Gate L13 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu109_layer13_exp0 [label="Expert 0 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer13_exp1 [label="Expert 1 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer13_exp2 [label="Expert 2 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer13_exp3 [label="Expert 3 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer13_exp4 [label="Expert 4 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer13_exp5 [label="Expert 5 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer13_exp6 [label="Expert 6 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer13_exp7 [label="Expert 7 L13 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu109_layer13 [label="MLP L13 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu109_layer14 [label="Attention L14 GPU109\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu109_layer14 [label="Gate L14 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu109_layer14_exp0 [label="Expert 0 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer14_exp1 [label="Expert 1 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer14_exp2 [label="Expert 2 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer14_exp3 [label="Expert 3 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer14_exp4 [label="Expert 4 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer14_exp5 [label="Expert 5 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer14_exp6 [label="Expert 6 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer14_exp7 [label="Expert 7 L14 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu109_layer14 [label="MLP L14 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu109_layer15 [label="Attention L15 GPU109\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu109_layer15 [label="Gate L15 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu109_layer15_exp0 [label="Expert 0 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer15_exp1 [label="Expert 1 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer15_exp2 [label="Expert 2 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer15_exp3 [label="Expert 3 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer15_exp4 [label="Expert 4 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer15_exp5 [label="Expert 5 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer15_exp6 [label="Expert 6 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu109_layer15_exp7 [label="Expert 7 L15 GPU109\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu109_layer15 [label="MLP L15 GPU109\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu110_layer12 [label="Attention L12 GPU110\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu110_layer12 [label="Gate L12 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu110_layer12_exp0 [label="Expert 0 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer12_exp1 [label="Expert 1 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer12_exp2 [label="Expert 2 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer12_exp3 [label="Expert 3 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer12_exp4 [label="Expert 4 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer12_exp5 [label="Expert 5 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer12_exp6 [label="Expert 6 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer12_exp7 [label="Expert 7 L12 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu110_layer12 [label="MLP L12 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu110_layer13 [label="Attention L13 GPU110\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu110_layer13 [label="Gate L13 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu110_layer13_exp0 [label="Expert 0 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer13_exp1 [label="Expert 1 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer13_exp2 [label="Expert 2 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer13_exp3 [label="Expert 3 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer13_exp4 [label="Expert 4 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer13_exp5 [label="Expert 5 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer13_exp6 [label="Expert 6 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer13_exp7 [label="Expert 7 L13 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu110_layer13 [label="MLP L13 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu110_layer14 [label="Attention L14 GPU110\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu110_layer14 [label="Gate L14 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu110_layer14_exp0 [label="Expert 0 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer14_exp1 [label="Expert 1 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer14_exp2 [label="Expert 2 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer14_exp3 [label="Expert 3 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer14_exp4 [label="Expert 4 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer14_exp5 [label="Expert 5 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer14_exp6 [label="Expert 6 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer14_exp7 [label="Expert 7 L14 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu110_layer14 [label="MLP L14 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu110_layer15 [label="Attention L15 GPU110\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu110_layer15 [label="Gate L15 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu110_layer15_exp0 [label="Expert 0 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer15_exp1 [label="Expert 1 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer15_exp2 [label="Expert 2 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer15_exp3 [label="Expert 3 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer15_exp4 [label="Expert 4 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer15_exp5 [label="Expert 5 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer15_exp6 [label="Expert 6 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu110_layer15_exp7 [label="Expert 7 L15 GPU110\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu110_layer15 [label="MLP L15 GPU110\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu111_layer12 [label="Attention L12 GPU111\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu111_layer12 [label="Gate L12 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu111_layer12_exp0 [label="Expert 0 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer12_exp1 [label="Expert 1 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer12_exp2 [label="Expert 2 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer12_exp3 [label="Expert 3 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer12_exp4 [label="Expert 4 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer12_exp5 [label="Expert 5 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer12_exp6 [label="Expert 6 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer12_exp7 [label="Expert 7 L12 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu111_layer12 [label="MLP L12 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu111_layer13 [label="Attention L13 GPU111\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu111_layer13 [label="Gate L13 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu111_layer13_exp0 [label="Expert 0 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer13_exp1 [label="Expert 1 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer13_exp2 [label="Expert 2 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer13_exp3 [label="Expert 3 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer13_exp4 [label="Expert 4 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer13_exp5 [label="Expert 5 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer13_exp6 [label="Expert 6 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer13_exp7 [label="Expert 7 L13 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu111_layer13 [label="MLP L13 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu111_layer14 [label="Attention L14 GPU111\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu111_layer14 [label="Gate L14 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu111_layer14_exp0 [label="Expert 0 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer14_exp1 [label="Expert 1 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer14_exp2 [label="Expert 2 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer14_exp3 [label="Expert 3 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer14_exp4 [label="Expert 4 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer14_exp5 [label="Expert 5 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer14_exp6 [label="Expert 6 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer14_exp7 [label="Expert 7 L14 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu111_layer14 [label="MLP L14 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu111_layer15 [label="Attention L15 GPU111\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu111_layer15 [label="Gate L15 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu111_layer15_exp0 [label="Expert 0 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer15_exp1 [label="Expert 1 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer15_exp2 [label="Expert 2 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer15_exp3 [label="Expert 3 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer15_exp4 [label="Expert 4 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer15_exp5 [label="Expert 5 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer15_exp6 [label="Expert 6 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu111_layer15_exp7 [label="Expert 7 L15 GPU111\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu111_layer15 [label="MLP L15 GPU111\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // EP Group 7 - GPUs [112-127]
    subgraph cluster_ep7 {
        label="EP Group 7\nGPUs [112-127]";
        style=filled;
        fillcolor=lightyellow;
        color=blue;
        penwidth=2;
        
        // PP Stage 0 - GPUs [112-115]
        subgraph cluster_pp7_0 {
            label="PP Stage 0\nGPUs [112-115]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu112_layer0 [label="Attention L0 GPU112\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu112_layer0 [label="Gate L0 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu112_layer0_exp0 [label="Expert 0 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer0_exp1 [label="Expert 1 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer0_exp2 [label="Expert 2 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer0_exp3 [label="Expert 3 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer0_exp4 [label="Expert 4 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer0_exp5 [label="Expert 5 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer0_exp6 [label="Expert 6 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer0_exp7 [label="Expert 7 L0 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu112_layer0 [label="MLP L0 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu112_layer1 [label="Attention L1 GPU112\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu112_layer1 [label="Gate L1 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu112_layer1_exp0 [label="Expert 0 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer1_exp1 [label="Expert 1 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer1_exp2 [label="Expert 2 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer1_exp3 [label="Expert 3 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer1_exp4 [label="Expert 4 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer1_exp5 [label="Expert 5 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer1_exp6 [label="Expert 6 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer1_exp7 [label="Expert 7 L1 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu112_layer1 [label="MLP L1 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu112_layer2 [label="Attention L2 GPU112\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu112_layer2 [label="Gate L2 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu112_layer2_exp0 [label="Expert 0 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer2_exp1 [label="Expert 1 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer2_exp2 [label="Expert 2 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer2_exp3 [label="Expert 3 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer2_exp4 [label="Expert 4 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer2_exp5 [label="Expert 5 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer2_exp6 [label="Expert 6 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer2_exp7 [label="Expert 7 L2 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu112_layer2 [label="MLP L2 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu112_layer3 [label="Attention L3 GPU112\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu112_layer3 [label="Gate L3 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu112_layer3_exp0 [label="Expert 0 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer3_exp1 [label="Expert 1 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer3_exp2 [label="Expert 2 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer3_exp3 [label="Expert 3 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer3_exp4 [label="Expert 4 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer3_exp5 [label="Expert 5 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer3_exp6 [label="Expert 6 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu112_layer3_exp7 [label="Expert 7 L3 GPU112\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu112_layer3 [label="MLP L3 GPU112\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu113_layer0 [label="Attention L0 GPU113\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu113_layer0 [label="Gate L0 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu113_layer0_exp0 [label="Expert 0 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer0_exp1 [label="Expert 1 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer0_exp2 [label="Expert 2 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer0_exp3 [label="Expert 3 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer0_exp4 [label="Expert 4 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer0_exp5 [label="Expert 5 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer0_exp6 [label="Expert 6 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer0_exp7 [label="Expert 7 L0 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu113_layer0 [label="MLP L0 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu113_layer1 [label="Attention L1 GPU113\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu113_layer1 [label="Gate L1 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu113_layer1_exp0 [label="Expert 0 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer1_exp1 [label="Expert 1 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer1_exp2 [label="Expert 2 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer1_exp3 [label="Expert 3 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer1_exp4 [label="Expert 4 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer1_exp5 [label="Expert 5 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer1_exp6 [label="Expert 6 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer1_exp7 [label="Expert 7 L1 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu113_layer1 [label="MLP L1 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu113_layer2 [label="Attention L2 GPU113\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu113_layer2 [label="Gate L2 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu113_layer2_exp0 [label="Expert 0 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer2_exp1 [label="Expert 1 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer2_exp2 [label="Expert 2 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer2_exp3 [label="Expert 3 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer2_exp4 [label="Expert 4 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer2_exp5 [label="Expert 5 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer2_exp6 [label="Expert 6 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer2_exp7 [label="Expert 7 L2 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu113_layer2 [label="MLP L2 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu113_layer3 [label="Attention L3 GPU113\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu113_layer3 [label="Gate L3 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu113_layer3_exp0 [label="Expert 0 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer3_exp1 [label="Expert 1 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer3_exp2 [label="Expert 2 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer3_exp3 [label="Expert 3 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer3_exp4 [label="Expert 4 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer3_exp5 [label="Expert 5 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer3_exp6 [label="Expert 6 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu113_layer3_exp7 [label="Expert 7 L3 GPU113\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu113_layer3 [label="MLP L3 GPU113\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu114_layer0 [label="Attention L0 GPU114\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu114_layer0 [label="Gate L0 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu114_layer0_exp0 [label="Expert 0 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer0_exp1 [label="Expert 1 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer0_exp2 [label="Expert 2 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer0_exp3 [label="Expert 3 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer0_exp4 [label="Expert 4 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer0_exp5 [label="Expert 5 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer0_exp6 [label="Expert 6 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer0_exp7 [label="Expert 7 L0 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu114_layer0 [label="MLP L0 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu114_layer1 [label="Attention L1 GPU114\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu114_layer1 [label="Gate L1 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu114_layer1_exp0 [label="Expert 0 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer1_exp1 [label="Expert 1 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer1_exp2 [label="Expert 2 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer1_exp3 [label="Expert 3 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer1_exp4 [label="Expert 4 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer1_exp5 [label="Expert 5 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer1_exp6 [label="Expert 6 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer1_exp7 [label="Expert 7 L1 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu114_layer1 [label="MLP L1 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu114_layer2 [label="Attention L2 GPU114\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu114_layer2 [label="Gate L2 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu114_layer2_exp0 [label="Expert 0 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer2_exp1 [label="Expert 1 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer2_exp2 [label="Expert 2 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer2_exp3 [label="Expert 3 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer2_exp4 [label="Expert 4 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer2_exp5 [label="Expert 5 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer2_exp6 [label="Expert 6 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer2_exp7 [label="Expert 7 L2 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu114_layer2 [label="MLP L2 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu114_layer3 [label="Attention L3 GPU114\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu114_layer3 [label="Gate L3 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu114_layer3_exp0 [label="Expert 0 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer3_exp1 [label="Expert 1 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer3_exp2 [label="Expert 2 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer3_exp3 [label="Expert 3 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer3_exp4 [label="Expert 4 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer3_exp5 [label="Expert 5 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer3_exp6 [label="Expert 6 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu114_layer3_exp7 [label="Expert 7 L3 GPU114\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu114_layer3 [label="MLP L3 GPU114\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu115_layer0 [label="Attention L0 GPU115\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu115_layer0 [label="Gate L0 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu115_layer0_exp0 [label="Expert 0 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer0_exp1 [label="Expert 1 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer0_exp2 [label="Expert 2 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer0_exp3 [label="Expert 3 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer0_exp4 [label="Expert 4 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer0_exp5 [label="Expert 5 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer0_exp6 [label="Expert 6 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer0_exp7 [label="Expert 7 L0 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu115_layer0 [label="MLP L0 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu115_layer1 [label="Attention L1 GPU115\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu115_layer1 [label="Gate L1 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu115_layer1_exp0 [label="Expert 0 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer1_exp1 [label="Expert 1 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer1_exp2 [label="Expert 2 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer1_exp3 [label="Expert 3 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer1_exp4 [label="Expert 4 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer1_exp5 [label="Expert 5 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer1_exp6 [label="Expert 6 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer1_exp7 [label="Expert 7 L1 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu115_layer1 [label="MLP L1 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu115_layer2 [label="Attention L2 GPU115\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu115_layer2 [label="Gate L2 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu115_layer2_exp0 [label="Expert 0 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer2_exp1 [label="Expert 1 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer2_exp2 [label="Expert 2 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer2_exp3 [label="Expert 3 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer2_exp4 [label="Expert 4 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer2_exp5 [label="Expert 5 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer2_exp6 [label="Expert 6 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer2_exp7 [label="Expert 7 L2 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu115_layer2 [label="MLP L2 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu115_layer3 [label="Attention L3 GPU115\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu115_layer3 [label="Gate L3 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu115_layer3_exp0 [label="Expert 0 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer3_exp1 [label="Expert 1 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer3_exp2 [label="Expert 2 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer3_exp3 [label="Expert 3 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer3_exp4 [label="Expert 4 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer3_exp5 [label="Expert 5 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer3_exp6 [label="Expert 6 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu115_layer3_exp7 [label="Expert 7 L3 GPU115\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu115_layer3 [label="MLP L3 GPU115\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 1 - GPUs [116-119]
        subgraph cluster_pp7_1 {
            label="PP Stage 1\nGPUs [116-119]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu116_layer4 [label="Attention L4 GPU116\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu116_layer4 [label="Gate L4 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu116_layer4_exp0 [label="Expert 0 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer4_exp1 [label="Expert 1 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer4_exp2 [label="Expert 2 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer4_exp3 [label="Expert 3 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer4_exp4 [label="Expert 4 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer4_exp5 [label="Expert 5 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer4_exp6 [label="Expert 6 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer4_exp7 [label="Expert 7 L4 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu116_layer4 [label="MLP L4 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu116_layer5 [label="Attention L5 GPU116\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu116_layer5 [label="Gate L5 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu116_layer5_exp0 [label="Expert 0 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer5_exp1 [label="Expert 1 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer5_exp2 [label="Expert 2 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer5_exp3 [label="Expert 3 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer5_exp4 [label="Expert 4 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer5_exp5 [label="Expert 5 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer5_exp6 [label="Expert 6 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer5_exp7 [label="Expert 7 L5 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu116_layer5 [label="MLP L5 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu116_layer6 [label="Attention L6 GPU116\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu116_layer6 [label="Gate L6 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu116_layer6_exp0 [label="Expert 0 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer6_exp1 [label="Expert 1 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer6_exp2 [label="Expert 2 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer6_exp3 [label="Expert 3 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer6_exp4 [label="Expert 4 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer6_exp5 [label="Expert 5 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer6_exp6 [label="Expert 6 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer6_exp7 [label="Expert 7 L6 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu116_layer6 [label="MLP L6 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu116_layer7 [label="Attention L7 GPU116\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu116_layer7 [label="Gate L7 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu116_layer7_exp0 [label="Expert 0 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer7_exp1 [label="Expert 1 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer7_exp2 [label="Expert 2 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer7_exp3 [label="Expert 3 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer7_exp4 [label="Expert 4 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer7_exp5 [label="Expert 5 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer7_exp6 [label="Expert 6 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu116_layer7_exp7 [label="Expert 7 L7 GPU116\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu116_layer7 [label="MLP L7 GPU116\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu117_layer4 [label="Attention L4 GPU117\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu117_layer4 [label="Gate L4 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu117_layer4_exp0 [label="Expert 0 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer4_exp1 [label="Expert 1 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer4_exp2 [label="Expert 2 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer4_exp3 [label="Expert 3 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer4_exp4 [label="Expert 4 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer4_exp5 [label="Expert 5 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer4_exp6 [label="Expert 6 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer4_exp7 [label="Expert 7 L4 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu117_layer4 [label="MLP L4 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu117_layer5 [label="Attention L5 GPU117\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu117_layer5 [label="Gate L5 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu117_layer5_exp0 [label="Expert 0 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer5_exp1 [label="Expert 1 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer5_exp2 [label="Expert 2 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer5_exp3 [label="Expert 3 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer5_exp4 [label="Expert 4 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer5_exp5 [label="Expert 5 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer5_exp6 [label="Expert 6 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer5_exp7 [label="Expert 7 L5 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu117_layer5 [label="MLP L5 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu117_layer6 [label="Attention L6 GPU117\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu117_layer6 [label="Gate L6 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu117_layer6_exp0 [label="Expert 0 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer6_exp1 [label="Expert 1 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer6_exp2 [label="Expert 2 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer6_exp3 [label="Expert 3 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer6_exp4 [label="Expert 4 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer6_exp5 [label="Expert 5 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer6_exp6 [label="Expert 6 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer6_exp7 [label="Expert 7 L6 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu117_layer6 [label="MLP L6 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu117_layer7 [label="Attention L7 GPU117\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu117_layer7 [label="Gate L7 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu117_layer7_exp0 [label="Expert 0 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer7_exp1 [label="Expert 1 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer7_exp2 [label="Expert 2 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer7_exp3 [label="Expert 3 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer7_exp4 [label="Expert 4 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer7_exp5 [label="Expert 5 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer7_exp6 [label="Expert 6 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu117_layer7_exp7 [label="Expert 7 L7 GPU117\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu117_layer7 [label="MLP L7 GPU117\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu118_layer4 [label="Attention L4 GPU118\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu118_layer4 [label="Gate L4 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu118_layer4_exp0 [label="Expert 0 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer4_exp1 [label="Expert 1 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer4_exp2 [label="Expert 2 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer4_exp3 [label="Expert 3 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer4_exp4 [label="Expert 4 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer4_exp5 [label="Expert 5 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer4_exp6 [label="Expert 6 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer4_exp7 [label="Expert 7 L4 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu118_layer4 [label="MLP L4 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu118_layer5 [label="Attention L5 GPU118\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu118_layer5 [label="Gate L5 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu118_layer5_exp0 [label="Expert 0 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer5_exp1 [label="Expert 1 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer5_exp2 [label="Expert 2 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer5_exp3 [label="Expert 3 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer5_exp4 [label="Expert 4 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer5_exp5 [label="Expert 5 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer5_exp6 [label="Expert 6 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer5_exp7 [label="Expert 7 L5 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu118_layer5 [label="MLP L5 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu118_layer6 [label="Attention L6 GPU118\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu118_layer6 [label="Gate L6 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu118_layer6_exp0 [label="Expert 0 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer6_exp1 [label="Expert 1 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer6_exp2 [label="Expert 2 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer6_exp3 [label="Expert 3 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer6_exp4 [label="Expert 4 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer6_exp5 [label="Expert 5 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer6_exp6 [label="Expert 6 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer6_exp7 [label="Expert 7 L6 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu118_layer6 [label="MLP L6 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu118_layer7 [label="Attention L7 GPU118\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu118_layer7 [label="Gate L7 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu118_layer7_exp0 [label="Expert 0 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer7_exp1 [label="Expert 1 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer7_exp2 [label="Expert 2 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer7_exp3 [label="Expert 3 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer7_exp4 [label="Expert 4 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer7_exp5 [label="Expert 5 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer7_exp6 [label="Expert 6 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu118_layer7_exp7 [label="Expert 7 L7 GPU118\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu118_layer7 [label="MLP L7 GPU118\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu119_layer4 [label="Attention L4 GPU119\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu119_layer4 [label="Gate L4 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu119_layer4_exp0 [label="Expert 0 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer4_exp1 [label="Expert 1 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer4_exp2 [label="Expert 2 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer4_exp3 [label="Expert 3 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer4_exp4 [label="Expert 4 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer4_exp5 [label="Expert 5 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer4_exp6 [label="Expert 6 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer4_exp7 [label="Expert 7 L4 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu119_layer4 [label="MLP L4 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu119_layer5 [label="Attention L5 GPU119\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu119_layer5 [label="Gate L5 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu119_layer5_exp0 [label="Expert 0 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer5_exp1 [label="Expert 1 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer5_exp2 [label="Expert 2 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer5_exp3 [label="Expert 3 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer5_exp4 [label="Expert 4 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer5_exp5 [label="Expert 5 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer5_exp6 [label="Expert 6 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer5_exp7 [label="Expert 7 L5 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu119_layer5 [label="MLP L5 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu119_layer6 [label="Attention L6 GPU119\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu119_layer6 [label="Gate L6 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu119_layer6_exp0 [label="Expert 0 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer6_exp1 [label="Expert 1 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer6_exp2 [label="Expert 2 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer6_exp3 [label="Expert 3 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer6_exp4 [label="Expert 4 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer6_exp5 [label="Expert 5 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer6_exp6 [label="Expert 6 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer6_exp7 [label="Expert 7 L6 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu119_layer6 [label="MLP L6 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu119_layer7 [label="Attention L7 GPU119\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu119_layer7 [label="Gate L7 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu119_layer7_exp0 [label="Expert 0 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer7_exp1 [label="Expert 1 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer7_exp2 [label="Expert 2 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer7_exp3 [label="Expert 3 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer7_exp4 [label="Expert 4 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer7_exp5 [label="Expert 5 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer7_exp6 [label="Expert 6 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu119_layer7_exp7 [label="Expert 7 L7 GPU119\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu119_layer7 [label="MLP L7 GPU119\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 2 - GPUs [120-123]
        subgraph cluster_pp7_2 {
            label="PP Stage 2\nGPUs [120-123]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu120_layer8 [label="Attention L8 GPU120\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu120_layer8 [label="Gate L8 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu120_layer8_exp0 [label="Expert 0 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer8_exp1 [label="Expert 1 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer8_exp2 [label="Expert 2 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer8_exp3 [label="Expert 3 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer8_exp4 [label="Expert 4 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer8_exp5 [label="Expert 5 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer8_exp6 [label="Expert 6 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer8_exp7 [label="Expert 7 L8 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu120_layer8 [label="MLP L8 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu120_layer9 [label="Attention L9 GPU120\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu120_layer9 [label="Gate L9 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu120_layer9_exp0 [label="Expert 0 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer9_exp1 [label="Expert 1 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer9_exp2 [label="Expert 2 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer9_exp3 [label="Expert 3 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer9_exp4 [label="Expert 4 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer9_exp5 [label="Expert 5 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer9_exp6 [label="Expert 6 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer9_exp7 [label="Expert 7 L9 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu120_layer9 [label="MLP L9 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu120_layer10 [label="Attention L10 GPU120\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu120_layer10 [label="Gate L10 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu120_layer10_exp0 [label="Expert 0 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer10_exp1 [label="Expert 1 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer10_exp2 [label="Expert 2 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer10_exp3 [label="Expert 3 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer10_exp4 [label="Expert 4 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer10_exp5 [label="Expert 5 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer10_exp6 [label="Expert 6 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer10_exp7 [label="Expert 7 L10 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu120_layer10 [label="MLP L10 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu120_layer11 [label="Attention L11 GPU120\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu120_layer11 [label="Gate L11 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu120_layer11_exp0 [label="Expert 0 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer11_exp1 [label="Expert 1 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer11_exp2 [label="Expert 2 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer11_exp3 [label="Expert 3 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer11_exp4 [label="Expert 4 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer11_exp5 [label="Expert 5 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer11_exp6 [label="Expert 6 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu120_layer11_exp7 [label="Expert 7 L11 GPU120\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu120_layer11 [label="MLP L11 GPU120\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu121_layer8 [label="Attention L8 GPU121\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu121_layer8 [label="Gate L8 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu121_layer8_exp0 [label="Expert 0 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer8_exp1 [label="Expert 1 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer8_exp2 [label="Expert 2 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer8_exp3 [label="Expert 3 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer8_exp4 [label="Expert 4 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer8_exp5 [label="Expert 5 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer8_exp6 [label="Expert 6 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer8_exp7 [label="Expert 7 L8 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu121_layer8 [label="MLP L8 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu121_layer9 [label="Attention L9 GPU121\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu121_layer9 [label="Gate L9 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu121_layer9_exp0 [label="Expert 0 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer9_exp1 [label="Expert 1 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer9_exp2 [label="Expert 2 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer9_exp3 [label="Expert 3 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer9_exp4 [label="Expert 4 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer9_exp5 [label="Expert 5 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer9_exp6 [label="Expert 6 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer9_exp7 [label="Expert 7 L9 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu121_layer9 [label="MLP L9 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu121_layer10 [label="Attention L10 GPU121\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu121_layer10 [label="Gate L10 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu121_layer10_exp0 [label="Expert 0 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer10_exp1 [label="Expert 1 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer10_exp2 [label="Expert 2 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer10_exp3 [label="Expert 3 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer10_exp4 [label="Expert 4 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer10_exp5 [label="Expert 5 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer10_exp6 [label="Expert 6 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer10_exp7 [label="Expert 7 L10 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu121_layer10 [label="MLP L10 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu121_layer11 [label="Attention L11 GPU121\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu121_layer11 [label="Gate L11 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu121_layer11_exp0 [label="Expert 0 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer11_exp1 [label="Expert 1 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer11_exp2 [label="Expert 2 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer11_exp3 [label="Expert 3 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer11_exp4 [label="Expert 4 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer11_exp5 [label="Expert 5 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer11_exp6 [label="Expert 6 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu121_layer11_exp7 [label="Expert 7 L11 GPU121\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu121_layer11 [label="MLP L11 GPU121\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu122_layer8 [label="Attention L8 GPU122\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu122_layer8 [label="Gate L8 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu122_layer8_exp0 [label="Expert 0 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer8_exp1 [label="Expert 1 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer8_exp2 [label="Expert 2 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer8_exp3 [label="Expert 3 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer8_exp4 [label="Expert 4 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer8_exp5 [label="Expert 5 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer8_exp6 [label="Expert 6 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer8_exp7 [label="Expert 7 L8 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu122_layer8 [label="MLP L8 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu122_layer9 [label="Attention L9 GPU122\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu122_layer9 [label="Gate L9 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu122_layer9_exp0 [label="Expert 0 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer9_exp1 [label="Expert 1 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer9_exp2 [label="Expert 2 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer9_exp3 [label="Expert 3 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer9_exp4 [label="Expert 4 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer9_exp5 [label="Expert 5 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer9_exp6 [label="Expert 6 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer9_exp7 [label="Expert 7 L9 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu122_layer9 [label="MLP L9 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu122_layer10 [label="Attention L10 GPU122\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu122_layer10 [label="Gate L10 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu122_layer10_exp0 [label="Expert 0 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer10_exp1 [label="Expert 1 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer10_exp2 [label="Expert 2 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer10_exp3 [label="Expert 3 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer10_exp4 [label="Expert 4 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer10_exp5 [label="Expert 5 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer10_exp6 [label="Expert 6 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer10_exp7 [label="Expert 7 L10 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu122_layer10 [label="MLP L10 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu122_layer11 [label="Attention L11 GPU122\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu122_layer11 [label="Gate L11 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu122_layer11_exp0 [label="Expert 0 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer11_exp1 [label="Expert 1 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer11_exp2 [label="Expert 2 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer11_exp3 [label="Expert 3 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer11_exp4 [label="Expert 4 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer11_exp5 [label="Expert 5 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer11_exp6 [label="Expert 6 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu122_layer11_exp7 [label="Expert 7 L11 GPU122\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu122_layer11 [label="MLP L11 GPU122\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu123_layer8 [label="Attention L8 GPU123\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu123_layer8 [label="Gate L8 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu123_layer8_exp0 [label="Expert 0 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer8_exp1 [label="Expert 1 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer8_exp2 [label="Expert 2 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer8_exp3 [label="Expert 3 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer8_exp4 [label="Expert 4 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer8_exp5 [label="Expert 5 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer8_exp6 [label="Expert 6 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer8_exp7 [label="Expert 7 L8 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu123_layer8 [label="MLP L8 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu123_layer9 [label="Attention L9 GPU123\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu123_layer9 [label="Gate L9 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu123_layer9_exp0 [label="Expert 0 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer9_exp1 [label="Expert 1 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer9_exp2 [label="Expert 2 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer9_exp3 [label="Expert 3 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer9_exp4 [label="Expert 4 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer9_exp5 [label="Expert 5 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer9_exp6 [label="Expert 6 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer9_exp7 [label="Expert 7 L9 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu123_layer9 [label="MLP L9 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu123_layer10 [label="Attention L10 GPU123\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu123_layer10 [label="Gate L10 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu123_layer10_exp0 [label="Expert 0 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer10_exp1 [label="Expert 1 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer10_exp2 [label="Expert 2 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer10_exp3 [label="Expert 3 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer10_exp4 [label="Expert 4 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer10_exp5 [label="Expert 5 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer10_exp6 [label="Expert 6 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer10_exp7 [label="Expert 7 L10 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu123_layer10 [label="MLP L10 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu123_layer11 [label="Attention L11 GPU123\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu123_layer11 [label="Gate L11 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu123_layer11_exp0 [label="Expert 0 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer11_exp1 [label="Expert 1 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer11_exp2 [label="Expert 2 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer11_exp3 [label="Expert 3 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer11_exp4 [label="Expert 4 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer11_exp5 [label="Expert 5 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer11_exp6 [label="Expert 6 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu123_layer11_exp7 [label="Expert 7 L11 GPU123\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu123_layer11 [label="MLP L11 GPU123\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

        // PP Stage 3 - GPUs [124-127]
        subgraph cluster_pp7_3 {
            label="PP Stage 3\nGPUs [124-127]";
            style=filled;
            fillcolor=lightcyan;
            color=green;
            penwidth=1.5;
            
            attn_gpu124_layer12 [label="Attention L12 GPU124\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu124_layer12 [label="Gate L12 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu124_layer12_exp0 [label="Expert 0 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer12_exp1 [label="Expert 1 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer12_exp2 [label="Expert 2 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer12_exp3 [label="Expert 3 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer12_exp4 [label="Expert 4 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer12_exp5 [label="Expert 5 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer12_exp6 [label="Expert 6 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer12_exp7 [label="Expert 7 L12 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu124_layer12 [label="MLP L12 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu124_layer13 [label="Attention L13 GPU124\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu124_layer13 [label="Gate L13 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu124_layer13_exp0 [label="Expert 0 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer13_exp1 [label="Expert 1 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer13_exp2 [label="Expert 2 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer13_exp3 [label="Expert 3 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer13_exp4 [label="Expert 4 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer13_exp5 [label="Expert 5 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer13_exp6 [label="Expert 6 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer13_exp7 [label="Expert 7 L13 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu124_layer13 [label="MLP L13 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu124_layer14 [label="Attention L14 GPU124\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu124_layer14 [label="Gate L14 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu124_layer14_exp0 [label="Expert 0 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer14_exp1 [label="Expert 1 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer14_exp2 [label="Expert 2 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer14_exp3 [label="Expert 3 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer14_exp4 [label="Expert 4 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer14_exp5 [label="Expert 5 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer14_exp6 [label="Expert 6 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer14_exp7 [label="Expert 7 L14 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu124_layer14 [label="MLP L14 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu124_layer15 [label="Attention L15 GPU124\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu124_layer15 [label="Gate L15 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu124_layer15_exp0 [label="Expert 0 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer15_exp1 [label="Expert 1 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer15_exp2 [label="Expert 2 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer15_exp3 [label="Expert 3 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer15_exp4 [label="Expert 4 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer15_exp5 [label="Expert 5 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer15_exp6 [label="Expert 6 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu124_layer15_exp7 [label="Expert 7 L15 GPU124\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu124_layer15 [label="MLP L15 GPU124\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu125_layer12 [label="Attention L12 GPU125\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu125_layer12 [label="Gate L12 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu125_layer12_exp0 [label="Expert 0 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer12_exp1 [label="Expert 1 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer12_exp2 [label="Expert 2 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer12_exp3 [label="Expert 3 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer12_exp4 [label="Expert 4 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer12_exp5 [label="Expert 5 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer12_exp6 [label="Expert 6 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer12_exp7 [label="Expert 7 L12 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu125_layer12 [label="MLP L12 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu125_layer13 [label="Attention L13 GPU125\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu125_layer13 [label="Gate L13 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu125_layer13_exp0 [label="Expert 0 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer13_exp1 [label="Expert 1 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer13_exp2 [label="Expert 2 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer13_exp3 [label="Expert 3 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer13_exp4 [label="Expert 4 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer13_exp5 [label="Expert 5 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer13_exp6 [label="Expert 6 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer13_exp7 [label="Expert 7 L13 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu125_layer13 [label="MLP L13 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu125_layer14 [label="Attention L14 GPU125\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu125_layer14 [label="Gate L14 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu125_layer14_exp0 [label="Expert 0 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer14_exp1 [label="Expert 1 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer14_exp2 [label="Expert 2 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer14_exp3 [label="Expert 3 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer14_exp4 [label="Expert 4 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer14_exp5 [label="Expert 5 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer14_exp6 [label="Expert 6 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer14_exp7 [label="Expert 7 L14 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu125_layer14 [label="MLP L14 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu125_layer15 [label="Attention L15 GPU125\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu125_layer15 [label="Gate L15 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu125_layer15_exp0 [label="Expert 0 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer15_exp1 [label="Expert 1 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer15_exp2 [label="Expert 2 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer15_exp3 [label="Expert 3 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer15_exp4 [label="Expert 4 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer15_exp5 [label="Expert 5 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer15_exp6 [label="Expert 6 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu125_layer15_exp7 [label="Expert 7 L15 GPU125\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu125_layer15 [label="MLP L15 GPU125\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu126_layer12 [label="Attention L12 GPU126\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu126_layer12 [label="Gate L12 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu126_layer12_exp0 [label="Expert 0 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer12_exp1 [label="Expert 1 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer12_exp2 [label="Expert 2 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer12_exp3 [label="Expert 3 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer12_exp4 [label="Expert 4 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer12_exp5 [label="Expert 5 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer12_exp6 [label="Expert 6 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer12_exp7 [label="Expert 7 L12 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu126_layer12 [label="MLP L12 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu126_layer13 [label="Attention L13 GPU126\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu126_layer13 [label="Gate L13 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu126_layer13_exp0 [label="Expert 0 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer13_exp1 [label="Expert 1 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer13_exp2 [label="Expert 2 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer13_exp3 [label="Expert 3 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer13_exp4 [label="Expert 4 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer13_exp5 [label="Expert 5 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer13_exp6 [label="Expert 6 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer13_exp7 [label="Expert 7 L13 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu126_layer13 [label="MLP L13 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu126_layer14 [label="Attention L14 GPU126\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu126_layer14 [label="Gate L14 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu126_layer14_exp0 [label="Expert 0 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer14_exp1 [label="Expert 1 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer14_exp2 [label="Expert 2 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer14_exp3 [label="Expert 3 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer14_exp4 [label="Expert 4 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer14_exp5 [label="Expert 5 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer14_exp6 [label="Expert 6 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer14_exp7 [label="Expert 7 L14 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu126_layer14 [label="MLP L14 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu126_layer15 [label="Attention L15 GPU126\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu126_layer15 [label="Gate L15 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu126_layer15_exp0 [label="Expert 0 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer15_exp1 [label="Expert 1 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer15_exp2 [label="Expert 2 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer15_exp3 [label="Expert 3 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer15_exp4 [label="Expert 4 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer15_exp5 [label="Expert 5 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer15_exp6 [label="Expert 6 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu126_layer15_exp7 [label="Expert 7 L15 GPU126\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu126_layer15 [label="MLP L15 GPU126\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu127_layer12 [label="Attention L12 GPU127\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu127_layer12 [label="Gate L12 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu127_layer12_exp0 [label="Expert 0 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer12_exp1 [label="Expert 1 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer12_exp2 [label="Expert 2 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer12_exp3 [label="Expert 3 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer12_exp4 [label="Expert 4 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer12_exp5 [label="Expert 5 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer12_exp6 [label="Expert 6 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer12_exp7 [label="Expert 7 L12 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu127_layer12 [label="MLP L12 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu127_layer13 [label="Attention L13 GPU127\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu127_layer13 [label="Gate L13 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu127_layer13_exp0 [label="Expert 0 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer13_exp1 [label="Expert 1 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer13_exp2 [label="Expert 2 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer13_exp3 [label="Expert 3 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer13_exp4 [label="Expert 4 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer13_exp5 [label="Expert 5 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer13_exp6 [label="Expert 6 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer13_exp7 [label="Expert 7 L13 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu127_layer13 [label="MLP L13 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu127_layer14 [label="Attention L14 GPU127\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu127_layer14 [label="Gate L14 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu127_layer14_exp0 [label="Expert 0 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer14_exp1 [label="Expert 1 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer14_exp2 [label="Expert 2 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer14_exp3 [label="Expert 3 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer14_exp4 [label="Expert 4 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer14_exp5 [label="Expert 5 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer14_exp6 [label="Expert 6 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer14_exp7 [label="Expert 7 L14 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu127_layer14 [label="MLP L14 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
            attn_gpu127_layer15 [label="Attention L15 GPU127\nInput: [batch_size=4, seq_len=1024, heads=16, d_k=64]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightblue];
            gate_gpu127_layer15 [label="Gate L15 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, top_2_experts]", shape=parallelogram, fillcolor=orange, style="filled,dashed", penwidth=2];
            expert_gpu127_layer15_exp0 [label="Expert 0 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer15_exp1 [label="Expert 1 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer15_exp2 [label="Expert 2 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer15_exp3 [label="Expert 3 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer15_exp4 [label="Expert 4 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer15_exp5 [label="Expert 5 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer15_exp6 [label="Expert 6 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            expert_gpu127_layer15_exp7 [label="Expert 7 L15 GPU127\nInput: [batch_size=0.5, seq_len=1024, hidden=1024]\nOutput: [batch_size=0.5, seq_len=1024, hidden=1024]", shape=box, fillcolor=pink];
            mlp_gpu127_layer15 [label="MLP L15 GPU127\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=box, fillcolor=lightgreen];
        }

    }

    // Communication nodes
    node [shape=ellipse, fillcolor=yellow];
    
    tp_allreduce_ep0_pp0_layer0 [label="TP All-Reduce\nEP0 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp0_layer1 [label="TP All-Reduce\nEP0 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp0_layer2 [label="TP All-Reduce\nEP0 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp0_layer3 [label="TP All-Reduce\nEP0 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp1_layer4 [label="TP All-Reduce\nEP0 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp1_layer5 [label="TP All-Reduce\nEP0 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp1_layer6 [label="TP All-Reduce\nEP0 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp1_layer7 [label="TP All-Reduce\nEP0 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp2_layer8 [label="TP All-Reduce\nEP0 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp2_layer9 [label="TP All-Reduce\nEP0 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp2_layer10 [label="TP All-Reduce\nEP0 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp2_layer11 [label="TP All-Reduce\nEP0 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp3_layer12 [label="TP All-Reduce\nEP0 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp3_layer13 [label="TP All-Reduce\nEP0 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp3_layer14 [label="TP All-Reduce\nEP0 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep0_pp3_layer15 [label="TP All-Reduce\nEP0 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp0_layer0 [label="TP All-Reduce\nEP1 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp0_layer1 [label="TP All-Reduce\nEP1 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp0_layer2 [label="TP All-Reduce\nEP1 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp0_layer3 [label="TP All-Reduce\nEP1 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp1_layer4 [label="TP All-Reduce\nEP1 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp1_layer5 [label="TP All-Reduce\nEP1 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp1_layer6 [label="TP All-Reduce\nEP1 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp1_layer7 [label="TP All-Reduce\nEP1 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp2_layer8 [label="TP All-Reduce\nEP1 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp2_layer9 [label="TP All-Reduce\nEP1 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp2_layer10 [label="TP All-Reduce\nEP1 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp2_layer11 [label="TP All-Reduce\nEP1 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp3_layer12 [label="TP All-Reduce\nEP1 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp3_layer13 [label="TP All-Reduce\nEP1 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp3_layer14 [label="TP All-Reduce\nEP1 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep1_pp3_layer15 [label="TP All-Reduce\nEP1 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp0_layer0 [label="TP All-Reduce\nEP2 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp0_layer1 [label="TP All-Reduce\nEP2 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp0_layer2 [label="TP All-Reduce\nEP2 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp0_layer3 [label="TP All-Reduce\nEP2 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp1_layer4 [label="TP All-Reduce\nEP2 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp1_layer5 [label="TP All-Reduce\nEP2 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp1_layer6 [label="TP All-Reduce\nEP2 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp1_layer7 [label="TP All-Reduce\nEP2 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp2_layer8 [label="TP All-Reduce\nEP2 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp2_layer9 [label="TP All-Reduce\nEP2 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp2_layer10 [label="TP All-Reduce\nEP2 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp2_layer11 [label="TP All-Reduce\nEP2 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp3_layer12 [label="TP All-Reduce\nEP2 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp3_layer13 [label="TP All-Reduce\nEP2 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp3_layer14 [label="TP All-Reduce\nEP2 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep2_pp3_layer15 [label="TP All-Reduce\nEP2 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp0_layer0 [label="TP All-Reduce\nEP3 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp0_layer1 [label="TP All-Reduce\nEP3 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp0_layer2 [label="TP All-Reduce\nEP3 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp0_layer3 [label="TP All-Reduce\nEP3 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp1_layer4 [label="TP All-Reduce\nEP3 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp1_layer5 [label="TP All-Reduce\nEP3 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp1_layer6 [label="TP All-Reduce\nEP3 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp1_layer7 [label="TP All-Reduce\nEP3 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp2_layer8 [label="TP All-Reduce\nEP3 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp2_layer9 [label="TP All-Reduce\nEP3 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp2_layer10 [label="TP All-Reduce\nEP3 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp2_layer11 [label="TP All-Reduce\nEP3 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp3_layer12 [label="TP All-Reduce\nEP3 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp3_layer13 [label="TP All-Reduce\nEP3 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp3_layer14 [label="TP All-Reduce\nEP3 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep3_pp3_layer15 [label="TP All-Reduce\nEP3 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp0_layer0 [label="TP All-Reduce\nEP4 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp0_layer1 [label="TP All-Reduce\nEP4 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp0_layer2 [label="TP All-Reduce\nEP4 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp0_layer3 [label="TP All-Reduce\nEP4 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp1_layer4 [label="TP All-Reduce\nEP4 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp1_layer5 [label="TP All-Reduce\nEP4 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp1_layer6 [label="TP All-Reduce\nEP4 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp1_layer7 [label="TP All-Reduce\nEP4 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp2_layer8 [label="TP All-Reduce\nEP4 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp2_layer9 [label="TP All-Reduce\nEP4 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp2_layer10 [label="TP All-Reduce\nEP4 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp2_layer11 [label="TP All-Reduce\nEP4 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp3_layer12 [label="TP All-Reduce\nEP4 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp3_layer13 [label="TP All-Reduce\nEP4 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp3_layer14 [label="TP All-Reduce\nEP4 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep4_pp3_layer15 [label="TP All-Reduce\nEP4 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp0_layer0 [label="TP All-Reduce\nEP5 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp0_layer1 [label="TP All-Reduce\nEP5 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp0_layer2 [label="TP All-Reduce\nEP5 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp0_layer3 [label="TP All-Reduce\nEP5 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp1_layer4 [label="TP All-Reduce\nEP5 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp1_layer5 [label="TP All-Reduce\nEP5 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp1_layer6 [label="TP All-Reduce\nEP5 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp1_layer7 [label="TP All-Reduce\nEP5 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp2_layer8 [label="TP All-Reduce\nEP5 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp2_layer9 [label="TP All-Reduce\nEP5 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp2_layer10 [label="TP All-Reduce\nEP5 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp2_layer11 [label="TP All-Reduce\nEP5 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp3_layer12 [label="TP All-Reduce\nEP5 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp3_layer13 [label="TP All-Reduce\nEP5 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp3_layer14 [label="TP All-Reduce\nEP5 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep5_pp3_layer15 [label="TP All-Reduce\nEP5 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp0_layer0 [label="TP All-Reduce\nEP6 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp0_layer1 [label="TP All-Reduce\nEP6 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp0_layer2 [label="TP All-Reduce\nEP6 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp0_layer3 [label="TP All-Reduce\nEP6 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp1_layer4 [label="TP All-Reduce\nEP6 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp1_layer5 [label="TP All-Reduce\nEP6 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp1_layer6 [label="TP All-Reduce\nEP6 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp1_layer7 [label="TP All-Reduce\nEP6 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp2_layer8 [label="TP All-Reduce\nEP6 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp2_layer9 [label="TP All-Reduce\nEP6 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp2_layer10 [label="TP All-Reduce\nEP6 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp2_layer11 [label="TP All-Reduce\nEP6 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp3_layer12 [label="TP All-Reduce\nEP6 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp3_layer13 [label="TP All-Reduce\nEP6 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp3_layer14 [label="TP All-Reduce\nEP6 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep6_pp3_layer15 [label="TP All-Reduce\nEP6 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp0_layer0 [label="TP All-Reduce\nEP7 PP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp0_layer1 [label="TP All-Reduce\nEP7 PP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp0_layer2 [label="TP All-Reduce\nEP7 PP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp0_layer3 [label="TP All-Reduce\nEP7 PP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp1_layer4 [label="TP All-Reduce\nEP7 PP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp1_layer5 [label="TP All-Reduce\nEP7 PP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp1_layer6 [label="TP All-Reduce\nEP7 PP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp1_layer7 [label="TP All-Reduce\nEP7 PP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp2_layer8 [label="TP All-Reduce\nEP7 PP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp2_layer9 [label="TP All-Reduce\nEP7 PP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp2_layer10 [label="TP All-Reduce\nEP7 PP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp2_layer11 [label="TP All-Reduce\nEP7 PP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp3_layer12 [label="TP All-Reduce\nEP7 PP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp3_layer13 [label="TP All-Reduce\nEP7 PP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp3_layer14 [label="TP All-Reduce\nEP7 PP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    tp_allreduce_ep7_pp3_layer15 [label="TP All-Reduce\nEP7 PP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer0 [label="EP All-to-All\nEP0 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer1 [label="EP All-to-All\nEP0 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer2 [label="EP All-to-All\nEP0 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer3 [label="EP All-to-All\nEP0 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer4 [label="EP All-to-All\nEP0 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer5 [label="EP All-to-All\nEP0 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer6 [label="EP All-to-All\nEP0 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer7 [label="EP All-to-All\nEP0 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer8 [label="EP All-to-All\nEP0 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer9 [label="EP All-to-All\nEP0 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer10 [label="EP All-to-All\nEP0 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer11 [label="EP All-to-All\nEP0 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer12 [label="EP All-to-All\nEP0 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer13 [label="EP All-to-All\nEP0 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer14 [label="EP All-to-All\nEP0 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep0_layer15 [label="EP All-to-All\nEP0 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer0 [label="EP All-to-All\nEP1 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer1 [label="EP All-to-All\nEP1 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer2 [label="EP All-to-All\nEP1 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer3 [label="EP All-to-All\nEP1 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer4 [label="EP All-to-All\nEP1 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer5 [label="EP All-to-All\nEP1 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer6 [label="EP All-to-All\nEP1 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer7 [label="EP All-to-All\nEP1 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer8 [label="EP All-to-All\nEP1 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer9 [label="EP All-to-All\nEP1 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer10 [label="EP All-to-All\nEP1 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer11 [label="EP All-to-All\nEP1 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer12 [label="EP All-to-All\nEP1 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer13 [label="EP All-to-All\nEP1 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer14 [label="EP All-to-All\nEP1 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep1_layer15 [label="EP All-to-All\nEP1 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer0 [label="EP All-to-All\nEP2 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer1 [label="EP All-to-All\nEP2 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer2 [label="EP All-to-All\nEP2 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer3 [label="EP All-to-All\nEP2 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer4 [label="EP All-to-All\nEP2 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer5 [label="EP All-to-All\nEP2 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer6 [label="EP All-to-All\nEP2 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer7 [label="EP All-to-All\nEP2 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer8 [label="EP All-to-All\nEP2 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer9 [label="EP All-to-All\nEP2 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer10 [label="EP All-to-All\nEP2 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer11 [label="EP All-to-All\nEP2 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer12 [label="EP All-to-All\nEP2 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer13 [label="EP All-to-All\nEP2 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer14 [label="EP All-to-All\nEP2 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep2_layer15 [label="EP All-to-All\nEP2 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer0 [label="EP All-to-All\nEP3 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer1 [label="EP All-to-All\nEP3 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer2 [label="EP All-to-All\nEP3 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer3 [label="EP All-to-All\nEP3 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer4 [label="EP All-to-All\nEP3 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer5 [label="EP All-to-All\nEP3 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer6 [label="EP All-to-All\nEP3 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer7 [label="EP All-to-All\nEP3 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer8 [label="EP All-to-All\nEP3 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer9 [label="EP All-to-All\nEP3 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer10 [label="EP All-to-All\nEP3 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer11 [label="EP All-to-All\nEP3 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer12 [label="EP All-to-All\nEP3 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer13 [label="EP All-to-All\nEP3 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer14 [label="EP All-to-All\nEP3 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep3_layer15 [label="EP All-to-All\nEP3 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer0 [label="EP All-to-All\nEP4 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer1 [label="EP All-to-All\nEP4 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer2 [label="EP All-to-All\nEP4 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer3 [label="EP All-to-All\nEP4 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer4 [label="EP All-to-All\nEP4 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer5 [label="EP All-to-All\nEP4 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer6 [label="EP All-to-All\nEP4 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer7 [label="EP All-to-All\nEP4 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer8 [label="EP All-to-All\nEP4 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer9 [label="EP All-to-All\nEP4 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer10 [label="EP All-to-All\nEP4 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer11 [label="EP All-to-All\nEP4 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer12 [label="EP All-to-All\nEP4 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer13 [label="EP All-to-All\nEP4 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer14 [label="EP All-to-All\nEP4 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep4_layer15 [label="EP All-to-All\nEP4 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer0 [label="EP All-to-All\nEP5 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer1 [label="EP All-to-All\nEP5 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer2 [label="EP All-to-All\nEP5 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer3 [label="EP All-to-All\nEP5 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer4 [label="EP All-to-All\nEP5 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer5 [label="EP All-to-All\nEP5 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer6 [label="EP All-to-All\nEP5 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer7 [label="EP All-to-All\nEP5 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer8 [label="EP All-to-All\nEP5 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer9 [label="EP All-to-All\nEP5 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer10 [label="EP All-to-All\nEP5 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer11 [label="EP All-to-All\nEP5 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer12 [label="EP All-to-All\nEP5 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer13 [label="EP All-to-All\nEP5 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer14 [label="EP All-to-All\nEP5 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep5_layer15 [label="EP All-to-All\nEP5 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer0 [label="EP All-to-All\nEP6 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer1 [label="EP All-to-All\nEP6 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer2 [label="EP All-to-All\nEP6 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer3 [label="EP All-to-All\nEP6 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer4 [label="EP All-to-All\nEP6 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer5 [label="EP All-to-All\nEP6 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer6 [label="EP All-to-All\nEP6 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer7 [label="EP All-to-All\nEP6 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer8 [label="EP All-to-All\nEP6 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer9 [label="EP All-to-All\nEP6 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer10 [label="EP All-to-All\nEP6 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer11 [label="EP All-to-All\nEP6 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer12 [label="EP All-to-All\nEP6 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer13 [label="EP All-to-All\nEP6 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer14 [label="EP All-to-All\nEP6 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep6_layer15 [label="EP All-to-All\nEP6 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer0 [label="EP All-to-All\nEP7 L0\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer1 [label="EP All-to-All\nEP7 L1\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer2 [label="EP All-to-All\nEP7 L2\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer3 [label="EP All-to-All\nEP7 L3\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer4 [label="EP All-to-All\nEP7 L4\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer5 [label="EP All-to-All\nEP7 L5\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer6 [label="EP All-to-All\nEP7 L6\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer7 [label="EP All-to-All\nEP7 L7\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer8 [label="EP All-to-All\nEP7 L8\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer9 [label="EP All-to-All\nEP7 L9\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer10 [label="EP All-to-All\nEP7 L10\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer11 [label="EP All-to-All\nEP7 L11\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer12 [label="EP All-to-All\nEP7 L12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer13 [label="EP All-to-All\nEP7 L13\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer14 [label="EP All-to-All\nEP7 L14\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    ep_alltoall_ep7_layer15 [label="EP All-to-All\nEP7 L15\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep0_stage0_to_1 [label="PP P2P\nEP0 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep0_stage1_to_2 [label="PP P2P\nEP0 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep0_stage2_to_3 [label="PP P2P\nEP0 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep1_stage0_to_1 [label="PP P2P\nEP1 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep1_stage1_to_2 [label="PP P2P\nEP1 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep1_stage2_to_3 [label="PP P2P\nEP1 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep2_stage0_to_1 [label="PP P2P\nEP2 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep2_stage1_to_2 [label="PP P2P\nEP2 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep2_stage2_to_3 [label="PP P2P\nEP2 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep3_stage0_to_1 [label="PP P2P\nEP3 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep3_stage1_to_2 [label="PP P2P\nEP3 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep3_stage2_to_3 [label="PP P2P\nEP3 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep4_stage0_to_1 [label="PP P2P\nEP4 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep4_stage1_to_2 [label="PP P2P\nEP4 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep4_stage2_to_3 [label="PP P2P\nEP4 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep5_stage0_to_1 [label="PP P2P\nEP5 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep5_stage1_to_2 [label="PP P2P\nEP5 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep5_stage2_to_3 [label="PP P2P\nEP5 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep6_stage0_to_1 [label="PP P2P\nEP6 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep6_stage1_to_2 [label="PP P2P\nEP6 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep6_stage2_to_3 [label="PP P2P\nEP6 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep7_stage0_to_1 [label="PP P2P\nEP7 Stage01\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep7_stage1_to_2 [label="PP P2P\nEP7 Stage12\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    pp_p2p_ep7_stage2_to_3 [label="PP P2P\nEP7 Stage23\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]"];
    output [shape=ellipse, label="Output\nInput: [batch_size=4, seq_len=1024, hidden=1024]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", fillcolor=lightcoral];
    
    // Main data flow edges
    edge [color=black, penwidth=1];
    
    input -> attn_gpu0_layer0;
    input -> attn_gpu16_layer0;
    input -> attn_gpu32_layer0;
    input -> attn_gpu48_layer0;
    input -> attn_gpu64_layer0;
    input -> attn_gpu80_layer0;
    input -> attn_gpu96_layer0;
    input -> attn_gpu112_layer0;
    attn_gpu0_layer0 -> gate_gpu0_layer0;
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> expert_gpu0_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu0_layer0 -> mlp_gpu0_layer0;
    mlp_gpu0_layer0 -> tp_allreduce_ep0_pp0_layer0;
    attn_gpu1_layer0 -> gate_gpu1_layer0;
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> expert_gpu1_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu1_layer0 -> mlp_gpu1_layer0;
    mlp_gpu1_layer0 -> tp_allreduce_ep0_pp0_layer0;
    attn_gpu2_layer0 -> gate_gpu2_layer0;
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> expert_gpu2_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu2_layer0 -> mlp_gpu2_layer0;
    mlp_gpu2_layer0 -> tp_allreduce_ep0_pp0_layer0;
    attn_gpu3_layer0 -> gate_gpu3_layer0;
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> expert_gpu3_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu3_layer0 -> mlp_gpu3_layer0;
    mlp_gpu3_layer0 -> tp_allreduce_ep0_pp0_layer0;
    attn_gpu0_layer1 -> gate_gpu0_layer1;
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> expert_gpu0_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu0_layer1 -> mlp_gpu0_layer1;
    mlp_gpu0_layer1 -> tp_allreduce_ep0_pp0_layer1;
    attn_gpu1_layer1 -> gate_gpu1_layer1;
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> expert_gpu1_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu1_layer1 -> mlp_gpu1_layer1;
    mlp_gpu1_layer1 -> tp_allreduce_ep0_pp0_layer1;
    attn_gpu2_layer1 -> gate_gpu2_layer1;
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> expert_gpu2_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu2_layer1 -> mlp_gpu2_layer1;
    mlp_gpu2_layer1 -> tp_allreduce_ep0_pp0_layer1;
    attn_gpu3_layer1 -> gate_gpu3_layer1;
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> expert_gpu3_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu3_layer1 -> mlp_gpu3_layer1;
    mlp_gpu3_layer1 -> tp_allreduce_ep0_pp0_layer1;
    attn_gpu0_layer2 -> gate_gpu0_layer2;
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> expert_gpu0_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu0_layer2 -> mlp_gpu0_layer2;
    mlp_gpu0_layer2 -> tp_allreduce_ep0_pp0_layer2;
    attn_gpu1_layer2 -> gate_gpu1_layer2;
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> expert_gpu1_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu1_layer2 -> mlp_gpu1_layer2;
    mlp_gpu1_layer2 -> tp_allreduce_ep0_pp0_layer2;
    attn_gpu2_layer2 -> gate_gpu2_layer2;
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> expert_gpu2_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu2_layer2 -> mlp_gpu2_layer2;
    mlp_gpu2_layer2 -> tp_allreduce_ep0_pp0_layer2;
    attn_gpu3_layer2 -> gate_gpu3_layer2;
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> expert_gpu3_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu3_layer2 -> mlp_gpu3_layer2;
    mlp_gpu3_layer2 -> tp_allreduce_ep0_pp0_layer2;
    attn_gpu0_layer3 -> gate_gpu0_layer3;
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> expert_gpu0_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu0_layer3 -> mlp_gpu0_layer3;
    mlp_gpu0_layer3 -> tp_allreduce_ep0_pp0_layer3;
    attn_gpu1_layer3 -> gate_gpu1_layer3;
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> expert_gpu1_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu1_layer3 -> mlp_gpu1_layer3;
    mlp_gpu1_layer3 -> tp_allreduce_ep0_pp0_layer3;
    attn_gpu2_layer3 -> gate_gpu2_layer3;
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> expert_gpu2_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu2_layer3 -> mlp_gpu2_layer3;
    mlp_gpu2_layer3 -> tp_allreduce_ep0_pp0_layer3;
    attn_gpu3_layer3 -> gate_gpu3_layer3;
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> expert_gpu3_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu3_layer3 -> mlp_gpu3_layer3;
    mlp_gpu3_layer3 -> tp_allreduce_ep0_pp0_layer3;
    attn_gpu4_layer4 -> gate_gpu4_layer4;
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> expert_gpu4_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu4_layer4 -> mlp_gpu4_layer4;
    mlp_gpu4_layer4 -> tp_allreduce_ep0_pp1_layer4;
    attn_gpu5_layer4 -> gate_gpu5_layer4;
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> expert_gpu5_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu5_layer4 -> mlp_gpu5_layer4;
    mlp_gpu5_layer4 -> tp_allreduce_ep0_pp1_layer4;
    attn_gpu6_layer4 -> gate_gpu6_layer4;
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> expert_gpu6_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu6_layer4 -> mlp_gpu6_layer4;
    mlp_gpu6_layer4 -> tp_allreduce_ep0_pp1_layer4;
    attn_gpu7_layer4 -> gate_gpu7_layer4;
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> expert_gpu7_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu7_layer4 -> mlp_gpu7_layer4;
    mlp_gpu7_layer4 -> tp_allreduce_ep0_pp1_layer4;
    attn_gpu4_layer5 -> gate_gpu4_layer5;
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> expert_gpu4_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu4_layer5 -> mlp_gpu4_layer5;
    mlp_gpu4_layer5 -> tp_allreduce_ep0_pp1_layer5;
    attn_gpu5_layer5 -> gate_gpu5_layer5;
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> expert_gpu5_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu5_layer5 -> mlp_gpu5_layer5;
    mlp_gpu5_layer5 -> tp_allreduce_ep0_pp1_layer5;
    attn_gpu6_layer5 -> gate_gpu6_layer5;
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> expert_gpu6_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu6_layer5 -> mlp_gpu6_layer5;
    mlp_gpu6_layer5 -> tp_allreduce_ep0_pp1_layer5;
    attn_gpu7_layer5 -> gate_gpu7_layer5;
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> expert_gpu7_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu7_layer5 -> mlp_gpu7_layer5;
    mlp_gpu7_layer5 -> tp_allreduce_ep0_pp1_layer5;
    attn_gpu4_layer6 -> gate_gpu4_layer6;
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> expert_gpu4_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu4_layer6 -> mlp_gpu4_layer6;
    mlp_gpu4_layer6 -> tp_allreduce_ep0_pp1_layer6;
    attn_gpu5_layer6 -> gate_gpu5_layer6;
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> expert_gpu5_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu5_layer6 -> mlp_gpu5_layer6;
    mlp_gpu5_layer6 -> tp_allreduce_ep0_pp1_layer6;
    attn_gpu6_layer6 -> gate_gpu6_layer6;
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> expert_gpu6_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu6_layer6 -> mlp_gpu6_layer6;
    mlp_gpu6_layer6 -> tp_allreduce_ep0_pp1_layer6;
    attn_gpu7_layer6 -> gate_gpu7_layer6;
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> expert_gpu7_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu7_layer6 -> mlp_gpu7_layer6;
    mlp_gpu7_layer6 -> tp_allreduce_ep0_pp1_layer6;
    attn_gpu4_layer7 -> gate_gpu4_layer7;
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> expert_gpu4_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu4_layer7 -> mlp_gpu4_layer7;
    mlp_gpu4_layer7 -> tp_allreduce_ep0_pp1_layer7;
    attn_gpu5_layer7 -> gate_gpu5_layer7;
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> expert_gpu5_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu5_layer7 -> mlp_gpu5_layer7;
    mlp_gpu5_layer7 -> tp_allreduce_ep0_pp1_layer7;
    attn_gpu6_layer7 -> gate_gpu6_layer7;
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> expert_gpu6_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu6_layer7 -> mlp_gpu6_layer7;
    mlp_gpu6_layer7 -> tp_allreduce_ep0_pp1_layer7;
    attn_gpu7_layer7 -> gate_gpu7_layer7;
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> expert_gpu7_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu7_layer7 -> mlp_gpu7_layer7;
    mlp_gpu7_layer7 -> tp_allreduce_ep0_pp1_layer7;
    attn_gpu8_layer8 -> gate_gpu8_layer8;
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> expert_gpu8_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu8_layer8 -> mlp_gpu8_layer8;
    mlp_gpu8_layer8 -> tp_allreduce_ep0_pp2_layer8;
    attn_gpu9_layer8 -> gate_gpu9_layer8;
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> expert_gpu9_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu9_layer8 -> mlp_gpu9_layer8;
    mlp_gpu9_layer8 -> tp_allreduce_ep0_pp2_layer8;
    attn_gpu10_layer8 -> gate_gpu10_layer8;
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> expert_gpu10_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu10_layer8 -> mlp_gpu10_layer8;
    mlp_gpu10_layer8 -> tp_allreduce_ep0_pp2_layer8;
    attn_gpu11_layer8 -> gate_gpu11_layer8;
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> expert_gpu11_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu11_layer8 -> mlp_gpu11_layer8;
    mlp_gpu11_layer8 -> tp_allreduce_ep0_pp2_layer8;
    attn_gpu8_layer9 -> gate_gpu8_layer9;
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> expert_gpu8_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu8_layer9 -> mlp_gpu8_layer9;
    mlp_gpu8_layer9 -> tp_allreduce_ep0_pp2_layer9;
    attn_gpu9_layer9 -> gate_gpu9_layer9;
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> expert_gpu9_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu9_layer9 -> mlp_gpu9_layer9;
    mlp_gpu9_layer9 -> tp_allreduce_ep0_pp2_layer9;
    attn_gpu10_layer9 -> gate_gpu10_layer9;
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> expert_gpu10_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu10_layer9 -> mlp_gpu10_layer9;
    mlp_gpu10_layer9 -> tp_allreduce_ep0_pp2_layer9;
    attn_gpu11_layer9 -> gate_gpu11_layer9;
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> expert_gpu11_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu11_layer9 -> mlp_gpu11_layer9;
    mlp_gpu11_layer9 -> tp_allreduce_ep0_pp2_layer9;
    attn_gpu8_layer10 -> gate_gpu8_layer10;
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> expert_gpu8_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu8_layer10 -> mlp_gpu8_layer10;
    mlp_gpu8_layer10 -> tp_allreduce_ep0_pp2_layer10;
    attn_gpu9_layer10 -> gate_gpu9_layer10;
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> expert_gpu9_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu9_layer10 -> mlp_gpu9_layer10;
    mlp_gpu9_layer10 -> tp_allreduce_ep0_pp2_layer10;
    attn_gpu10_layer10 -> gate_gpu10_layer10;
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> expert_gpu10_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu10_layer10 -> mlp_gpu10_layer10;
    mlp_gpu10_layer10 -> tp_allreduce_ep0_pp2_layer10;
    attn_gpu11_layer10 -> gate_gpu11_layer10;
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> expert_gpu11_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu11_layer10 -> mlp_gpu11_layer10;
    mlp_gpu11_layer10 -> tp_allreduce_ep0_pp2_layer10;
    attn_gpu8_layer11 -> gate_gpu8_layer11;
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> expert_gpu8_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu8_layer11 -> mlp_gpu8_layer11;
    mlp_gpu8_layer11 -> tp_allreduce_ep0_pp2_layer11;
    attn_gpu9_layer11 -> gate_gpu9_layer11;
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> expert_gpu9_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu9_layer11 -> mlp_gpu9_layer11;
    mlp_gpu9_layer11 -> tp_allreduce_ep0_pp2_layer11;
    attn_gpu10_layer11 -> gate_gpu10_layer11;
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> expert_gpu10_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu10_layer11 -> mlp_gpu10_layer11;
    mlp_gpu10_layer11 -> tp_allreduce_ep0_pp2_layer11;
    attn_gpu11_layer11 -> gate_gpu11_layer11;
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> expert_gpu11_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu11_layer11 -> mlp_gpu11_layer11;
    mlp_gpu11_layer11 -> tp_allreduce_ep0_pp2_layer11;
    attn_gpu12_layer12 -> gate_gpu12_layer12;
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> expert_gpu12_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu12_layer12 -> mlp_gpu12_layer12;
    mlp_gpu12_layer12 -> tp_allreduce_ep0_pp3_layer12;
    attn_gpu13_layer12 -> gate_gpu13_layer12;
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> expert_gpu13_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu13_layer12 -> mlp_gpu13_layer12;
    mlp_gpu13_layer12 -> tp_allreduce_ep0_pp3_layer12;
    attn_gpu14_layer12 -> gate_gpu14_layer12;
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> expert_gpu14_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu14_layer12 -> mlp_gpu14_layer12;
    mlp_gpu14_layer12 -> tp_allreduce_ep0_pp3_layer12;
    attn_gpu15_layer12 -> gate_gpu15_layer12;
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> expert_gpu15_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu15_layer12 -> mlp_gpu15_layer12;
    mlp_gpu15_layer12 -> tp_allreduce_ep0_pp3_layer12;
    attn_gpu12_layer13 -> gate_gpu12_layer13;
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> expert_gpu12_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu12_layer13 -> mlp_gpu12_layer13;
    mlp_gpu12_layer13 -> tp_allreduce_ep0_pp3_layer13;
    attn_gpu13_layer13 -> gate_gpu13_layer13;
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> expert_gpu13_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu13_layer13 -> mlp_gpu13_layer13;
    mlp_gpu13_layer13 -> tp_allreduce_ep0_pp3_layer13;
    attn_gpu14_layer13 -> gate_gpu14_layer13;
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> expert_gpu14_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu14_layer13 -> mlp_gpu14_layer13;
    mlp_gpu14_layer13 -> tp_allreduce_ep0_pp3_layer13;
    attn_gpu15_layer13 -> gate_gpu15_layer13;
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> expert_gpu15_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu15_layer13 -> mlp_gpu15_layer13;
    mlp_gpu15_layer13 -> tp_allreduce_ep0_pp3_layer13;
    attn_gpu12_layer14 -> gate_gpu12_layer14;
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> expert_gpu12_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu12_layer14 -> mlp_gpu12_layer14;
    mlp_gpu12_layer14 -> tp_allreduce_ep0_pp3_layer14;
    attn_gpu13_layer14 -> gate_gpu13_layer14;
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> expert_gpu13_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu13_layer14 -> mlp_gpu13_layer14;
    mlp_gpu13_layer14 -> tp_allreduce_ep0_pp3_layer14;
    attn_gpu14_layer14 -> gate_gpu14_layer14;
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> expert_gpu14_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu14_layer14 -> mlp_gpu14_layer14;
    mlp_gpu14_layer14 -> tp_allreduce_ep0_pp3_layer14;
    attn_gpu15_layer14 -> gate_gpu15_layer14;
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> expert_gpu15_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu15_layer14 -> mlp_gpu15_layer14;
    mlp_gpu15_layer14 -> tp_allreduce_ep0_pp3_layer14;
    attn_gpu12_layer15 -> gate_gpu12_layer15;
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> expert_gpu12_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu12_layer15 -> mlp_gpu12_layer15;
    mlp_gpu12_layer15 -> tp_allreduce_ep0_pp3_layer15;
    attn_gpu13_layer15 -> gate_gpu13_layer15;
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> expert_gpu13_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu13_layer15 -> mlp_gpu13_layer15;
    mlp_gpu13_layer15 -> tp_allreduce_ep0_pp3_layer15;
    attn_gpu14_layer15 -> gate_gpu14_layer15;
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> expert_gpu14_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu14_layer15 -> mlp_gpu14_layer15;
    mlp_gpu14_layer15 -> tp_allreduce_ep0_pp3_layer15;
    attn_gpu15_layer15 -> gate_gpu15_layer15;
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> expert_gpu15_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu15_layer15 -> mlp_gpu15_layer15;
    mlp_gpu15_layer15 -> tp_allreduce_ep0_pp3_layer15;
    attn_gpu16_layer0 -> gate_gpu16_layer0;
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> expert_gpu16_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu16_layer0 -> mlp_gpu16_layer0;
    mlp_gpu16_layer0 -> tp_allreduce_ep1_pp0_layer0;
    attn_gpu17_layer0 -> gate_gpu17_layer0;
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> expert_gpu17_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu17_layer0 -> mlp_gpu17_layer0;
    mlp_gpu17_layer0 -> tp_allreduce_ep1_pp0_layer0;
    attn_gpu18_layer0 -> gate_gpu18_layer0;
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> expert_gpu18_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu18_layer0 -> mlp_gpu18_layer0;
    mlp_gpu18_layer0 -> tp_allreduce_ep1_pp0_layer0;
    attn_gpu19_layer0 -> gate_gpu19_layer0;
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> expert_gpu19_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu19_layer0 -> mlp_gpu19_layer0;
    mlp_gpu19_layer0 -> tp_allreduce_ep1_pp0_layer0;
    attn_gpu16_layer1 -> gate_gpu16_layer1;
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> expert_gpu16_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu16_layer1 -> mlp_gpu16_layer1;
    mlp_gpu16_layer1 -> tp_allreduce_ep1_pp0_layer1;
    attn_gpu17_layer1 -> gate_gpu17_layer1;
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> expert_gpu17_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu17_layer1 -> mlp_gpu17_layer1;
    mlp_gpu17_layer1 -> tp_allreduce_ep1_pp0_layer1;
    attn_gpu18_layer1 -> gate_gpu18_layer1;
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> expert_gpu18_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu18_layer1 -> mlp_gpu18_layer1;
    mlp_gpu18_layer1 -> tp_allreduce_ep1_pp0_layer1;
    attn_gpu19_layer1 -> gate_gpu19_layer1;
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> expert_gpu19_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu19_layer1 -> mlp_gpu19_layer1;
    mlp_gpu19_layer1 -> tp_allreduce_ep1_pp0_layer1;
    attn_gpu16_layer2 -> gate_gpu16_layer2;
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> expert_gpu16_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu16_layer2 -> mlp_gpu16_layer2;
    mlp_gpu16_layer2 -> tp_allreduce_ep1_pp0_layer2;
    attn_gpu17_layer2 -> gate_gpu17_layer2;
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> expert_gpu17_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu17_layer2 -> mlp_gpu17_layer2;
    mlp_gpu17_layer2 -> tp_allreduce_ep1_pp0_layer2;
    attn_gpu18_layer2 -> gate_gpu18_layer2;
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> expert_gpu18_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu18_layer2 -> mlp_gpu18_layer2;
    mlp_gpu18_layer2 -> tp_allreduce_ep1_pp0_layer2;
    attn_gpu19_layer2 -> gate_gpu19_layer2;
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> expert_gpu19_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu19_layer2 -> mlp_gpu19_layer2;
    mlp_gpu19_layer2 -> tp_allreduce_ep1_pp0_layer2;
    attn_gpu16_layer3 -> gate_gpu16_layer3;
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> expert_gpu16_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu16_layer3 -> mlp_gpu16_layer3;
    mlp_gpu16_layer3 -> tp_allreduce_ep1_pp0_layer3;
    attn_gpu17_layer3 -> gate_gpu17_layer3;
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> expert_gpu17_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu17_layer3 -> mlp_gpu17_layer3;
    mlp_gpu17_layer3 -> tp_allreduce_ep1_pp0_layer3;
    attn_gpu18_layer3 -> gate_gpu18_layer3;
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> expert_gpu18_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu18_layer3 -> mlp_gpu18_layer3;
    mlp_gpu18_layer3 -> tp_allreduce_ep1_pp0_layer3;
    attn_gpu19_layer3 -> gate_gpu19_layer3;
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> expert_gpu19_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu19_layer3 -> mlp_gpu19_layer3;
    mlp_gpu19_layer3 -> tp_allreduce_ep1_pp0_layer3;
    attn_gpu20_layer4 -> gate_gpu20_layer4;
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> expert_gpu20_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu20_layer4 -> mlp_gpu20_layer4;
    mlp_gpu20_layer4 -> tp_allreduce_ep1_pp1_layer4;
    attn_gpu21_layer4 -> gate_gpu21_layer4;
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> expert_gpu21_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu21_layer4 -> mlp_gpu21_layer4;
    mlp_gpu21_layer4 -> tp_allreduce_ep1_pp1_layer4;
    attn_gpu22_layer4 -> gate_gpu22_layer4;
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> expert_gpu22_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu22_layer4 -> mlp_gpu22_layer4;
    mlp_gpu22_layer4 -> tp_allreduce_ep1_pp1_layer4;
    attn_gpu23_layer4 -> gate_gpu23_layer4;
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> expert_gpu23_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu23_layer4 -> mlp_gpu23_layer4;
    mlp_gpu23_layer4 -> tp_allreduce_ep1_pp1_layer4;
    attn_gpu20_layer5 -> gate_gpu20_layer5;
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> expert_gpu20_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu20_layer5 -> mlp_gpu20_layer5;
    mlp_gpu20_layer5 -> tp_allreduce_ep1_pp1_layer5;
    attn_gpu21_layer5 -> gate_gpu21_layer5;
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> expert_gpu21_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu21_layer5 -> mlp_gpu21_layer5;
    mlp_gpu21_layer5 -> tp_allreduce_ep1_pp1_layer5;
    attn_gpu22_layer5 -> gate_gpu22_layer5;
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> expert_gpu22_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu22_layer5 -> mlp_gpu22_layer5;
    mlp_gpu22_layer5 -> tp_allreduce_ep1_pp1_layer5;
    attn_gpu23_layer5 -> gate_gpu23_layer5;
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> expert_gpu23_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu23_layer5 -> mlp_gpu23_layer5;
    mlp_gpu23_layer5 -> tp_allreduce_ep1_pp1_layer5;
    attn_gpu20_layer6 -> gate_gpu20_layer6;
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> expert_gpu20_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu20_layer6 -> mlp_gpu20_layer6;
    mlp_gpu20_layer6 -> tp_allreduce_ep1_pp1_layer6;
    attn_gpu21_layer6 -> gate_gpu21_layer6;
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> expert_gpu21_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu21_layer6 -> mlp_gpu21_layer6;
    mlp_gpu21_layer6 -> tp_allreduce_ep1_pp1_layer6;
    attn_gpu22_layer6 -> gate_gpu22_layer6;
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> expert_gpu22_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu22_layer6 -> mlp_gpu22_layer6;
    mlp_gpu22_layer6 -> tp_allreduce_ep1_pp1_layer6;
    attn_gpu23_layer6 -> gate_gpu23_layer6;
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> expert_gpu23_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu23_layer6 -> mlp_gpu23_layer6;
    mlp_gpu23_layer6 -> tp_allreduce_ep1_pp1_layer6;
    attn_gpu20_layer7 -> gate_gpu20_layer7;
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> expert_gpu20_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu20_layer7 -> mlp_gpu20_layer7;
    mlp_gpu20_layer7 -> tp_allreduce_ep1_pp1_layer7;
    attn_gpu21_layer7 -> gate_gpu21_layer7;
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> expert_gpu21_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu21_layer7 -> mlp_gpu21_layer7;
    mlp_gpu21_layer7 -> tp_allreduce_ep1_pp1_layer7;
    attn_gpu22_layer7 -> gate_gpu22_layer7;
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> expert_gpu22_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu22_layer7 -> mlp_gpu22_layer7;
    mlp_gpu22_layer7 -> tp_allreduce_ep1_pp1_layer7;
    attn_gpu23_layer7 -> gate_gpu23_layer7;
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> expert_gpu23_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu23_layer7 -> mlp_gpu23_layer7;
    mlp_gpu23_layer7 -> tp_allreduce_ep1_pp1_layer7;
    attn_gpu24_layer8 -> gate_gpu24_layer8;
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> expert_gpu24_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu24_layer8 -> mlp_gpu24_layer8;
    mlp_gpu24_layer8 -> tp_allreduce_ep1_pp2_layer8;
    attn_gpu25_layer8 -> gate_gpu25_layer8;
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> expert_gpu25_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu25_layer8 -> mlp_gpu25_layer8;
    mlp_gpu25_layer8 -> tp_allreduce_ep1_pp2_layer8;
    attn_gpu26_layer8 -> gate_gpu26_layer8;
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> expert_gpu26_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu26_layer8 -> mlp_gpu26_layer8;
    mlp_gpu26_layer8 -> tp_allreduce_ep1_pp2_layer8;
    attn_gpu27_layer8 -> gate_gpu27_layer8;
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> expert_gpu27_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu27_layer8 -> mlp_gpu27_layer8;
    mlp_gpu27_layer8 -> tp_allreduce_ep1_pp2_layer8;
    attn_gpu24_layer9 -> gate_gpu24_layer9;
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> expert_gpu24_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu24_layer9 -> mlp_gpu24_layer9;
    mlp_gpu24_layer9 -> tp_allreduce_ep1_pp2_layer9;
    attn_gpu25_layer9 -> gate_gpu25_layer9;
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> expert_gpu25_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu25_layer9 -> mlp_gpu25_layer9;
    mlp_gpu25_layer9 -> tp_allreduce_ep1_pp2_layer9;
    attn_gpu26_layer9 -> gate_gpu26_layer9;
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> expert_gpu26_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu26_layer9 -> mlp_gpu26_layer9;
    mlp_gpu26_layer9 -> tp_allreduce_ep1_pp2_layer9;
    attn_gpu27_layer9 -> gate_gpu27_layer9;
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> expert_gpu27_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu27_layer9 -> mlp_gpu27_layer9;
    mlp_gpu27_layer9 -> tp_allreduce_ep1_pp2_layer9;
    attn_gpu24_layer10 -> gate_gpu24_layer10;
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> expert_gpu24_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu24_layer10 -> mlp_gpu24_layer10;
    mlp_gpu24_layer10 -> tp_allreduce_ep1_pp2_layer10;
    attn_gpu25_layer10 -> gate_gpu25_layer10;
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> expert_gpu25_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu25_layer10 -> mlp_gpu25_layer10;
    mlp_gpu25_layer10 -> tp_allreduce_ep1_pp2_layer10;
    attn_gpu26_layer10 -> gate_gpu26_layer10;
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> expert_gpu26_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu26_layer10 -> mlp_gpu26_layer10;
    mlp_gpu26_layer10 -> tp_allreduce_ep1_pp2_layer10;
    attn_gpu27_layer10 -> gate_gpu27_layer10;
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> expert_gpu27_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu27_layer10 -> mlp_gpu27_layer10;
    mlp_gpu27_layer10 -> tp_allreduce_ep1_pp2_layer10;
    attn_gpu24_layer11 -> gate_gpu24_layer11;
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> expert_gpu24_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu24_layer11 -> mlp_gpu24_layer11;
    mlp_gpu24_layer11 -> tp_allreduce_ep1_pp2_layer11;
    attn_gpu25_layer11 -> gate_gpu25_layer11;
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> expert_gpu25_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu25_layer11 -> mlp_gpu25_layer11;
    mlp_gpu25_layer11 -> tp_allreduce_ep1_pp2_layer11;
    attn_gpu26_layer11 -> gate_gpu26_layer11;
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> expert_gpu26_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu26_layer11 -> mlp_gpu26_layer11;
    mlp_gpu26_layer11 -> tp_allreduce_ep1_pp2_layer11;
    attn_gpu27_layer11 -> gate_gpu27_layer11;
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> expert_gpu27_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu27_layer11 -> mlp_gpu27_layer11;
    mlp_gpu27_layer11 -> tp_allreduce_ep1_pp2_layer11;
    attn_gpu28_layer12 -> gate_gpu28_layer12;
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> expert_gpu28_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu28_layer12 -> mlp_gpu28_layer12;
    mlp_gpu28_layer12 -> tp_allreduce_ep1_pp3_layer12;
    attn_gpu29_layer12 -> gate_gpu29_layer12;
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> expert_gpu29_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu29_layer12 -> mlp_gpu29_layer12;
    mlp_gpu29_layer12 -> tp_allreduce_ep1_pp3_layer12;
    attn_gpu30_layer12 -> gate_gpu30_layer12;
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> expert_gpu30_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu30_layer12 -> mlp_gpu30_layer12;
    mlp_gpu30_layer12 -> tp_allreduce_ep1_pp3_layer12;
    attn_gpu31_layer12 -> gate_gpu31_layer12;
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> expert_gpu31_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu31_layer12 -> mlp_gpu31_layer12;
    mlp_gpu31_layer12 -> tp_allreduce_ep1_pp3_layer12;
    attn_gpu28_layer13 -> gate_gpu28_layer13;
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> expert_gpu28_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu28_layer13 -> mlp_gpu28_layer13;
    mlp_gpu28_layer13 -> tp_allreduce_ep1_pp3_layer13;
    attn_gpu29_layer13 -> gate_gpu29_layer13;
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> expert_gpu29_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu29_layer13 -> mlp_gpu29_layer13;
    mlp_gpu29_layer13 -> tp_allreduce_ep1_pp3_layer13;
    attn_gpu30_layer13 -> gate_gpu30_layer13;
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> expert_gpu30_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu30_layer13 -> mlp_gpu30_layer13;
    mlp_gpu30_layer13 -> tp_allreduce_ep1_pp3_layer13;
    attn_gpu31_layer13 -> gate_gpu31_layer13;
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> expert_gpu31_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu31_layer13 -> mlp_gpu31_layer13;
    mlp_gpu31_layer13 -> tp_allreduce_ep1_pp3_layer13;
    attn_gpu28_layer14 -> gate_gpu28_layer14;
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> expert_gpu28_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu28_layer14 -> mlp_gpu28_layer14;
    mlp_gpu28_layer14 -> tp_allreduce_ep1_pp3_layer14;
    attn_gpu29_layer14 -> gate_gpu29_layer14;
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> expert_gpu29_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu29_layer14 -> mlp_gpu29_layer14;
    mlp_gpu29_layer14 -> tp_allreduce_ep1_pp3_layer14;
    attn_gpu30_layer14 -> gate_gpu30_layer14;
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> expert_gpu30_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu30_layer14 -> mlp_gpu30_layer14;
    mlp_gpu30_layer14 -> tp_allreduce_ep1_pp3_layer14;
    attn_gpu31_layer14 -> gate_gpu31_layer14;
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> expert_gpu31_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu31_layer14 -> mlp_gpu31_layer14;
    mlp_gpu31_layer14 -> tp_allreduce_ep1_pp3_layer14;
    attn_gpu28_layer15 -> gate_gpu28_layer15;
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> expert_gpu28_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu28_layer15 -> mlp_gpu28_layer15;
    mlp_gpu28_layer15 -> tp_allreduce_ep1_pp3_layer15;
    attn_gpu29_layer15 -> gate_gpu29_layer15;
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> expert_gpu29_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu29_layer15 -> mlp_gpu29_layer15;
    mlp_gpu29_layer15 -> tp_allreduce_ep1_pp3_layer15;
    attn_gpu30_layer15 -> gate_gpu30_layer15;
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> expert_gpu30_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu30_layer15 -> mlp_gpu30_layer15;
    mlp_gpu30_layer15 -> tp_allreduce_ep1_pp3_layer15;
    attn_gpu31_layer15 -> gate_gpu31_layer15;
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> expert_gpu31_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu31_layer15 -> mlp_gpu31_layer15;
    mlp_gpu31_layer15 -> tp_allreduce_ep1_pp3_layer15;
    attn_gpu32_layer0 -> gate_gpu32_layer0;
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> expert_gpu32_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu32_layer0 -> mlp_gpu32_layer0;
    mlp_gpu32_layer0 -> tp_allreduce_ep2_pp0_layer0;
    attn_gpu33_layer0 -> gate_gpu33_layer0;
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> expert_gpu33_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu33_layer0 -> mlp_gpu33_layer0;
    mlp_gpu33_layer0 -> tp_allreduce_ep2_pp0_layer0;
    attn_gpu34_layer0 -> gate_gpu34_layer0;
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> expert_gpu34_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu34_layer0 -> mlp_gpu34_layer0;
    mlp_gpu34_layer0 -> tp_allreduce_ep2_pp0_layer0;
    attn_gpu35_layer0 -> gate_gpu35_layer0;
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> expert_gpu35_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu35_layer0 -> mlp_gpu35_layer0;
    mlp_gpu35_layer0 -> tp_allreduce_ep2_pp0_layer0;
    attn_gpu32_layer1 -> gate_gpu32_layer1;
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> expert_gpu32_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu32_layer1 -> mlp_gpu32_layer1;
    mlp_gpu32_layer1 -> tp_allreduce_ep2_pp0_layer1;
    attn_gpu33_layer1 -> gate_gpu33_layer1;
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> expert_gpu33_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu33_layer1 -> mlp_gpu33_layer1;
    mlp_gpu33_layer1 -> tp_allreduce_ep2_pp0_layer1;
    attn_gpu34_layer1 -> gate_gpu34_layer1;
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> expert_gpu34_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu34_layer1 -> mlp_gpu34_layer1;
    mlp_gpu34_layer1 -> tp_allreduce_ep2_pp0_layer1;
    attn_gpu35_layer1 -> gate_gpu35_layer1;
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> expert_gpu35_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu35_layer1 -> mlp_gpu35_layer1;
    mlp_gpu35_layer1 -> tp_allreduce_ep2_pp0_layer1;
    attn_gpu32_layer2 -> gate_gpu32_layer2;
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> expert_gpu32_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu32_layer2 -> mlp_gpu32_layer2;
    mlp_gpu32_layer2 -> tp_allreduce_ep2_pp0_layer2;
    attn_gpu33_layer2 -> gate_gpu33_layer2;
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> expert_gpu33_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu33_layer2 -> mlp_gpu33_layer2;
    mlp_gpu33_layer2 -> tp_allreduce_ep2_pp0_layer2;
    attn_gpu34_layer2 -> gate_gpu34_layer2;
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> expert_gpu34_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu34_layer2 -> mlp_gpu34_layer2;
    mlp_gpu34_layer2 -> tp_allreduce_ep2_pp0_layer2;
    attn_gpu35_layer2 -> gate_gpu35_layer2;
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> expert_gpu35_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu35_layer2 -> mlp_gpu35_layer2;
    mlp_gpu35_layer2 -> tp_allreduce_ep2_pp0_layer2;
    attn_gpu32_layer3 -> gate_gpu32_layer3;
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> expert_gpu32_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu32_layer3 -> mlp_gpu32_layer3;
    mlp_gpu32_layer3 -> tp_allreduce_ep2_pp0_layer3;
    attn_gpu33_layer3 -> gate_gpu33_layer3;
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> expert_gpu33_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu33_layer3 -> mlp_gpu33_layer3;
    mlp_gpu33_layer3 -> tp_allreduce_ep2_pp0_layer3;
    attn_gpu34_layer3 -> gate_gpu34_layer3;
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> expert_gpu34_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu34_layer3 -> mlp_gpu34_layer3;
    mlp_gpu34_layer3 -> tp_allreduce_ep2_pp0_layer3;
    attn_gpu35_layer3 -> gate_gpu35_layer3;
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> expert_gpu35_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu35_layer3 -> mlp_gpu35_layer3;
    mlp_gpu35_layer3 -> tp_allreduce_ep2_pp0_layer3;
    attn_gpu36_layer4 -> gate_gpu36_layer4;
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> expert_gpu36_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu36_layer4 -> mlp_gpu36_layer4;
    mlp_gpu36_layer4 -> tp_allreduce_ep2_pp1_layer4;
    attn_gpu37_layer4 -> gate_gpu37_layer4;
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> expert_gpu37_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu37_layer4 -> mlp_gpu37_layer4;
    mlp_gpu37_layer4 -> tp_allreduce_ep2_pp1_layer4;
    attn_gpu38_layer4 -> gate_gpu38_layer4;
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> expert_gpu38_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu38_layer4 -> mlp_gpu38_layer4;
    mlp_gpu38_layer4 -> tp_allreduce_ep2_pp1_layer4;
    attn_gpu39_layer4 -> gate_gpu39_layer4;
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> expert_gpu39_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu39_layer4 -> mlp_gpu39_layer4;
    mlp_gpu39_layer4 -> tp_allreduce_ep2_pp1_layer4;
    attn_gpu36_layer5 -> gate_gpu36_layer5;
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> expert_gpu36_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu36_layer5 -> mlp_gpu36_layer5;
    mlp_gpu36_layer5 -> tp_allreduce_ep2_pp1_layer5;
    attn_gpu37_layer5 -> gate_gpu37_layer5;
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> expert_gpu37_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu37_layer5 -> mlp_gpu37_layer5;
    mlp_gpu37_layer5 -> tp_allreduce_ep2_pp1_layer5;
    attn_gpu38_layer5 -> gate_gpu38_layer5;
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> expert_gpu38_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu38_layer5 -> mlp_gpu38_layer5;
    mlp_gpu38_layer5 -> tp_allreduce_ep2_pp1_layer5;
    attn_gpu39_layer5 -> gate_gpu39_layer5;
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> expert_gpu39_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu39_layer5 -> mlp_gpu39_layer5;
    mlp_gpu39_layer5 -> tp_allreduce_ep2_pp1_layer5;
    attn_gpu36_layer6 -> gate_gpu36_layer6;
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> expert_gpu36_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu36_layer6 -> mlp_gpu36_layer6;
    mlp_gpu36_layer6 -> tp_allreduce_ep2_pp1_layer6;
    attn_gpu37_layer6 -> gate_gpu37_layer6;
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> expert_gpu37_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu37_layer6 -> mlp_gpu37_layer6;
    mlp_gpu37_layer6 -> tp_allreduce_ep2_pp1_layer6;
    attn_gpu38_layer6 -> gate_gpu38_layer6;
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> expert_gpu38_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu38_layer6 -> mlp_gpu38_layer6;
    mlp_gpu38_layer6 -> tp_allreduce_ep2_pp1_layer6;
    attn_gpu39_layer6 -> gate_gpu39_layer6;
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> expert_gpu39_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu39_layer6 -> mlp_gpu39_layer6;
    mlp_gpu39_layer6 -> tp_allreduce_ep2_pp1_layer6;
    attn_gpu36_layer7 -> gate_gpu36_layer7;
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> expert_gpu36_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu36_layer7 -> mlp_gpu36_layer7;
    mlp_gpu36_layer7 -> tp_allreduce_ep2_pp1_layer7;
    attn_gpu37_layer7 -> gate_gpu37_layer7;
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> expert_gpu37_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu37_layer7 -> mlp_gpu37_layer7;
    mlp_gpu37_layer7 -> tp_allreduce_ep2_pp1_layer7;
    attn_gpu38_layer7 -> gate_gpu38_layer7;
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> expert_gpu38_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu38_layer7 -> mlp_gpu38_layer7;
    mlp_gpu38_layer7 -> tp_allreduce_ep2_pp1_layer7;
    attn_gpu39_layer7 -> gate_gpu39_layer7;
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> expert_gpu39_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu39_layer7 -> mlp_gpu39_layer7;
    mlp_gpu39_layer7 -> tp_allreduce_ep2_pp1_layer7;
    attn_gpu40_layer8 -> gate_gpu40_layer8;
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> expert_gpu40_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu40_layer8 -> mlp_gpu40_layer8;
    mlp_gpu40_layer8 -> tp_allreduce_ep2_pp2_layer8;
    attn_gpu41_layer8 -> gate_gpu41_layer8;
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> expert_gpu41_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu41_layer8 -> mlp_gpu41_layer8;
    mlp_gpu41_layer8 -> tp_allreduce_ep2_pp2_layer8;
    attn_gpu42_layer8 -> gate_gpu42_layer8;
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> expert_gpu42_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu42_layer8 -> mlp_gpu42_layer8;
    mlp_gpu42_layer8 -> tp_allreduce_ep2_pp2_layer8;
    attn_gpu43_layer8 -> gate_gpu43_layer8;
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> expert_gpu43_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu43_layer8 -> mlp_gpu43_layer8;
    mlp_gpu43_layer8 -> tp_allreduce_ep2_pp2_layer8;
    attn_gpu40_layer9 -> gate_gpu40_layer9;
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> expert_gpu40_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu40_layer9 -> mlp_gpu40_layer9;
    mlp_gpu40_layer9 -> tp_allreduce_ep2_pp2_layer9;
    attn_gpu41_layer9 -> gate_gpu41_layer9;
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> expert_gpu41_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu41_layer9 -> mlp_gpu41_layer9;
    mlp_gpu41_layer9 -> tp_allreduce_ep2_pp2_layer9;
    attn_gpu42_layer9 -> gate_gpu42_layer9;
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> expert_gpu42_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu42_layer9 -> mlp_gpu42_layer9;
    mlp_gpu42_layer9 -> tp_allreduce_ep2_pp2_layer9;
    attn_gpu43_layer9 -> gate_gpu43_layer9;
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> expert_gpu43_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu43_layer9 -> mlp_gpu43_layer9;
    mlp_gpu43_layer9 -> tp_allreduce_ep2_pp2_layer9;
    attn_gpu40_layer10 -> gate_gpu40_layer10;
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> expert_gpu40_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu40_layer10 -> mlp_gpu40_layer10;
    mlp_gpu40_layer10 -> tp_allreduce_ep2_pp2_layer10;
    attn_gpu41_layer10 -> gate_gpu41_layer10;
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> expert_gpu41_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu41_layer10 -> mlp_gpu41_layer10;
    mlp_gpu41_layer10 -> tp_allreduce_ep2_pp2_layer10;
    attn_gpu42_layer10 -> gate_gpu42_layer10;
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> expert_gpu42_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu42_layer10 -> mlp_gpu42_layer10;
    mlp_gpu42_layer10 -> tp_allreduce_ep2_pp2_layer10;
    attn_gpu43_layer10 -> gate_gpu43_layer10;
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> expert_gpu43_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu43_layer10 -> mlp_gpu43_layer10;
    mlp_gpu43_layer10 -> tp_allreduce_ep2_pp2_layer10;
    attn_gpu40_layer11 -> gate_gpu40_layer11;
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> expert_gpu40_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu40_layer11 -> mlp_gpu40_layer11;
    mlp_gpu40_layer11 -> tp_allreduce_ep2_pp2_layer11;
    attn_gpu41_layer11 -> gate_gpu41_layer11;
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> expert_gpu41_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu41_layer11 -> mlp_gpu41_layer11;
    mlp_gpu41_layer11 -> tp_allreduce_ep2_pp2_layer11;
    attn_gpu42_layer11 -> gate_gpu42_layer11;
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> expert_gpu42_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu42_layer11 -> mlp_gpu42_layer11;
    mlp_gpu42_layer11 -> tp_allreduce_ep2_pp2_layer11;
    attn_gpu43_layer11 -> gate_gpu43_layer11;
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> expert_gpu43_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu43_layer11 -> mlp_gpu43_layer11;
    mlp_gpu43_layer11 -> tp_allreduce_ep2_pp2_layer11;
    attn_gpu44_layer12 -> gate_gpu44_layer12;
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> expert_gpu44_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu44_layer12 -> mlp_gpu44_layer12;
    mlp_gpu44_layer12 -> tp_allreduce_ep2_pp3_layer12;
    attn_gpu45_layer12 -> gate_gpu45_layer12;
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> expert_gpu45_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu45_layer12 -> mlp_gpu45_layer12;
    mlp_gpu45_layer12 -> tp_allreduce_ep2_pp3_layer12;
    attn_gpu46_layer12 -> gate_gpu46_layer12;
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> expert_gpu46_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu46_layer12 -> mlp_gpu46_layer12;
    mlp_gpu46_layer12 -> tp_allreduce_ep2_pp3_layer12;
    attn_gpu47_layer12 -> gate_gpu47_layer12;
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> expert_gpu47_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu47_layer12 -> mlp_gpu47_layer12;
    mlp_gpu47_layer12 -> tp_allreduce_ep2_pp3_layer12;
    attn_gpu44_layer13 -> gate_gpu44_layer13;
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> expert_gpu44_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu44_layer13 -> mlp_gpu44_layer13;
    mlp_gpu44_layer13 -> tp_allreduce_ep2_pp3_layer13;
    attn_gpu45_layer13 -> gate_gpu45_layer13;
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> expert_gpu45_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu45_layer13 -> mlp_gpu45_layer13;
    mlp_gpu45_layer13 -> tp_allreduce_ep2_pp3_layer13;
    attn_gpu46_layer13 -> gate_gpu46_layer13;
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> expert_gpu46_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu46_layer13 -> mlp_gpu46_layer13;
    mlp_gpu46_layer13 -> tp_allreduce_ep2_pp3_layer13;
    attn_gpu47_layer13 -> gate_gpu47_layer13;
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> expert_gpu47_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu47_layer13 -> mlp_gpu47_layer13;
    mlp_gpu47_layer13 -> tp_allreduce_ep2_pp3_layer13;
    attn_gpu44_layer14 -> gate_gpu44_layer14;
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> expert_gpu44_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu44_layer14 -> mlp_gpu44_layer14;
    mlp_gpu44_layer14 -> tp_allreduce_ep2_pp3_layer14;
    attn_gpu45_layer14 -> gate_gpu45_layer14;
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> expert_gpu45_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu45_layer14 -> mlp_gpu45_layer14;
    mlp_gpu45_layer14 -> tp_allreduce_ep2_pp3_layer14;
    attn_gpu46_layer14 -> gate_gpu46_layer14;
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> expert_gpu46_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu46_layer14 -> mlp_gpu46_layer14;
    mlp_gpu46_layer14 -> tp_allreduce_ep2_pp3_layer14;
    attn_gpu47_layer14 -> gate_gpu47_layer14;
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> expert_gpu47_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu47_layer14 -> mlp_gpu47_layer14;
    mlp_gpu47_layer14 -> tp_allreduce_ep2_pp3_layer14;
    attn_gpu44_layer15 -> gate_gpu44_layer15;
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> expert_gpu44_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu44_layer15 -> mlp_gpu44_layer15;
    mlp_gpu44_layer15 -> tp_allreduce_ep2_pp3_layer15;
    attn_gpu45_layer15 -> gate_gpu45_layer15;
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> expert_gpu45_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu45_layer15 -> mlp_gpu45_layer15;
    mlp_gpu45_layer15 -> tp_allreduce_ep2_pp3_layer15;
    attn_gpu46_layer15 -> gate_gpu46_layer15;
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> expert_gpu46_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu46_layer15 -> mlp_gpu46_layer15;
    mlp_gpu46_layer15 -> tp_allreduce_ep2_pp3_layer15;
    attn_gpu47_layer15 -> gate_gpu47_layer15;
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> expert_gpu47_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu47_layer15 -> mlp_gpu47_layer15;
    mlp_gpu47_layer15 -> tp_allreduce_ep2_pp3_layer15;
    attn_gpu48_layer0 -> gate_gpu48_layer0;
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> expert_gpu48_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu48_layer0 -> mlp_gpu48_layer0;
    mlp_gpu48_layer0 -> tp_allreduce_ep3_pp0_layer0;
    attn_gpu49_layer0 -> gate_gpu49_layer0;
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> expert_gpu49_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu49_layer0 -> mlp_gpu49_layer0;
    mlp_gpu49_layer0 -> tp_allreduce_ep3_pp0_layer0;
    attn_gpu50_layer0 -> gate_gpu50_layer0;
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> expert_gpu50_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu50_layer0 -> mlp_gpu50_layer0;
    mlp_gpu50_layer0 -> tp_allreduce_ep3_pp0_layer0;
    attn_gpu51_layer0 -> gate_gpu51_layer0;
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> expert_gpu51_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu51_layer0 -> mlp_gpu51_layer0;
    mlp_gpu51_layer0 -> tp_allreduce_ep3_pp0_layer0;
    attn_gpu48_layer1 -> gate_gpu48_layer1;
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> expert_gpu48_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu48_layer1 -> mlp_gpu48_layer1;
    mlp_gpu48_layer1 -> tp_allreduce_ep3_pp0_layer1;
    attn_gpu49_layer1 -> gate_gpu49_layer1;
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> expert_gpu49_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu49_layer1 -> mlp_gpu49_layer1;
    mlp_gpu49_layer1 -> tp_allreduce_ep3_pp0_layer1;
    attn_gpu50_layer1 -> gate_gpu50_layer1;
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> expert_gpu50_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu50_layer1 -> mlp_gpu50_layer1;
    mlp_gpu50_layer1 -> tp_allreduce_ep3_pp0_layer1;
    attn_gpu51_layer1 -> gate_gpu51_layer1;
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> expert_gpu51_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu51_layer1 -> mlp_gpu51_layer1;
    mlp_gpu51_layer1 -> tp_allreduce_ep3_pp0_layer1;
    attn_gpu48_layer2 -> gate_gpu48_layer2;
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> expert_gpu48_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu48_layer2 -> mlp_gpu48_layer2;
    mlp_gpu48_layer2 -> tp_allreduce_ep3_pp0_layer2;
    attn_gpu49_layer2 -> gate_gpu49_layer2;
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> expert_gpu49_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu49_layer2 -> mlp_gpu49_layer2;
    mlp_gpu49_layer2 -> tp_allreduce_ep3_pp0_layer2;
    attn_gpu50_layer2 -> gate_gpu50_layer2;
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> expert_gpu50_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu50_layer2 -> mlp_gpu50_layer2;
    mlp_gpu50_layer2 -> tp_allreduce_ep3_pp0_layer2;
    attn_gpu51_layer2 -> gate_gpu51_layer2;
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> expert_gpu51_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu51_layer2 -> mlp_gpu51_layer2;
    mlp_gpu51_layer2 -> tp_allreduce_ep3_pp0_layer2;
    attn_gpu48_layer3 -> gate_gpu48_layer3;
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> expert_gpu48_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu48_layer3 -> mlp_gpu48_layer3;
    mlp_gpu48_layer3 -> tp_allreduce_ep3_pp0_layer3;
    attn_gpu49_layer3 -> gate_gpu49_layer3;
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> expert_gpu49_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu49_layer3 -> mlp_gpu49_layer3;
    mlp_gpu49_layer3 -> tp_allreduce_ep3_pp0_layer3;
    attn_gpu50_layer3 -> gate_gpu50_layer3;
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> expert_gpu50_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu50_layer3 -> mlp_gpu50_layer3;
    mlp_gpu50_layer3 -> tp_allreduce_ep3_pp0_layer3;
    attn_gpu51_layer3 -> gate_gpu51_layer3;
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> expert_gpu51_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu51_layer3 -> mlp_gpu51_layer3;
    mlp_gpu51_layer3 -> tp_allreduce_ep3_pp0_layer3;
    attn_gpu52_layer4 -> gate_gpu52_layer4;
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> expert_gpu52_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu52_layer4 -> mlp_gpu52_layer4;
    mlp_gpu52_layer4 -> tp_allreduce_ep3_pp1_layer4;
    attn_gpu53_layer4 -> gate_gpu53_layer4;
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> expert_gpu53_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu53_layer4 -> mlp_gpu53_layer4;
    mlp_gpu53_layer4 -> tp_allreduce_ep3_pp1_layer4;
    attn_gpu54_layer4 -> gate_gpu54_layer4;
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> expert_gpu54_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu54_layer4 -> mlp_gpu54_layer4;
    mlp_gpu54_layer4 -> tp_allreduce_ep3_pp1_layer4;
    attn_gpu55_layer4 -> gate_gpu55_layer4;
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> expert_gpu55_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu55_layer4 -> mlp_gpu55_layer4;
    mlp_gpu55_layer4 -> tp_allreduce_ep3_pp1_layer4;
    attn_gpu52_layer5 -> gate_gpu52_layer5;
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> expert_gpu52_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu52_layer5 -> mlp_gpu52_layer5;
    mlp_gpu52_layer5 -> tp_allreduce_ep3_pp1_layer5;
    attn_gpu53_layer5 -> gate_gpu53_layer5;
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> expert_gpu53_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu53_layer5 -> mlp_gpu53_layer5;
    mlp_gpu53_layer5 -> tp_allreduce_ep3_pp1_layer5;
    attn_gpu54_layer5 -> gate_gpu54_layer5;
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> expert_gpu54_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu54_layer5 -> mlp_gpu54_layer5;
    mlp_gpu54_layer5 -> tp_allreduce_ep3_pp1_layer5;
    attn_gpu55_layer5 -> gate_gpu55_layer5;
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> expert_gpu55_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu55_layer5 -> mlp_gpu55_layer5;
    mlp_gpu55_layer5 -> tp_allreduce_ep3_pp1_layer5;
    attn_gpu52_layer6 -> gate_gpu52_layer6;
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> expert_gpu52_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu52_layer6 -> mlp_gpu52_layer6;
    mlp_gpu52_layer6 -> tp_allreduce_ep3_pp1_layer6;
    attn_gpu53_layer6 -> gate_gpu53_layer6;
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> expert_gpu53_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu53_layer6 -> mlp_gpu53_layer6;
    mlp_gpu53_layer6 -> tp_allreduce_ep3_pp1_layer6;
    attn_gpu54_layer6 -> gate_gpu54_layer6;
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> expert_gpu54_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu54_layer6 -> mlp_gpu54_layer6;
    mlp_gpu54_layer6 -> tp_allreduce_ep3_pp1_layer6;
    attn_gpu55_layer6 -> gate_gpu55_layer6;
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> expert_gpu55_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu55_layer6 -> mlp_gpu55_layer6;
    mlp_gpu55_layer6 -> tp_allreduce_ep3_pp1_layer6;
    attn_gpu52_layer7 -> gate_gpu52_layer7;
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> expert_gpu52_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu52_layer7 -> mlp_gpu52_layer7;
    mlp_gpu52_layer7 -> tp_allreduce_ep3_pp1_layer7;
    attn_gpu53_layer7 -> gate_gpu53_layer7;
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> expert_gpu53_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu53_layer7 -> mlp_gpu53_layer7;
    mlp_gpu53_layer7 -> tp_allreduce_ep3_pp1_layer7;
    attn_gpu54_layer7 -> gate_gpu54_layer7;
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> expert_gpu54_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu54_layer7 -> mlp_gpu54_layer7;
    mlp_gpu54_layer7 -> tp_allreduce_ep3_pp1_layer7;
    attn_gpu55_layer7 -> gate_gpu55_layer7;
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> expert_gpu55_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu55_layer7 -> mlp_gpu55_layer7;
    mlp_gpu55_layer7 -> tp_allreduce_ep3_pp1_layer7;
    attn_gpu56_layer8 -> gate_gpu56_layer8;
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> expert_gpu56_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu56_layer8 -> mlp_gpu56_layer8;
    mlp_gpu56_layer8 -> tp_allreduce_ep3_pp2_layer8;
    attn_gpu57_layer8 -> gate_gpu57_layer8;
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> expert_gpu57_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu57_layer8 -> mlp_gpu57_layer8;
    mlp_gpu57_layer8 -> tp_allreduce_ep3_pp2_layer8;
    attn_gpu58_layer8 -> gate_gpu58_layer8;
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> expert_gpu58_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu58_layer8 -> mlp_gpu58_layer8;
    mlp_gpu58_layer8 -> tp_allreduce_ep3_pp2_layer8;
    attn_gpu59_layer8 -> gate_gpu59_layer8;
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> expert_gpu59_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu59_layer8 -> mlp_gpu59_layer8;
    mlp_gpu59_layer8 -> tp_allreduce_ep3_pp2_layer8;
    attn_gpu56_layer9 -> gate_gpu56_layer9;
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> expert_gpu56_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu56_layer9 -> mlp_gpu56_layer9;
    mlp_gpu56_layer9 -> tp_allreduce_ep3_pp2_layer9;
    attn_gpu57_layer9 -> gate_gpu57_layer9;
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> expert_gpu57_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu57_layer9 -> mlp_gpu57_layer9;
    mlp_gpu57_layer9 -> tp_allreduce_ep3_pp2_layer9;
    attn_gpu58_layer9 -> gate_gpu58_layer9;
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> expert_gpu58_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu58_layer9 -> mlp_gpu58_layer9;
    mlp_gpu58_layer9 -> tp_allreduce_ep3_pp2_layer9;
    attn_gpu59_layer9 -> gate_gpu59_layer9;
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> expert_gpu59_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu59_layer9 -> mlp_gpu59_layer9;
    mlp_gpu59_layer9 -> tp_allreduce_ep3_pp2_layer9;
    attn_gpu56_layer10 -> gate_gpu56_layer10;
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> expert_gpu56_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu56_layer10 -> mlp_gpu56_layer10;
    mlp_gpu56_layer10 -> tp_allreduce_ep3_pp2_layer10;
    attn_gpu57_layer10 -> gate_gpu57_layer10;
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> expert_gpu57_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu57_layer10 -> mlp_gpu57_layer10;
    mlp_gpu57_layer10 -> tp_allreduce_ep3_pp2_layer10;
    attn_gpu58_layer10 -> gate_gpu58_layer10;
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> expert_gpu58_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu58_layer10 -> mlp_gpu58_layer10;
    mlp_gpu58_layer10 -> tp_allreduce_ep3_pp2_layer10;
    attn_gpu59_layer10 -> gate_gpu59_layer10;
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> expert_gpu59_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu59_layer10 -> mlp_gpu59_layer10;
    mlp_gpu59_layer10 -> tp_allreduce_ep3_pp2_layer10;
    attn_gpu56_layer11 -> gate_gpu56_layer11;
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> expert_gpu56_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu56_layer11 -> mlp_gpu56_layer11;
    mlp_gpu56_layer11 -> tp_allreduce_ep3_pp2_layer11;
    attn_gpu57_layer11 -> gate_gpu57_layer11;
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> expert_gpu57_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu57_layer11 -> mlp_gpu57_layer11;
    mlp_gpu57_layer11 -> tp_allreduce_ep3_pp2_layer11;
    attn_gpu58_layer11 -> gate_gpu58_layer11;
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> expert_gpu58_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu58_layer11 -> mlp_gpu58_layer11;
    mlp_gpu58_layer11 -> tp_allreduce_ep3_pp2_layer11;
    attn_gpu59_layer11 -> gate_gpu59_layer11;
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> expert_gpu59_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu59_layer11 -> mlp_gpu59_layer11;
    mlp_gpu59_layer11 -> tp_allreduce_ep3_pp2_layer11;
    attn_gpu60_layer12 -> gate_gpu60_layer12;
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> expert_gpu60_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu60_layer12 -> mlp_gpu60_layer12;
    mlp_gpu60_layer12 -> tp_allreduce_ep3_pp3_layer12;
    attn_gpu61_layer12 -> gate_gpu61_layer12;
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> expert_gpu61_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu61_layer12 -> mlp_gpu61_layer12;
    mlp_gpu61_layer12 -> tp_allreduce_ep3_pp3_layer12;
    attn_gpu62_layer12 -> gate_gpu62_layer12;
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> expert_gpu62_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu62_layer12 -> mlp_gpu62_layer12;
    mlp_gpu62_layer12 -> tp_allreduce_ep3_pp3_layer12;
    attn_gpu63_layer12 -> gate_gpu63_layer12;
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> expert_gpu63_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu63_layer12 -> mlp_gpu63_layer12;
    mlp_gpu63_layer12 -> tp_allreduce_ep3_pp3_layer12;
    attn_gpu60_layer13 -> gate_gpu60_layer13;
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> expert_gpu60_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu60_layer13 -> mlp_gpu60_layer13;
    mlp_gpu60_layer13 -> tp_allreduce_ep3_pp3_layer13;
    attn_gpu61_layer13 -> gate_gpu61_layer13;
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> expert_gpu61_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu61_layer13 -> mlp_gpu61_layer13;
    mlp_gpu61_layer13 -> tp_allreduce_ep3_pp3_layer13;
    attn_gpu62_layer13 -> gate_gpu62_layer13;
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> expert_gpu62_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu62_layer13 -> mlp_gpu62_layer13;
    mlp_gpu62_layer13 -> tp_allreduce_ep3_pp3_layer13;
    attn_gpu63_layer13 -> gate_gpu63_layer13;
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> expert_gpu63_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu63_layer13 -> mlp_gpu63_layer13;
    mlp_gpu63_layer13 -> tp_allreduce_ep3_pp3_layer13;
    attn_gpu60_layer14 -> gate_gpu60_layer14;
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> expert_gpu60_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu60_layer14 -> mlp_gpu60_layer14;
    mlp_gpu60_layer14 -> tp_allreduce_ep3_pp3_layer14;
    attn_gpu61_layer14 -> gate_gpu61_layer14;
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> expert_gpu61_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu61_layer14 -> mlp_gpu61_layer14;
    mlp_gpu61_layer14 -> tp_allreduce_ep3_pp3_layer14;
    attn_gpu62_layer14 -> gate_gpu62_layer14;
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> expert_gpu62_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu62_layer14 -> mlp_gpu62_layer14;
    mlp_gpu62_layer14 -> tp_allreduce_ep3_pp3_layer14;
    attn_gpu63_layer14 -> gate_gpu63_layer14;
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> expert_gpu63_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu63_layer14 -> mlp_gpu63_layer14;
    mlp_gpu63_layer14 -> tp_allreduce_ep3_pp3_layer14;
    attn_gpu60_layer15 -> gate_gpu60_layer15;
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> expert_gpu60_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu60_layer15 -> mlp_gpu60_layer15;
    mlp_gpu60_layer15 -> tp_allreduce_ep3_pp3_layer15;
    attn_gpu61_layer15 -> gate_gpu61_layer15;
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> expert_gpu61_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu61_layer15 -> mlp_gpu61_layer15;
    mlp_gpu61_layer15 -> tp_allreduce_ep3_pp3_layer15;
    attn_gpu62_layer15 -> gate_gpu62_layer15;
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> expert_gpu62_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu62_layer15 -> mlp_gpu62_layer15;
    mlp_gpu62_layer15 -> tp_allreduce_ep3_pp3_layer15;
    attn_gpu63_layer15 -> gate_gpu63_layer15;
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> expert_gpu63_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu63_layer15 -> mlp_gpu63_layer15;
    mlp_gpu63_layer15 -> tp_allreduce_ep3_pp3_layer15;
    attn_gpu64_layer0 -> gate_gpu64_layer0;
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> expert_gpu64_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu64_layer0 -> mlp_gpu64_layer0;
    mlp_gpu64_layer0 -> tp_allreduce_ep4_pp0_layer0;
    attn_gpu65_layer0 -> gate_gpu65_layer0;
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> expert_gpu65_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu65_layer0 -> mlp_gpu65_layer0;
    mlp_gpu65_layer0 -> tp_allreduce_ep4_pp0_layer0;
    attn_gpu66_layer0 -> gate_gpu66_layer0;
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> expert_gpu66_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu66_layer0 -> mlp_gpu66_layer0;
    mlp_gpu66_layer0 -> tp_allreduce_ep4_pp0_layer0;
    attn_gpu67_layer0 -> gate_gpu67_layer0;
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> expert_gpu67_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu67_layer0 -> mlp_gpu67_layer0;
    mlp_gpu67_layer0 -> tp_allreduce_ep4_pp0_layer0;
    attn_gpu64_layer1 -> gate_gpu64_layer1;
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> expert_gpu64_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu64_layer1 -> mlp_gpu64_layer1;
    mlp_gpu64_layer1 -> tp_allreduce_ep4_pp0_layer1;
    attn_gpu65_layer1 -> gate_gpu65_layer1;
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> expert_gpu65_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu65_layer1 -> mlp_gpu65_layer1;
    mlp_gpu65_layer1 -> tp_allreduce_ep4_pp0_layer1;
    attn_gpu66_layer1 -> gate_gpu66_layer1;
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> expert_gpu66_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu66_layer1 -> mlp_gpu66_layer1;
    mlp_gpu66_layer1 -> tp_allreduce_ep4_pp0_layer1;
    attn_gpu67_layer1 -> gate_gpu67_layer1;
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> expert_gpu67_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu67_layer1 -> mlp_gpu67_layer1;
    mlp_gpu67_layer1 -> tp_allreduce_ep4_pp0_layer1;
    attn_gpu64_layer2 -> gate_gpu64_layer2;
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> expert_gpu64_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu64_layer2 -> mlp_gpu64_layer2;
    mlp_gpu64_layer2 -> tp_allreduce_ep4_pp0_layer2;
    attn_gpu65_layer2 -> gate_gpu65_layer2;
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> expert_gpu65_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu65_layer2 -> mlp_gpu65_layer2;
    mlp_gpu65_layer2 -> tp_allreduce_ep4_pp0_layer2;
    attn_gpu66_layer2 -> gate_gpu66_layer2;
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> expert_gpu66_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu66_layer2 -> mlp_gpu66_layer2;
    mlp_gpu66_layer2 -> tp_allreduce_ep4_pp0_layer2;
    attn_gpu67_layer2 -> gate_gpu67_layer2;
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> expert_gpu67_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu67_layer2 -> mlp_gpu67_layer2;
    mlp_gpu67_layer2 -> tp_allreduce_ep4_pp0_layer2;
    attn_gpu64_layer3 -> gate_gpu64_layer3;
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> expert_gpu64_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu64_layer3 -> mlp_gpu64_layer3;
    mlp_gpu64_layer3 -> tp_allreduce_ep4_pp0_layer3;
    attn_gpu65_layer3 -> gate_gpu65_layer3;
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> expert_gpu65_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu65_layer3 -> mlp_gpu65_layer3;
    mlp_gpu65_layer3 -> tp_allreduce_ep4_pp0_layer3;
    attn_gpu66_layer3 -> gate_gpu66_layer3;
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> expert_gpu66_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu66_layer3 -> mlp_gpu66_layer3;
    mlp_gpu66_layer3 -> tp_allreduce_ep4_pp0_layer3;
    attn_gpu67_layer3 -> gate_gpu67_layer3;
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> expert_gpu67_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu67_layer3 -> mlp_gpu67_layer3;
    mlp_gpu67_layer3 -> tp_allreduce_ep4_pp0_layer3;
    attn_gpu68_layer4 -> gate_gpu68_layer4;
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> expert_gpu68_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu68_layer4 -> mlp_gpu68_layer4;
    mlp_gpu68_layer4 -> tp_allreduce_ep4_pp1_layer4;
    attn_gpu69_layer4 -> gate_gpu69_layer4;
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> expert_gpu69_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu69_layer4 -> mlp_gpu69_layer4;
    mlp_gpu69_layer4 -> tp_allreduce_ep4_pp1_layer4;
    attn_gpu70_layer4 -> gate_gpu70_layer4;
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> expert_gpu70_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu70_layer4 -> mlp_gpu70_layer4;
    mlp_gpu70_layer4 -> tp_allreduce_ep4_pp1_layer4;
    attn_gpu71_layer4 -> gate_gpu71_layer4;
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> expert_gpu71_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu71_layer4 -> mlp_gpu71_layer4;
    mlp_gpu71_layer4 -> tp_allreduce_ep4_pp1_layer4;
    attn_gpu68_layer5 -> gate_gpu68_layer5;
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> expert_gpu68_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu68_layer5 -> mlp_gpu68_layer5;
    mlp_gpu68_layer5 -> tp_allreduce_ep4_pp1_layer5;
    attn_gpu69_layer5 -> gate_gpu69_layer5;
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> expert_gpu69_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu69_layer5 -> mlp_gpu69_layer5;
    mlp_gpu69_layer5 -> tp_allreduce_ep4_pp1_layer5;
    attn_gpu70_layer5 -> gate_gpu70_layer5;
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> expert_gpu70_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu70_layer5 -> mlp_gpu70_layer5;
    mlp_gpu70_layer5 -> tp_allreduce_ep4_pp1_layer5;
    attn_gpu71_layer5 -> gate_gpu71_layer5;
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> expert_gpu71_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu71_layer5 -> mlp_gpu71_layer5;
    mlp_gpu71_layer5 -> tp_allreduce_ep4_pp1_layer5;
    attn_gpu68_layer6 -> gate_gpu68_layer6;
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> expert_gpu68_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu68_layer6 -> mlp_gpu68_layer6;
    mlp_gpu68_layer6 -> tp_allreduce_ep4_pp1_layer6;
    attn_gpu69_layer6 -> gate_gpu69_layer6;
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> expert_gpu69_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu69_layer6 -> mlp_gpu69_layer6;
    mlp_gpu69_layer6 -> tp_allreduce_ep4_pp1_layer6;
    attn_gpu70_layer6 -> gate_gpu70_layer6;
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> expert_gpu70_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu70_layer6 -> mlp_gpu70_layer6;
    mlp_gpu70_layer6 -> tp_allreduce_ep4_pp1_layer6;
    attn_gpu71_layer6 -> gate_gpu71_layer6;
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> expert_gpu71_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu71_layer6 -> mlp_gpu71_layer6;
    mlp_gpu71_layer6 -> tp_allreduce_ep4_pp1_layer6;
    attn_gpu68_layer7 -> gate_gpu68_layer7;
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> expert_gpu68_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu68_layer7 -> mlp_gpu68_layer7;
    mlp_gpu68_layer7 -> tp_allreduce_ep4_pp1_layer7;
    attn_gpu69_layer7 -> gate_gpu69_layer7;
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> expert_gpu69_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu69_layer7 -> mlp_gpu69_layer7;
    mlp_gpu69_layer7 -> tp_allreduce_ep4_pp1_layer7;
    attn_gpu70_layer7 -> gate_gpu70_layer7;
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> expert_gpu70_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu70_layer7 -> mlp_gpu70_layer7;
    mlp_gpu70_layer7 -> tp_allreduce_ep4_pp1_layer7;
    attn_gpu71_layer7 -> gate_gpu71_layer7;
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> expert_gpu71_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu71_layer7 -> mlp_gpu71_layer7;
    mlp_gpu71_layer7 -> tp_allreduce_ep4_pp1_layer7;
    attn_gpu72_layer8 -> gate_gpu72_layer8;
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> expert_gpu72_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu72_layer8 -> mlp_gpu72_layer8;
    mlp_gpu72_layer8 -> tp_allreduce_ep4_pp2_layer8;
    attn_gpu73_layer8 -> gate_gpu73_layer8;
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> expert_gpu73_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu73_layer8 -> mlp_gpu73_layer8;
    mlp_gpu73_layer8 -> tp_allreduce_ep4_pp2_layer8;
    attn_gpu74_layer8 -> gate_gpu74_layer8;
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> expert_gpu74_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu74_layer8 -> mlp_gpu74_layer8;
    mlp_gpu74_layer8 -> tp_allreduce_ep4_pp2_layer8;
    attn_gpu75_layer8 -> gate_gpu75_layer8;
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> expert_gpu75_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu75_layer8 -> mlp_gpu75_layer8;
    mlp_gpu75_layer8 -> tp_allreduce_ep4_pp2_layer8;
    attn_gpu72_layer9 -> gate_gpu72_layer9;
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> expert_gpu72_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu72_layer9 -> mlp_gpu72_layer9;
    mlp_gpu72_layer9 -> tp_allreduce_ep4_pp2_layer9;
    attn_gpu73_layer9 -> gate_gpu73_layer9;
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> expert_gpu73_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu73_layer9 -> mlp_gpu73_layer9;
    mlp_gpu73_layer9 -> tp_allreduce_ep4_pp2_layer9;
    attn_gpu74_layer9 -> gate_gpu74_layer9;
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> expert_gpu74_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu74_layer9 -> mlp_gpu74_layer9;
    mlp_gpu74_layer9 -> tp_allreduce_ep4_pp2_layer9;
    attn_gpu75_layer9 -> gate_gpu75_layer9;
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> expert_gpu75_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu75_layer9 -> mlp_gpu75_layer9;
    mlp_gpu75_layer9 -> tp_allreduce_ep4_pp2_layer9;
    attn_gpu72_layer10 -> gate_gpu72_layer10;
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> expert_gpu72_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu72_layer10 -> mlp_gpu72_layer10;
    mlp_gpu72_layer10 -> tp_allreduce_ep4_pp2_layer10;
    attn_gpu73_layer10 -> gate_gpu73_layer10;
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> expert_gpu73_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu73_layer10 -> mlp_gpu73_layer10;
    mlp_gpu73_layer10 -> tp_allreduce_ep4_pp2_layer10;
    attn_gpu74_layer10 -> gate_gpu74_layer10;
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> expert_gpu74_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu74_layer10 -> mlp_gpu74_layer10;
    mlp_gpu74_layer10 -> tp_allreduce_ep4_pp2_layer10;
    attn_gpu75_layer10 -> gate_gpu75_layer10;
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> expert_gpu75_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu75_layer10 -> mlp_gpu75_layer10;
    mlp_gpu75_layer10 -> tp_allreduce_ep4_pp2_layer10;
    attn_gpu72_layer11 -> gate_gpu72_layer11;
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> expert_gpu72_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu72_layer11 -> mlp_gpu72_layer11;
    mlp_gpu72_layer11 -> tp_allreduce_ep4_pp2_layer11;
    attn_gpu73_layer11 -> gate_gpu73_layer11;
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> expert_gpu73_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu73_layer11 -> mlp_gpu73_layer11;
    mlp_gpu73_layer11 -> tp_allreduce_ep4_pp2_layer11;
    attn_gpu74_layer11 -> gate_gpu74_layer11;
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> expert_gpu74_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu74_layer11 -> mlp_gpu74_layer11;
    mlp_gpu74_layer11 -> tp_allreduce_ep4_pp2_layer11;
    attn_gpu75_layer11 -> gate_gpu75_layer11;
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> expert_gpu75_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu75_layer11 -> mlp_gpu75_layer11;
    mlp_gpu75_layer11 -> tp_allreduce_ep4_pp2_layer11;
    attn_gpu76_layer12 -> gate_gpu76_layer12;
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> expert_gpu76_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu76_layer12 -> mlp_gpu76_layer12;
    mlp_gpu76_layer12 -> tp_allreduce_ep4_pp3_layer12;
    attn_gpu77_layer12 -> gate_gpu77_layer12;
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> expert_gpu77_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu77_layer12 -> mlp_gpu77_layer12;
    mlp_gpu77_layer12 -> tp_allreduce_ep4_pp3_layer12;
    attn_gpu78_layer12 -> gate_gpu78_layer12;
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> expert_gpu78_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu78_layer12 -> mlp_gpu78_layer12;
    mlp_gpu78_layer12 -> tp_allreduce_ep4_pp3_layer12;
    attn_gpu79_layer12 -> gate_gpu79_layer12;
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> expert_gpu79_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu79_layer12 -> mlp_gpu79_layer12;
    mlp_gpu79_layer12 -> tp_allreduce_ep4_pp3_layer12;
    attn_gpu76_layer13 -> gate_gpu76_layer13;
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> expert_gpu76_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu76_layer13 -> mlp_gpu76_layer13;
    mlp_gpu76_layer13 -> tp_allreduce_ep4_pp3_layer13;
    attn_gpu77_layer13 -> gate_gpu77_layer13;
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> expert_gpu77_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu77_layer13 -> mlp_gpu77_layer13;
    mlp_gpu77_layer13 -> tp_allreduce_ep4_pp3_layer13;
    attn_gpu78_layer13 -> gate_gpu78_layer13;
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> expert_gpu78_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu78_layer13 -> mlp_gpu78_layer13;
    mlp_gpu78_layer13 -> tp_allreduce_ep4_pp3_layer13;
    attn_gpu79_layer13 -> gate_gpu79_layer13;
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> expert_gpu79_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu79_layer13 -> mlp_gpu79_layer13;
    mlp_gpu79_layer13 -> tp_allreduce_ep4_pp3_layer13;
    attn_gpu76_layer14 -> gate_gpu76_layer14;
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> expert_gpu76_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu76_layer14 -> mlp_gpu76_layer14;
    mlp_gpu76_layer14 -> tp_allreduce_ep4_pp3_layer14;
    attn_gpu77_layer14 -> gate_gpu77_layer14;
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> expert_gpu77_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu77_layer14 -> mlp_gpu77_layer14;
    mlp_gpu77_layer14 -> tp_allreduce_ep4_pp3_layer14;
    attn_gpu78_layer14 -> gate_gpu78_layer14;
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> expert_gpu78_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu78_layer14 -> mlp_gpu78_layer14;
    mlp_gpu78_layer14 -> tp_allreduce_ep4_pp3_layer14;
    attn_gpu79_layer14 -> gate_gpu79_layer14;
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> expert_gpu79_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu79_layer14 -> mlp_gpu79_layer14;
    mlp_gpu79_layer14 -> tp_allreduce_ep4_pp3_layer14;
    attn_gpu76_layer15 -> gate_gpu76_layer15;
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> expert_gpu76_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu76_layer15 -> mlp_gpu76_layer15;
    mlp_gpu76_layer15 -> tp_allreduce_ep4_pp3_layer15;
    attn_gpu77_layer15 -> gate_gpu77_layer15;
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> expert_gpu77_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu77_layer15 -> mlp_gpu77_layer15;
    mlp_gpu77_layer15 -> tp_allreduce_ep4_pp3_layer15;
    attn_gpu78_layer15 -> gate_gpu78_layer15;
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> expert_gpu78_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu78_layer15 -> mlp_gpu78_layer15;
    mlp_gpu78_layer15 -> tp_allreduce_ep4_pp3_layer15;
    attn_gpu79_layer15 -> gate_gpu79_layer15;
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> expert_gpu79_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu79_layer15 -> mlp_gpu79_layer15;
    mlp_gpu79_layer15 -> tp_allreduce_ep4_pp3_layer15;
    attn_gpu80_layer0 -> gate_gpu80_layer0;
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> expert_gpu80_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu80_layer0 -> mlp_gpu80_layer0;
    mlp_gpu80_layer0 -> tp_allreduce_ep5_pp0_layer0;
    attn_gpu81_layer0 -> gate_gpu81_layer0;
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> expert_gpu81_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu81_layer0 -> mlp_gpu81_layer0;
    mlp_gpu81_layer0 -> tp_allreduce_ep5_pp0_layer0;
    attn_gpu82_layer0 -> gate_gpu82_layer0;
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> expert_gpu82_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu82_layer0 -> mlp_gpu82_layer0;
    mlp_gpu82_layer0 -> tp_allreduce_ep5_pp0_layer0;
    attn_gpu83_layer0 -> gate_gpu83_layer0;
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> expert_gpu83_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu83_layer0 -> mlp_gpu83_layer0;
    mlp_gpu83_layer0 -> tp_allreduce_ep5_pp0_layer0;
    attn_gpu80_layer1 -> gate_gpu80_layer1;
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> expert_gpu80_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu80_layer1 -> mlp_gpu80_layer1;
    mlp_gpu80_layer1 -> tp_allreduce_ep5_pp0_layer1;
    attn_gpu81_layer1 -> gate_gpu81_layer1;
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> expert_gpu81_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu81_layer1 -> mlp_gpu81_layer1;
    mlp_gpu81_layer1 -> tp_allreduce_ep5_pp0_layer1;
    attn_gpu82_layer1 -> gate_gpu82_layer1;
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> expert_gpu82_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu82_layer1 -> mlp_gpu82_layer1;
    mlp_gpu82_layer1 -> tp_allreduce_ep5_pp0_layer1;
    attn_gpu83_layer1 -> gate_gpu83_layer1;
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> expert_gpu83_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu83_layer1 -> mlp_gpu83_layer1;
    mlp_gpu83_layer1 -> tp_allreduce_ep5_pp0_layer1;
    attn_gpu80_layer2 -> gate_gpu80_layer2;
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> expert_gpu80_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu80_layer2 -> mlp_gpu80_layer2;
    mlp_gpu80_layer2 -> tp_allreduce_ep5_pp0_layer2;
    attn_gpu81_layer2 -> gate_gpu81_layer2;
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> expert_gpu81_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu81_layer2 -> mlp_gpu81_layer2;
    mlp_gpu81_layer2 -> tp_allreduce_ep5_pp0_layer2;
    attn_gpu82_layer2 -> gate_gpu82_layer2;
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> expert_gpu82_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu82_layer2 -> mlp_gpu82_layer2;
    mlp_gpu82_layer2 -> tp_allreduce_ep5_pp0_layer2;
    attn_gpu83_layer2 -> gate_gpu83_layer2;
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> expert_gpu83_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu83_layer2 -> mlp_gpu83_layer2;
    mlp_gpu83_layer2 -> tp_allreduce_ep5_pp0_layer2;
    attn_gpu80_layer3 -> gate_gpu80_layer3;
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> expert_gpu80_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu80_layer3 -> mlp_gpu80_layer3;
    mlp_gpu80_layer3 -> tp_allreduce_ep5_pp0_layer3;
    attn_gpu81_layer3 -> gate_gpu81_layer3;
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> expert_gpu81_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu81_layer3 -> mlp_gpu81_layer3;
    mlp_gpu81_layer3 -> tp_allreduce_ep5_pp0_layer3;
    attn_gpu82_layer3 -> gate_gpu82_layer3;
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> expert_gpu82_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu82_layer3 -> mlp_gpu82_layer3;
    mlp_gpu82_layer3 -> tp_allreduce_ep5_pp0_layer3;
    attn_gpu83_layer3 -> gate_gpu83_layer3;
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> expert_gpu83_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu83_layer3 -> mlp_gpu83_layer3;
    mlp_gpu83_layer3 -> tp_allreduce_ep5_pp0_layer3;
    attn_gpu84_layer4 -> gate_gpu84_layer4;
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> expert_gpu84_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu84_layer4 -> mlp_gpu84_layer4;
    mlp_gpu84_layer4 -> tp_allreduce_ep5_pp1_layer4;
    attn_gpu85_layer4 -> gate_gpu85_layer4;
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> expert_gpu85_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu85_layer4 -> mlp_gpu85_layer4;
    mlp_gpu85_layer4 -> tp_allreduce_ep5_pp1_layer4;
    attn_gpu86_layer4 -> gate_gpu86_layer4;
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> expert_gpu86_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu86_layer4 -> mlp_gpu86_layer4;
    mlp_gpu86_layer4 -> tp_allreduce_ep5_pp1_layer4;
    attn_gpu87_layer4 -> gate_gpu87_layer4;
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> expert_gpu87_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu87_layer4 -> mlp_gpu87_layer4;
    mlp_gpu87_layer4 -> tp_allreduce_ep5_pp1_layer4;
    attn_gpu84_layer5 -> gate_gpu84_layer5;
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> expert_gpu84_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu84_layer5 -> mlp_gpu84_layer5;
    mlp_gpu84_layer5 -> tp_allreduce_ep5_pp1_layer5;
    attn_gpu85_layer5 -> gate_gpu85_layer5;
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> expert_gpu85_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu85_layer5 -> mlp_gpu85_layer5;
    mlp_gpu85_layer5 -> tp_allreduce_ep5_pp1_layer5;
    attn_gpu86_layer5 -> gate_gpu86_layer5;
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> expert_gpu86_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu86_layer5 -> mlp_gpu86_layer5;
    mlp_gpu86_layer5 -> tp_allreduce_ep5_pp1_layer5;
    attn_gpu87_layer5 -> gate_gpu87_layer5;
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> expert_gpu87_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu87_layer5 -> mlp_gpu87_layer5;
    mlp_gpu87_layer5 -> tp_allreduce_ep5_pp1_layer5;
    attn_gpu84_layer6 -> gate_gpu84_layer6;
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> expert_gpu84_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu84_layer6 -> mlp_gpu84_layer6;
    mlp_gpu84_layer6 -> tp_allreduce_ep5_pp1_layer6;
    attn_gpu85_layer6 -> gate_gpu85_layer6;
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> expert_gpu85_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu85_layer6 -> mlp_gpu85_layer6;
    mlp_gpu85_layer6 -> tp_allreduce_ep5_pp1_layer6;
    attn_gpu86_layer6 -> gate_gpu86_layer6;
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> expert_gpu86_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu86_layer6 -> mlp_gpu86_layer6;
    mlp_gpu86_layer6 -> tp_allreduce_ep5_pp1_layer6;
    attn_gpu87_layer6 -> gate_gpu87_layer6;
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> expert_gpu87_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu87_layer6 -> mlp_gpu87_layer6;
    mlp_gpu87_layer6 -> tp_allreduce_ep5_pp1_layer6;
    attn_gpu84_layer7 -> gate_gpu84_layer7;
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> expert_gpu84_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu84_layer7 -> mlp_gpu84_layer7;
    mlp_gpu84_layer7 -> tp_allreduce_ep5_pp1_layer7;
    attn_gpu85_layer7 -> gate_gpu85_layer7;
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> expert_gpu85_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu85_layer7 -> mlp_gpu85_layer7;
    mlp_gpu85_layer7 -> tp_allreduce_ep5_pp1_layer7;
    attn_gpu86_layer7 -> gate_gpu86_layer7;
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> expert_gpu86_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu86_layer7 -> mlp_gpu86_layer7;
    mlp_gpu86_layer7 -> tp_allreduce_ep5_pp1_layer7;
    attn_gpu87_layer7 -> gate_gpu87_layer7;
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> expert_gpu87_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu87_layer7 -> mlp_gpu87_layer7;
    mlp_gpu87_layer7 -> tp_allreduce_ep5_pp1_layer7;
    attn_gpu88_layer8 -> gate_gpu88_layer8;
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> expert_gpu88_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu88_layer8 -> mlp_gpu88_layer8;
    mlp_gpu88_layer8 -> tp_allreduce_ep5_pp2_layer8;
    attn_gpu89_layer8 -> gate_gpu89_layer8;
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> expert_gpu89_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu89_layer8 -> mlp_gpu89_layer8;
    mlp_gpu89_layer8 -> tp_allreduce_ep5_pp2_layer8;
    attn_gpu90_layer8 -> gate_gpu90_layer8;
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> expert_gpu90_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu90_layer8 -> mlp_gpu90_layer8;
    mlp_gpu90_layer8 -> tp_allreduce_ep5_pp2_layer8;
    attn_gpu91_layer8 -> gate_gpu91_layer8;
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> expert_gpu91_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu91_layer8 -> mlp_gpu91_layer8;
    mlp_gpu91_layer8 -> tp_allreduce_ep5_pp2_layer8;
    attn_gpu88_layer9 -> gate_gpu88_layer9;
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> expert_gpu88_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu88_layer9 -> mlp_gpu88_layer9;
    mlp_gpu88_layer9 -> tp_allreduce_ep5_pp2_layer9;
    attn_gpu89_layer9 -> gate_gpu89_layer9;
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> expert_gpu89_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu89_layer9 -> mlp_gpu89_layer9;
    mlp_gpu89_layer9 -> tp_allreduce_ep5_pp2_layer9;
    attn_gpu90_layer9 -> gate_gpu90_layer9;
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> expert_gpu90_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu90_layer9 -> mlp_gpu90_layer9;
    mlp_gpu90_layer9 -> tp_allreduce_ep5_pp2_layer9;
    attn_gpu91_layer9 -> gate_gpu91_layer9;
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> expert_gpu91_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu91_layer9 -> mlp_gpu91_layer9;
    mlp_gpu91_layer9 -> tp_allreduce_ep5_pp2_layer9;
    attn_gpu88_layer10 -> gate_gpu88_layer10;
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> expert_gpu88_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu88_layer10 -> mlp_gpu88_layer10;
    mlp_gpu88_layer10 -> tp_allreduce_ep5_pp2_layer10;
    attn_gpu89_layer10 -> gate_gpu89_layer10;
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> expert_gpu89_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu89_layer10 -> mlp_gpu89_layer10;
    mlp_gpu89_layer10 -> tp_allreduce_ep5_pp2_layer10;
    attn_gpu90_layer10 -> gate_gpu90_layer10;
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> expert_gpu90_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu90_layer10 -> mlp_gpu90_layer10;
    mlp_gpu90_layer10 -> tp_allreduce_ep5_pp2_layer10;
    attn_gpu91_layer10 -> gate_gpu91_layer10;
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> expert_gpu91_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu91_layer10 -> mlp_gpu91_layer10;
    mlp_gpu91_layer10 -> tp_allreduce_ep5_pp2_layer10;
    attn_gpu88_layer11 -> gate_gpu88_layer11;
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> expert_gpu88_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu88_layer11 -> mlp_gpu88_layer11;
    mlp_gpu88_layer11 -> tp_allreduce_ep5_pp2_layer11;
    attn_gpu89_layer11 -> gate_gpu89_layer11;
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> expert_gpu89_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu89_layer11 -> mlp_gpu89_layer11;
    mlp_gpu89_layer11 -> tp_allreduce_ep5_pp2_layer11;
    attn_gpu90_layer11 -> gate_gpu90_layer11;
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> expert_gpu90_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu90_layer11 -> mlp_gpu90_layer11;
    mlp_gpu90_layer11 -> tp_allreduce_ep5_pp2_layer11;
    attn_gpu91_layer11 -> gate_gpu91_layer11;
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> expert_gpu91_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu91_layer11 -> mlp_gpu91_layer11;
    mlp_gpu91_layer11 -> tp_allreduce_ep5_pp2_layer11;
    attn_gpu92_layer12 -> gate_gpu92_layer12;
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> expert_gpu92_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu92_layer12 -> mlp_gpu92_layer12;
    mlp_gpu92_layer12 -> tp_allreduce_ep5_pp3_layer12;
    attn_gpu93_layer12 -> gate_gpu93_layer12;
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> expert_gpu93_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu93_layer12 -> mlp_gpu93_layer12;
    mlp_gpu93_layer12 -> tp_allreduce_ep5_pp3_layer12;
    attn_gpu94_layer12 -> gate_gpu94_layer12;
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> expert_gpu94_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu94_layer12 -> mlp_gpu94_layer12;
    mlp_gpu94_layer12 -> tp_allreduce_ep5_pp3_layer12;
    attn_gpu95_layer12 -> gate_gpu95_layer12;
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> expert_gpu95_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu95_layer12 -> mlp_gpu95_layer12;
    mlp_gpu95_layer12 -> tp_allreduce_ep5_pp3_layer12;
    attn_gpu92_layer13 -> gate_gpu92_layer13;
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> expert_gpu92_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu92_layer13 -> mlp_gpu92_layer13;
    mlp_gpu92_layer13 -> tp_allreduce_ep5_pp3_layer13;
    attn_gpu93_layer13 -> gate_gpu93_layer13;
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> expert_gpu93_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu93_layer13 -> mlp_gpu93_layer13;
    mlp_gpu93_layer13 -> tp_allreduce_ep5_pp3_layer13;
    attn_gpu94_layer13 -> gate_gpu94_layer13;
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> expert_gpu94_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu94_layer13 -> mlp_gpu94_layer13;
    mlp_gpu94_layer13 -> tp_allreduce_ep5_pp3_layer13;
    attn_gpu95_layer13 -> gate_gpu95_layer13;
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> expert_gpu95_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu95_layer13 -> mlp_gpu95_layer13;
    mlp_gpu95_layer13 -> tp_allreduce_ep5_pp3_layer13;
    attn_gpu92_layer14 -> gate_gpu92_layer14;
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> expert_gpu92_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu92_layer14 -> mlp_gpu92_layer14;
    mlp_gpu92_layer14 -> tp_allreduce_ep5_pp3_layer14;
    attn_gpu93_layer14 -> gate_gpu93_layer14;
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> expert_gpu93_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu93_layer14 -> mlp_gpu93_layer14;
    mlp_gpu93_layer14 -> tp_allreduce_ep5_pp3_layer14;
    attn_gpu94_layer14 -> gate_gpu94_layer14;
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> expert_gpu94_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu94_layer14 -> mlp_gpu94_layer14;
    mlp_gpu94_layer14 -> tp_allreduce_ep5_pp3_layer14;
    attn_gpu95_layer14 -> gate_gpu95_layer14;
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> expert_gpu95_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu95_layer14 -> mlp_gpu95_layer14;
    mlp_gpu95_layer14 -> tp_allreduce_ep5_pp3_layer14;
    attn_gpu92_layer15 -> gate_gpu92_layer15;
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> expert_gpu92_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu92_layer15 -> mlp_gpu92_layer15;
    mlp_gpu92_layer15 -> tp_allreduce_ep5_pp3_layer15;
    attn_gpu93_layer15 -> gate_gpu93_layer15;
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> expert_gpu93_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu93_layer15 -> mlp_gpu93_layer15;
    mlp_gpu93_layer15 -> tp_allreduce_ep5_pp3_layer15;
    attn_gpu94_layer15 -> gate_gpu94_layer15;
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> expert_gpu94_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu94_layer15 -> mlp_gpu94_layer15;
    mlp_gpu94_layer15 -> tp_allreduce_ep5_pp3_layer15;
    attn_gpu95_layer15 -> gate_gpu95_layer15;
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> expert_gpu95_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu95_layer15 -> mlp_gpu95_layer15;
    mlp_gpu95_layer15 -> tp_allreduce_ep5_pp3_layer15;
    attn_gpu96_layer0 -> gate_gpu96_layer0;
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> expert_gpu96_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu96_layer0 -> mlp_gpu96_layer0;
    mlp_gpu96_layer0 -> tp_allreduce_ep6_pp0_layer0;
    attn_gpu97_layer0 -> gate_gpu97_layer0;
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> expert_gpu97_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu97_layer0 -> mlp_gpu97_layer0;
    mlp_gpu97_layer0 -> tp_allreduce_ep6_pp0_layer0;
    attn_gpu98_layer0 -> gate_gpu98_layer0;
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> expert_gpu98_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu98_layer0 -> mlp_gpu98_layer0;
    mlp_gpu98_layer0 -> tp_allreduce_ep6_pp0_layer0;
    attn_gpu99_layer0 -> gate_gpu99_layer0;
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> expert_gpu99_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu99_layer0 -> mlp_gpu99_layer0;
    mlp_gpu99_layer0 -> tp_allreduce_ep6_pp0_layer0;
    attn_gpu96_layer1 -> gate_gpu96_layer1;
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> expert_gpu96_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu96_layer1 -> mlp_gpu96_layer1;
    mlp_gpu96_layer1 -> tp_allreduce_ep6_pp0_layer1;
    attn_gpu97_layer1 -> gate_gpu97_layer1;
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> expert_gpu97_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu97_layer1 -> mlp_gpu97_layer1;
    mlp_gpu97_layer1 -> tp_allreduce_ep6_pp0_layer1;
    attn_gpu98_layer1 -> gate_gpu98_layer1;
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> expert_gpu98_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu98_layer1 -> mlp_gpu98_layer1;
    mlp_gpu98_layer1 -> tp_allreduce_ep6_pp0_layer1;
    attn_gpu99_layer1 -> gate_gpu99_layer1;
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> expert_gpu99_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu99_layer1 -> mlp_gpu99_layer1;
    mlp_gpu99_layer1 -> tp_allreduce_ep6_pp0_layer1;
    attn_gpu96_layer2 -> gate_gpu96_layer2;
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> expert_gpu96_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu96_layer2 -> mlp_gpu96_layer2;
    mlp_gpu96_layer2 -> tp_allreduce_ep6_pp0_layer2;
    attn_gpu97_layer2 -> gate_gpu97_layer2;
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> expert_gpu97_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu97_layer2 -> mlp_gpu97_layer2;
    mlp_gpu97_layer2 -> tp_allreduce_ep6_pp0_layer2;
    attn_gpu98_layer2 -> gate_gpu98_layer2;
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> expert_gpu98_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu98_layer2 -> mlp_gpu98_layer2;
    mlp_gpu98_layer2 -> tp_allreduce_ep6_pp0_layer2;
    attn_gpu99_layer2 -> gate_gpu99_layer2;
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> expert_gpu99_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu99_layer2 -> mlp_gpu99_layer2;
    mlp_gpu99_layer2 -> tp_allreduce_ep6_pp0_layer2;
    attn_gpu96_layer3 -> gate_gpu96_layer3;
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> expert_gpu96_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu96_layer3 -> mlp_gpu96_layer3;
    mlp_gpu96_layer3 -> tp_allreduce_ep6_pp0_layer3;
    attn_gpu97_layer3 -> gate_gpu97_layer3;
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> expert_gpu97_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu97_layer3 -> mlp_gpu97_layer3;
    mlp_gpu97_layer3 -> tp_allreduce_ep6_pp0_layer3;
    attn_gpu98_layer3 -> gate_gpu98_layer3;
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> expert_gpu98_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu98_layer3 -> mlp_gpu98_layer3;
    mlp_gpu98_layer3 -> tp_allreduce_ep6_pp0_layer3;
    attn_gpu99_layer3 -> gate_gpu99_layer3;
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> expert_gpu99_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu99_layer3 -> mlp_gpu99_layer3;
    mlp_gpu99_layer3 -> tp_allreduce_ep6_pp0_layer3;
    attn_gpu100_layer4 -> gate_gpu100_layer4;
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> expert_gpu100_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu100_layer4 -> mlp_gpu100_layer4;
    mlp_gpu100_layer4 -> tp_allreduce_ep6_pp1_layer4;
    attn_gpu101_layer4 -> gate_gpu101_layer4;
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> expert_gpu101_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu101_layer4 -> mlp_gpu101_layer4;
    mlp_gpu101_layer4 -> tp_allreduce_ep6_pp1_layer4;
    attn_gpu102_layer4 -> gate_gpu102_layer4;
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> expert_gpu102_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu102_layer4 -> mlp_gpu102_layer4;
    mlp_gpu102_layer4 -> tp_allreduce_ep6_pp1_layer4;
    attn_gpu103_layer4 -> gate_gpu103_layer4;
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> expert_gpu103_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu103_layer4 -> mlp_gpu103_layer4;
    mlp_gpu103_layer4 -> tp_allreduce_ep6_pp1_layer4;
    attn_gpu100_layer5 -> gate_gpu100_layer5;
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> expert_gpu100_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu100_layer5 -> mlp_gpu100_layer5;
    mlp_gpu100_layer5 -> tp_allreduce_ep6_pp1_layer5;
    attn_gpu101_layer5 -> gate_gpu101_layer5;
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> expert_gpu101_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu101_layer5 -> mlp_gpu101_layer5;
    mlp_gpu101_layer5 -> tp_allreduce_ep6_pp1_layer5;
    attn_gpu102_layer5 -> gate_gpu102_layer5;
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> expert_gpu102_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu102_layer5 -> mlp_gpu102_layer5;
    mlp_gpu102_layer5 -> tp_allreduce_ep6_pp1_layer5;
    attn_gpu103_layer5 -> gate_gpu103_layer5;
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> expert_gpu103_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu103_layer5 -> mlp_gpu103_layer5;
    mlp_gpu103_layer5 -> tp_allreduce_ep6_pp1_layer5;
    attn_gpu100_layer6 -> gate_gpu100_layer6;
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> expert_gpu100_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu100_layer6 -> mlp_gpu100_layer6;
    mlp_gpu100_layer6 -> tp_allreduce_ep6_pp1_layer6;
    attn_gpu101_layer6 -> gate_gpu101_layer6;
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> expert_gpu101_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu101_layer6 -> mlp_gpu101_layer6;
    mlp_gpu101_layer6 -> tp_allreduce_ep6_pp1_layer6;
    attn_gpu102_layer6 -> gate_gpu102_layer6;
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> expert_gpu102_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu102_layer6 -> mlp_gpu102_layer6;
    mlp_gpu102_layer6 -> tp_allreduce_ep6_pp1_layer6;
    attn_gpu103_layer6 -> gate_gpu103_layer6;
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> expert_gpu103_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu103_layer6 -> mlp_gpu103_layer6;
    mlp_gpu103_layer6 -> tp_allreduce_ep6_pp1_layer6;
    attn_gpu100_layer7 -> gate_gpu100_layer7;
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> expert_gpu100_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu100_layer7 -> mlp_gpu100_layer7;
    mlp_gpu100_layer7 -> tp_allreduce_ep6_pp1_layer7;
    attn_gpu101_layer7 -> gate_gpu101_layer7;
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> expert_gpu101_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu101_layer7 -> mlp_gpu101_layer7;
    mlp_gpu101_layer7 -> tp_allreduce_ep6_pp1_layer7;
    attn_gpu102_layer7 -> gate_gpu102_layer7;
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> expert_gpu102_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu102_layer7 -> mlp_gpu102_layer7;
    mlp_gpu102_layer7 -> tp_allreduce_ep6_pp1_layer7;
    attn_gpu103_layer7 -> gate_gpu103_layer7;
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> expert_gpu103_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu103_layer7 -> mlp_gpu103_layer7;
    mlp_gpu103_layer7 -> tp_allreduce_ep6_pp1_layer7;
    attn_gpu104_layer8 -> gate_gpu104_layer8;
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> expert_gpu104_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu104_layer8 -> mlp_gpu104_layer8;
    mlp_gpu104_layer8 -> tp_allreduce_ep6_pp2_layer8;
    attn_gpu105_layer8 -> gate_gpu105_layer8;
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> expert_gpu105_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu105_layer8 -> mlp_gpu105_layer8;
    mlp_gpu105_layer8 -> tp_allreduce_ep6_pp2_layer8;
    attn_gpu106_layer8 -> gate_gpu106_layer8;
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> expert_gpu106_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu106_layer8 -> mlp_gpu106_layer8;
    mlp_gpu106_layer8 -> tp_allreduce_ep6_pp2_layer8;
    attn_gpu107_layer8 -> gate_gpu107_layer8;
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> expert_gpu107_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu107_layer8 -> mlp_gpu107_layer8;
    mlp_gpu107_layer8 -> tp_allreduce_ep6_pp2_layer8;
    attn_gpu104_layer9 -> gate_gpu104_layer9;
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> expert_gpu104_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu104_layer9 -> mlp_gpu104_layer9;
    mlp_gpu104_layer9 -> tp_allreduce_ep6_pp2_layer9;
    attn_gpu105_layer9 -> gate_gpu105_layer9;
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> expert_gpu105_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu105_layer9 -> mlp_gpu105_layer9;
    mlp_gpu105_layer9 -> tp_allreduce_ep6_pp2_layer9;
    attn_gpu106_layer9 -> gate_gpu106_layer9;
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> expert_gpu106_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu106_layer9 -> mlp_gpu106_layer9;
    mlp_gpu106_layer9 -> tp_allreduce_ep6_pp2_layer9;
    attn_gpu107_layer9 -> gate_gpu107_layer9;
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> expert_gpu107_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu107_layer9 -> mlp_gpu107_layer9;
    mlp_gpu107_layer9 -> tp_allreduce_ep6_pp2_layer9;
    attn_gpu104_layer10 -> gate_gpu104_layer10;
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> expert_gpu104_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu104_layer10 -> mlp_gpu104_layer10;
    mlp_gpu104_layer10 -> tp_allreduce_ep6_pp2_layer10;
    attn_gpu105_layer10 -> gate_gpu105_layer10;
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> expert_gpu105_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu105_layer10 -> mlp_gpu105_layer10;
    mlp_gpu105_layer10 -> tp_allreduce_ep6_pp2_layer10;
    attn_gpu106_layer10 -> gate_gpu106_layer10;
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> expert_gpu106_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu106_layer10 -> mlp_gpu106_layer10;
    mlp_gpu106_layer10 -> tp_allreduce_ep6_pp2_layer10;
    attn_gpu107_layer10 -> gate_gpu107_layer10;
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> expert_gpu107_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu107_layer10 -> mlp_gpu107_layer10;
    mlp_gpu107_layer10 -> tp_allreduce_ep6_pp2_layer10;
    attn_gpu104_layer11 -> gate_gpu104_layer11;
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> expert_gpu104_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu104_layer11 -> mlp_gpu104_layer11;
    mlp_gpu104_layer11 -> tp_allreduce_ep6_pp2_layer11;
    attn_gpu105_layer11 -> gate_gpu105_layer11;
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> expert_gpu105_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu105_layer11 -> mlp_gpu105_layer11;
    mlp_gpu105_layer11 -> tp_allreduce_ep6_pp2_layer11;
    attn_gpu106_layer11 -> gate_gpu106_layer11;
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> expert_gpu106_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu106_layer11 -> mlp_gpu106_layer11;
    mlp_gpu106_layer11 -> tp_allreduce_ep6_pp2_layer11;
    attn_gpu107_layer11 -> gate_gpu107_layer11;
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> expert_gpu107_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu107_layer11 -> mlp_gpu107_layer11;
    mlp_gpu107_layer11 -> tp_allreduce_ep6_pp2_layer11;
    attn_gpu108_layer12 -> gate_gpu108_layer12;
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> expert_gpu108_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu108_layer12 -> mlp_gpu108_layer12;
    mlp_gpu108_layer12 -> tp_allreduce_ep6_pp3_layer12;
    attn_gpu109_layer12 -> gate_gpu109_layer12;
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> expert_gpu109_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu109_layer12 -> mlp_gpu109_layer12;
    mlp_gpu109_layer12 -> tp_allreduce_ep6_pp3_layer12;
    attn_gpu110_layer12 -> gate_gpu110_layer12;
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> expert_gpu110_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu110_layer12 -> mlp_gpu110_layer12;
    mlp_gpu110_layer12 -> tp_allreduce_ep6_pp3_layer12;
    attn_gpu111_layer12 -> gate_gpu111_layer12;
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> expert_gpu111_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu111_layer12 -> mlp_gpu111_layer12;
    mlp_gpu111_layer12 -> tp_allreduce_ep6_pp3_layer12;
    attn_gpu108_layer13 -> gate_gpu108_layer13;
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> expert_gpu108_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu108_layer13 -> mlp_gpu108_layer13;
    mlp_gpu108_layer13 -> tp_allreduce_ep6_pp3_layer13;
    attn_gpu109_layer13 -> gate_gpu109_layer13;
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> expert_gpu109_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu109_layer13 -> mlp_gpu109_layer13;
    mlp_gpu109_layer13 -> tp_allreduce_ep6_pp3_layer13;
    attn_gpu110_layer13 -> gate_gpu110_layer13;
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> expert_gpu110_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu110_layer13 -> mlp_gpu110_layer13;
    mlp_gpu110_layer13 -> tp_allreduce_ep6_pp3_layer13;
    attn_gpu111_layer13 -> gate_gpu111_layer13;
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> expert_gpu111_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu111_layer13 -> mlp_gpu111_layer13;
    mlp_gpu111_layer13 -> tp_allreduce_ep6_pp3_layer13;
    attn_gpu108_layer14 -> gate_gpu108_layer14;
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> expert_gpu108_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu108_layer14 -> mlp_gpu108_layer14;
    mlp_gpu108_layer14 -> tp_allreduce_ep6_pp3_layer14;
    attn_gpu109_layer14 -> gate_gpu109_layer14;
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> expert_gpu109_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu109_layer14 -> mlp_gpu109_layer14;
    mlp_gpu109_layer14 -> tp_allreduce_ep6_pp3_layer14;
    attn_gpu110_layer14 -> gate_gpu110_layer14;
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> expert_gpu110_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu110_layer14 -> mlp_gpu110_layer14;
    mlp_gpu110_layer14 -> tp_allreduce_ep6_pp3_layer14;
    attn_gpu111_layer14 -> gate_gpu111_layer14;
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> expert_gpu111_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu111_layer14 -> mlp_gpu111_layer14;
    mlp_gpu111_layer14 -> tp_allreduce_ep6_pp3_layer14;
    attn_gpu108_layer15 -> gate_gpu108_layer15;
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> expert_gpu108_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu108_layer15 -> mlp_gpu108_layer15;
    mlp_gpu108_layer15 -> tp_allreduce_ep6_pp3_layer15;
    attn_gpu109_layer15 -> gate_gpu109_layer15;
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> expert_gpu109_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu109_layer15 -> mlp_gpu109_layer15;
    mlp_gpu109_layer15 -> tp_allreduce_ep6_pp3_layer15;
    attn_gpu110_layer15 -> gate_gpu110_layer15;
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> expert_gpu110_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu110_layer15 -> mlp_gpu110_layer15;
    mlp_gpu110_layer15 -> tp_allreduce_ep6_pp3_layer15;
    attn_gpu111_layer15 -> gate_gpu111_layer15;
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> expert_gpu111_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu111_layer15 -> mlp_gpu111_layer15;
    mlp_gpu111_layer15 -> tp_allreduce_ep6_pp3_layer15;
    attn_gpu112_layer0 -> gate_gpu112_layer0;
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> expert_gpu112_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu112_layer0 -> mlp_gpu112_layer0;
    mlp_gpu112_layer0 -> tp_allreduce_ep7_pp0_layer0;
    attn_gpu113_layer0 -> gate_gpu113_layer0;
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> expert_gpu113_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu113_layer0 -> mlp_gpu113_layer0;
    mlp_gpu113_layer0 -> tp_allreduce_ep7_pp0_layer0;
    attn_gpu114_layer0 -> gate_gpu114_layer0;
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> expert_gpu114_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu114_layer0 -> mlp_gpu114_layer0;
    mlp_gpu114_layer0 -> tp_allreduce_ep7_pp0_layer0;
    attn_gpu115_layer0 -> gate_gpu115_layer0;
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp0 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp1 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp2 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp3 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp4 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp5 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp6 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> expert_gpu115_layer0_exp7 [style=dashed, penwidth=2];
    gate_gpu115_layer0 -> mlp_gpu115_layer0;
    mlp_gpu115_layer0 -> tp_allreduce_ep7_pp0_layer0;
    attn_gpu112_layer1 -> gate_gpu112_layer1;
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> expert_gpu112_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu112_layer1 -> mlp_gpu112_layer1;
    mlp_gpu112_layer1 -> tp_allreduce_ep7_pp0_layer1;
    attn_gpu113_layer1 -> gate_gpu113_layer1;
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> expert_gpu113_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu113_layer1 -> mlp_gpu113_layer1;
    mlp_gpu113_layer1 -> tp_allreduce_ep7_pp0_layer1;
    attn_gpu114_layer1 -> gate_gpu114_layer1;
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> expert_gpu114_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu114_layer1 -> mlp_gpu114_layer1;
    mlp_gpu114_layer1 -> tp_allreduce_ep7_pp0_layer1;
    attn_gpu115_layer1 -> gate_gpu115_layer1;
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp0 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp1 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp2 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp3 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp4 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp5 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp6 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> expert_gpu115_layer1_exp7 [style=dashed, penwidth=2];
    gate_gpu115_layer1 -> mlp_gpu115_layer1;
    mlp_gpu115_layer1 -> tp_allreduce_ep7_pp0_layer1;
    attn_gpu112_layer2 -> gate_gpu112_layer2;
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> expert_gpu112_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu112_layer2 -> mlp_gpu112_layer2;
    mlp_gpu112_layer2 -> tp_allreduce_ep7_pp0_layer2;
    attn_gpu113_layer2 -> gate_gpu113_layer2;
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> expert_gpu113_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu113_layer2 -> mlp_gpu113_layer2;
    mlp_gpu113_layer2 -> tp_allreduce_ep7_pp0_layer2;
    attn_gpu114_layer2 -> gate_gpu114_layer2;
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> expert_gpu114_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu114_layer2 -> mlp_gpu114_layer2;
    mlp_gpu114_layer2 -> tp_allreduce_ep7_pp0_layer2;
    attn_gpu115_layer2 -> gate_gpu115_layer2;
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp0 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp1 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp2 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp3 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp4 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp5 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp6 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> expert_gpu115_layer2_exp7 [style=dashed, penwidth=2];
    gate_gpu115_layer2 -> mlp_gpu115_layer2;
    mlp_gpu115_layer2 -> tp_allreduce_ep7_pp0_layer2;
    attn_gpu112_layer3 -> gate_gpu112_layer3;
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> expert_gpu112_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu112_layer3 -> mlp_gpu112_layer3;
    mlp_gpu112_layer3 -> tp_allreduce_ep7_pp0_layer3;
    attn_gpu113_layer3 -> gate_gpu113_layer3;
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> expert_gpu113_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu113_layer3 -> mlp_gpu113_layer3;
    mlp_gpu113_layer3 -> tp_allreduce_ep7_pp0_layer3;
    attn_gpu114_layer3 -> gate_gpu114_layer3;
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> expert_gpu114_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu114_layer3 -> mlp_gpu114_layer3;
    mlp_gpu114_layer3 -> tp_allreduce_ep7_pp0_layer3;
    attn_gpu115_layer3 -> gate_gpu115_layer3;
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp0 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp1 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp2 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp3 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp4 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp5 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp6 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> expert_gpu115_layer3_exp7 [style=dashed, penwidth=2];
    gate_gpu115_layer3 -> mlp_gpu115_layer3;
    mlp_gpu115_layer3 -> tp_allreduce_ep7_pp0_layer3;
    attn_gpu116_layer4 -> gate_gpu116_layer4;
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> expert_gpu116_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu116_layer4 -> mlp_gpu116_layer4;
    mlp_gpu116_layer4 -> tp_allreduce_ep7_pp1_layer4;
    attn_gpu117_layer4 -> gate_gpu117_layer4;
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> expert_gpu117_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu117_layer4 -> mlp_gpu117_layer4;
    mlp_gpu117_layer4 -> tp_allreduce_ep7_pp1_layer4;
    attn_gpu118_layer4 -> gate_gpu118_layer4;
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> expert_gpu118_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu118_layer4 -> mlp_gpu118_layer4;
    mlp_gpu118_layer4 -> tp_allreduce_ep7_pp1_layer4;
    attn_gpu119_layer4 -> gate_gpu119_layer4;
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp0 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp1 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp2 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp3 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp4 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp5 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp6 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> expert_gpu119_layer4_exp7 [style=dashed, penwidth=2];
    gate_gpu119_layer4 -> mlp_gpu119_layer4;
    mlp_gpu119_layer4 -> tp_allreduce_ep7_pp1_layer4;
    attn_gpu116_layer5 -> gate_gpu116_layer5;
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> expert_gpu116_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu116_layer5 -> mlp_gpu116_layer5;
    mlp_gpu116_layer5 -> tp_allreduce_ep7_pp1_layer5;
    attn_gpu117_layer5 -> gate_gpu117_layer5;
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> expert_gpu117_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu117_layer5 -> mlp_gpu117_layer5;
    mlp_gpu117_layer5 -> tp_allreduce_ep7_pp1_layer5;
    attn_gpu118_layer5 -> gate_gpu118_layer5;
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> expert_gpu118_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu118_layer5 -> mlp_gpu118_layer5;
    mlp_gpu118_layer5 -> tp_allreduce_ep7_pp1_layer5;
    attn_gpu119_layer5 -> gate_gpu119_layer5;
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp0 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp1 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp2 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp3 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp4 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp5 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp6 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> expert_gpu119_layer5_exp7 [style=dashed, penwidth=2];
    gate_gpu119_layer5 -> mlp_gpu119_layer5;
    mlp_gpu119_layer5 -> tp_allreduce_ep7_pp1_layer5;
    attn_gpu116_layer6 -> gate_gpu116_layer6;
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> expert_gpu116_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu116_layer6 -> mlp_gpu116_layer6;
    mlp_gpu116_layer6 -> tp_allreduce_ep7_pp1_layer6;
    attn_gpu117_layer6 -> gate_gpu117_layer6;
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> expert_gpu117_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu117_layer6 -> mlp_gpu117_layer6;
    mlp_gpu117_layer6 -> tp_allreduce_ep7_pp1_layer6;
    attn_gpu118_layer6 -> gate_gpu118_layer6;
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> expert_gpu118_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu118_layer6 -> mlp_gpu118_layer6;
    mlp_gpu118_layer6 -> tp_allreduce_ep7_pp1_layer6;
    attn_gpu119_layer6 -> gate_gpu119_layer6;
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp0 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp1 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp2 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp3 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp4 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp5 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp6 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> expert_gpu119_layer6_exp7 [style=dashed, penwidth=2];
    gate_gpu119_layer6 -> mlp_gpu119_layer6;
    mlp_gpu119_layer6 -> tp_allreduce_ep7_pp1_layer6;
    attn_gpu116_layer7 -> gate_gpu116_layer7;
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> expert_gpu116_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu116_layer7 -> mlp_gpu116_layer7;
    mlp_gpu116_layer7 -> tp_allreduce_ep7_pp1_layer7;
    attn_gpu117_layer7 -> gate_gpu117_layer7;
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> expert_gpu117_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu117_layer7 -> mlp_gpu117_layer7;
    mlp_gpu117_layer7 -> tp_allreduce_ep7_pp1_layer7;
    attn_gpu118_layer7 -> gate_gpu118_layer7;
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> expert_gpu118_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu118_layer7 -> mlp_gpu118_layer7;
    mlp_gpu118_layer7 -> tp_allreduce_ep7_pp1_layer7;
    attn_gpu119_layer7 -> gate_gpu119_layer7;
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp0 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp1 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp2 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp3 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp4 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp5 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp6 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> expert_gpu119_layer7_exp7 [style=dashed, penwidth=2];
    gate_gpu119_layer7 -> mlp_gpu119_layer7;
    mlp_gpu119_layer7 -> tp_allreduce_ep7_pp1_layer7;
    attn_gpu120_layer8 -> gate_gpu120_layer8;
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> expert_gpu120_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu120_layer8 -> mlp_gpu120_layer8;
    mlp_gpu120_layer8 -> tp_allreduce_ep7_pp2_layer8;
    attn_gpu121_layer8 -> gate_gpu121_layer8;
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> expert_gpu121_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu121_layer8 -> mlp_gpu121_layer8;
    mlp_gpu121_layer8 -> tp_allreduce_ep7_pp2_layer8;
    attn_gpu122_layer8 -> gate_gpu122_layer8;
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> expert_gpu122_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu122_layer8 -> mlp_gpu122_layer8;
    mlp_gpu122_layer8 -> tp_allreduce_ep7_pp2_layer8;
    attn_gpu123_layer8 -> gate_gpu123_layer8;
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp0 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp1 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp2 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp3 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp4 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp5 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp6 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> expert_gpu123_layer8_exp7 [style=dashed, penwidth=2];
    gate_gpu123_layer8 -> mlp_gpu123_layer8;
    mlp_gpu123_layer8 -> tp_allreduce_ep7_pp2_layer8;
    attn_gpu120_layer9 -> gate_gpu120_layer9;
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> expert_gpu120_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu120_layer9 -> mlp_gpu120_layer9;
    mlp_gpu120_layer9 -> tp_allreduce_ep7_pp2_layer9;
    attn_gpu121_layer9 -> gate_gpu121_layer9;
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> expert_gpu121_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu121_layer9 -> mlp_gpu121_layer9;
    mlp_gpu121_layer9 -> tp_allreduce_ep7_pp2_layer9;
    attn_gpu122_layer9 -> gate_gpu122_layer9;
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> expert_gpu122_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu122_layer9 -> mlp_gpu122_layer9;
    mlp_gpu122_layer9 -> tp_allreduce_ep7_pp2_layer9;
    attn_gpu123_layer9 -> gate_gpu123_layer9;
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp0 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp1 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp2 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp3 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp4 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp5 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp6 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> expert_gpu123_layer9_exp7 [style=dashed, penwidth=2];
    gate_gpu123_layer9 -> mlp_gpu123_layer9;
    mlp_gpu123_layer9 -> tp_allreduce_ep7_pp2_layer9;
    attn_gpu120_layer10 -> gate_gpu120_layer10;
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> expert_gpu120_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu120_layer10 -> mlp_gpu120_layer10;
    mlp_gpu120_layer10 -> tp_allreduce_ep7_pp2_layer10;
    attn_gpu121_layer10 -> gate_gpu121_layer10;
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> expert_gpu121_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu121_layer10 -> mlp_gpu121_layer10;
    mlp_gpu121_layer10 -> tp_allreduce_ep7_pp2_layer10;
    attn_gpu122_layer10 -> gate_gpu122_layer10;
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> expert_gpu122_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu122_layer10 -> mlp_gpu122_layer10;
    mlp_gpu122_layer10 -> tp_allreduce_ep7_pp2_layer10;
    attn_gpu123_layer10 -> gate_gpu123_layer10;
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp0 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp1 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp2 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp3 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp4 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp5 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp6 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> expert_gpu123_layer10_exp7 [style=dashed, penwidth=2];
    gate_gpu123_layer10 -> mlp_gpu123_layer10;
    mlp_gpu123_layer10 -> tp_allreduce_ep7_pp2_layer10;
    attn_gpu120_layer11 -> gate_gpu120_layer11;
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> expert_gpu120_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu120_layer11 -> mlp_gpu120_layer11;
    mlp_gpu120_layer11 -> tp_allreduce_ep7_pp2_layer11;
    attn_gpu121_layer11 -> gate_gpu121_layer11;
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> expert_gpu121_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu121_layer11 -> mlp_gpu121_layer11;
    mlp_gpu121_layer11 -> tp_allreduce_ep7_pp2_layer11;
    attn_gpu122_layer11 -> gate_gpu122_layer11;
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> expert_gpu122_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu122_layer11 -> mlp_gpu122_layer11;
    mlp_gpu122_layer11 -> tp_allreduce_ep7_pp2_layer11;
    attn_gpu123_layer11 -> gate_gpu123_layer11;
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp0 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp1 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp2 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp3 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp4 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp5 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp6 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> expert_gpu123_layer11_exp7 [style=dashed, penwidth=2];
    gate_gpu123_layer11 -> mlp_gpu123_layer11;
    mlp_gpu123_layer11 -> tp_allreduce_ep7_pp2_layer11;
    attn_gpu124_layer12 -> gate_gpu124_layer12;
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> expert_gpu124_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu124_layer12 -> mlp_gpu124_layer12;
    mlp_gpu124_layer12 -> tp_allreduce_ep7_pp3_layer12;
    attn_gpu125_layer12 -> gate_gpu125_layer12;
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> expert_gpu125_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu125_layer12 -> mlp_gpu125_layer12;
    mlp_gpu125_layer12 -> tp_allreduce_ep7_pp3_layer12;
    attn_gpu126_layer12 -> gate_gpu126_layer12;
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> expert_gpu126_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu126_layer12 -> mlp_gpu126_layer12;
    mlp_gpu126_layer12 -> tp_allreduce_ep7_pp3_layer12;
    attn_gpu127_layer12 -> gate_gpu127_layer12;
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp0 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp1 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp2 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp3 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp4 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp5 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp6 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> expert_gpu127_layer12_exp7 [style=dashed, penwidth=2];
    gate_gpu127_layer12 -> mlp_gpu127_layer12;
    mlp_gpu127_layer12 -> tp_allreduce_ep7_pp3_layer12;
    attn_gpu124_layer13 -> gate_gpu124_layer13;
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> expert_gpu124_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu124_layer13 -> mlp_gpu124_layer13;
    mlp_gpu124_layer13 -> tp_allreduce_ep7_pp3_layer13;
    attn_gpu125_layer13 -> gate_gpu125_layer13;
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> expert_gpu125_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu125_layer13 -> mlp_gpu125_layer13;
    mlp_gpu125_layer13 -> tp_allreduce_ep7_pp3_layer13;
    attn_gpu126_layer13 -> gate_gpu126_layer13;
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> expert_gpu126_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu126_layer13 -> mlp_gpu126_layer13;
    mlp_gpu126_layer13 -> tp_allreduce_ep7_pp3_layer13;
    attn_gpu127_layer13 -> gate_gpu127_layer13;
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp0 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp1 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp2 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp3 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp4 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp5 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp6 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> expert_gpu127_layer13_exp7 [style=dashed, penwidth=2];
    gate_gpu127_layer13 -> mlp_gpu127_layer13;
    mlp_gpu127_layer13 -> tp_allreduce_ep7_pp3_layer13;
    attn_gpu124_layer14 -> gate_gpu124_layer14;
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> expert_gpu124_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu124_layer14 -> mlp_gpu124_layer14;
    mlp_gpu124_layer14 -> tp_allreduce_ep7_pp3_layer14;
    attn_gpu125_layer14 -> gate_gpu125_layer14;
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> expert_gpu125_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu125_layer14 -> mlp_gpu125_layer14;
    mlp_gpu125_layer14 -> tp_allreduce_ep7_pp3_layer14;
    attn_gpu126_layer14 -> gate_gpu126_layer14;
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> expert_gpu126_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu126_layer14 -> mlp_gpu126_layer14;
    mlp_gpu126_layer14 -> tp_allreduce_ep7_pp3_layer14;
    attn_gpu127_layer14 -> gate_gpu127_layer14;
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp0 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp1 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp2 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp3 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp4 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp5 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp6 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> expert_gpu127_layer14_exp7 [style=dashed, penwidth=2];
    gate_gpu127_layer14 -> mlp_gpu127_layer14;
    mlp_gpu127_layer14 -> tp_allreduce_ep7_pp3_layer14;
    attn_gpu124_layer15 -> gate_gpu124_layer15;
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> expert_gpu124_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu124_layer15 -> mlp_gpu124_layer15;
    mlp_gpu124_layer15 -> tp_allreduce_ep7_pp3_layer15;
    attn_gpu125_layer15 -> gate_gpu125_layer15;
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> expert_gpu125_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu125_layer15 -> mlp_gpu125_layer15;
    mlp_gpu125_layer15 -> tp_allreduce_ep7_pp3_layer15;
    attn_gpu126_layer15 -> gate_gpu126_layer15;
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> expert_gpu126_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu126_layer15 -> mlp_gpu126_layer15;
    mlp_gpu126_layer15 -> tp_allreduce_ep7_pp3_layer15;
    attn_gpu127_layer15 -> gate_gpu127_layer15;
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp0 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp1 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp2 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp3 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp4 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp5 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp6 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> expert_gpu127_layer15_exp7 [style=dashed, penwidth=2];
    gate_gpu127_layer15 -> mlp_gpu127_layer15;
    mlp_gpu127_layer15 -> tp_allreduce_ep7_pp3_layer15;
    mlp_gpu0_layer3 -> pp_p2p_ep0_stage0_to_1;
    pp_p2p_ep0_stage0_to_1 -> attn_gpu4_layer4;
    mlp_gpu1_layer3 -> pp_p2p_ep0_stage0_to_1;
    pp_p2p_ep0_stage0_to_1 -> attn_gpu5_layer4;
    mlp_gpu2_layer3 -> pp_p2p_ep0_stage0_to_1;
    pp_p2p_ep0_stage0_to_1 -> attn_gpu6_layer4;
    mlp_gpu3_layer3 -> pp_p2p_ep0_stage0_to_1;
    pp_p2p_ep0_stage0_to_1 -> attn_gpu7_layer4;
    mlp_gpu4_layer7 -> pp_p2p_ep0_stage1_to_2;
    pp_p2p_ep0_stage1_to_2 -> attn_gpu8_layer8;
    mlp_gpu5_layer7 -> pp_p2p_ep0_stage1_to_2;
    pp_p2p_ep0_stage1_to_2 -> attn_gpu9_layer8;
    mlp_gpu6_layer7 -> pp_p2p_ep0_stage1_to_2;
    pp_p2p_ep0_stage1_to_2 -> attn_gpu10_layer8;
    mlp_gpu7_layer7 -> pp_p2p_ep0_stage1_to_2;
    pp_p2p_ep0_stage1_to_2 -> attn_gpu11_layer8;
    mlp_gpu8_layer11 -> pp_p2p_ep0_stage2_to_3;
    pp_p2p_ep0_stage2_to_3 -> attn_gpu12_layer12;
    mlp_gpu9_layer11 -> pp_p2p_ep0_stage2_to_3;
    pp_p2p_ep0_stage2_to_3 -> attn_gpu13_layer12;
    mlp_gpu10_layer11 -> pp_p2p_ep0_stage2_to_3;
    pp_p2p_ep0_stage2_to_3 -> attn_gpu14_layer12;
    mlp_gpu11_layer11 -> pp_p2p_ep0_stage2_to_3;
    pp_p2p_ep0_stage2_to_3 -> attn_gpu15_layer12;
    mlp_gpu16_layer3 -> pp_p2p_ep1_stage0_to_1;
    pp_p2p_ep1_stage0_to_1 -> attn_gpu20_layer4;
    mlp_gpu17_layer3 -> pp_p2p_ep1_stage0_to_1;
    pp_p2p_ep1_stage0_to_1 -> attn_gpu21_layer4;
    mlp_gpu18_layer3 -> pp_p2p_ep1_stage0_to_1;
    pp_p2p_ep1_stage0_to_1 -> attn_gpu22_layer4;
    mlp_gpu19_layer3 -> pp_p2p_ep1_stage0_to_1;
    pp_p2p_ep1_stage0_to_1 -> attn_gpu23_layer4;
    mlp_gpu20_layer7 -> pp_p2p_ep1_stage1_to_2;
    pp_p2p_ep1_stage1_to_2 -> attn_gpu24_layer8;
    mlp_gpu21_layer7 -> pp_p2p_ep1_stage1_to_2;
    pp_p2p_ep1_stage1_to_2 -> attn_gpu25_layer8;
    mlp_gpu22_layer7 -> pp_p2p_ep1_stage1_to_2;
    pp_p2p_ep1_stage1_to_2 -> attn_gpu26_layer8;
    mlp_gpu23_layer7 -> pp_p2p_ep1_stage1_to_2;
    pp_p2p_ep1_stage1_to_2 -> attn_gpu27_layer8;
    mlp_gpu24_layer11 -> pp_p2p_ep1_stage2_to_3;
    pp_p2p_ep1_stage2_to_3 -> attn_gpu28_layer12;
    mlp_gpu25_layer11 -> pp_p2p_ep1_stage2_to_3;
    pp_p2p_ep1_stage2_to_3 -> attn_gpu29_layer12;
    mlp_gpu26_layer11 -> pp_p2p_ep1_stage2_to_3;
    pp_p2p_ep1_stage2_to_3 -> attn_gpu30_layer12;
    mlp_gpu27_layer11 -> pp_p2p_ep1_stage2_to_3;
    pp_p2p_ep1_stage2_to_3 -> attn_gpu31_layer12;
    mlp_gpu32_layer3 -> pp_p2p_ep2_stage0_to_1;
    pp_p2p_ep2_stage0_to_1 -> attn_gpu36_layer4;
    mlp_gpu33_layer3 -> pp_p2p_ep2_stage0_to_1;
    pp_p2p_ep2_stage0_to_1 -> attn_gpu37_layer4;
    mlp_gpu34_layer3 -> pp_p2p_ep2_stage0_to_1;
    pp_p2p_ep2_stage0_to_1 -> attn_gpu38_layer4;
    mlp_gpu35_layer3 -> pp_p2p_ep2_stage0_to_1;
    pp_p2p_ep2_stage0_to_1 -> attn_gpu39_layer4;
    mlp_gpu36_layer7 -> pp_p2p_ep2_stage1_to_2;
    pp_p2p_ep2_stage1_to_2 -> attn_gpu40_layer8;
    mlp_gpu37_layer7 -> pp_p2p_ep2_stage1_to_2;
    pp_p2p_ep2_stage1_to_2 -> attn_gpu41_layer8;
    mlp_gpu38_layer7 -> pp_p2p_ep2_stage1_to_2;
    pp_p2p_ep2_stage1_to_2 -> attn_gpu42_layer8;
    mlp_gpu39_layer7 -> pp_p2p_ep2_stage1_to_2;
    pp_p2p_ep2_stage1_to_2 -> attn_gpu43_layer8;
    mlp_gpu40_layer11 -> pp_p2p_ep2_stage2_to_3;
    pp_p2p_ep2_stage2_to_3 -> attn_gpu44_layer12;
    mlp_gpu41_layer11 -> pp_p2p_ep2_stage2_to_3;
    pp_p2p_ep2_stage2_to_3 -> attn_gpu45_layer12;
    mlp_gpu42_layer11 -> pp_p2p_ep2_stage2_to_3;
    pp_p2p_ep2_stage2_to_3 -> attn_gpu46_layer12;
    mlp_gpu43_layer11 -> pp_p2p_ep2_stage2_to_3;
    pp_p2p_ep2_stage2_to_3 -> attn_gpu47_layer12;
    mlp_gpu48_layer3 -> pp_p2p_ep3_stage0_to_1;
    pp_p2p_ep3_stage0_to_1 -> attn_gpu52_layer4;
    mlp_gpu49_layer3 -> pp_p2p_ep3_stage0_to_1;
    pp_p2p_ep3_stage0_to_1 -> attn_gpu53_layer4;
    mlp_gpu50_layer3 -> pp_p2p_ep3_stage0_to_1;
    pp_p2p_ep3_stage0_to_1 -> attn_gpu54_layer4;
    mlp_gpu51_layer3 -> pp_p2p_ep3_stage0_to_1;
    pp_p2p_ep3_stage0_to_1 -> attn_gpu55_layer4;
    mlp_gpu52_layer7 -> pp_p2p_ep3_stage1_to_2;
    pp_p2p_ep3_stage1_to_2 -> attn_gpu56_layer8;
    mlp_gpu53_layer7 -> pp_p2p_ep3_stage1_to_2;
    pp_p2p_ep3_stage1_to_2 -> attn_gpu57_layer8;
    mlp_gpu54_layer7 -> pp_p2p_ep3_stage1_to_2;
    pp_p2p_ep3_stage1_to_2 -> attn_gpu58_layer8;
    mlp_gpu55_layer7 -> pp_p2p_ep3_stage1_to_2;
    pp_p2p_ep3_stage1_to_2 -> attn_gpu59_layer8;
    mlp_gpu56_layer11 -> pp_p2p_ep3_stage2_to_3;
    pp_p2p_ep3_stage2_to_3 -> attn_gpu60_layer12;
    mlp_gpu57_layer11 -> pp_p2p_ep3_stage2_to_3;
    pp_p2p_ep3_stage2_to_3 -> attn_gpu61_layer12;
    mlp_gpu58_layer11 -> pp_p2p_ep3_stage2_to_3;
    pp_p2p_ep3_stage2_to_3 -> attn_gpu62_layer12;
    mlp_gpu59_layer11 -> pp_p2p_ep3_stage2_to_3;
    pp_p2p_ep3_stage2_to_3 -> attn_gpu63_layer12;
    mlp_gpu64_layer3 -> pp_p2p_ep4_stage0_to_1;
    pp_p2p_ep4_stage0_to_1 -> attn_gpu68_layer4;
    mlp_gpu65_layer3 -> pp_p2p_ep4_stage0_to_1;
    pp_p2p_ep4_stage0_to_1 -> attn_gpu69_layer4;
    mlp_gpu66_layer3 -> pp_p2p_ep4_stage0_to_1;
    pp_p2p_ep4_stage0_to_1 -> attn_gpu70_layer4;
    mlp_gpu67_layer3 -> pp_p2p_ep4_stage0_to_1;
    pp_p2p_ep4_stage0_to_1 -> attn_gpu71_layer4;
    mlp_gpu68_layer7 -> pp_p2p_ep4_stage1_to_2;
    pp_p2p_ep4_stage1_to_2 -> attn_gpu72_layer8;
    mlp_gpu69_layer7 -> pp_p2p_ep4_stage1_to_2;
    pp_p2p_ep4_stage1_to_2 -> attn_gpu73_layer8;
    mlp_gpu70_layer7 -> pp_p2p_ep4_stage1_to_2;
    pp_p2p_ep4_stage1_to_2 -> attn_gpu74_layer8;
    mlp_gpu71_layer7 -> pp_p2p_ep4_stage1_to_2;
    pp_p2p_ep4_stage1_to_2 -> attn_gpu75_layer8;
    mlp_gpu72_layer11 -> pp_p2p_ep4_stage2_to_3;
    pp_p2p_ep4_stage2_to_3 -> attn_gpu76_layer12;
    mlp_gpu73_layer11 -> pp_p2p_ep4_stage2_to_3;
    pp_p2p_ep4_stage2_to_3 -> attn_gpu77_layer12;
    mlp_gpu74_layer11 -> pp_p2p_ep4_stage2_to_3;
    pp_p2p_ep4_stage2_to_3 -> attn_gpu78_layer12;
    mlp_gpu75_layer11 -> pp_p2p_ep4_stage2_to_3;
    pp_p2p_ep4_stage2_to_3 -> attn_gpu79_layer12;
    mlp_gpu80_layer3 -> pp_p2p_ep5_stage0_to_1;
    pp_p2p_ep5_stage0_to_1 -> attn_gpu84_layer4;
    mlp_gpu81_layer3 -> pp_p2p_ep5_stage0_to_1;
    pp_p2p_ep5_stage0_to_1 -> attn_gpu85_layer4;
    mlp_gpu82_layer3 -> pp_p2p_ep5_stage0_to_1;
    pp_p2p_ep5_stage0_to_1 -> attn_gpu86_layer4;
    mlp_gpu83_layer3 -> pp_p2p_ep5_stage0_to_1;
    pp_p2p_ep5_stage0_to_1 -> attn_gpu87_layer4;
    mlp_gpu84_layer7 -> pp_p2p_ep5_stage1_to_2;
    pp_p2p_ep5_stage1_to_2 -> attn_gpu88_layer8;
    mlp_gpu85_layer7 -> pp_p2p_ep5_stage1_to_2;
    pp_p2p_ep5_stage1_to_2 -> attn_gpu89_layer8;
    mlp_gpu86_layer7 -> pp_p2p_ep5_stage1_to_2;
    pp_p2p_ep5_stage1_to_2 -> attn_gpu90_layer8;
    mlp_gpu87_layer7 -> pp_p2p_ep5_stage1_to_2;
    pp_p2p_ep5_stage1_to_2 -> attn_gpu91_layer8;
    mlp_gpu88_layer11 -> pp_p2p_ep5_stage2_to_3;
    pp_p2p_ep5_stage2_to_3 -> attn_gpu92_layer12;
    mlp_gpu89_layer11 -> pp_p2p_ep5_stage2_to_3;
    pp_p2p_ep5_stage2_to_3 -> attn_gpu93_layer12;
    mlp_gpu90_layer11 -> pp_p2p_ep5_stage2_to_3;
    pp_p2p_ep5_stage2_to_3 -> attn_gpu94_layer12;
    mlp_gpu91_layer11 -> pp_p2p_ep5_stage2_to_3;
    pp_p2p_ep5_stage2_to_3 -> attn_gpu95_layer12;
    mlp_gpu96_layer3 -> pp_p2p_ep6_stage0_to_1;
    pp_p2p_ep6_stage0_to_1 -> attn_gpu100_layer4;
    mlp_gpu97_layer3 -> pp_p2p_ep6_stage0_to_1;
    pp_p2p_ep6_stage0_to_1 -> attn_gpu101_layer4;
    mlp_gpu98_layer3 -> pp_p2p_ep6_stage0_to_1;
    pp_p2p_ep6_stage0_to_1 -> attn_gpu102_layer4;
    mlp_gpu99_layer3 -> pp_p2p_ep6_stage0_to_1;
    pp_p2p_ep6_stage0_to_1 -> attn_gpu103_layer4;
    mlp_gpu100_layer7 -> pp_p2p_ep6_stage1_to_2;
    pp_p2p_ep6_stage1_to_2 -> attn_gpu104_layer8;
    mlp_gpu101_layer7 -> pp_p2p_ep6_stage1_to_2;
    pp_p2p_ep6_stage1_to_2 -> attn_gpu105_layer8;
    mlp_gpu102_layer7 -> pp_p2p_ep6_stage1_to_2;
    pp_p2p_ep6_stage1_to_2 -> attn_gpu106_layer8;
    mlp_gpu103_layer7 -> pp_p2p_ep6_stage1_to_2;
    pp_p2p_ep6_stage1_to_2 -> attn_gpu107_layer8;
    mlp_gpu104_layer11 -> pp_p2p_ep6_stage2_to_3;
    pp_p2p_ep6_stage2_to_3 -> attn_gpu108_layer12;
    mlp_gpu105_layer11 -> pp_p2p_ep6_stage2_to_3;
    pp_p2p_ep6_stage2_to_3 -> attn_gpu109_layer12;
    mlp_gpu106_layer11 -> pp_p2p_ep6_stage2_to_3;
    pp_p2p_ep6_stage2_to_3 -> attn_gpu110_layer12;
    mlp_gpu107_layer11 -> pp_p2p_ep6_stage2_to_3;
    pp_p2p_ep6_stage2_to_3 -> attn_gpu111_layer12;
    mlp_gpu112_layer3 -> pp_p2p_ep7_stage0_to_1;
    pp_p2p_ep7_stage0_to_1 -> attn_gpu116_layer4;
    mlp_gpu113_layer3 -> pp_p2p_ep7_stage0_to_1;
    pp_p2p_ep7_stage0_to_1 -> attn_gpu117_layer4;
    mlp_gpu114_layer3 -> pp_p2p_ep7_stage0_to_1;
    pp_p2p_ep7_stage0_to_1 -> attn_gpu118_layer4;
    mlp_gpu115_layer3 -> pp_p2p_ep7_stage0_to_1;
    pp_p2p_ep7_stage0_to_1 -> attn_gpu119_layer4;
    mlp_gpu116_layer7 -> pp_p2p_ep7_stage1_to_2;
    pp_p2p_ep7_stage1_to_2 -> attn_gpu120_layer8;
    mlp_gpu117_layer7 -> pp_p2p_ep7_stage1_to_2;
    pp_p2p_ep7_stage1_to_2 -> attn_gpu121_layer8;
    mlp_gpu118_layer7 -> pp_p2p_ep7_stage1_to_2;
    pp_p2p_ep7_stage1_to_2 -> attn_gpu122_layer8;
    mlp_gpu119_layer7 -> pp_p2p_ep7_stage1_to_2;
    pp_p2p_ep7_stage1_to_2 -> attn_gpu123_layer8;
    mlp_gpu120_layer11 -> pp_p2p_ep7_stage2_to_3;
    pp_p2p_ep7_stage2_to_3 -> attn_gpu124_layer12;
    mlp_gpu121_layer11 -> pp_p2p_ep7_stage2_to_3;
    pp_p2p_ep7_stage2_to_3 -> attn_gpu125_layer12;
    mlp_gpu122_layer11 -> pp_p2p_ep7_stage2_to_3;
    pp_p2p_ep7_stage2_to_3 -> attn_gpu126_layer12;
    mlp_gpu123_layer11 -> pp_p2p_ep7_stage2_to_3;
    pp_p2p_ep7_stage2_to_3 -> attn_gpu127_layer12;
    mlp_gpu12_layer15 -> output;
    mlp_gpu28_layer15 -> output;
    mlp_gpu44_layer15 -> output;
    mlp_gpu60_layer15 -> output;
    mlp_gpu76_layer15 -> output;
    mlp_gpu92_layer15 -> output;
    mlp_gpu108_layer15 -> output;
    mlp_gpu124_layer15 -> output;
    ep_agg_ep0_layer0 [label="Expert Aggregation\nEP0 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer1 [label="Expert Aggregation\nEP0 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer2 [label="Expert Aggregation\nEP0 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer3 [label="Expert Aggregation\nEP0 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer4 [label="Expert Aggregation\nEP0 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer5 [label="Expert Aggregation\nEP0 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer6 [label="Expert Aggregation\nEP0 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer7 [label="Expert Aggregation\nEP0 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer8 [label="Expert Aggregation\nEP0 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer9 [label="Expert Aggregation\nEP0 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer10 [label="Expert Aggregation\nEP0 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer11 [label="Expert Aggregation\nEP0 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer12 [label="Expert Aggregation\nEP0 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer13 [label="Expert Aggregation\nEP0 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer14 [label="Expert Aggregation\nEP0 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep0_layer15 [label="Expert Aggregation\nEP0 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer0 [label="Expert Aggregation\nEP1 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer1 [label="Expert Aggregation\nEP1 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer2 [label="Expert Aggregation\nEP1 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer3 [label="Expert Aggregation\nEP1 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer4 [label="Expert Aggregation\nEP1 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer5 [label="Expert Aggregation\nEP1 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer6 [label="Expert Aggregation\nEP1 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer7 [label="Expert Aggregation\nEP1 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer8 [label="Expert Aggregation\nEP1 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer9 [label="Expert Aggregation\nEP1 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer10 [label="Expert Aggregation\nEP1 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer11 [label="Expert Aggregation\nEP1 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer12 [label="Expert Aggregation\nEP1 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer13 [label="Expert Aggregation\nEP1 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer14 [label="Expert Aggregation\nEP1 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep1_layer15 [label="Expert Aggregation\nEP1 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer0 [label="Expert Aggregation\nEP2 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer1 [label="Expert Aggregation\nEP2 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer2 [label="Expert Aggregation\nEP2 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer3 [label="Expert Aggregation\nEP2 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer4 [label="Expert Aggregation\nEP2 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer5 [label="Expert Aggregation\nEP2 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer6 [label="Expert Aggregation\nEP2 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer7 [label="Expert Aggregation\nEP2 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer8 [label="Expert Aggregation\nEP2 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer9 [label="Expert Aggregation\nEP2 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer10 [label="Expert Aggregation\nEP2 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer11 [label="Expert Aggregation\nEP2 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer12 [label="Expert Aggregation\nEP2 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer13 [label="Expert Aggregation\nEP2 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer14 [label="Expert Aggregation\nEP2 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep2_layer15 [label="Expert Aggregation\nEP2 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer0 [label="Expert Aggregation\nEP3 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer1 [label="Expert Aggregation\nEP3 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer2 [label="Expert Aggregation\nEP3 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer3 [label="Expert Aggregation\nEP3 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer4 [label="Expert Aggregation\nEP3 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer5 [label="Expert Aggregation\nEP3 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer6 [label="Expert Aggregation\nEP3 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer7 [label="Expert Aggregation\nEP3 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer8 [label="Expert Aggregation\nEP3 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer9 [label="Expert Aggregation\nEP3 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer10 [label="Expert Aggregation\nEP3 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer11 [label="Expert Aggregation\nEP3 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer12 [label="Expert Aggregation\nEP3 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer13 [label="Expert Aggregation\nEP3 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer14 [label="Expert Aggregation\nEP3 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep3_layer15 [label="Expert Aggregation\nEP3 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer0 [label="Expert Aggregation\nEP4 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer1 [label="Expert Aggregation\nEP4 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer2 [label="Expert Aggregation\nEP4 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer3 [label="Expert Aggregation\nEP4 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer4 [label="Expert Aggregation\nEP4 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer5 [label="Expert Aggregation\nEP4 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer6 [label="Expert Aggregation\nEP4 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer7 [label="Expert Aggregation\nEP4 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer8 [label="Expert Aggregation\nEP4 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer9 [label="Expert Aggregation\nEP4 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer10 [label="Expert Aggregation\nEP4 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer11 [label="Expert Aggregation\nEP4 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer12 [label="Expert Aggregation\nEP4 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer13 [label="Expert Aggregation\nEP4 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer14 [label="Expert Aggregation\nEP4 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep4_layer15 [label="Expert Aggregation\nEP4 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer0 [label="Expert Aggregation\nEP5 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer1 [label="Expert Aggregation\nEP5 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer2 [label="Expert Aggregation\nEP5 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer3 [label="Expert Aggregation\nEP5 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer4 [label="Expert Aggregation\nEP5 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer5 [label="Expert Aggregation\nEP5 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer6 [label="Expert Aggregation\nEP5 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer7 [label="Expert Aggregation\nEP5 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer8 [label="Expert Aggregation\nEP5 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer9 [label="Expert Aggregation\nEP5 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer10 [label="Expert Aggregation\nEP5 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer11 [label="Expert Aggregation\nEP5 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer12 [label="Expert Aggregation\nEP5 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer13 [label="Expert Aggregation\nEP5 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer14 [label="Expert Aggregation\nEP5 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep5_layer15 [label="Expert Aggregation\nEP5 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer0 [label="Expert Aggregation\nEP6 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer1 [label="Expert Aggregation\nEP6 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer2 [label="Expert Aggregation\nEP6 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer3 [label="Expert Aggregation\nEP6 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer4 [label="Expert Aggregation\nEP6 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer5 [label="Expert Aggregation\nEP6 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer6 [label="Expert Aggregation\nEP6 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer7 [label="Expert Aggregation\nEP6 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer8 [label="Expert Aggregation\nEP6 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer9 [label="Expert Aggregation\nEP6 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer10 [label="Expert Aggregation\nEP6 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer11 [label="Expert Aggregation\nEP6 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer12 [label="Expert Aggregation\nEP6 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer13 [label="Expert Aggregation\nEP6 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer14 [label="Expert Aggregation\nEP6 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep6_layer15 [label="Expert Aggregation\nEP6 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer0 [label="Expert Aggregation\nEP7 L0\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer1 [label="Expert Aggregation\nEP7 L1\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer2 [label="Expert Aggregation\nEP7 L2\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer3 [label="Expert Aggregation\nEP7 L3\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer4 [label="Expert Aggregation\nEP7 L4\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer5 [label="Expert Aggregation\nEP7 L5\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer6 [label="Expert Aggregation\nEP7 L6\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer7 [label="Expert Aggregation\nEP7 L7\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer8 [label="Expert Aggregation\nEP7 L8\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer9 [label="Expert Aggregation\nEP7 L9\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer10 [label="Expert Aggregation\nEP7 L10\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer11 [label="Expert Aggregation\nEP7 L11\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer12 [label="Expert Aggregation\nEP7 L12\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer13 [label="Expert Aggregation\nEP7 L13\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer14 [label="Expert Aggregation\nEP7 L14\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
    ep_agg_ep7_layer15 [label="Expert Aggregation\nEP7 L15\nInput: [batch_size=4, seq_len=1024, experts=8]\nOutput: [batch_size=4, seq_len=1024, hidden=1024]", shape=parallelogram, fillcolor=gold];
}
