// LLM Parallel Strategy - Simplified Overview
digraph {
	dpi=300 rankdir=LR size="20,15"
	input [label="Input\n[batch=4, seq=10240, hidden=512]" fillcolor=white shape=ellipse style=filled]
	prefill [label="PREFILL PHASE\nPP=4, EP=4, TP=2, SP=2\n64 GPUs" fillcolor=lightblue shape=box style=filled]
	decode [label="DECODE PHASE\nPP=4, EP=4, TP=2, SP=1\n32 GPUs" fillcolor=lightgreen shape=box style=filled]
	output [label="Output\nGenerated Tokens" fillcolor=white shape=ellipse style=filled]
	comm [label="Communication Patterns:\n• All-Reduce (TP)\n• All-Gather (SP)\n• All-to-All (EP)\n• Pipeline (PP)" fillcolor=yellow shape=parallelogram style=filled]
	input -> prefill
	prefill -> decode [label="KV Cache\nTransfer"]
	decode -> output
}
