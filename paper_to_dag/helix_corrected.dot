digraph helix_transformer_dag {
	graph [rankdir=TB, splines=ortho]
	node [fillcolor=lightblue, shape=ellipse, style=filled]
	input [fillcolor=lightgreen, label="Input\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=parallelogram]
	layer0_ln1 [fillcolor=lightcoral, label="LayerNorm1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nAll 16 devices", shape=rectangle]
	layer0_qkv_device0 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 0\nHeads: 0-7, Dim: 0-31", shape=rectangle]
	layer0_attn_device0 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 0", shape=rectangle]
	layer0_out_proj_device0 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 0", shape=rectangle]
	layer0_qkv_device1 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 1\nHeads: 0-7, Dim: 32-63", shape=rectangle]
	layer0_attn_device1 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 1", shape=rectangle]
	layer0_out_proj_device1 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 1", shape=rectangle]
	layer0_qkv_device2 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 2\nHeads: 0-7, Dim: 64-95", shape=rectangle]
	layer0_attn_device2 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 2", shape=rectangle]
	layer0_out_proj_device2 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 2", shape=rectangle]
	layer0_qkv_device3 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 3\nHeads: 0-7, Dim: 96-127", shape=rectangle]
	layer0_attn_device3 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 3", shape=rectangle]
	layer0_out_proj_device3 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 3", shape=rectangle]
	layer0_qkv_device4 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 4\nHeads: 8-15, Dim: 0-31", shape=rectangle]
	layer0_attn_device4 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 4", shape=rectangle]
	layer0_out_proj_device4 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 4", shape=rectangle]
	layer0_qkv_device5 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 5\nHeads: 8-15, Dim: 32-63", shape=rectangle]
	layer0_attn_device5 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 5", shape=rectangle]
	layer0_out_proj_device5 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 5", shape=rectangle]
	layer0_qkv_device6 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 6\nHeads: 8-15, Dim: 64-95", shape=rectangle]
	layer0_attn_device6 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 6", shape=rectangle]
	layer0_out_proj_device6 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 6", shape=rectangle]
	layer0_qkv_device7 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 7\nHeads: 8-15, Dim: 96-127", shape=rectangle]
	layer0_attn_device7 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 7", shape=rectangle]
	layer0_out_proj_device7 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 7", shape=rectangle]
	layer0_qkv_device8 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 8\nHeads: 16-23, Dim: 0-31", shape=rectangle]
	layer0_attn_device8 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 8", shape=rectangle]
	layer0_out_proj_device8 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 8", shape=rectangle]
	layer0_qkv_device9 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 9\nHeads: 16-23, Dim: 32-63", shape=rectangle]
	layer0_attn_device9 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 9", shape=rectangle]
	layer0_out_proj_device9 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 9", shape=rectangle]
	layer0_qkv_device10 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 10\nHeads: 16-23, Dim: 64-95", shape=rectangle]
	layer0_attn_device10 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 10", shape=rectangle]
	layer0_out_proj_device10 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 10", shape=rectangle]
	layer0_qkv_device11 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 11\nHeads: 16-23, Dim: 96-127", shape=rectangle]
	layer0_attn_device11 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 11", shape=rectangle]
	layer0_out_proj_device11 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 11", shape=rectangle]
	layer0_qkv_device12 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 12\nHeads: 24-31, Dim: 0-31", shape=rectangle]
	layer0_attn_device12 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 12", shape=rectangle]
	layer0_out_proj_device12 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 12", shape=rectangle]
	layer0_qkv_device13 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 13\nHeads: 24-31, Dim: 32-63", shape=rectangle]
	layer0_attn_device13 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 13", shape=rectangle]
	layer0_out_proj_device13 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 13", shape=rectangle]
	layer0_qkv_device14 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 14\nHeads: 24-31, Dim: 64-95", shape=rectangle]
	layer0_attn_device14 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 14", shape=rectangle]
	layer0_out_proj_device14 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 14", shape=rectangle]
	layer0_qkv_device15 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 15\nHeads: 24-31, Dim: 96-127", shape=rectangle]
	layer0_attn_device15 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 15", shape=rectangle]
	layer0_out_proj_device15 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 15", shape=rectangle]
	layer0_agg_dim_group0 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 0", shape=parallelogram]
	layer0_agg_dim_group1 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 1", shape=parallelogram]
	layer0_agg_dim_group2 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 2", shape=parallelogram]
	layer0_agg_dim_group3 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 3", shape=parallelogram]
	layer0_agg_heads [fillcolor=orange, label="Head Concat\n[batch_size=128, seq_len=10000, heads=32, head_dim=128]", shape=parallelogram]
	layer0_res1 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=ellipse]
	layer0_ln2 [fillcolor=lightcoral, label="LayerNorm2\n[batch_size=128, seq_len=10000, hidden_size=4096]\nAll 16 devices", shape=rectangle]
	layer0_mlp_fc1_device0 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 0", shape=rectangle]
	layer0_mlp_fc2_device0 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 0", shape=rectangle]
	layer0_mlp_fc1_device1 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 1", shape=rectangle]
	layer0_mlp_fc2_device1 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 1", shape=rectangle]
	layer0_mlp_fc1_device2 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 2", shape=rectangle]
	layer0_mlp_fc2_device2 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 2", shape=rectangle]
	layer0_mlp_fc1_device3 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 3", shape=rectangle]
	layer0_mlp_fc2_device3 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 3", shape=rectangle]
	layer0_mlp_fc1_device4 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 4", shape=rectangle]
	layer0_mlp_fc2_device4 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 4", shape=rectangle]
	layer0_mlp_fc1_device5 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 5", shape=rectangle]
	layer0_mlp_fc2_device5 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 5", shape=rectangle]
	layer0_mlp_fc1_device6 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 6", shape=rectangle]
	layer0_mlp_fc2_device6 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 6", shape=rectangle]
	layer0_mlp_fc1_device7 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 7", shape=rectangle]
	layer0_mlp_fc2_device7 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 7", shape=rectangle]
	layer0_mlp_fc1_device8 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 8", shape=rectangle]
	layer0_mlp_fc2_device8 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 8", shape=rectangle]
	layer0_mlp_fc1_device9 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 9", shape=rectangle]
	layer0_mlp_fc2_device9 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 9", shape=rectangle]
	layer0_mlp_fc1_device10 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 10", shape=rectangle]
	layer0_mlp_fc2_device10 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 10", shape=rectangle]
	layer0_mlp_fc1_device11 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 11", shape=rectangle]
	layer0_mlp_fc2_device11 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 11", shape=rectangle]
	layer0_mlp_fc1_device12 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 12", shape=rectangle]
	layer0_mlp_fc2_device12 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 12", shape=rectangle]
	layer0_mlp_fc1_device13 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 13", shape=rectangle]
	layer0_mlp_fc2_device13 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 13", shape=rectangle]
	layer0_mlp_fc1_device14 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 14", shape=rectangle]
	layer0_mlp_fc2_device14 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 14", shape=rectangle]
	layer0_mlp_fc1_device15 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 15", shape=rectangle]
	layer0_mlp_fc2_device15 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 15", shape=rectangle]
	layer0_mlp_agg [fillcolor=orange, label="MLP Concat\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=parallelogram]
	layer0_res2 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=ellipse]
	layer1_ln1 [fillcolor=lightcoral, label="LayerNorm1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nAll 16 devices", shape=rectangle]
	layer1_qkv_device0 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 0\nHeads: 0-7, Dim: 0-31", shape=rectangle]
	layer1_attn_device0 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 0", shape=rectangle]
	layer1_out_proj_device0 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 0", shape=rectangle]
	layer1_qkv_device1 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 1\nHeads: 0-7, Dim: 32-63", shape=rectangle]
	layer1_attn_device1 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 1", shape=rectangle]
	layer1_out_proj_device1 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 1", shape=rectangle]
	layer1_qkv_device2 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 2\nHeads: 0-7, Dim: 64-95", shape=rectangle]
	layer1_attn_device2 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 2", shape=rectangle]
	layer1_out_proj_device2 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 2", shape=rectangle]
	layer1_qkv_device3 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 3\nHeads: 0-7, Dim: 96-127", shape=rectangle]
	layer1_attn_device3 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 3", shape=rectangle]
	layer1_out_proj_device3 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 3", shape=rectangle]
	layer1_qkv_device4 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 4\nHeads: 8-15, Dim: 0-31", shape=rectangle]
	layer1_attn_device4 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 4", shape=rectangle]
	layer1_out_proj_device4 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 4", shape=rectangle]
	layer1_qkv_device5 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 5\nHeads: 8-15, Dim: 32-63", shape=rectangle]
	layer1_attn_device5 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 5", shape=rectangle]
	layer1_out_proj_device5 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 5", shape=rectangle]
	layer1_qkv_device6 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 6\nHeads: 8-15, Dim: 64-95", shape=rectangle]
	layer1_attn_device6 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 6", shape=rectangle]
	layer1_out_proj_device6 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 6", shape=rectangle]
	layer1_qkv_device7 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 7\nHeads: 8-15, Dim: 96-127", shape=rectangle]
	layer1_attn_device7 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 7", shape=rectangle]
	layer1_out_proj_device7 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 7", shape=rectangle]
	layer1_qkv_device8 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 8\nHeads: 16-23, Dim: 0-31", shape=rectangle]
	layer1_attn_device8 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 8", shape=rectangle]
	layer1_out_proj_device8 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 8", shape=rectangle]
	layer1_qkv_device9 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 9\nHeads: 16-23, Dim: 32-63", shape=rectangle]
	layer1_attn_device9 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 9", shape=rectangle]
	layer1_out_proj_device9 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 9", shape=rectangle]
	layer1_qkv_device10 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 10\nHeads: 16-23, Dim: 64-95", shape=rectangle]
	layer1_attn_device10 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 10", shape=rectangle]
	layer1_out_proj_device10 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 10", shape=rectangle]
	layer1_qkv_device11 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 11\nHeads: 16-23, Dim: 96-127", shape=rectangle]
	layer1_attn_device11 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 11", shape=rectangle]
	layer1_out_proj_device11 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 11", shape=rectangle]
	layer1_qkv_device12 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 12\nHeads: 24-31, Dim: 0-31", shape=rectangle]
	layer1_attn_device12 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 12", shape=rectangle]
	layer1_out_proj_device12 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 12", shape=rectangle]
	layer1_qkv_device13 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 13\nHeads: 24-31, Dim: 32-63", shape=rectangle]
	layer1_attn_device13 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 13", shape=rectangle]
	layer1_out_proj_device13 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 13", shape=rectangle]
	layer1_qkv_device14 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 14\nHeads: 24-31, Dim: 64-95", shape=rectangle]
	layer1_attn_device14 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 14", shape=rectangle]
	layer1_out_proj_device14 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 14", shape=rectangle]
	layer1_qkv_device15 [label="QKV Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 15\nHeads: 24-31, Dim: 96-127", shape=rectangle]
	layer1_attn_device15 [label="Attention\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 15", shape=rectangle]
	layer1_out_proj_device15 [label="Output Projection\n[batch_size=128, seq_len=10000, heads=8, head_dim=32]\nDevice: 15", shape=rectangle]
	layer1_agg_dim_group0 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 0", shape=parallelogram]
	layer1_agg_dim_group1 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 1", shape=parallelogram]
	layer1_agg_dim_group2 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 2", shape=parallelogram]
	layer1_agg_dim_group3 [fillcolor=orange, label="Dimension Concat\n[batch_size=128, seq_len=10000, heads=8, head_dim=128]\nHead Group: 3", shape=parallelogram]
	layer1_agg_heads [fillcolor=orange, label="Head Concat\n[batch_size=128, seq_len=10000, heads=32, head_dim=128]", shape=parallelogram]
	layer1_res1 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=ellipse]
	layer1_ln2 [fillcolor=lightcoral, label="LayerNorm2\n[batch_size=128, seq_len=10000, hidden_size=4096]\nAll 16 devices", shape=rectangle]
	layer1_mlp_fc1_device0 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 0", shape=rectangle]
	layer1_mlp_fc2_device0 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 0", shape=rectangle]
	layer1_mlp_fc1_device1 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 1", shape=rectangle]
	layer1_mlp_fc2_device1 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 1", shape=rectangle]
	layer1_mlp_fc1_device2 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 2", shape=rectangle]
	layer1_mlp_fc2_device2 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 2", shape=rectangle]
	layer1_mlp_fc1_device3 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 3", shape=rectangle]
	layer1_mlp_fc2_device3 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 3", shape=rectangle]
	layer1_mlp_fc1_device4 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 4", shape=rectangle]
	layer1_mlp_fc2_device4 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 4", shape=rectangle]
	layer1_mlp_fc1_device5 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 5", shape=rectangle]
	layer1_mlp_fc2_device5 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 5", shape=rectangle]
	layer1_mlp_fc1_device6 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 6", shape=rectangle]
	layer1_mlp_fc2_device6 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 6", shape=rectangle]
	layer1_mlp_fc1_device7 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 7", shape=rectangle]
	layer1_mlp_fc2_device7 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 7", shape=rectangle]
	layer1_mlp_fc1_device8 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 8", shape=rectangle]
	layer1_mlp_fc2_device8 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 8", shape=rectangle]
	layer1_mlp_fc1_device9 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 9", shape=rectangle]
	layer1_mlp_fc2_device9 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 9", shape=rectangle]
	layer1_mlp_fc1_device10 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 10", shape=rectangle]
	layer1_mlp_fc2_device10 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 10", shape=rectangle]
	layer1_mlp_fc1_device11 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 11", shape=rectangle]
	layer1_mlp_fc2_device11 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 11", shape=rectangle]
	layer1_mlp_fc1_device12 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 12", shape=rectangle]
	layer1_mlp_fc2_device12 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 12", shape=rectangle]
	layer1_mlp_fc1_device13 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 13", shape=rectangle]
	layer1_mlp_fc2_device13 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 13", shape=rectangle]
	layer1_mlp_fc1_device14 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 14", shape=rectangle]
	layer1_mlp_fc2_device14 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 14", shape=rectangle]
	layer1_mlp_fc1_device15 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=2048]\nDevice: 15", shape=rectangle]
	layer1_mlp_fc2_device15 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=256]\nDevice: 15", shape=rectangle]
	layer1_mlp_agg [fillcolor=orange, label="MLP Concat\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=parallelogram]
	layer1_res2 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=ellipse]
	output [fillcolor=lightgreen, label="Output\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=parallelogram]
	input -> layer0_ln1
	layer0_ln1 -> layer0_qkv_device0
	layer0_qkv_device0 -> layer0_attn_device0
	layer0_attn_device0 -> layer0_out_proj_device0
	layer0_out_proj_device0 -> layer0_agg_dim_group0
	layer0_ln1 -> layer0_qkv_device1
	layer0_qkv_device1 -> layer0_attn_device1
	layer0_attn_device1 -> layer0_out_proj_device1
	layer0_out_proj_device1 -> layer0_agg_dim_group0
	layer0_ln1 -> layer0_qkv_device2
	layer0_qkv_device2 -> layer0_attn_device2
	layer0_attn_device2 -> layer0_out_proj_device2
	layer0_out_proj_device2 -> layer0_agg_dim_group0
	layer0_ln1 -> layer0_qkv_device3
	layer0_qkv_device3 -> layer0_attn_device3
	layer0_attn_device3 -> layer0_out_proj_device3
	layer0_out_proj_device3 -> layer0_agg_dim_group0
	layer0_ln1 -> layer0_qkv_device4
	layer0_qkv_device4 -> layer0_attn_device4
	layer0_attn_device4 -> layer0_out_proj_device4
	layer0_out_proj_device4 -> layer0_agg_dim_group1
	layer0_ln1 -> layer0_qkv_device5
	layer0_qkv_device5 -> layer0_attn_device5
	layer0_attn_device5 -> layer0_out_proj_device5
	layer0_out_proj_device5 -> layer0_agg_dim_group1
	layer0_ln1 -> layer0_qkv_device6
	layer0_qkv_device6 -> layer0_attn_device6
	layer0_attn_device6 -> layer0_out_proj_device6
	layer0_out_proj_device6 -> layer0_agg_dim_group1
	layer0_ln1 -> layer0_qkv_device7
	layer0_qkv_device7 -> layer0_attn_device7
	layer0_attn_device7 -> layer0_out_proj_device7
	layer0_out_proj_device7 -> layer0_agg_dim_group1
	layer0_ln1 -> layer0_qkv_device8
	layer0_qkv_device8 -> layer0_attn_device8
	layer0_attn_device8 -> layer0_out_proj_device8
	layer0_out_proj_device8 -> layer0_agg_dim_group2
	layer0_ln1 -> layer0_qkv_device9
	layer0_qkv_device9 -> layer0_attn_device9
	layer0_attn_device9 -> layer0_out_proj_device9
	layer0_out_proj_device9 -> layer0_agg_dim_group2
	layer0_ln1 -> layer0_qkv_device10
	layer0_qkv_device10 -> layer0_attn_device10
	layer0_attn_device10 -> layer0_out_proj_device10
	layer0_out_proj_device10 -> layer0_agg_dim_group2
	layer0_ln1 -> layer0_qkv_device11
	layer0_qkv_device11 -> layer0_attn_device11
	layer0_attn_device11 -> layer0_out_proj_device11
	layer0_out_proj_device11 -> layer0_agg_dim_group2
	layer0_ln1 -> layer0_qkv_device12
	layer0_qkv_device12 -> layer0_attn_device12
	layer0_attn_device12 -> layer0_out_proj_device12
	layer0_out_proj_device12 -> layer0_agg_dim_group3
	layer0_ln1 -> layer0_qkv_device13
	layer0_qkv_device13 -> layer0_attn_device13
	layer0_attn_device13 -> layer0_out_proj_device13
	layer0_out_proj_device13 -> layer0_agg_dim_group3
	layer0_ln1 -> layer0_qkv_device14
	layer0_qkv_device14 -> layer0_attn_device14
	layer0_attn_device14 -> layer0_out_proj_device14
	layer0_out_proj_device14 -> layer0_agg_dim_group3
	layer0_ln1 -> layer0_qkv_device15
	layer0_qkv_device15 -> layer0_attn_device15
	layer0_attn_device15 -> layer0_out_proj_device15
	layer0_out_proj_device15 -> layer0_agg_dim_group3
	layer0_agg_dim_group0 -> layer0_agg_heads
	layer0_agg_dim_group1 -> layer0_agg_heads
	layer0_agg_dim_group2 -> layer0_agg_heads
	layer0_agg_dim_group3 -> layer0_agg_heads
	layer0_agg_heads -> layer0_res1
	input -> layer0_res1 [style=dashed]
	layer0_res1 -> layer0_ln2
	layer0_ln2 -> layer0_mlp_fc1_device0
	layer0_mlp_fc1_device0 -> layer0_mlp_fc2_device0
	layer0_mlp_fc2_device0 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device1
	layer0_mlp_fc1_device1 -> layer0_mlp_fc2_device1
	layer0_mlp_fc2_device1 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device2
	layer0_mlp_fc1_device2 -> layer0_mlp_fc2_device2
	layer0_mlp_fc2_device2 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device3
	layer0_mlp_fc1_device3 -> layer0_mlp_fc2_device3
	layer0_mlp_fc2_device3 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device4
	layer0_mlp_fc1_device4 -> layer0_mlp_fc2_device4
	layer0_mlp_fc2_device4 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device5
	layer0_mlp_fc1_device5 -> layer0_mlp_fc2_device5
	layer0_mlp_fc2_device5 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device6
	layer0_mlp_fc1_device6 -> layer0_mlp_fc2_device6
	layer0_mlp_fc2_device6 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device7
	layer0_mlp_fc1_device7 -> layer0_mlp_fc2_device7
	layer0_mlp_fc2_device7 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device8
	layer0_mlp_fc1_device8 -> layer0_mlp_fc2_device8
	layer0_mlp_fc2_device8 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device9
	layer0_mlp_fc1_device9 -> layer0_mlp_fc2_device9
	layer0_mlp_fc2_device9 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device10
	layer0_mlp_fc1_device10 -> layer0_mlp_fc2_device10
	layer0_mlp_fc2_device10 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device11
	layer0_mlp_fc1_device11 -> layer0_mlp_fc2_device11
	layer0_mlp_fc2_device11 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device12
	layer0_mlp_fc1_device12 -> layer0_mlp_fc2_device12
	layer0_mlp_fc2_device12 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device13
	layer0_mlp_fc1_device13 -> layer0_mlp_fc2_device13
	layer0_mlp_fc2_device13 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device14
	layer0_mlp_fc1_device14 -> layer0_mlp_fc2_device14
	layer0_mlp_fc2_device14 -> layer0_mlp_agg
	layer0_ln2 -> layer0_mlp_fc1_device15
	layer0_mlp_fc1_device15 -> layer0_mlp_fc2_device15
	layer0_mlp_fc2_device15 -> layer0_mlp_agg
	layer0_mlp_agg -> layer0_res2
	layer0_res1 -> layer0_res2 [style=dashed]
	layer0_res2 -> layer1_ln1
	layer1_ln1 -> layer1_qkv_device0
	layer1_qkv_device0 -> layer1_attn_device0
	layer1_attn_device0 -> layer1_out_proj_device0
	layer1_out_proj_device0 -> layer1_agg_dim_group0
	layer1_ln1 -> layer1_qkv_device1
	layer1_qkv_device1 -> layer1_attn_device1
	layer1_attn_device1 -> layer1_out_proj_device1
	layer1_out_proj_device1 -> layer1_agg_dim_group0
	layer1_ln1 -> layer1_qkv_device2
	layer1_qkv_device2 -> layer1_attn_device2
	layer1_attn_device2 -> layer1_out_proj_device2
	layer1_out_proj_device2 -> layer1_agg_dim_group0
	layer1_ln1 -> layer1_qkv_device3
	layer1_qkv_device3 -> layer1_attn_device3
	layer1_attn_device3 -> layer1_out_proj_device3
	layer1_out_proj_device3 -> layer1_agg_dim_group0
	layer1_ln1 -> layer1_qkv_device4
	layer1_qkv_device4 -> layer1_attn_device4
	layer1_attn_device4 -> layer1_out_proj_device4
	layer1_out_proj_device4 -> layer1_agg_dim_group1
	layer1_ln1 -> layer1_qkv_device5
	layer1_qkv_device5 -> layer1_attn_device5
	layer1_attn_device5 -> layer1_out_proj_device5
	layer1_out_proj_device5 -> layer1_agg_dim_group1
	layer1_ln1 -> layer1_qkv_device6
	layer1_qkv_device6 -> layer1_attn_device6
	layer1_attn_device6 -> layer1_out_proj_device6
	layer1_out_proj_device6 -> layer1_agg_dim_group1
	layer1_ln1 -> layer1_qkv_device7
	layer1_qkv_device7 -> layer1_attn_device7
	layer1_attn_device7 -> layer1_out_proj_device7
	layer1_out_proj_device7 -> layer1_agg_dim_group1
	layer1_ln1 -> layer1_qkv_device8
	layer1_qkv_device8 -> layer1_attn_device8
	layer1_attn_device8 -> layer1_out_proj_device8
	layer1_out_proj_device8 -> layer1_agg_dim_group2
	layer1_ln1 -> layer1_qkv_device9
	layer1_qkv_device9 -> layer1_attn_device9
	layer1_attn_device9 -> layer1_out_proj_device9
	layer1_out_proj_device9 -> layer1_agg_dim_group2
	layer1_ln1 -> layer1_qkv_device10
	layer1_qkv_device10 -> layer1_attn_device10
	layer1_attn_device10 -> layer1_out_proj_device10
	layer1_out_proj_device10 -> layer1_agg_dim_group2
	layer1_ln1 -> layer1_qkv_device11
	layer1_qkv_device11 -> layer1_attn_device11
	layer1_attn_device11 -> layer1_out_proj_device11
	layer1_out_proj_device11 -> layer1_agg_dim_group2
	layer1_ln1 -> layer1_qkv_device12
	layer1_qkv_device12 -> layer1_attn_device12
	layer1_attn_device12 -> layer1_out_proj_device12
	layer1_out_proj_device12 -> layer1_agg_dim_group3
	layer1_ln1 -> layer1_qkv_device13
	layer1_qkv_device13 -> layer1_attn_device13
	layer1_attn_device13 -> layer1_out_proj_device13
	layer1_out_proj_device13 -> layer1_agg_dim_group3
	layer1_ln1 -> layer1_qkv_device14
	layer1_qkv_device14 -> layer1_attn_device14
	layer1_attn_device14 -> layer1_out_proj_device14
	layer1_out_proj_device14 -> layer1_agg_dim_group3
	layer1_ln1 -> layer1_qkv_device15
	layer1_qkv_device15 -> layer1_attn_device15
	layer1_attn_device15 -> layer1_out_proj_device15
	layer1_out_proj_device15 -> layer1_agg_dim_group3
	layer1_agg_dim_group0 -> layer1_agg_heads
	layer1_agg_dim_group1 -> layer1_agg_heads
	layer1_agg_dim_group2 -> layer1_agg_heads
	layer1_agg_dim_group3 -> layer1_agg_heads
	layer1_agg_heads -> layer1_res1
	layer1_ln1 -> layer1_res1 [style=dashed]
	layer1_res1 -> layer1_ln2
	layer1_ln2 -> layer1_mlp_fc1_device0
	layer1_mlp_fc1_device0 -> layer1_mlp_fc2_device0
	layer1_mlp_fc2_device0 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device1
	layer1_mlp_fc1_device1 -> layer1_mlp_fc2_device1
	layer1_mlp_fc2_device1 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device2
	layer1_mlp_fc1_device2 -> layer1_mlp_fc2_device2
	layer1_mlp_fc2_device2 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device3
	layer1_mlp_fc1_device3 -> layer1_mlp_fc2_device3
	layer1_mlp_fc2_device3 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device4
	layer1_mlp_fc1_device4 -> layer1_mlp_fc2_device4
	layer1_mlp_fc2_device4 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device5
	layer1_mlp_fc1_device5 -> layer1_mlp_fc2_device5
	layer1_mlp_fc2_device5 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device6
	layer1_mlp_fc1_device6 -> layer1_mlp_fc2_device6
	layer1_mlp_fc2_device6 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device7
	layer1_mlp_fc1_device7 -> layer1_mlp_fc2_device7
	layer1_mlp_fc2_device7 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device8
	layer1_mlp_fc1_device8 -> layer1_mlp_fc2_device8
	layer1_mlp_fc2_device8 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device9
	layer1_mlp_fc1_device9 -> layer1_mlp_fc2_device9
	layer1_mlp_fc2_device9 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device10
	layer1_mlp_fc1_device10 -> layer1_mlp_fc2_device10
	layer1_mlp_fc2_device10 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device11
	layer1_mlp_fc1_device11 -> layer1_mlp_fc2_device11
	layer1_mlp_fc2_device11 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device12
	layer1_mlp_fc1_device12 -> layer1_mlp_fc2_device12
	layer1_mlp_fc2_device12 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device13
	layer1_mlp_fc1_device13 -> layer1_mlp_fc2_device13
	layer1_mlp_fc2_device13 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device14
	layer1_mlp_fc1_device14 -> layer1_mlp_fc2_device14
	layer1_mlp_fc2_device14 -> layer1_mlp_agg
	layer1_ln2 -> layer1_mlp_fc1_device15
	layer1_mlp_fc1_device15 -> layer1_mlp_fc2_device15
	layer1_mlp_fc2_device15 -> layer1_mlp_agg
	layer1_mlp_agg -> layer1_res2
	layer1_res1 -> layer1_res2 [style=dashed]
	layer1_res2 -> output
}