digraph Proposed_Large_EP16_Final {
    rankdir=TB;
    splines=ortho;
    node [fontname="Arial", fontsize=10];
    
    // Model Input
    input [label="Model Input\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
           shape=box, style=filled, fillcolor=lightblue];
    
    // Layer 0
    layer0_mha [label="MHA Layer 0\nAll 16 GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                shape=rectangle, style=filled, fillcolor=lightcyan];
    layer0_ln1 [label="LayerNorm MHA 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                shape=rectangle];
    layer0_add1 [label="MHA Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                 shape=rectangle, style=filled, fillcolor=yellow];
    
    layer0_gate [label="Gate Layer 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", 
                 shape=parallelogram, style=filled, fillcolor=lightgreen];
    
    // Expert Cluster 0 (4 representative experts shown with [repeated 4x])
    layer0_route_gpu0 [label="Route to Expert 0\nGPU 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", 
                       shape=ellipse, style=dashed, fillcolor=orange];
    layer0_expert0 [label="MLP Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", 
                    shape=rectangle, style=filled, fillcolor=lightcoral];
    layer0_agg_gpu0 [label="Aggregate from Expert 0\nGPU 0\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                       shape=ellipse, style=filled, fillcolor=lightpink];
    
    layer0_route_gpu1 [label="Route to Expert 1\nGPU 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", 
                       shape=ellipse, style=dashed, fillcolor=orange];
    layer0_expert1 [label="MLP Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", 
                    shape=rectangle, style=filled, fillcolor=lightcoral];
    layer0_agg_gpu1 [label="Aggregate from Expert 1\nGPU 1\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                       shape=ellipse, style=filled, fillcolor=lightpink];
    
    layer0_route_gpu15 [label="Route to Expert 15\nGPU 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", 
                        shape=ellipse, style=dashed, fillcolor=orange];
    layer0_expert15 [label="MLP Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [tokens_per_expert, hidden_dim=4096]", 
                     shape=rectangle, style=filled, fillcolor=lightcoral];
    layer0_agg_gpu15 [label="Aggregate from Expert 15\nGPU 15\nInput: [tokens_per_expert, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                        shape=ellipse, style=filled, fillcolor=lightpink];
    
    layer0_moe_agg [label="MoE Final Aggregation 0\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                    shape=ellipse, style=filled, fillcolor=gold];
    layer0_ln2 [label="LayerNorm MoE 0\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                shape=rectangle];
    layer0_output [label="MoE Residual Add 0\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                   shape=rectangle, style=filled, fillcolor=yellow];
    
    // Layer 1
    layer1_mha [label="MHA Layer 1\nAll 16 GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                shape=rectangle, style=filled, fillcolor=lightcyan];
    layer1_ln1 [label="LayerNorm MHA 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                shape=rectangle];
    layer1_add1 [label="MHA Residual Add 1\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                 shape=rectangle, style=filled, fillcolor=yellow];
    layer1_gate [label="Gate Layer 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", 
                 shape=parallelogram, style=filled, fillcolor=lightgreen];
    layer1_moe_agg [label="MoE Final Aggregation 1\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                    shape=ellipse, style=filled, fillcolor=gold];
    layer1_ln2 [label="LayerNorm MoE 1\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                shape=rectangle];
    layer1_output [label="MoE Residual Add 1\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                   shape=rectangle, style=filled, fillcolor=yellow];
    
    // Layer 15 (final layer)
    layer15_mha [label="MHA Layer 15\nAll 16 GPUs\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                 shape=rectangle, style=filled, fillcolor=lightcyan];
    layer15_ln1 [label="LayerNorm MHA 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                 shape=rectangle];
    layer15_add1 [label="MHA Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                  shape=rectangle, style=filled, fillcolor=yellow];
    layer15_gate [label="Gate Layer 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, num_experts=16]", 
                  shape=parallelogram, style=filled, fillcolor=lightgreen];
    layer15_moe_agg [label="MoE Final Aggregation 15\nInput: 16×[batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                     shape=ellipse, style=filled, fillcolor=gold];
    layer15_ln2 [label="LayerNorm MoE 15\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                 shape=rectangle];
    layer15_output [label="MoE Residual Add 15\nInput1: [batch_size=128, seq_len=10000, hidden_dim=4096]\nInput2: [batch_size=128, seq_len=10000, hidden_dim=4096]\nOutput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
                    shape=rectangle, style=filled, fillcolor=yellow];
    
    // Hidden layers 2-14 (represented by cluster)
    hidden_layers [label="Layers 2-14 (13 layers)\n[Repeating pattern - 16 experts/GPU]", 
                   shape=rectangle, style=dashed, fillcolor=lightgray];
    layer1_output -> hidden_layers;
    hidden_layers -> layer15_mha;
    
    // Connections
    input -> layer0_mha;
    layer0_mha -> layer0_ln1;
    layer0_ln1 -> layer0_add1;
    input -> layer0_add1;
    layer0_add1 -> layer0_gate;
    layer0_gate -> layer0_route_gpu0;
    layer0_gate -> layer0_route_gpu1;
    layer0_gate -> layer0_route_gpu15;
    layer0_route_gpu0 -> layer0_expert0;
    layer0_route_gpu1 -> layer0_expert1;
    layer0_route_gpu15 -> layer0_expert15;
    layer0_expert0 -> layer0_agg_gpu0;
    layer0_expert1 -> layer0_agg_gpu1;
    layer0_expert15 -> layer0_agg_gpu15;
    layer0_agg_gpu0 -> layer0_moe_agg;
    layer0_agg_gpu1 -> layer0_moe_agg;
    layer0_agg_gpu15 -> layer0_moe_agg;
    layer0_moe_agg -> layer0_ln2;
    layer0_ln2 -> layer0_output;
    layer0_add1 -> layer0_output;
    
    // Connections - Layer 1
    layer0_output -> layer1_mha;
    layer1_mha -> layer1_ln1;
    layer1_ln1 -> layer1_add1;
    layer0_output -> layer1_add1;
    layer1_add1 -> layer1_gate;
    layer1_gate -> layer1_moe_agg;
    layer1_moe_agg -> layer1_ln2;
    layer1_ln2 -> layer1_output;
    layer1_add1 -> layer1_output;
    
    // Connections - Layer 15
    layer14_output -> layer15_mha;
    layer15_mha -> layer15_ln1;
    layer15_ln1 -> layer15_add1;
    layer14_output -> layer15_add1;
    layer15_add1 -> layer15_gate;
    layer15_gate -> layer15_moe_agg;
    layer15_moe_agg -> layer15_ln2;
    layer15_ln2 -> layer15_output;
    layer15_add1 -> layer15_output;
    
    // Final output
    output [label="Model Output\nInput: [batch_size=128, seq_len=10000, hidden_dim=4096]", 
            shape=box, style=filled, fillcolor=lightblue];
    layer15_output -> output;
}