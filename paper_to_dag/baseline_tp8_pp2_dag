digraph Baseline_TP8_PP2 {
	fontname=Arial fontsize=12 rankdir=TB
	node [fillcolor=lightblue shape=ellipse style=filled]
	node [fillcolor=lightgreen shape=rectangle style=filled]
	node [fillcolor=yellow shape=parallelogram style=filled]
	node [fillcolor=orange shape=diamond style=filled]
	input [label="Input\n[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightblue shape=ellipse]
	node [fillcolor=lightcoral shape=rectangle style=filled]
	layer_0_mha_tp8 [label="Layer 0 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_0_mlp_tp8 [label="Layer 0 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	input -> layer_0_mha_tp8
	layer_0_mha_tp8 -> layer_0_mlp_tp8
	layer_1_mha_tp8 [label="Layer 1 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_1_mlp_tp8 [label="Layer 1 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_0_mlp_tp8 -> layer_1_mha_tp8
	layer_1_mha_tp8 -> layer_1_mlp_tp8
	layer_2_mha_tp8 [label="Layer 2 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_2_mlp_tp8 [label="Layer 2 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_1_mlp_tp8 -> layer_2_mha_tp8
	layer_2_mha_tp8 -> layer_2_mlp_tp8
	layer_3_mha_tp8 [label="Layer 3 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_3_mlp_tp8 [label="Layer 3 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_2_mlp_tp8 -> layer_3_mha_tp8
	layer_3_mha_tp8 -> layer_3_mlp_tp8
	layer_4_mha_tp8 [label="Layer 4 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_4_mlp_tp8 [label="Layer 4 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_3_mlp_tp8 -> layer_4_mha_tp8
	layer_4_mha_tp8 -> layer_4_mlp_tp8
	layer_5_mha_tp8 [label="Layer 5 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_5_mlp_tp8 [label="Layer 5 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_4_mlp_tp8 -> layer_5_mha_tp8
	layer_5_mha_tp8 -> layer_5_mlp_tp8
	layer_6_mha_tp8 [label="Layer 6 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_6_mlp_tp8 [label="Layer 6 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_5_mlp_tp8 -> layer_6_mha_tp8
	layer_6_mha_tp8 -> layer_6_mlp_tp8
	layer_7_mha_tp8 [label="Layer 7 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_7_mlp_tp8 [label="Layer 7 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightcoral shape=rectangle]
	layer_6_mlp_tp8 -> layer_7_mha_tp8
	layer_7_mha_tp8 -> layer_7_mlp_tp8
	stage0_to_stage1 [label="Pipeline Communication\nStage 0 → Stage 1\nGPU 7 → GPU 8" fillcolor=orange shape=diamond]
	layer_7_mlp_tp8 -> stage0_to_stage1
	node [fillcolor=lightsteelblue shape=rectangle style=filled]
	layer_8_mha_tp8 [label="Layer 8 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_8_mlp_tp8 [label="Layer 8 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	stage0_to_stage1 -> layer_8_mha_tp8
	layer_8_mha_tp8 -> layer_8_mlp_tp8
	layer_9_mha_tp8 [label="Layer 9 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_9_mlp_tp8 [label="Layer 9 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_8_mlp_tp8 -> layer_9_mha_tp8
	layer_9_mha_tp8 -> layer_9_mlp_tp8
	layer_10_mha_tp8 [label="Layer 10 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_10_mlp_tp8 [label="Layer 10 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_9_mlp_tp8 -> layer_10_mha_tp8
	layer_10_mha_tp8 -> layer_10_mlp_tp8
	layer_11_mha_tp8 [label="Layer 11 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_11_mlp_tp8 [label="Layer 11 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_10_mlp_tp8 -> layer_11_mha_tp8
	layer_11_mha_tp8 -> layer_11_mlp_tp8
	layer_12_mha_tp8 [label="Layer 12 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_12_mlp_tp8 [label="Layer 12 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_11_mlp_tp8 -> layer_12_mha_tp8
	layer_12_mha_tp8 -> layer_12_mlp_tp8
	layer_13_mha_tp8 [label="Layer 13 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_13_mlp_tp8 [label="Layer 13 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_12_mlp_tp8 -> layer_13_mha_tp8
	layer_13_mha_tp8 -> layer_13_mlp_tp8
	layer_14_mha_tp8 [label="Layer 14 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_14_mlp_tp8 [label="Layer 14 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_13_mlp_tp8 -> layer_14_mha_tp8
	layer_14_mha_tp8 -> layer_14_mlp_tp8
	layer_15_mha_tp8 [label="Layer 15 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_15_mlp_tp8 [label="Layer 15 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightsteelblue shape=rectangle]
	layer_14_mlp_tp8 -> layer_15_mha_tp8
	layer_15_mha_tp8 -> layer_15_mlp_tp8
	output [label="Output\n[batch_size=128, seq_len=10000, d_model=4096]" fillcolor=lightblue shape=ellipse]
	layer_15_mlp_tp8 -> output
}
