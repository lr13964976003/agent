digraph Baseline_TP8_PP2 {
	graph [bb="0,0,747,3853.5",
		fontname=Arial,
		fontsize=12,
		rankdir=TB
	];
	node [fillcolor=lightsteelblue,
		label="\N",
		shape=rectangle,
		style=filled
	];
	input	[fillcolor=lightblue,
		height=0.74639,
		label="Input\n[batch_size=128, seq_len=10000, d_model=4096]",
		pos="373.5,3826.6",
		shape=ellipse,
		width=7.3853];
	layer_0_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 0 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,3729.7",
		width=10.375];
	input -> layer_0_mha_tp8	[pos="e,373.5,3763.8 373.5,3799.7 373.5,3791.7 373.5,3782.7 373.5,3773.9"];
	layer_0_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 0 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,3618.2",
		width=10.375];
	layer_0_mha_tp8 -> layer_0_mlp_tp8	[pos="e,373.5,3660 373.5,3695.6 373.5,3687.5 373.5,3678.7 373.5,3670.1"];
	layer_1_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 1 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,3506.7",
		width=10.375];
	layer_0_mlp_tp8 -> layer_1_mha_tp8	[pos="e,373.5,3541 373.5,3576.5 373.5,3568.3 373.5,3559.6 373.5,3551.3"];
	layer_1_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 1 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,3395.2",
		width=10.375];
	layer_1_mha_tp8 -> layer_1_mlp_tp8	[pos="e,373.5,3437 373.5,3472.6 373.5,3464.5 373.5,3455.7 373.5,3447.1"];
	layer_2_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 2 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,3283.7",
		width=10.375];
	layer_1_mlp_tp8 -> layer_2_mha_tp8	[pos="e,373.5,3318 373.5,3353.5 373.5,3345.3 373.5,3336.6 373.5,3328.3"];
	layer_2_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 2 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,3172.2",
		width=10.375];
	layer_2_mha_tp8 -> layer_2_mlp_tp8	[pos="e,373.5,3214 373.5,3249.6 373.5,3241.5 373.5,3232.7 373.5,3224.1"];
	layer_3_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 3 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,3060.7",
		width=10.375];
	layer_2_mlp_tp8 -> layer_3_mha_tp8	[pos="e,373.5,3095 373.5,3130.5 373.5,3122.3 373.5,3113.6 373.5,3105.3"];
	layer_3_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 3 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,2949.2",
		width=10.375];
	layer_3_mha_tp8 -> layer_3_mlp_tp8	[pos="e,373.5,2991 373.5,3026.6 373.5,3018.5 373.5,3009.7 373.5,3001.1"];
	layer_4_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 4 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,2837.7",
		width=10.375];
	layer_3_mlp_tp8 -> layer_4_mha_tp8	[pos="e,373.5,2872 373.5,2907.5 373.5,2899.3 373.5,2890.6 373.5,2882.3"];
	layer_4_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 4 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,2726.2",
		width=10.375];
	layer_4_mha_tp8 -> layer_4_mlp_tp8	[pos="e,373.5,2768 373.5,2803.6 373.5,2795.5 373.5,2786.7 373.5,2778.1"];
	layer_5_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 5 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,2614.7",
		width=10.375];
	layer_4_mlp_tp8 -> layer_5_mha_tp8	[pos="e,373.5,2649 373.5,2684.5 373.5,2676.3 373.5,2667.6 373.5,2659.3"];
	layer_5_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 5 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,2503.2",
		width=10.375];
	layer_5_mha_tp8 -> layer_5_mlp_tp8	[pos="e,373.5,2545 373.5,2580.6 373.5,2572.5 373.5,2563.7 373.5,2555.1"];
	layer_6_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 6 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,2391.7",
		width=10.375];
	layer_5_mlp_tp8 -> layer_6_mha_tp8	[pos="e,373.5,2426 373.5,2461.5 373.5,2453.3 373.5,2444.6 373.5,2436.3"];
	layer_6_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 6 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,2280.2",
		width=10.375];
	layer_6_mha_tp8 -> layer_6_mlp_tp8	[pos="e,373.5,2322 373.5,2357.6 373.5,2349.5 373.5,2340.7 373.5,2332.1"];
	layer_7_mha_tp8	[fillcolor=lightcoral,
		height=0.94444,
		label="Layer 7 MHA\nTensor Parallel=8\nGPU 0-7\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,2168.7",
		width=10.375];
	layer_6_mlp_tp8 -> layer_7_mha_tp8	[pos="e,373.5,2203 373.5,2238.5 373.5,2230.3 373.5,2221.6 373.5,2213.3"];
	layer_7_mlp_tp8	[fillcolor=lightcoral,
		height=1.1528,
		label="Layer 7 MLP+Experts\nTensor Parallel=8\nGPU 0-7\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,2057.2",
		width=10.375];
	layer_7_mha_tp8 -> layer_7_mlp_tp8	[pos="e,373.5,2099 373.5,2134.6 373.5,2126.5 373.5,2117.7 373.5,2109.1"];
	stage0_to_stage1	[fillcolor=orange,
		height=1.4722,
		label="Pipeline Communication\nStage 0 → Stage 1\nGPU 7 → GPU 8",
		pos="373.5,1926.7",
		shape=diamond,
		width=5.3056];
	layer_7_mlp_tp8 -> stage0_to_stage1	[pos="e,373.5,1979.9 373.5,2015.6 373.5,2007.4 373.5,1998.7 373.5,1990"];
	layer_8_mha_tp8	[height=0.94444,
		label="Layer 8 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,1803.7",
		width=10.375];
	stage0_to_stage1 -> layer_8_mha_tp8	[pos="e,373.5,1837.8 373.5,1873.7 373.5,1865.2 373.5,1856.4 373.5,1848.1"];
	layer_8_mlp_tp8	[height=1.1528,
		label="Layer 8 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,1692.2",
		width=10.375];
	layer_8_mha_tp8 -> layer_8_mlp_tp8	[pos="e,373.5,1734 373.5,1769.6 373.5,1761.5 373.5,1752.7 373.5,1744.1"];
	layer_9_mha_tp8	[height=0.94444,
		label="Layer 9 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,1580.7",
		width=10.375];
	layer_8_mlp_tp8 -> layer_9_mha_tp8	[pos="e,373.5,1615 373.5,1650.5 373.5,1642.3 373.5,1633.6 373.5,1625.3"];
	layer_9_mlp_tp8	[height=1.1528,
		label="Layer 9 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=\
128, seq_len=10000, d_model=4096]",
		pos="373.5,1469.2",
		width=10.375];
	layer_9_mha_tp8 -> layer_9_mlp_tp8	[pos="e,373.5,1511 373.5,1546.6 373.5,1538.5 373.5,1529.7 373.5,1521.1"];
	layer_10_mha_tp8	[height=0.94444,
		label="Layer 10 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,1357.7",
		width=10.375];
	layer_9_mlp_tp8 -> layer_10_mha_tp8	[pos="e,373.5,1392 373.5,1427.5 373.5,1419.3 373.5,1410.6 373.5,1402.3"];
	layer_10_mlp_tp8	[height=1.1528,
		label="Layer 10 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_\
size=128, seq_len=10000, d_model=4096]",
		pos="373.5,1246.2",
		width=10.375];
	layer_10_mha_tp8 -> layer_10_mlp_tp8	[pos="e,373.5,1288 373.5,1323.6 373.5,1315.5 373.5,1306.7 373.5,1298.1"];
	layer_11_mha_tp8	[height=0.94444,
		label="Layer 11 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,1134.7",
		width=10.375];
	layer_10_mlp_tp8 -> layer_11_mha_tp8	[pos="e,373.5,1169 373.5,1204.5 373.5,1196.3 373.5,1187.6 373.5,1179.3"];
	layer_11_mlp_tp8	[height=1.1528,
		label="Layer 11 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_\
size=128, seq_len=10000, d_model=4096]",
		pos="373.5,1023.2",
		width=10.375];
	layer_11_mha_tp8 -> layer_11_mlp_tp8	[pos="e,373.5,1065 373.5,1100.6 373.5,1092.5 373.5,1083.7 373.5,1075.1"];
	layer_12_mha_tp8	[height=0.94444,
		label="Layer 12 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,911.74",
		width=10.375];
	layer_11_mlp_tp8 -> layer_12_mha_tp8	[pos="e,373.5,946.04 373.5,981.47 373.5,973.26 373.5,964.6 373.5,956.3"];
	layer_12_mlp_tp8	[height=1.1528,
		label="Layer 12 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_\
size=128, seq_len=10000, d_model=4096]",
		pos="373.5,800.24",
		width=10.375];
	layer_12_mha_tp8 -> layer_12_mlp_tp8	[pos="e,373.5,842.02 373.5,877.59 373.5,869.54 373.5,860.74 373.5,852.08"];
	layer_13_mha_tp8	[height=0.94444,
		label="Layer 13 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,688.74",
		width=10.375];
	layer_12_mlp_tp8 -> layer_13_mha_tp8	[pos="e,373.5,723.04 373.5,758.47 373.5,750.26 373.5,741.6 373.5,733.3"];
	layer_13_mlp_tp8	[height=1.1528,
		label="Layer 13 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_\
size=128, seq_len=10000, d_model=4096]",
		pos="373.5,577.24",
		width=10.375];
	layer_13_mha_tp8 -> layer_13_mlp_tp8	[pos="e,373.5,619.02 373.5,654.59 373.5,646.54 373.5,637.74 373.5,629.08"];
	layer_14_mha_tp8	[height=0.94444,
		label="Layer 14 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,465.74",
		width=10.375];
	layer_13_mlp_tp8 -> layer_14_mha_tp8	[pos="e,373.5,500.04 373.5,535.47 373.5,527.26 373.5,518.6 373.5,510.3"];
	layer_14_mlp_tp8	[height=1.1528,
		label="Layer 14 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_\
size=128, seq_len=10000, d_model=4096]",
		pos="373.5,354.24",
		width=10.375];
	layer_14_mha_tp8 -> layer_14_mlp_tp8	[pos="e,373.5,396.02 373.5,431.59 373.5,423.54 373.5,414.74 373.5,406.08"];
	layer_15_mha_tp8	[height=0.94444,
		label="Layer 15 MHA\nTensor Parallel=8\nGPU 8-15\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_size=128, seq_len=10000, d_model=\
4096]",
		pos="373.5,242.74",
		width=10.375];
	layer_14_mlp_tp8 -> layer_15_mha_tp8	[pos="e,373.5,277.04 373.5,312.47 373.5,304.26 373.5,295.6 373.5,287.3"];
	layer_15_mlp_tp8	[height=1.1528,
		label="Layer 15 MLP+Experts\nTensor Parallel=8\nGPU 8-15\n16 experts colocated\n[batch_size=128, seq_len=10000, d_model=4096]→[batch_\
size=128, seq_len=10000, d_model=4096]",
		pos="373.5,131.24",
		width=10.375];
	layer_15_mha_tp8 -> layer_15_mlp_tp8	[pos="e,373.5,173.02 373.5,208.59 373.5,200.54 373.5,191.74 373.5,183.08"];
	output	[fillcolor=lightblue,
		height=0.74639,
		label="Output\n[batch_size=128, seq_len=10000, d_model=4096]",
		pos="373.5,26.87",
		shape=ellipse,
		width=7.3853];
	layer_15_mlp_tp8 -> output	[pos="e,373.5,53.764 373.5,89.487 373.5,81.018 373.5,72.153 373.5,63.87"];
}
