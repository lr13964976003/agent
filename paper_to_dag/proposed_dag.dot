digraph G {
    rankdir=TB;
    node [shape=rectangle, style=filled];
    
    // Title
    label="Proposed: Ring Attention + Sequence Parallelism (16 devices)";
    labelloc="t";
    fontsize=24;
    
    // Input node
    input [shape=ellipse, label="Input\nInput: [batch=1024, seq=10000, hidden=8192]\nGPU: Host"];
    
    // Sequence split across devices
    sequence_split [shape=parallelogram, label="Sequence Split\nInput: [batch=1024, seq=10000, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=8192] x 16\nGPU: All 16"];
    
    // ===== Layer 0 =====
    subgraph cluster_layer0 {
        label="Layer 0 (All Devices)";
        style=dashed;
        
        // Loop through all devices
        subgraph cluster_device0 {
            label="GPU 0 (Seq 0-624)";
            style=dotted;
            
            // Embedding for device 0
            emb0 [label="Embedding\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            
            // Layer Norm
            ln0_0 [label="LayerNorm\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            
            // QKV projections (full parameters, local computation)
            q_proj0_0 [label="Q Projection\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            k_proj0_0 [label="K Projection\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            v_proj0_0 [label="V Projection\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            
            // Ring attention components
            ring_attn0_0 [label="Local Q Computation\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, heads=16, dim=512]\nGPU: 0"];
            
            // KV ring exchange for 16 stages
            kv_send0_0 [shape=ellipse, label="Send KV\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: → GPU 1\nGPU: 0"];
            kv_recv0_0 [shape=ellipse, label="Recv KV\nInput: ← GPU 15\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            
            // Attention computation
            attn0_0 [label="Attention\nInput: Q[batch=1024, seq=625, heads=16, dim=512]\n       K[batch=1024, seq=625, heads=16, dim=512]\n       V[batch=1024, seq=625, heads=16, dim=512]\nOutput: [batch=1024, seq=625, heads=16, dim=512]\nGPU: 0"];
            
            // Output projection
            out_proj0_0 [label="Output Projection\nInput: [batch=1024, seq=625, heads=16, dim=512]\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            
            // Residual
            residual0_0 [shape=parallelogram, label="Add\nInput: [batch=1024, seq=625, hidden=8192] x2\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            
            // MLP
            gate0_0 [label="Gate Projection\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=32768]\nGPU: 0"];
            up0_0 [label="Up Projection\nInput: [batch=1024, seq=625, hidden=8192]\nOutput: [batch=1024, seq=625, hidden=32768]\nGPU: 0"];
            silu0_0 [label="SiLU\nInput: [batch=1024, seq=625, hidden=32768]\nOutput: [batch=1024, seq=625, hidden=32768]\nGPU: 0"];
            mul0_0 [label="Elementwise Mul\nInput: [batch=1024, seq=625, hidden=32768] x2\nOutput: [batch=1024, seq=625, hidden=32768]\nGPU: 0"];
            down0_0 [label="Down Projection\nInput: [batch=1024, seq=625, hidden=32768]\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
            residual_mlp0_0 [shape=parallelogram, label="Add\nInput: [batch=1024, seq=625, hidden=8192] x2\nOutput: [batch=1024, seq=625, hidden=8192]\nGPU: 0"];
        }
        
        // Repeat for device 1 (simplified representation)
        device1_rep [label="...\nGPU 1-15\n(Same structure as GPU 0)"];
    }
    
    // Ring topology connections
    ring_topology [shape=ellipse, label="Ring Topology\nKV exchange in 16 stages\nGPU 0→1→2→...→15→0"];
    
    // ===== Layer 1 (repeated for all layers) =====
    subgraph cluster_layer1_all {
        label="Layer 1-3 (All Devices - Same Structure)";
        style=dashed;
        
        layer1_rep [label="Layers 1-3\nSame structure repeated\n16 devices, 625 tokens each"];
    }
    
    // Sequence gather
    sequence_gather [shape=parallelogram, label="Sequence Gather\nInput: [batch=1024, seq=625, hidden=8192] x 16\nOutput: [batch=1024, seq=10000, hidden=8192]\nGPU: All 16"];
    
    // Final output
    output [shape=ellipse, label="Output\nInput: [batch=1024, seq=10000, hidden=8192]\nOutput: [batch=1024, seq=10000, vocab_size]\nGPU: All 16"];
    
    // Connections
    input -> sequence_split;
    
    // Device 0 connections
    sequence_split -> emb0;
    emb0 -> ln0_0;
    ln0_0 -> q_proj0_0;
    ln0_0 -> k_proj0_0;
    ln0_0 -> v_proj0_0;
    
    q_proj0_0 -> ring_attn0_0;
    k_proj0_0 -> kv_send0_0;
    v_proj0_0 -> kv_send0_0;
    
    kv_recv0_0 -> attn0_0;
    ring_attn0_0 -> attn0_0;
    
    attn0_0 -> out_proj0_0;
    out_proj0_0 -> residual0_0;
    emb0 -> residual0_0;
    
    residual0_0 -> gate0_0;
    residual0_0 -> up0_0;
    gate0_0 -> silu0_0;
    up0_0 -> mul0_0;
    silu0_0 -> mul0_0;
    mul0_0 -> down0_0;
    down0_0 -> residual_mlp0_0;
    residual0_0 -> residual_mlp0_0;
    
    // Connect to layer representation
    residual_mlp0_0 -> layer1_rep;
    layer1_rep -> sequence_gather;
    sequence_gather -> output;
    
    // Ring topology visualization
    kv_send0_0 -> ring_topology [style=dashed];
    ring_topology -> kv_recv0_0 [style=dashed];