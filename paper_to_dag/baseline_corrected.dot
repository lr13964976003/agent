digraph baseline_transformer_dag {
	graph [rankdir=TB, splines=ortho]
	node [fillcolor=lightblue, shape=ellipse, style=filled]
	subgraph cluster_pipeline_stage_0 {
		graph [label="Pipeline Stage 0 (Devices 0-7)", style="rounded,filled", fillcolor=lightyellow]
		input [fillcolor=lightgreen, label="Input\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=parallelogram]
		layer0_ln1 [fillcolor=lightcoral, label="LayerNorm1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=rectangle]
		layer0_qkv_device0 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 0", shape=rectangle]
		layer0_attn_device0 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 0", shape=rectangle]
		layer0_out_proj_device0 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 0", shape=rectangle]
		layer0_qkv_device1 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 1", shape=rectangle]
		layer0_attn_device1 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 1", shape=rectangle]
		layer0_out_proj_device1 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 1", shape=rectangle]
		layer0_qkv_device2 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 2", shape=rectangle]
		layer0_attn_device2 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 2", shape=rectangle]
		layer0_out_proj_device2 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 2", shape=rectangle]
		layer0_qkv_device3 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 3", shape=rectangle]
		layer0_attn_device3 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 3", shape=rectangle]
		layer0_out_proj_device3 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 3", shape=rectangle]
		layer0_qkv_device4 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 4", shape=rectangle]
		layer0_attn_device4 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 4", shape=rectangle]
		layer0_out_proj_device4 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 4", shape=rectangle]
		layer0_qkv_device5 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 5", shape=rectangle]
		layer0_attn_device5 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 5", shape=rectangle]
		layer0_out_proj_device5 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 5", shape=rectangle]
		layer0_qkv_device6 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 6", shape=rectangle]
		layer0_attn_device6 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 6", shape=rectangle]
		layer0_out_proj_device6 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 6", shape=rectangle]
		layer0_qkv_device7 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 7", shape=rectangle]
		layer0_attn_device7 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 7", shape=rectangle]
		layer0_out_proj_device7 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 7", shape=rectangle]
		layer0_attn_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=parallelogram]
		layer0_res1 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=ellipse]
		layer0_ln2 [fillcolor=lightcoral, label="LayerNorm2\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=rectangle]
		layer0_mlp_fc1_device0 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 0", shape=rectangle]
		layer0_mlp_fc2_device0 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 0", shape=rectangle]
		layer0_mlp_fc1_device1 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 1", shape=rectangle]
		layer0_mlp_fc2_device1 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 1", shape=rectangle]
		layer0_mlp_fc1_device2 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 2", shape=rectangle]
		layer0_mlp_fc2_device2 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 2", shape=rectangle]
		layer0_mlp_fc1_device3 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 3", shape=rectangle]
		layer0_mlp_fc2_device3 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 3", shape=rectangle]
		layer0_mlp_fc1_device4 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 4", shape=rectangle]
		layer0_mlp_fc2_device4 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 4", shape=rectangle]
		layer0_mlp_fc1_device5 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 5", shape=rectangle]
		layer0_mlp_fc2_device5 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 5", shape=rectangle]
		layer0_mlp_fc1_device6 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 6", shape=rectangle]
		layer0_mlp_fc2_device6 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 6", shape=rectangle]
		layer0_mlp_fc1_device7 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 7", shape=rectangle]
		layer0_mlp_fc2_device7 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 7", shape=rectangle]
		layer0_mlp_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=parallelogram]
		layer0_res2 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=ellipse]
		layer1_ln1 [fillcolor=lightcoral, label="LayerNorm1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=rectangle]
		layer1_qkv_device0 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 0", shape=rectangle]
		layer1_attn_device0 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 0", shape=rectangle]
		layer1_out_proj_device0 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 0", shape=rectangle]
		layer1_qkv_device1 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 1", shape=rectangle]
		layer1_attn_device1 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 1", shape=rectangle]
		layer1_out_proj_device1 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 1", shape=rectangle]
		layer1_qkv_device2 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 2", shape=rectangle]
		layer1_attn_device2 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 2", shape=rectangle]
		layer1_out_proj_device2 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 2", shape=rectangle]
		layer1_qkv_device3 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 3", shape=rectangle]
		layer1_attn_device3 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 3", shape=rectangle]
		layer1_out_proj_device3 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 3", shape=rectangle]
		layer1_qkv_device4 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 4", shape=rectangle]
		layer1_attn_device4 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 4", shape=rectangle]
		layer1_out_proj_device4 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 4", shape=rectangle]
		layer1_qkv_device5 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 5", shape=rectangle]
		layer1_attn_device5 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 5", shape=rectangle]
		layer1_out_proj_device5 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 5", shape=rectangle]
		layer1_qkv_device6 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 6", shape=rectangle]
		layer1_attn_device6 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 6", shape=rectangle]
		layer1_out_proj_device6 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 6", shape=rectangle]
		layer1_qkv_device7 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 7", shape=rectangle]
		layer1_attn_device7 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 7", shape=rectangle]
		layer1_out_proj_device7 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 7", shape=rectangle]
		layer1_attn_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=parallelogram]
		layer1_res1 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=ellipse]
		layer1_ln2 [fillcolor=lightcoral, label="LayerNorm2\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=rectangle]
		layer1_mlp_fc1_device0 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 0", shape=rectangle]
		layer1_mlp_fc2_device0 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 0", shape=rectangle]
		layer1_mlp_fc1_device1 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 1", shape=rectangle]
		layer1_mlp_fc2_device1 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 1", shape=rectangle]
		layer1_mlp_fc1_device2 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 2", shape=rectangle]
		layer1_mlp_fc2_device2 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 2", shape=rectangle]
		layer1_mlp_fc1_device3 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 3", shape=rectangle]
		layer1_mlp_fc2_device3 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 3", shape=rectangle]
		layer1_mlp_fc1_device4 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 4", shape=rectangle]
		layer1_mlp_fc2_device4 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 4", shape=rectangle]
		layer1_mlp_fc1_device5 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 5", shape=rectangle]
		layer1_mlp_fc2_device5 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 5", shape=rectangle]
		layer1_mlp_fc1_device6 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 6", shape=rectangle]
		layer1_mlp_fc2_device6 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 6", shape=rectangle]
		layer1_mlp_fc1_device7 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 7", shape=rectangle]
		layer1_mlp_fc2_device7 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 7", shape=rectangle]
		layer1_mlp_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=parallelogram]
		layer1_res2 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7", shape=ellipse]
	}
	subgraph cluster_pipeline_stage_1 {
		graph [label="Pipeline Stage 1 (Devices 8-15)", style="rounded,filled", fillcolor=lightyellow]
		layer2_ln1 [fillcolor=lightcoral, label="LayerNorm1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=rectangle]
		layer2_qkv_device8 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 8", shape=rectangle]
		layer2_attn_device8 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 8", shape=rectangle]
		layer2_out_proj_device8 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 8", shape=rectangle]
		layer2_qkv_device9 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 9", shape=rectangle]
		layer2_attn_device9 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 9", shape=rectangle]
		layer2_out_proj_device9 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 9", shape=rectangle]
		layer2_qkv_device10 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 10", shape=rectangle]
		layer2_attn_device10 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 10", shape=rectangle]
		layer2_out_proj_device10 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 10", shape=rectangle]
		layer2_qkv_device11 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 11", shape=rectangle]
		layer2_attn_device11 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 11", shape=rectangle]
		layer2_out_proj_device11 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 11", shape=rectangle]
		layer2_qkv_device12 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 12", shape=rectangle]
		layer2_attn_device12 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 12", shape=rectangle]
		layer2_out_proj_device12 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 12", shape=rectangle]
		layer2_qkv_device13 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 13", shape=rectangle]
		layer2_attn_device13 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 13", shape=rectangle]
		layer2_out_proj_device13 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 13", shape=rectangle]
		layer2_qkv_device14 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 14", shape=rectangle]
		layer2_attn_device14 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 14", shape=rectangle]
		layer2_out_proj_device14 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 14", shape=rectangle]
		layer2_qkv_device15 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 15", shape=rectangle]
		layer2_attn_device15 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 15", shape=rectangle]
		layer2_out_proj_device15 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 15", shape=rectangle]
		layer2_attn_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=parallelogram]
		layer2_res1 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=ellipse]
		layer2_ln2 [fillcolor=lightcoral, label="LayerNorm2\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=rectangle]
		layer2_mlp_fc1_device8 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 8", shape=rectangle]
		layer2_mlp_fc2_device8 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 8", shape=rectangle]
		layer2_mlp_fc1_device9 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 9", shape=rectangle]
		layer2_mlp_fc2_device9 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 9", shape=rectangle]
		layer2_mlp_fc1_device10 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 10", shape=rectangle]
		layer2_mlp_fc2_device10 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 10", shape=rectangle]
		layer2_mlp_fc1_device11 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 11", shape=rectangle]
		layer2_mlp_fc2_device11 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 11", shape=rectangle]
		layer2_mlp_fc1_device12 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 12", shape=rectangle]
		layer2_mlp_fc2_device12 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 12", shape=rectangle]
		layer2_mlp_fc1_device13 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 13", shape=rectangle]
		layer2_mlp_fc2_device13 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 13", shape=rectangle]
		layer2_mlp_fc1_device14 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 14", shape=rectangle]
		layer2_mlp_fc2_device14 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 14", shape=rectangle]
		layer2_mlp_fc1_device15 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 15", shape=rectangle]
		layer2_mlp_fc2_device15 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 15", shape=rectangle]
		layer2_mlp_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=parallelogram]
		layer2_res2 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=ellipse]
		layer3_ln1 [fillcolor=lightcoral, label="LayerNorm1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=rectangle]
		layer3_qkv_device8 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 8", shape=rectangle]
		layer3_attn_device8 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 8", shape=rectangle]
		layer3_out_proj_device8 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 8", shape=rectangle]
		layer3_qkv_device9 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 9", shape=rectangle]
		layer3_attn_device9 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 9", shape=rectangle]
		layer3_out_proj_device9 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 9", shape=rectangle]
		layer3_qkv_device10 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 10", shape=rectangle]
		layer3_attn_device10 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 10", shape=rectangle]
		layer3_out_proj_device10 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 10", shape=rectangle]
		layer3_qkv_device11 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 11", shape=rectangle]
		layer3_attn_device11 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 11", shape=rectangle]
		layer3_out_proj_device11 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 11", shape=rectangle]
		layer3_qkv_device12 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 12", shape=rectangle]
		layer3_attn_device12 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 12", shape=rectangle]
		layer3_out_proj_device12 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 12", shape=rectangle]
		layer3_qkv_device13 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 13", shape=rectangle]
		layer3_attn_device13 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 13", shape=rectangle]
		layer3_out_proj_device13 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 13", shape=rectangle]
		layer3_qkv_device14 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 14", shape=rectangle]
		layer3_attn_device14 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 14", shape=rectangle]
		layer3_out_proj_device14 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 14", shape=rectangle]
		layer3_qkv_device15 [label="QKV Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 15", shape=rectangle]
		layer3_attn_device15 [label="Attention\n[batch_size=128, seq_len=10000, heads=4, head_dim=128]\nDevice: 15", shape=rectangle]
		layer3_out_proj_device15 [label="Output Projection\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 15", shape=rectangle]
		layer3_attn_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=parallelogram]
		layer3_res1 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=ellipse]
		layer3_ln2 [fillcolor=lightcoral, label="LayerNorm2\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=rectangle]
		layer3_mlp_fc1_device8 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 8", shape=rectangle]
		layer3_mlp_fc2_device8 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 8", shape=rectangle]
		layer3_mlp_fc1_device9 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 9", shape=rectangle]
		layer3_mlp_fc2_device9 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 9", shape=rectangle]
		layer3_mlp_fc1_device10 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 10", shape=rectangle]
		layer3_mlp_fc2_device10 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 10", shape=rectangle]
		layer3_mlp_fc1_device11 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 11", shape=rectangle]
		layer3_mlp_fc2_device11 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 11", shape=rectangle]
		layer3_mlp_fc1_device12 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 12", shape=rectangle]
		layer3_mlp_fc2_device12 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 12", shape=rectangle]
		layer3_mlp_fc1_device13 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 13", shape=rectangle]
		layer3_mlp_fc2_device13 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 13", shape=rectangle]
		layer3_mlp_fc1_device14 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 14", shape=rectangle]
		layer3_mlp_fc2_device14 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 14", shape=rectangle]
		layer3_mlp_fc1_device15 [label="MLP FC1\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevice: 15", shape=rectangle]
		layer3_mlp_fc2_device15 [label="MLP FC2\n[batch_size=128, seq_len=10000, hidden_size=512]\nDevice: 15", shape=rectangle]
		layer3_mlp_allreduce [fillcolor=orange, label="All-Reduce\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=parallelogram]
		layer3_res2 [fillcolor=lightgreen, label="Residual Add\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 8-15", shape=ellipse]
	}
	pipeline_send [fillcolor=purple, label="Pipeline Send\n[batch_size=128, seq_len=10000, hidden_size=4096]\nDevices: 0-7 â†’ 8-15", shape=parallelogram]
	output [fillcolor=lightgreen, label="Output\n[batch_size=128, seq_len=10000, hidden_size=4096]", shape=parallelogram]
	input -> layer0_ln1
	layer0_ln1 -> layer0_qkv_device0
	layer0_qkv_device0 -> layer0_attn_device0
	layer0_attn_device0 -> layer0_out_proj_device0
	layer0_out_proj_device0 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_device1
	layer0_qkv_device1 -> layer0_attn_device1
	layer0_attn_device1 -> layer0_out_proj_device1
	layer0_out_proj_device1 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_device2
	layer0_qkv_device2 -> layer0_attn_device2
	layer0_attn_device2 -> layer0_out_proj_device2
	layer0_out_proj_device2 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_device3
	layer0_qkv_device3 -> layer0_attn_device3
	layer0_attn_device3 -> layer0_out_proj_device3
	layer0_out_proj_device3 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_device4
	layer0_qkv_device4 -> layer0_attn_device4
	layer0_attn_device4 -> layer0_out_proj_device4
	layer0_out_proj_device4 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_device5
	layer0_qkv_device5 -> layer0_attn_device5
	layer0_attn_device5 -> layer0_out_proj_device5
	layer0_out_proj_device5 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_device6
	layer0_qkv_device6 -> layer0_attn_device6
	layer0_attn_device6 -> layer0_out_proj_device6
	layer0_out_proj_device6 -> layer0_attn_allreduce
	layer0_ln1 -> layer0_qkv_device7
	layer0_qkv_device7 -> layer0_attn_device7
	layer0_attn_device7 -> layer0_out_proj_device7
	layer0_out_proj_device7 -> layer0_attn_allreduce
	layer0_attn_allreduce -> layer0_res1
	input -> layer0_res1 [style=dashed]
	layer0_res1 -> layer0_ln2
	layer0_ln2 -> layer0_mlp_fc1_device0
	layer0_mlp_fc1_device0 -> layer0_mlp_fc2_device0
	layer0_mlp_fc2_device0 -> layer0_mlp_allreduce
	layer0_ln2 -> layer0_mlp_fc1_device1
	layer0_mlp_fc1_device1 -> layer0_mlp_fc2_device1
	layer0_mlp_fc2_device1 -> layer0_mlp_allreduce
	layer0_ln2 -> layer0_mlp_fc1_device2
	layer0_mlp_fc1_device2 -> layer0_mlp_fc2_device2
	layer0_mlp_fc2_device2 -> layer0_mlp_allreduce
	layer0_ln2 -> layer0_mlp_fc1_device3
	layer0_mlp_fc1_device3 -> layer0_mlp_fc2_device3
	layer0_mlp_fc2_device3 -> layer0_mlp_allreduce
	layer0_ln2 -> layer0_mlp_fc1_device4
	layer0_mlp_fc1_device4 -> layer0_mlp_fc2_device4
	layer0_mlp_fc2_device4 -> layer0_mlp_allreduce
	layer0_ln2 -> layer0_mlp_fc1_device5
	layer0_mlp_fc1_device5 -> layer0_mlp_fc2_device5
	layer0_mlp_fc2_device5 -> layer0_mlp_allreduce
	layer0_ln2 -> layer0_mlp_fc1_device6
	layer0_mlp_fc1_device6 -> layer0_mlp_fc2_device6
	layer0_mlp_fc2_device6 -> layer0_mlp_allreduce
	layer0_ln2 -> layer0_mlp_fc1_device7
	layer0_mlp_fc1_device7 -> layer0_mlp_fc2_device7
	layer0_mlp_fc2_device7 -> layer0_mlp_allreduce
	layer0_mlp_allreduce -> layer0_res2
	layer0_res1 -> layer0_res2 [style=dashed]
	layer0_res2 -> layer1_ln1
	layer1_ln1 -> layer1_qkv_device0
	layer1_qkv_device0 -> layer1_attn_device0
	layer1_attn_device0 -> layer1_out_proj_device0
	layer1_out_proj_device0 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_device1
	layer1_qkv_device1 -> layer1_attn_device1
	layer1_attn_device1 -> layer1_out_proj_device1
	layer1_out_proj_device1 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_device2
	layer1_qkv_device2 -> layer1_attn_device2
	layer1_attn_device2 -> layer1_out_proj_device2
	layer1_out_proj_device2 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_device3
	layer1_qkv_device3 -> layer1_attn_device3
	layer1_attn_device3 -> layer1_out_proj_device3
	layer1_out_proj_device3 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_device4
	layer1_qkv_device4 -> layer1_attn_device4
	layer1_attn_device4 -> layer1_out_proj_device4
	layer1_out_proj_device4 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_device5
	layer1_qkv_device5 -> layer1_attn_device5
	layer1_attn_device5 -> layer1_out_proj_device5
	layer1_out_proj_device5 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_device6
	layer1_qkv_device6 -> layer1_attn_device6
	layer1_attn_device6 -> layer1_out_proj_device6
	layer1_out_proj_device6 -> layer1_attn_allreduce
	layer1_ln1 -> layer1_qkv_device7
	layer1_qkv_device7 -> layer1_attn_device7
	layer1_attn_device7 -> layer1_out_proj_device7
	layer1_out_proj_device7 -> layer1_attn_allreduce
	layer1_attn_allreduce -> layer1_res1
	layer1_ln1 -> layer1_res1 [style=dashed]
	layer1_res1 -> layer1_ln2
	layer1_ln2 -> layer1_mlp_fc1_device0
	layer1_mlp_fc1_device0 -> layer1_mlp_fc2_device0
	layer1_mlp_fc2_device0 -> layer1_mlp_allreduce
	layer1_ln2 -> layer1_mlp_fc1_device1
	layer1_mlp_fc1_device1 -> layer1_mlp_fc2_device1
	layer1_mlp_fc2_device1 -> layer1_mlp_allreduce
	layer1_ln2 -> layer1_mlp_fc1_device2
	layer1_mlp_fc1_device2 -> layer1_mlp_fc2_device2
	layer1_mlp_fc2_device2 -> layer1_mlp_allreduce
	layer1_ln2 -> layer1_mlp_fc1_device3
	layer1_mlp_fc1_device3 -> layer1_mlp_fc2_device3
	layer1_mlp_fc2_device3 -> layer1_mlp_allreduce
	layer1_ln2 -> layer1_mlp_fc1_device4
	layer1_mlp_fc1_device4 -> layer1_mlp_fc2_device4
	layer1_mlp_fc2_device4 -> layer1_mlp_allreduce
	layer1_ln2 -> layer1_mlp_fc1_device5
	layer1_mlp_fc1_device5 -> layer1_mlp_fc2_device5
	layer1_mlp_fc2_device5 -> layer1_mlp_allreduce
	layer1_ln2 -> layer1_mlp_fc1_device6
	layer1_mlp_fc1_device6 -> layer1_mlp_fc2_device6
	layer1_mlp_fc2_device6 -> layer1_mlp_allreduce
	layer1_ln2 -> layer1_mlp_fc1_device7
	layer1_mlp_fc1_device7 -> layer1_mlp_fc2_device7
	layer1_mlp_fc2_device7 -> layer1_mlp_allreduce
	layer1_mlp_allreduce -> layer1_res2
	layer1_res1 -> layer1_res2 [style=dashed]
	layer1_res2 -> pipeline_send
	pipeline_send -> layer2_ln1
	layer2_ln1 -> layer2_qkv_device8
	layer2_qkv_device8 -> layer2_attn_device8
	layer2_attn_device8 -> layer2_out_proj_device8
	layer2_out_proj_device8 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_device9
	layer2_qkv_device9 -> layer2_attn_device9
	layer2_attn_device9 -> layer2_out_proj_device9
	layer2_out_proj_device9 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_device10
	layer2_qkv_device10 -> layer2_attn_device10
	layer2_attn_device10 -> layer2_out_proj_device10
	layer2_out_proj_device10 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_device11
	layer2_qkv_device11 -> layer2_attn_device11
	layer2_attn_device11 -> layer2_out_proj_device11
	layer2_out_proj_device11 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_device12
	layer2_qkv_device12 -> layer2_attn_device12
	layer2_attn_device12 -> layer2_out_proj_device12
	layer2_out_proj_device12 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_device13
	layer2_qkv_device13 -> layer2_attn_device13
	layer2_attn_device13 -> layer2_out_proj_device13
	layer2_out_proj_device13 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_device14
	layer2_qkv_device14 -> layer2_attn_device14
	layer2_attn_device14 -> layer2_out_proj_device14
	layer2_out_proj_device14 -> layer2_attn_allreduce
	layer2_ln1 -> layer2_qkv_device15
	layer2_qkv_device15 -> layer2_attn_device15
	layer2_attn_device15 -> layer2_out_proj_device15
	layer2_out_proj_device15 -> layer2_attn_allreduce
	layer2_attn_allreduce -> layer2_res1
	layer2_ln1 -> layer2_res1 [style=dashed]
	layer2_res1 -> layer2_ln2
	layer2_ln2 -> layer2_mlp_fc1_device8
	layer2_mlp_fc1_device8 -> layer2_mlp_fc2_device8
	layer2_mlp_fc2_device8 -> layer2_mlp_allreduce
	layer2_ln2 -> layer2_mlp_fc1_device9
	layer2_mlp_fc1_device9 -> layer2_mlp_fc2_device9
	layer2_mlp_fc2_device9 -> layer2_mlp_allreduce
	layer2_ln2 -> layer2_mlp_fc1_device10
	layer2_mlp_fc1_device10 -> layer2_mlp_fc2_device10
	layer2_mlp_fc2_device10 -> layer2_mlp_allreduce
	layer2_ln2 -> layer2_mlp_fc1_device11
	layer2_mlp_fc1_device11 -> layer2_mlp_fc2_device11
	layer2_mlp_fc2_device11 -> layer2_mlp_allreduce
	layer2_ln2 -> layer2_mlp_fc1_device12
	layer2_mlp_fc1_device12 -> layer2_mlp_fc2_device12
	layer2_mlp_fc2_device12 -> layer2_mlp_allreduce
	layer2_ln2 -> layer2_mlp_fc1_device13
	layer2_mlp_fc1_device13 -> layer2_mlp_fc2_device13
	layer2_mlp_fc2_device13 -> layer2_mlp_allreduce
	layer2_ln2 -> layer2_mlp_fc1_device14
	layer2_mlp_fc1_device14 -> layer2_mlp_fc2_device14
	layer2_mlp_fc2_device14 -> layer2_mlp_allreduce
	layer2_ln2 -> layer2_mlp_fc1_device15
	layer2_mlp_fc1_device15 -> layer2_mlp_fc2_device15
	layer2_mlp_fc2_device15 -> layer2_mlp_allreduce
	layer2_mlp_allreduce -> layer2_res2
	layer2_res1 -> layer2_res2 [style=dashed]
	layer2_res2 -> layer3_ln1
	layer3_ln1 -> layer3_qkv_device8
	layer3_qkv_device8 -> layer3_attn_device8
	layer3_attn_device8 -> layer3_out_proj_device8
	layer3_out_proj_device8 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_device9
	layer3_qkv_device9 -> layer3_attn_device9
	layer3_attn_device9 -> layer3_out_proj_device9
	layer3_out_proj_device9 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_device10
	layer3_qkv_device10 -> layer3_attn_device10
	layer3_attn_device10 -> layer3_out_proj_device10
	layer3_out_proj_device10 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_device11
	layer3_qkv_device11 -> layer3_attn_device11
	layer3_attn_device11 -> layer3_out_proj_device11
	layer3_out_proj_device11 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_device12
	layer3_qkv_device12 -> layer3_attn_device12
	layer3_attn_device12 -> layer3_out_proj_device12
	layer3_out_proj_device12 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_device13
	layer3_qkv_device13 -> layer3_attn_device13
	layer3_attn_device13 -> layer3_out_proj_device13
	layer3_out_proj_device13 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_device14
	layer3_qkv_device14 -> layer3_attn_device14
	layer3_attn_device14 -> layer3_out_proj_device14
	layer3_out_proj_device14 -> layer3_attn_allreduce
	layer3_ln1 -> layer3_qkv_device15
	layer3_qkv_device15 -> layer3_attn_device15
	layer3_attn_device15 -> layer3_out_proj_device15
	layer3_out_proj_device15 -> layer3_attn_allreduce
	layer3_attn_allreduce -> layer3_res1
	layer3_ln1 -> layer3_res1 [style=dashed]
	layer3_res1 -> layer3_ln2
	layer3_ln2 -> layer3_mlp_fc1_device8
	layer3_mlp_fc1_device8 -> layer3_mlp_fc2_device8
	layer3_mlp_fc2_device8 -> layer3_mlp_allreduce
	layer3_ln2 -> layer3_mlp_fc1_device9
	layer3_mlp_fc1_device9 -> layer3_mlp_fc2_device9
	layer3_mlp_fc2_device9 -> layer3_mlp_allreduce
	layer3_ln2 -> layer3_mlp_fc1_device10
	layer3_mlp_fc1_device10 -> layer3_mlp_fc2_device10
	layer3_mlp_fc2_device10 -> layer3_mlp_allreduce
	layer3_ln2 -> layer3_mlp_fc1_device11
	layer3_mlp_fc1_device11 -> layer3_mlp_fc2_device11
	layer3_mlp_fc2_device11 -> layer3_mlp_allreduce
	layer3_ln2 -> layer3_mlp_fc1_device12
	layer3_mlp_fc1_device12 -> layer3_mlp_fc2_device12
	layer3_mlp_fc2_device12 -> layer3_mlp_allreduce
	layer3_ln2 -> layer3_mlp_fc1_device13
	layer3_mlp_fc1_device13 -> layer3_mlp_fc2_device13
	layer3_mlp_fc2_device13 -> layer3_mlp_allreduce
	layer3_ln2 -> layer3_mlp_fc1_device14
	layer3_mlp_fc1_device14 -> layer3_mlp_fc2_device14
	layer3_mlp_fc2_device14 -> layer3_mlp_allreduce
	layer3_ln2 -> layer3_mlp_fc1_device15
	layer3_mlp_fc1_device15 -> layer3_mlp_fc2_device15
	layer3_mlp_fc2_device15 -> layer3_mlp_allreduce
	layer3_mlp_allreduce -> layer3_res2
	layer3_res1 -> layer3_res2 [style=dashed]
	layer3_res2 -> output
}