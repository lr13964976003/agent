digraph ma_separation {
    rankdir=TB;
    compound=true;
    ranksep=2.0;
    nodesep=0.8;
    
    subgraph cluster_attention {
        label="Attention Computation (GPUs 0-11)";
        style=rounded;
        color=blue;
        bgcolor=lightblue;
        
        input [shape=ellipse, style=filled, fillcolor=lightgreen, label="Model Input\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs"];
        
        embed [label="Token Embedding\nInput: [batch_size=1024, seq_len=2048]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        ln1_l0 [label="LayerNorm L0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        qkv_l0_gpu0 [label="QKV Projection\nGPU 0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 0"];
        qkv_l0_gpu1 [label="QKV Projection\nGPU 1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 1"];
        qkv_l0_gpu2 [label="QKV Projection\nGPU 2\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 2"];
        qkv_l0_gpu3 [label="QKV Projection\nGPU 3\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 3"];
        qkv_l0_gpu4 [label="QKV Projection\nGPU 4\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 4"];
        qkv_l0_gpu5 [label="QKV Projection\nGPU 5\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 5"];
        qkv_l0_gpu6 [label="QKV Projection\nGPU 6\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 6"];
        qkv_l0_gpu7 [label="QKV Projection\nGPU 7\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128, 3]\nGPU: 7"];
        qkv_l0_gpu8 [label="QKV Projection\nGPU 8\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 8"];
        qkv_l0_gpu9 [label="QKV Projection\nGPU 9\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 9"];
        qkv_l0_gpu10 [label="QKV Projection\nGPU 10\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 10"];
        qkv_l0_gpu11 [label="QKV Projection\nGPU 11\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128, 3]\nGPU: 11"];
        
        scores_l0_gpu0 [label="Attention Scores\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 0"];
        scores_l0_gpu1 [label="Attention Scores\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 1"];
        scores_l0_gpu2 [label="Attention Scores\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 2"];
        scores_l0_gpu3 [label="Attention Scores\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 3"];
        scores_l0_gpu4 [label="Attention Scores\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 4"];
        scores_l0_gpu5 [label="Attention Scores\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 5"];
        scores_l0_gpu6 [label="Attention Scores\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 6"];
        scores_l0_gpu7 [label="Attention Scores\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 7"];
        scores_l0_gpu8 [label="Attention Scores\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 8"];
        scores_l0_gpu9 [label="Attention Scores\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 9"];
        scores_l0_gpu10 [label="Attention Scores\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 10"];
        scores_l0_gpu11 [label="Attention Scores\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 11"];
        
        softmax_l0_gpu0 [label="Softmax\nGPU 0\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 0"];
        softmax_l0_gpu1 [label="Softmax\nGPU 1\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 1"];
        softmax_l0_gpu2 [label="Softmax\nGPU 2\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 2"];
        softmax_l0_gpu3 [label="Softmax\nGPU 3\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 3"];
        softmax_l0_gpu4 [label="Softmax\nGPU 4\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 4"];
        softmax_l0_gpu5 [label="Softmax\nGPU 5\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 5"];
        softmax_l0_gpu6 [label="Softmax\nGPU 6\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 6"];
        softmax_l0_gpu7 [label="Softmax\nGPU 7\nInput/Output: [batch_size=1024, seq_len=2048, heads=3, seq_len=2048]\nGPU: 7"];
        softmax_l0_gpu8 [label="Softmax\nGPU 8\nInput/Output: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 8"];
        softmax_l0_gpu9 [label="Softmax\nGPU 9\nInput/Output: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 9"];
        softmax_l0_gpu10 [label="Softmax\nGPU 10\nInput/Output: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 10"];
        softmax_l0_gpu11 [label="Softmax\nGPU 11\nInput/Output: [batch_size=1024, seq_len=2048, heads=2, seq_len=2048]\nGPU: 11"];
        
        attn_out_l0_gpu0 [label="Attention Output\nGPU 0\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 0"];
        attn_out_l0_gpu1 [label="Attention Output\nGPU 1\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 1"];
        attn_out_l0_gpu2 [label="Attention Output\nGPU 2\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 2"];
        attn_out_l0_gpu3 [label="Attention Output\nGPU 3\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 3"];
        attn_out_l0_gpu4 [label="Attention Output\nGPU 4\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 4"];
        attn_out_l0_gpu5 [label="Attention Output\nGPU 5\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 5"];
        attn_out_l0_gpu6 [label="Attention Output\nGPU 6\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 6"];
        attn_out_l0_gpu7 [label="Attention Output\nGPU 7\nInput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=3, head_dim=128]\nGPU: 7"];
        attn_out_l0_gpu8 [label="Attention Output\nGPU 8\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 8"];
        attn_out_l0_gpu9 [label="Attention Output\nGPU 9\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 9"];
        attn_out_l0_gpu10 [label="Attention Output\nGPU 10\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 10"];
        attn_out_l0_gpu11 [label="Attention Output\nGPU 11\nInput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nOutput: [batch_size=1024, seq_len=2048, heads=2, head_dim=128]\nGPU: 11"];
        
        o_proj_l0_gpu0 [label="O Projection\nGPU 0\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 0"];
        o_proj_l0_gpu1 [label="O Projection\nGPU 1\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 1"];
        o_proj_l0_gpu2 [label="O Projection\nGPU 2\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 2"];
        o_proj_l0_gpu3 [label="O Projection\nGPU 3\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 3"];
        o_proj_l0_gpu4 [label="O Projection\nGPU 4\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 4"];
        o_proj_l0_gpu5 [label="O Projection\nGPU 5\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 5"];
        o_proj_l0_gpu6 [label="O Projection\nGPU 6\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 6"];
        o_proj_l0_gpu7 [label="O Projection\nGPU 7\nInput: [batch_size=1024, seq_len=2048, dim=384]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 7"];
        o_proj_l0_gpu8 [label="O Projection\nGPU 8\nInput: [batch_size=1024, seq_len=2048, dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 8"];
        o_proj_l0_gpu9 [label="O Projection\nGPU 9\nInput: [batch_size=1024, seq_len=2048, dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 9"];
        o_proj_l0_gpu10 [label="O Projection\nGPU 10\nInput: [batch_size=1024, seq_len=2048, dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 10"];
        o_proj_l0_gpu11 [label="O Projection\nGPU 11\nInput: [batch_size=1024, seq_len=2048, dim=256]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=341]\nGPU: 11"];
        
        all_reduce_attn_l0 [shape=ellipse, style=filled, fillcolor=yellow, label="Attention All-Reduce\nHierarchical\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        res1_l0 [shape=ellipse, style=filled, fillcolor=pink, label="Residual Add L0\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        broadcast_l0 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Broadcast to MoE\nLayer 0\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        ln1_l1 [label="LayerNorm L1\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 0-11"];
        
        // Similar nodes for Layer 1-3 would be defined here
        // Due to space, we summarize the pattern for clarity
    }
    
    subgraph cluster_moe {
        label="MoE Computation (GPUs 12-15)";
        style=rounded;
        color=red;
        bgcolor=lightcoral;
        
        recv_moe_l0 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Receive from Attention\nLayer 0\nFrom: GPUs 0-11\nTo: GPUs 12-15\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        ln_moe_l0_gpu12 [label="LayerNorm\nGPU 12\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        ln_moe_l0_gpu13 [label="LayerNorm\nGPU 13\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        ln_moe_l0_gpu14 [label="LayerNorm\nGPU 14\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        ln_moe_l0_gpu15 [label="LayerNorm\nGPU 15\nInput/Output: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        
        gate_l0_gpu12 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 12"];
        gate_l0_gpu13 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 13"];
        gate_l0_gpu14 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 14"];
        gate_l0_gpu15 [shape=parallelogram, style=filled, fillcolor=orange, label="Gate Network\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, num_experts=16]\nGPU: 15"];
        
        // Experts for GPU 12
        expert_l0_0_gpu12 [label="Expert 0\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_l0_1_gpu12 [label="Expert 1\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_l0_2_gpu12 [label="Expert 2\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_l0_3_gpu12 [label="Expert 3\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        
        // Experts for GPU 13
        expert_l0_4_gpu13 [label="Expert 4\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_l0_5_gpu13 [label="Expert 5\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_l0_6_gpu13 [label="Expert 6\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_l0_7_gpu13 [label="Expert 7\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        
        // Experts for GPU 14
        expert_l0_8_gpu14 [label="Expert 8\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_l0_9_gpu14 [label="Expert 9\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_l0_10_gpu14 [label="Expert 10\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_l0_11_gpu14 [label="Expert 11\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        
        // Experts for GPU 15
        expert_l0_12_gpu15 [label="Expert 12\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        expert_l0_13_gpu15 [label="Expert 13\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        expert_l0_14_gpu15 [label="Expert 14\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        expert_l0_15_gpu15 [label="Expert 15\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        
        expert_agg_l0_gpu12 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12"];
        expert_agg_l0_gpu13 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 13"];
        expert_agg_l0_gpu14 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 14"];
        expert_agg_l0_gpu15 [shape=ellipse, style=filled, fillcolor=lightyellow, label="Expert Aggregation\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096, top_k=2]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 15"];
        
        all_to_all_l0_gpu12 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All\nGPU 12\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
        all_to_all_l0_gpu13 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All\nGPU 13\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
        all_to_all_l0_gpu14 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All\nGPU 14\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
        all_to_all_l0_gpu15 [shape=ellipse, style=filled, fillcolor=yellow, label="All-to-All\nGPU 15\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nGPU: 12-15"];
        
        return_l0 [shape=parallelogram, style=filled, fillcolor=lightcyan, label="Return to Attention\nLayer 0\nFrom: GPUs 12-15\nTo: GPUs 0-11\nData: [batch_size=1024, seq_len=2048, hidden_dim=4096]"];
        
        // Similar structure for Layers 1-3 would follow
    }
    
    // Layer 0 flows
    input -> embed;
    embed -> ln1_l0;
    ln1_l0 -> qkv_l0_gpu0;
    ln1_l0 -> qkv_l0_gpu1;
    ln1_l0 -> qkv_l0_gpu2;
    ln1_l0 -> qkv_l0_gpu3;
    ln1_l0 -> qkv_l0_gpu4;
    ln1_l0 -> qkv_l0_gpu5;
    ln1_l0 -> qkv_l0_gpu6;
    ln1_l0 -> qkv_l0_gpu7;
    ln1_l0 -> qkv_l0_gpu8;
    ln1_l0 -> qkv_l0_gpu9;
    ln1_l0 -> qkv_l0_gpu10;
    ln1_l0 -> qkv_l0_gpu11;
    
    qkv_l0_gpu0 -> scores_l0_gpu0;
    qkv_l0_gpu1 -> scores_l0_gpu1;
    qkv_l0_gpu2 -> scores_l0_gpu2;
    qkv_l0_gpu3 -> scores_l0_gpu3;
    qkv_l0_gpu4 -> scores_l0_gpu4;
    qkv_l0_gpu5 -> scores_l0_gpu5;
    qkv_l0_gpu6 -> scores_l0_gpu6;
    qkv_l0_gpu7 -> scores_l0_gpu7;
    qkv_l0_gpu8 -> scores_l0_gpu8;
    qkv_l0_gpu9 -> scores_l0_gpu9;
    qkv_l0_gpu10 -> scores_l0_gpu10;
    qkv_l0_gpu11 -> scores_l0_gpu11;
    
    scores_l0_gpu0 -> softmax_l0_gpu0;
    scores_l0_gpu1 -> softmax_l0_gpu1;
    scores_l0_gpu2 -> softmax_l0_gpu2;
    scores_l0_gpu3 -> softmax_l0_gpu3;
    scores_l0_gpu4 -> softmax_l0_gpu4;
    scores_l0_gpu5 -> softmax_l0_gpu5;
    scores_l0_gpu6 -> softmax_l0_gpu6;
    scores_l0_gpu7 -> softmax_l0_gpu7;
    scores_l0_gpu8 -> softmax_l0_gpu8;
    scores_l0_gpu9 -> softmax_l0_gpu9;
    scores_l0_gpu10 -> softmax_l0_gpu10;
    scores_l0_gpu11 -> softmax_l0_gpu11;
    
    softmax_l0_gpu0 -> attn_out_l0_gpu0;
    softmax_l0_gpu1 -> attn_out_l0_gpu1;
    softmax_l0_gpu2 -> attn_out_l0_gpu2;
    softmax_l0_gpu3 -> attn_out_l0_gpu3;
    softmax_l0_gpu4 -> attn_out_l0_gpu4;
    softmax_l0_gpu5 -> attn_out_l0_gpu5;
    softmax_l0_gpu6 -> attn_out_l0_gpu6;
    softmax_l0_gpu7 -> attn_out_l0_gpu7;
    softmax_l0_gpu8 -> attn_out_l0_gpu8;
    softmax_l0_gpu9 -> attn_out_l0_gpu9;
    softmax_l0_gpu10 -> attn_out_l0_gpu10;
    softmax_l0_gpu11 -> attn_out_l0_gpu11;
    
    attn_out_l0_gpu0 -> o_proj_l0_gpu0;
    attn_out_l0_gpu1 -> o_proj_l0_gpu1;
    attn_out_l0_gpu2 -> o_proj_l0_gpu2;
    attn_out_l0_gpu3 -> o_proj_l0_gpu3;
    attn_out_l0_gpu4 -> o_proj_l0_gpu4;
    attn_out_l0_gpu5 -> o_proj_l0_gpu5;
    attn_out_l0_gpu6 -> o_proj_l0_gpu6;
    attn_out_l0_gpu7 -> o_proj_l0_gpu7;
    attn_out_l0_gpu8 -> o_proj_l0_gpu8;
    attn_out_l0_gpu9 -> o_proj_l0_gpu9;
    attn_out_l0_gpu10 -> o_proj_l0_gpu10;
    attn_out_l0_gpu11 -> o_proj_l0_gpu11;
    
    o_proj_l0_gpu0 -> all_reduce_attn_l0;
    o_proj_l0_gpu1 -> all_reduce_attn_l0;
    o_proj_l0_gpu2 -> all_reduce_attn_l0;
    o_proj_l0_gpu3 -> all_reduce_attn_l0;
    o_proj_l0_gpu4 -> all_reduce_attn_l0;
    o_proj_l0_gpu5 -> all_reduce_attn_l0;
    o_proj_l0_gpu6 -> all_reduce_attn_l0;
    o_proj_l0_gpu7 -> all_reduce_attn_l0;
    o_proj_l0_gpu8 -> all_reduce_attn_l0;
    o_proj_l0_gpu9 -> all_reduce_attn_l0;
    o_proj_l0_gpu10 -> all_reduce_attn_l0;
    o_proj_l0_gpu11 -> all_reduce_attn_l0;
    
    all_reduce_attn_l0 -> res1_l0;
    res1_l0 -> broadcast_l0;
    
    // MoE flow
    broadcast_l0 -> recv_moe_l0 [lhead=cluster_moe, ltail=cluster_attention];
    
    recv_moe_l0 -> ln_moe_l0_gpu12;
    recv_moe_l0 -> ln_moe_l0_gpu13;
    recv_moe_l0 -> ln_moe_l0_gpu14;
    recv_moe_l0 -> ln_moe_l0_gpu15;
    
    ln_moe_l0_gpu12 -> gate_l0_gpu12;
    ln_moe_l0_gpu13 -> gate_l0_gpu13;
    ln_moe_l0_gpu14 -> gate_l0_gpu14;
    ln_moe_l0_gpu15 -> gate_l0_gpu15;
    
    recv_moe_l0 -> expert_l0_0_gpu12;
    recv_moe_l0 -> expert_l0_1_gpu12;
    recv_moe_l0 -> expert_l0_2_gpu12;
    recv_moe_l0 -> expert_l0_3_gpu12;
    recv_moe_l0 -> expert_l0_4_gpu13;
    recv_moe_l0 -> expert_l0_5_gpu13;
    recv_moe_l0 -> expert_l0_6_gpu13;
    recv_moe_l0 -> expert_l0_7_gpu13;
    recv_moe_l0 -> expert_l0_8_gpu14;
    recv_moe_l0 -> expert_l0_9_gpu14;
    recv_moe_l0 -> expert_l0_10_gpu14;
    recv_moe_l0 -> expert_l0_11_gpu14;
    recv_moe_l0 -> expert_l0_12_gpu15;
    recv_moe_l0 -> expert_l0_13_gpu15;
    recv_moe_l0 -> expert_l0_14_gpu15;
    recv_moe_l0 -> expert_l0_15_gpu15;
    
    gate_l0_gpu12 -> expert_l0_0_gpu12 [style=dashed];
    gate_l0_gpu12 -> expert_l0_1_gpu12 [style=dashed];
    gate_l0_gpu12 -> expert_l0_2_gpu12 [style=dashed];
    gate_l0_gpu12 -> expert_l0_3_gpu12 [style=dashed];
    gate_l0_gpu13 -> expert_l0_4_gpu13 [style=dashed];
    gate_l0_gpu13 -> expert_l0_5_gpu13 [style=dashed];
    gate_l0_gpu13 -> expert_l0_6_gpu13 [style=dashed];
    gate_l0_gpu13 -> expert_l0_7_gpu13 [style=dashed];
    gate_l0_gpu14 -> expert_l0_8_gpu14 [style=dashed];
    gate_l0_gpu14 -> expert_l0_9_gpu14 [style=dashed];
    gate_l0_gpu14 -> expert_l0_10_gpu14 [style=dashed];
    gate_l0_gpu14 -> expert_l0_11_gpu14 [style=dashed];
    gate_l0_gpu15 -> expert_l0_12_gpu15 [style=dashed];
    gate_l0_gpu15 -> expert_l0_13_gpu15 [style=dashed];
    gate_l0_gpu15 -> expert_l0_14_gpu15 [style=dashed];
    gate_l0_gpu15 -> expert_l0_15_gpu15 [style=dashed];
    
    expert_l0_0_gpu12 -> expert_agg_l0_gpu12;
    expert_l0_1_gpu12 -> expert_agg_l0_gpu12;
    expert_l0_2_gpu12 -> expert_agg_l0_gpu12;
    expert_l0_3_gpu12 -> expert_agg_l0_gpu12;
    expert_l0_4_gpu13 -> expert_agg_l0_gpu13;
    expert_l0_5_gpu13 -> expert_agg_l0_gpu13;
    expert_l0_6_gpu13 -> expert_agg_l0_gpu13;
    expert_l0_7_gpu13 -> expert_agg_l0_gpu13;
    expert_l0_8_gpu14 -> expert_agg_l0_gpu14;
    expert_l0_9_gpu14 -> expert_agg_l0_gpu14;
    expert_l0_10_gpu14 -> expert_agg_l0_gpu14;
    expert_l0_11_gpu14 -> expert_agg_l0_gpu14;
    expert_l0_12_gpu15 -> expert_agg_l0_gpu15;
    expert_l0_13_gpu15 -> expert_agg_l0_gpu15;
    expert_l0_14_gpu15 -> expert_agg_l0_gpu15;
    expert_l0_15_gpu15 -> expert_agg_l0_gpu15;
    
    expert_agg_l0_gpu12 -> all_to_all_l0_gpu12;
    expert_agg_l0_gpu13 -> all_to_all_l0_gpu13;
    expert_agg_l0_gpu14 -> all_to_all_l0_gpu14;
    expert_agg_l0_gpu15 -> all_to_all_l0_gpu15;
    
    all_to_all_l0_gpu12 -> return_l0 [lhead=cluster_attention, ltail=cluster_moe];
    all_to_all_l0_gpu13 -> return_l0 [lhead=cluster_attention, ltail=cluster_moe];
    all_to_all_l0_gpu14 -> return_l0 [lhead=cluster_attention, ltail=cluster_moe];
    all_to_all_l0_gpu15 -> return_l0 [lhead=cluster_attention, ltail=cluster_moe];
    
    // Continue with Layer 1-3 flows
    return_l0 -> ln1_l1;
    
    // Final output
    final_output [shape=ellipse, style=filled, fillcolor=lightgreen, label="Final Output\nInput: [batch_size=1024, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=1024, seq_len=2048, vocab_size=50265]\nGPU: 0-11"];
}