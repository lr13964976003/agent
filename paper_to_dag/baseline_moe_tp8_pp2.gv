digraph baseline_moe_tp8_pp2 {
	rankdir=TB splines=ortho
	input [label="Input\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all" shape=ellipse]
	L0_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L0_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L0_S0_TP0_GPU0_qkv_linear -> L0_S0_TP0_GPU0_qkv_slice
	L0_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L0_S0_TP0_GPU0_qkv_slice -> L0_S0_TP0_GPU0_qk_matmul
	L0_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L0_S0_TP0_GPU0_qk_matmul -> L0_S0_TP0_GPU0_softmax
	L0_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L0_S0_TP0_GPU0_softmax -> L0_S0_TP0_GPU0_attn_v
	L0_S0_TP0_GPU0_qkv_slice -> L0_S0_TP0_GPU0_attn_v
	L0_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP0_GPU0_mha_ar
	L0_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP0_GPU0_mha_ar -> L0_S0_TP0_GPU0_mha_res
	L0_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L0_S0_TP0_GPU0_mha_res -> L0_S0_TP0_GPU0_gate
	L0_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L0_S0_TP0_GPU0_gate -> L0_S0_TP0_GPU0_expert0 [style=dashed]
	L0_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L0_S0_TP0_GPU0_gate -> L0_S0_TP0_GPU0_expert1 [style=dashed]
	L0_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP0_GPU0_moe_ar
	L0_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP0_GPU0_moe_ar -> L0_S0_TP0_GPU0_moe_res
	L0_S0_TP0_GPU0_mha_res -> L0_S0_TP0_GPU0_moe_res
	L0_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP0_GPU0_moe_res -> L0_S0_TP0_GPU0_norm
	L0_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L0_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L0_S0_TP1_GPU1_qkv_linear -> L0_S0_TP1_GPU1_qkv_slice
	L0_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L0_S0_TP1_GPU1_qkv_slice -> L0_S0_TP1_GPU1_qk_matmul
	L0_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L0_S0_TP1_GPU1_qk_matmul -> L0_S0_TP1_GPU1_softmax
	L0_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L0_S0_TP1_GPU1_softmax -> L0_S0_TP1_GPU1_attn_v
	L0_S0_TP1_GPU1_qkv_slice -> L0_S0_TP1_GPU1_attn_v
	L0_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP1_GPU1_mha_ar
	L0_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP1_GPU1_mha_ar -> L0_S0_TP1_GPU1_mha_res
	L0_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L0_S0_TP1_GPU1_mha_res -> L0_S0_TP1_GPU1_gate
	L0_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L0_S0_TP1_GPU1_gate -> L0_S0_TP1_GPU1_expert0 [style=dashed]
	L0_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L0_S0_TP1_GPU1_gate -> L0_S0_TP1_GPU1_expert1 [style=dashed]
	L0_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP1_GPU1_moe_ar
	L0_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP1_GPU1_moe_ar -> L0_S0_TP1_GPU1_moe_res
	L0_S0_TP1_GPU1_mha_res -> L0_S0_TP1_GPU1_moe_res
	L0_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP1_GPU1_moe_res -> L0_S0_TP1_GPU1_norm
	L0_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L0_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L0_S0_TP2_GPU2_qkv_linear -> L0_S0_TP2_GPU2_qkv_slice
	L0_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L0_S0_TP2_GPU2_qkv_slice -> L0_S0_TP2_GPU2_qk_matmul
	L0_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L0_S0_TP2_GPU2_qk_matmul -> L0_S0_TP2_GPU2_softmax
	L0_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L0_S0_TP2_GPU2_softmax -> L0_S0_TP2_GPU2_attn_v
	L0_S0_TP2_GPU2_qkv_slice -> L0_S0_TP2_GPU2_attn_v
	L0_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP2_GPU2_mha_ar
	L0_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP2_GPU2_mha_ar -> L0_S0_TP2_GPU2_mha_res
	L0_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L0_S0_TP2_GPU2_mha_res -> L0_S0_TP2_GPU2_gate
	L0_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L0_S0_TP2_GPU2_gate -> L0_S0_TP2_GPU2_expert0 [style=dashed]
	L0_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L0_S0_TP2_GPU2_gate -> L0_S0_TP2_GPU2_expert1 [style=dashed]
	L0_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP2_GPU2_moe_ar
	L0_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP2_GPU2_moe_ar -> L0_S0_TP2_GPU2_moe_res
	L0_S0_TP2_GPU2_mha_res -> L0_S0_TP2_GPU2_moe_res
	L0_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP2_GPU2_moe_res -> L0_S0_TP2_GPU2_norm
	L0_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L0_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L0_S0_TP3_GPU3_qkv_linear -> L0_S0_TP3_GPU3_qkv_slice
	L0_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L0_S0_TP3_GPU3_qkv_slice -> L0_S0_TP3_GPU3_qk_matmul
	L0_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L0_S0_TP3_GPU3_qk_matmul -> L0_S0_TP3_GPU3_softmax
	L0_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L0_S0_TP3_GPU3_softmax -> L0_S0_TP3_GPU3_attn_v
	L0_S0_TP3_GPU3_qkv_slice -> L0_S0_TP3_GPU3_attn_v
	L0_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP3_GPU3_mha_ar
	L0_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP3_GPU3_mha_ar -> L0_S0_TP3_GPU3_mha_res
	L0_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L0_S0_TP3_GPU3_mha_res -> L0_S0_TP3_GPU3_gate
	L0_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L0_S0_TP3_GPU3_gate -> L0_S0_TP3_GPU3_expert0 [style=dashed]
	L0_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L0_S0_TP3_GPU3_gate -> L0_S0_TP3_GPU3_expert1 [style=dashed]
	L0_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP3_GPU3_moe_ar
	L0_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP3_GPU3_moe_ar -> L0_S0_TP3_GPU3_moe_res
	L0_S0_TP3_GPU3_mha_res -> L0_S0_TP3_GPU3_moe_res
	L0_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP3_GPU3_moe_res -> L0_S0_TP3_GPU3_norm
	L0_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L0_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L0_S0_TP4_GPU4_qkv_linear -> L0_S0_TP4_GPU4_qkv_slice
	L0_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L0_S0_TP4_GPU4_qkv_slice -> L0_S0_TP4_GPU4_qk_matmul
	L0_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L0_S0_TP4_GPU4_qk_matmul -> L0_S0_TP4_GPU4_softmax
	L0_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L0_S0_TP4_GPU4_softmax -> L0_S0_TP4_GPU4_attn_v
	L0_S0_TP4_GPU4_qkv_slice -> L0_S0_TP4_GPU4_attn_v
	L0_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP4_GPU4_mha_ar
	L0_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP4_GPU4_mha_ar -> L0_S0_TP4_GPU4_mha_res
	L0_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L0_S0_TP4_GPU4_mha_res -> L0_S0_TP4_GPU4_gate
	L0_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L0_S0_TP4_GPU4_gate -> L0_S0_TP4_GPU4_expert0 [style=dashed]
	L0_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L0_S0_TP4_GPU4_gate -> L0_S0_TP4_GPU4_expert1 [style=dashed]
	L0_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP4_GPU4_moe_ar
	L0_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP4_GPU4_moe_ar -> L0_S0_TP4_GPU4_moe_res
	L0_S0_TP4_GPU4_mha_res -> L0_S0_TP4_GPU4_moe_res
	L0_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP4_GPU4_moe_res -> L0_S0_TP4_GPU4_norm
	L0_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L0_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L0_S0_TP5_GPU5_qkv_linear -> L0_S0_TP5_GPU5_qkv_slice
	L0_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L0_S0_TP5_GPU5_qkv_slice -> L0_S0_TP5_GPU5_qk_matmul
	L0_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L0_S0_TP5_GPU5_qk_matmul -> L0_S0_TP5_GPU5_softmax
	L0_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L0_S0_TP5_GPU5_softmax -> L0_S0_TP5_GPU5_attn_v
	L0_S0_TP5_GPU5_qkv_slice -> L0_S0_TP5_GPU5_attn_v
	L0_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP5_GPU5_mha_ar
	L0_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP5_GPU5_mha_ar -> L0_S0_TP5_GPU5_mha_res
	L0_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L0_S0_TP5_GPU5_mha_res -> L0_S0_TP5_GPU5_gate
	L0_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L0_S0_TP5_GPU5_gate -> L0_S0_TP5_GPU5_expert0 [style=dashed]
	L0_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L0_S0_TP5_GPU5_gate -> L0_S0_TP5_GPU5_expert1 [style=dashed]
	L0_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP5_GPU5_moe_ar
	L0_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP5_GPU5_moe_ar -> L0_S0_TP5_GPU5_moe_res
	L0_S0_TP5_GPU5_mha_res -> L0_S0_TP5_GPU5_moe_res
	L0_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP5_GPU5_moe_res -> L0_S0_TP5_GPU5_norm
	L0_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L0_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L0_S0_TP6_GPU6_qkv_linear -> L0_S0_TP6_GPU6_qkv_slice
	L0_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L0_S0_TP6_GPU6_qkv_slice -> L0_S0_TP6_GPU6_qk_matmul
	L0_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L0_S0_TP6_GPU6_qk_matmul -> L0_S0_TP6_GPU6_softmax
	L0_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L0_S0_TP6_GPU6_softmax -> L0_S0_TP6_GPU6_attn_v
	L0_S0_TP6_GPU6_qkv_slice -> L0_S0_TP6_GPU6_attn_v
	L0_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP6_GPU6_mha_ar
	L0_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP6_GPU6_mha_ar -> L0_S0_TP6_GPU6_mha_res
	L0_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L0_S0_TP6_GPU6_mha_res -> L0_S0_TP6_GPU6_gate
	L0_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L0_S0_TP6_GPU6_gate -> L0_S0_TP6_GPU6_expert0 [style=dashed]
	L0_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L0_S0_TP6_GPU6_gate -> L0_S0_TP6_GPU6_expert1 [style=dashed]
	L0_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP6_GPU6_moe_ar
	L0_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP6_GPU6_moe_ar -> L0_S0_TP6_GPU6_moe_res
	L0_S0_TP6_GPU6_mha_res -> L0_S0_TP6_GPU6_moe_res
	L0_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP6_GPU6_moe_res -> L0_S0_TP6_GPU6_norm
	L0_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L0_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L0_S0_TP7_GPU7_qkv_linear -> L0_S0_TP7_GPU7_qkv_slice
	L0_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L0_S0_TP7_GPU7_qkv_slice -> L0_S0_TP7_GPU7_qk_matmul
	L0_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L0_S0_TP7_GPU7_qk_matmul -> L0_S0_TP7_GPU7_softmax
	L0_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L0_S0_TP7_GPU7_softmax -> L0_S0_TP7_GPU7_attn_v
	L0_S0_TP7_GPU7_qkv_slice -> L0_S0_TP7_GPU7_attn_v
	L0_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP1_GPU1_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP2_GPU2_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP3_GPU3_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP4_GPU4_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP5_GPU5_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP6_GPU6_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP7_GPU7_attn_v -> L0_S0_TP7_GPU7_mha_ar
	L0_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP7_GPU7_mha_ar -> L0_S0_TP7_GPU7_mha_res
	L0_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L0_S0_TP7_GPU7_mha_res -> L0_S0_TP7_GPU7_gate
	L0_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L0_S0_TP7_GPU7_gate -> L0_S0_TP7_GPU7_expert0 [style=dashed]
	L0_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L0_S0_TP7_GPU7_gate -> L0_S0_TP7_GPU7_expert1 [style=dashed]
	L0_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L0_S0_TP0_GPU0_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP0_GPU0_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP1_GPU1_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP1_GPU1_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP2_GPU2_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP2_GPU2_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP3_GPU3_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP3_GPU3_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP4_GPU4_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP4_GPU4_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP5_GPU5_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP5_GPU5_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP6_GPU6_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP6_GPU6_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP7_GPU7_expert0 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP7_GPU7_expert1 -> L0_S0_TP7_GPU7_moe_ar
	L0_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP7_GPU7_moe_ar -> L0_S0_TP7_GPU7_moe_res
	L0_S0_TP7_GPU7_mha_res -> L0_S0_TP7_GPU7_moe_res
	L0_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L0_S0_TP7_GPU7_moe_res -> L0_S0_TP7_GPU7_norm
	L1_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L1_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L1_S0_TP0_GPU0_qkv_linear -> L1_S0_TP0_GPU0_qkv_slice
	L1_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L1_S0_TP0_GPU0_qkv_slice -> L1_S0_TP0_GPU0_qk_matmul
	L1_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L1_S0_TP0_GPU0_qk_matmul -> L1_S0_TP0_GPU0_softmax
	L1_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L1_S0_TP0_GPU0_softmax -> L1_S0_TP0_GPU0_attn_v
	L1_S0_TP0_GPU0_qkv_slice -> L1_S0_TP0_GPU0_attn_v
	L1_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP0_GPU0_mha_ar
	L1_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP0_GPU0_mha_ar -> L1_S0_TP0_GPU0_mha_res
	L1_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L1_S0_TP0_GPU0_mha_res -> L1_S0_TP0_GPU0_gate
	L1_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L1_S0_TP0_GPU0_gate -> L1_S0_TP0_GPU0_expert0 [style=dashed]
	L1_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L1_S0_TP0_GPU0_gate -> L1_S0_TP0_GPU0_expert1 [style=dashed]
	L1_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP0_GPU0_moe_ar
	L1_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP0_GPU0_moe_ar -> L1_S0_TP0_GPU0_moe_res
	L1_S0_TP0_GPU0_mha_res -> L1_S0_TP0_GPU0_moe_res
	L1_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP0_GPU0_moe_res -> L1_S0_TP0_GPU0_norm
	L1_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L1_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L1_S0_TP1_GPU1_qkv_linear -> L1_S0_TP1_GPU1_qkv_slice
	L1_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L1_S0_TP1_GPU1_qkv_slice -> L1_S0_TP1_GPU1_qk_matmul
	L1_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L1_S0_TP1_GPU1_qk_matmul -> L1_S0_TP1_GPU1_softmax
	L1_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L1_S0_TP1_GPU1_softmax -> L1_S0_TP1_GPU1_attn_v
	L1_S0_TP1_GPU1_qkv_slice -> L1_S0_TP1_GPU1_attn_v
	L1_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP1_GPU1_mha_ar
	L1_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP1_GPU1_mha_ar -> L1_S0_TP1_GPU1_mha_res
	L1_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L1_S0_TP1_GPU1_mha_res -> L1_S0_TP1_GPU1_gate
	L1_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L1_S0_TP1_GPU1_gate -> L1_S0_TP1_GPU1_expert0 [style=dashed]
	L1_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L1_S0_TP1_GPU1_gate -> L1_S0_TP1_GPU1_expert1 [style=dashed]
	L1_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP1_GPU1_moe_ar
	L1_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP1_GPU1_moe_ar -> L1_S0_TP1_GPU1_moe_res
	L1_S0_TP1_GPU1_mha_res -> L1_S0_TP1_GPU1_moe_res
	L1_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP1_GPU1_moe_res -> L1_S0_TP1_GPU1_norm
	L1_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L1_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L1_S0_TP2_GPU2_qkv_linear -> L1_S0_TP2_GPU2_qkv_slice
	L1_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L1_S0_TP2_GPU2_qkv_slice -> L1_S0_TP2_GPU2_qk_matmul
	L1_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L1_S0_TP2_GPU2_qk_matmul -> L1_S0_TP2_GPU2_softmax
	L1_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L1_S0_TP2_GPU2_softmax -> L1_S0_TP2_GPU2_attn_v
	L1_S0_TP2_GPU2_qkv_slice -> L1_S0_TP2_GPU2_attn_v
	L1_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP2_GPU2_mha_ar
	L1_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP2_GPU2_mha_ar -> L1_S0_TP2_GPU2_mha_res
	L1_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L1_S0_TP2_GPU2_mha_res -> L1_S0_TP2_GPU2_gate
	L1_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L1_S0_TP2_GPU2_gate -> L1_S0_TP2_GPU2_expert0 [style=dashed]
	L1_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L1_S0_TP2_GPU2_gate -> L1_S0_TP2_GPU2_expert1 [style=dashed]
	L1_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP2_GPU2_moe_ar
	L1_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP2_GPU2_moe_ar -> L1_S0_TP2_GPU2_moe_res
	L1_S0_TP2_GPU2_mha_res -> L1_S0_TP2_GPU2_moe_res
	L1_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP2_GPU2_moe_res -> L1_S0_TP2_GPU2_norm
	L1_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L1_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L1_S0_TP3_GPU3_qkv_linear -> L1_S0_TP3_GPU3_qkv_slice
	L1_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L1_S0_TP3_GPU3_qkv_slice -> L1_S0_TP3_GPU3_qk_matmul
	L1_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L1_S0_TP3_GPU3_qk_matmul -> L1_S0_TP3_GPU3_softmax
	L1_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L1_S0_TP3_GPU3_softmax -> L1_S0_TP3_GPU3_attn_v
	L1_S0_TP3_GPU3_qkv_slice -> L1_S0_TP3_GPU3_attn_v
	L1_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP3_GPU3_mha_ar
	L1_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP3_GPU3_mha_ar -> L1_S0_TP3_GPU3_mha_res
	L1_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L1_S0_TP3_GPU3_mha_res -> L1_S0_TP3_GPU3_gate
	L1_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L1_S0_TP3_GPU3_gate -> L1_S0_TP3_GPU3_expert0 [style=dashed]
	L1_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L1_S0_TP3_GPU3_gate -> L1_S0_TP3_GPU3_expert1 [style=dashed]
	L1_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP3_GPU3_moe_ar
	L1_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP3_GPU3_moe_ar -> L1_S0_TP3_GPU3_moe_res
	L1_S0_TP3_GPU3_mha_res -> L1_S0_TP3_GPU3_moe_res
	L1_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP3_GPU3_moe_res -> L1_S0_TP3_GPU3_norm
	L1_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L1_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L1_S0_TP4_GPU4_qkv_linear -> L1_S0_TP4_GPU4_qkv_slice
	L1_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L1_S0_TP4_GPU4_qkv_slice -> L1_S0_TP4_GPU4_qk_matmul
	L1_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L1_S0_TP4_GPU4_qk_matmul -> L1_S0_TP4_GPU4_softmax
	L1_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L1_S0_TP4_GPU4_softmax -> L1_S0_TP4_GPU4_attn_v
	L1_S0_TP4_GPU4_qkv_slice -> L1_S0_TP4_GPU4_attn_v
	L1_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP4_GPU4_mha_ar
	L1_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP4_GPU4_mha_ar -> L1_S0_TP4_GPU4_mha_res
	L1_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L1_S0_TP4_GPU4_mha_res -> L1_S0_TP4_GPU4_gate
	L1_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L1_S0_TP4_GPU4_gate -> L1_S0_TP4_GPU4_expert0 [style=dashed]
	L1_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L1_S0_TP4_GPU4_gate -> L1_S0_TP4_GPU4_expert1 [style=dashed]
	L1_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP4_GPU4_moe_ar
	L1_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP4_GPU4_moe_ar -> L1_S0_TP4_GPU4_moe_res
	L1_S0_TP4_GPU4_mha_res -> L1_S0_TP4_GPU4_moe_res
	L1_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP4_GPU4_moe_res -> L1_S0_TP4_GPU4_norm
	L1_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L1_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L1_S0_TP5_GPU5_qkv_linear -> L1_S0_TP5_GPU5_qkv_slice
	L1_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L1_S0_TP5_GPU5_qkv_slice -> L1_S0_TP5_GPU5_qk_matmul
	L1_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L1_S0_TP5_GPU5_qk_matmul -> L1_S0_TP5_GPU5_softmax
	L1_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L1_S0_TP5_GPU5_softmax -> L1_S0_TP5_GPU5_attn_v
	L1_S0_TP5_GPU5_qkv_slice -> L1_S0_TP5_GPU5_attn_v
	L1_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP5_GPU5_mha_ar
	L1_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP5_GPU5_mha_ar -> L1_S0_TP5_GPU5_mha_res
	L1_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L1_S0_TP5_GPU5_mha_res -> L1_S0_TP5_GPU5_gate
	L1_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L1_S0_TP5_GPU5_gate -> L1_S0_TP5_GPU5_expert0 [style=dashed]
	L1_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L1_S0_TP5_GPU5_gate -> L1_S0_TP5_GPU5_expert1 [style=dashed]
	L1_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP5_GPU5_moe_ar
	L1_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP5_GPU5_moe_ar -> L1_S0_TP5_GPU5_moe_res
	L1_S0_TP5_GPU5_mha_res -> L1_S0_TP5_GPU5_moe_res
	L1_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP5_GPU5_moe_res -> L1_S0_TP5_GPU5_norm
	L1_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L1_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L1_S0_TP6_GPU6_qkv_linear -> L1_S0_TP6_GPU6_qkv_slice
	L1_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L1_S0_TP6_GPU6_qkv_slice -> L1_S0_TP6_GPU6_qk_matmul
	L1_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L1_S0_TP6_GPU6_qk_matmul -> L1_S0_TP6_GPU6_softmax
	L1_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L1_S0_TP6_GPU6_softmax -> L1_S0_TP6_GPU6_attn_v
	L1_S0_TP6_GPU6_qkv_slice -> L1_S0_TP6_GPU6_attn_v
	L1_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP6_GPU6_mha_ar
	L1_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP6_GPU6_mha_ar -> L1_S0_TP6_GPU6_mha_res
	L1_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L1_S0_TP6_GPU6_mha_res -> L1_S0_TP6_GPU6_gate
	L1_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L1_S0_TP6_GPU6_gate -> L1_S0_TP6_GPU6_expert0 [style=dashed]
	L1_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L1_S0_TP6_GPU6_gate -> L1_S0_TP6_GPU6_expert1 [style=dashed]
	L1_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP6_GPU6_moe_ar
	L1_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP6_GPU6_moe_ar -> L1_S0_TP6_GPU6_moe_res
	L1_S0_TP6_GPU6_mha_res -> L1_S0_TP6_GPU6_moe_res
	L1_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP6_GPU6_moe_res -> L1_S0_TP6_GPU6_norm
	L1_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L1_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L1_S0_TP7_GPU7_qkv_linear -> L1_S0_TP7_GPU7_qkv_slice
	L1_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L1_S0_TP7_GPU7_qkv_slice -> L1_S0_TP7_GPU7_qk_matmul
	L1_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L1_S0_TP7_GPU7_qk_matmul -> L1_S0_TP7_GPU7_softmax
	L1_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L1_S0_TP7_GPU7_softmax -> L1_S0_TP7_GPU7_attn_v
	L1_S0_TP7_GPU7_qkv_slice -> L1_S0_TP7_GPU7_attn_v
	L1_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP1_GPU1_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP2_GPU2_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP3_GPU3_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP4_GPU4_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP5_GPU5_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP6_GPU6_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP7_GPU7_attn_v -> L1_S0_TP7_GPU7_mha_ar
	L1_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP7_GPU7_mha_ar -> L1_S0_TP7_GPU7_mha_res
	L1_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L1_S0_TP7_GPU7_mha_res -> L1_S0_TP7_GPU7_gate
	L1_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L1_S0_TP7_GPU7_gate -> L1_S0_TP7_GPU7_expert0 [style=dashed]
	L1_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L1_S0_TP7_GPU7_gate -> L1_S0_TP7_GPU7_expert1 [style=dashed]
	L1_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L1_S0_TP0_GPU0_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP0_GPU0_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP1_GPU1_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP1_GPU1_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP2_GPU2_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP2_GPU2_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP3_GPU3_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP3_GPU3_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP4_GPU4_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP4_GPU4_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP5_GPU5_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP5_GPU5_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP6_GPU6_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP6_GPU6_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP7_GPU7_expert0 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP7_GPU7_expert1 -> L1_S0_TP7_GPU7_moe_ar
	L1_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP7_GPU7_moe_ar -> L1_S0_TP7_GPU7_moe_res
	L1_S0_TP7_GPU7_mha_res -> L1_S0_TP7_GPU7_moe_res
	L1_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L1_S0_TP7_GPU7_moe_res -> L1_S0_TP7_GPU7_norm
	L2_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L2_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L2_S0_TP0_GPU0_qkv_linear -> L2_S0_TP0_GPU0_qkv_slice
	L2_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L2_S0_TP0_GPU0_qkv_slice -> L2_S0_TP0_GPU0_qk_matmul
	L2_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L2_S0_TP0_GPU0_qk_matmul -> L2_S0_TP0_GPU0_softmax
	L2_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L2_S0_TP0_GPU0_softmax -> L2_S0_TP0_GPU0_attn_v
	L2_S0_TP0_GPU0_qkv_slice -> L2_S0_TP0_GPU0_attn_v
	L2_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP0_GPU0_mha_ar
	L2_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP0_GPU0_mha_ar -> L2_S0_TP0_GPU0_mha_res
	L2_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L2_S0_TP0_GPU0_mha_res -> L2_S0_TP0_GPU0_gate
	L2_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L2_S0_TP0_GPU0_gate -> L2_S0_TP0_GPU0_expert0 [style=dashed]
	L2_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L2_S0_TP0_GPU0_gate -> L2_S0_TP0_GPU0_expert1 [style=dashed]
	L2_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP0_GPU0_moe_ar
	L2_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP0_GPU0_moe_ar -> L2_S0_TP0_GPU0_moe_res
	L2_S0_TP0_GPU0_mha_res -> L2_S0_TP0_GPU0_moe_res
	L2_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP0_GPU0_moe_res -> L2_S0_TP0_GPU0_norm
	L2_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L2_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L2_S0_TP1_GPU1_qkv_linear -> L2_S0_TP1_GPU1_qkv_slice
	L2_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L2_S0_TP1_GPU1_qkv_slice -> L2_S0_TP1_GPU1_qk_matmul
	L2_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L2_S0_TP1_GPU1_qk_matmul -> L2_S0_TP1_GPU1_softmax
	L2_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L2_S0_TP1_GPU1_softmax -> L2_S0_TP1_GPU1_attn_v
	L2_S0_TP1_GPU1_qkv_slice -> L2_S0_TP1_GPU1_attn_v
	L2_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP1_GPU1_mha_ar
	L2_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP1_GPU1_mha_ar -> L2_S0_TP1_GPU1_mha_res
	L2_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L2_S0_TP1_GPU1_mha_res -> L2_S0_TP1_GPU1_gate
	L2_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L2_S0_TP1_GPU1_gate -> L2_S0_TP1_GPU1_expert0 [style=dashed]
	L2_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L2_S0_TP1_GPU1_gate -> L2_S0_TP1_GPU1_expert1 [style=dashed]
	L2_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP1_GPU1_moe_ar
	L2_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP1_GPU1_moe_ar -> L2_S0_TP1_GPU1_moe_res
	L2_S0_TP1_GPU1_mha_res -> L2_S0_TP1_GPU1_moe_res
	L2_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP1_GPU1_moe_res -> L2_S0_TP1_GPU1_norm
	L2_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L2_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L2_S0_TP2_GPU2_qkv_linear -> L2_S0_TP2_GPU2_qkv_slice
	L2_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L2_S0_TP2_GPU2_qkv_slice -> L2_S0_TP2_GPU2_qk_matmul
	L2_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L2_S0_TP2_GPU2_qk_matmul -> L2_S0_TP2_GPU2_softmax
	L2_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L2_S0_TP2_GPU2_softmax -> L2_S0_TP2_GPU2_attn_v
	L2_S0_TP2_GPU2_qkv_slice -> L2_S0_TP2_GPU2_attn_v
	L2_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP2_GPU2_mha_ar
	L2_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP2_GPU2_mha_ar -> L2_S0_TP2_GPU2_mha_res
	L2_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L2_S0_TP2_GPU2_mha_res -> L2_S0_TP2_GPU2_gate
	L2_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L2_S0_TP2_GPU2_gate -> L2_S0_TP2_GPU2_expert0 [style=dashed]
	L2_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L2_S0_TP2_GPU2_gate -> L2_S0_TP2_GPU2_expert1 [style=dashed]
	L2_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP2_GPU2_moe_ar
	L2_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP2_GPU2_moe_ar -> L2_S0_TP2_GPU2_moe_res
	L2_S0_TP2_GPU2_mha_res -> L2_S0_TP2_GPU2_moe_res
	L2_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP2_GPU2_moe_res -> L2_S0_TP2_GPU2_norm
	L2_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L2_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L2_S0_TP3_GPU3_qkv_linear -> L2_S0_TP3_GPU3_qkv_slice
	L2_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L2_S0_TP3_GPU3_qkv_slice -> L2_S0_TP3_GPU3_qk_matmul
	L2_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L2_S0_TP3_GPU3_qk_matmul -> L2_S0_TP3_GPU3_softmax
	L2_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L2_S0_TP3_GPU3_softmax -> L2_S0_TP3_GPU3_attn_v
	L2_S0_TP3_GPU3_qkv_slice -> L2_S0_TP3_GPU3_attn_v
	L2_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP3_GPU3_mha_ar
	L2_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP3_GPU3_mha_ar -> L2_S0_TP3_GPU3_mha_res
	L2_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L2_S0_TP3_GPU3_mha_res -> L2_S0_TP3_GPU3_gate
	L2_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L2_S0_TP3_GPU3_gate -> L2_S0_TP3_GPU3_expert0 [style=dashed]
	L2_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L2_S0_TP3_GPU3_gate -> L2_S0_TP3_GPU3_expert1 [style=dashed]
	L2_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP3_GPU3_moe_ar
	L2_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP3_GPU3_moe_ar -> L2_S0_TP3_GPU3_moe_res
	L2_S0_TP3_GPU3_mha_res -> L2_S0_TP3_GPU3_moe_res
	L2_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP3_GPU3_moe_res -> L2_S0_TP3_GPU3_norm
	L2_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L2_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L2_S0_TP4_GPU4_qkv_linear -> L2_S0_TP4_GPU4_qkv_slice
	L2_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L2_S0_TP4_GPU4_qkv_slice -> L2_S0_TP4_GPU4_qk_matmul
	L2_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L2_S0_TP4_GPU4_qk_matmul -> L2_S0_TP4_GPU4_softmax
	L2_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L2_S0_TP4_GPU4_softmax -> L2_S0_TP4_GPU4_attn_v
	L2_S0_TP4_GPU4_qkv_slice -> L2_S0_TP4_GPU4_attn_v
	L2_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP4_GPU4_mha_ar
	L2_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP4_GPU4_mha_ar -> L2_S0_TP4_GPU4_mha_res
	L2_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L2_S0_TP4_GPU4_mha_res -> L2_S0_TP4_GPU4_gate
	L2_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L2_S0_TP4_GPU4_gate -> L2_S0_TP4_GPU4_expert0 [style=dashed]
	L2_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L2_S0_TP4_GPU4_gate -> L2_S0_TP4_GPU4_expert1 [style=dashed]
	L2_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP4_GPU4_moe_ar
	L2_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP4_GPU4_moe_ar -> L2_S0_TP4_GPU4_moe_res
	L2_S0_TP4_GPU4_mha_res -> L2_S0_TP4_GPU4_moe_res
	L2_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP4_GPU4_moe_res -> L2_S0_TP4_GPU4_norm
	L2_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L2_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L2_S0_TP5_GPU5_qkv_linear -> L2_S0_TP5_GPU5_qkv_slice
	L2_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L2_S0_TP5_GPU5_qkv_slice -> L2_S0_TP5_GPU5_qk_matmul
	L2_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L2_S0_TP5_GPU5_qk_matmul -> L2_S0_TP5_GPU5_softmax
	L2_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L2_S0_TP5_GPU5_softmax -> L2_S0_TP5_GPU5_attn_v
	L2_S0_TP5_GPU5_qkv_slice -> L2_S0_TP5_GPU5_attn_v
	L2_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP5_GPU5_mha_ar
	L2_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP5_GPU5_mha_ar -> L2_S0_TP5_GPU5_mha_res
	L2_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L2_S0_TP5_GPU5_mha_res -> L2_S0_TP5_GPU5_gate
	L2_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L2_S0_TP5_GPU5_gate -> L2_S0_TP5_GPU5_expert0 [style=dashed]
	L2_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L2_S0_TP5_GPU5_gate -> L2_S0_TP5_GPU5_expert1 [style=dashed]
	L2_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP5_GPU5_moe_ar
	L2_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP5_GPU5_moe_ar -> L2_S0_TP5_GPU5_moe_res
	L2_S0_TP5_GPU5_mha_res -> L2_S0_TP5_GPU5_moe_res
	L2_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP5_GPU5_moe_res -> L2_S0_TP5_GPU5_norm
	L2_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L2_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L2_S0_TP6_GPU6_qkv_linear -> L2_S0_TP6_GPU6_qkv_slice
	L2_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L2_S0_TP6_GPU6_qkv_slice -> L2_S0_TP6_GPU6_qk_matmul
	L2_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L2_S0_TP6_GPU6_qk_matmul -> L2_S0_TP6_GPU6_softmax
	L2_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L2_S0_TP6_GPU6_softmax -> L2_S0_TP6_GPU6_attn_v
	L2_S0_TP6_GPU6_qkv_slice -> L2_S0_TP6_GPU6_attn_v
	L2_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP6_GPU6_mha_ar
	L2_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP6_GPU6_mha_ar -> L2_S0_TP6_GPU6_mha_res
	L2_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L2_S0_TP6_GPU6_mha_res -> L2_S0_TP6_GPU6_gate
	L2_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L2_S0_TP6_GPU6_gate -> L2_S0_TP6_GPU6_expert0 [style=dashed]
	L2_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L2_S0_TP6_GPU6_gate -> L2_S0_TP6_GPU6_expert1 [style=dashed]
	L2_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP6_GPU6_moe_ar
	L2_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP6_GPU6_moe_ar -> L2_S0_TP6_GPU6_moe_res
	L2_S0_TP6_GPU6_mha_res -> L2_S0_TP6_GPU6_moe_res
	L2_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP6_GPU6_moe_res -> L2_S0_TP6_GPU6_norm
	L2_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L2_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L2_S0_TP7_GPU7_qkv_linear -> L2_S0_TP7_GPU7_qkv_slice
	L2_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L2_S0_TP7_GPU7_qkv_slice -> L2_S0_TP7_GPU7_qk_matmul
	L2_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L2_S0_TP7_GPU7_qk_matmul -> L2_S0_TP7_GPU7_softmax
	L2_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L2_S0_TP7_GPU7_softmax -> L2_S0_TP7_GPU7_attn_v
	L2_S0_TP7_GPU7_qkv_slice -> L2_S0_TP7_GPU7_attn_v
	L2_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP1_GPU1_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP2_GPU2_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP3_GPU3_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP4_GPU4_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP5_GPU5_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP6_GPU6_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP7_GPU7_attn_v -> L2_S0_TP7_GPU7_mha_ar
	L2_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP7_GPU7_mha_ar -> L2_S0_TP7_GPU7_mha_res
	L2_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L2_S0_TP7_GPU7_mha_res -> L2_S0_TP7_GPU7_gate
	L2_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L2_S0_TP7_GPU7_gate -> L2_S0_TP7_GPU7_expert0 [style=dashed]
	L2_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L2_S0_TP7_GPU7_gate -> L2_S0_TP7_GPU7_expert1 [style=dashed]
	L2_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L2_S0_TP0_GPU0_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP0_GPU0_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP1_GPU1_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP1_GPU1_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP2_GPU2_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP2_GPU2_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP3_GPU3_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP3_GPU3_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP4_GPU4_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP4_GPU4_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP5_GPU5_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP5_GPU5_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP6_GPU6_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP6_GPU6_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP7_GPU7_expert0 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP7_GPU7_expert1 -> L2_S0_TP7_GPU7_moe_ar
	L2_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP7_GPU7_moe_ar -> L2_S0_TP7_GPU7_moe_res
	L2_S0_TP7_GPU7_mha_res -> L2_S0_TP7_GPU7_moe_res
	L2_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L2_S0_TP7_GPU7_moe_res -> L2_S0_TP7_GPU7_norm
	L3_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L3_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L3_S0_TP0_GPU0_qkv_linear -> L3_S0_TP0_GPU0_qkv_slice
	L3_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L3_S0_TP0_GPU0_qkv_slice -> L3_S0_TP0_GPU0_qk_matmul
	L3_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L3_S0_TP0_GPU0_qk_matmul -> L3_S0_TP0_GPU0_softmax
	L3_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L3_S0_TP0_GPU0_softmax -> L3_S0_TP0_GPU0_attn_v
	L3_S0_TP0_GPU0_qkv_slice -> L3_S0_TP0_GPU0_attn_v
	L3_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP0_GPU0_mha_ar
	L3_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP0_GPU0_mha_ar -> L3_S0_TP0_GPU0_mha_res
	L3_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L3_S0_TP0_GPU0_mha_res -> L3_S0_TP0_GPU0_gate
	L3_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L3_S0_TP0_GPU0_gate -> L3_S0_TP0_GPU0_expert0 [style=dashed]
	L3_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L3_S0_TP0_GPU0_gate -> L3_S0_TP0_GPU0_expert1 [style=dashed]
	L3_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP0_GPU0_moe_ar
	L3_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP0_GPU0_moe_ar -> L3_S0_TP0_GPU0_moe_res
	L3_S0_TP0_GPU0_mha_res -> L3_S0_TP0_GPU0_moe_res
	L3_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP0_GPU0_moe_res -> L3_S0_TP0_GPU0_norm
	L3_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L3_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L3_S0_TP1_GPU1_qkv_linear -> L3_S0_TP1_GPU1_qkv_slice
	L3_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L3_S0_TP1_GPU1_qkv_slice -> L3_S0_TP1_GPU1_qk_matmul
	L3_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L3_S0_TP1_GPU1_qk_matmul -> L3_S0_TP1_GPU1_softmax
	L3_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L3_S0_TP1_GPU1_softmax -> L3_S0_TP1_GPU1_attn_v
	L3_S0_TP1_GPU1_qkv_slice -> L3_S0_TP1_GPU1_attn_v
	L3_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP1_GPU1_mha_ar
	L3_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP1_GPU1_mha_ar -> L3_S0_TP1_GPU1_mha_res
	L3_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L3_S0_TP1_GPU1_mha_res -> L3_S0_TP1_GPU1_gate
	L3_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L3_S0_TP1_GPU1_gate -> L3_S0_TP1_GPU1_expert0 [style=dashed]
	L3_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L3_S0_TP1_GPU1_gate -> L3_S0_TP1_GPU1_expert1 [style=dashed]
	L3_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP1_GPU1_moe_ar
	L3_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP1_GPU1_moe_ar -> L3_S0_TP1_GPU1_moe_res
	L3_S0_TP1_GPU1_mha_res -> L3_S0_TP1_GPU1_moe_res
	L3_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP1_GPU1_moe_res -> L3_S0_TP1_GPU1_norm
	L3_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L3_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L3_S0_TP2_GPU2_qkv_linear -> L3_S0_TP2_GPU2_qkv_slice
	L3_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L3_S0_TP2_GPU2_qkv_slice -> L3_S0_TP2_GPU2_qk_matmul
	L3_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L3_S0_TP2_GPU2_qk_matmul -> L3_S0_TP2_GPU2_softmax
	L3_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L3_S0_TP2_GPU2_softmax -> L3_S0_TP2_GPU2_attn_v
	L3_S0_TP2_GPU2_qkv_slice -> L3_S0_TP2_GPU2_attn_v
	L3_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP2_GPU2_mha_ar
	L3_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP2_GPU2_mha_ar -> L3_S0_TP2_GPU2_mha_res
	L3_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L3_S0_TP2_GPU2_mha_res -> L3_S0_TP2_GPU2_gate
	L3_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L3_S0_TP2_GPU2_gate -> L3_S0_TP2_GPU2_expert0 [style=dashed]
	L3_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L3_S0_TP2_GPU2_gate -> L3_S0_TP2_GPU2_expert1 [style=dashed]
	L3_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP2_GPU2_moe_ar
	L3_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP2_GPU2_moe_ar -> L3_S0_TP2_GPU2_moe_res
	L3_S0_TP2_GPU2_mha_res -> L3_S0_TP2_GPU2_moe_res
	L3_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP2_GPU2_moe_res -> L3_S0_TP2_GPU2_norm
	L3_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L3_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L3_S0_TP3_GPU3_qkv_linear -> L3_S0_TP3_GPU3_qkv_slice
	L3_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L3_S0_TP3_GPU3_qkv_slice -> L3_S0_TP3_GPU3_qk_matmul
	L3_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L3_S0_TP3_GPU3_qk_matmul -> L3_S0_TP3_GPU3_softmax
	L3_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L3_S0_TP3_GPU3_softmax -> L3_S0_TP3_GPU3_attn_v
	L3_S0_TP3_GPU3_qkv_slice -> L3_S0_TP3_GPU3_attn_v
	L3_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP3_GPU3_mha_ar
	L3_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP3_GPU3_mha_ar -> L3_S0_TP3_GPU3_mha_res
	L3_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L3_S0_TP3_GPU3_mha_res -> L3_S0_TP3_GPU3_gate
	L3_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L3_S0_TP3_GPU3_gate -> L3_S0_TP3_GPU3_expert0 [style=dashed]
	L3_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L3_S0_TP3_GPU3_gate -> L3_S0_TP3_GPU3_expert1 [style=dashed]
	L3_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP3_GPU3_moe_ar
	L3_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP3_GPU3_moe_ar -> L3_S0_TP3_GPU3_moe_res
	L3_S0_TP3_GPU3_mha_res -> L3_S0_TP3_GPU3_moe_res
	L3_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP3_GPU3_moe_res -> L3_S0_TP3_GPU3_norm
	L3_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L3_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L3_S0_TP4_GPU4_qkv_linear -> L3_S0_TP4_GPU4_qkv_slice
	L3_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L3_S0_TP4_GPU4_qkv_slice -> L3_S0_TP4_GPU4_qk_matmul
	L3_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L3_S0_TP4_GPU4_qk_matmul -> L3_S0_TP4_GPU4_softmax
	L3_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L3_S0_TP4_GPU4_softmax -> L3_S0_TP4_GPU4_attn_v
	L3_S0_TP4_GPU4_qkv_slice -> L3_S0_TP4_GPU4_attn_v
	L3_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP4_GPU4_mha_ar
	L3_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP4_GPU4_mha_ar -> L3_S0_TP4_GPU4_mha_res
	L3_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L3_S0_TP4_GPU4_mha_res -> L3_S0_TP4_GPU4_gate
	L3_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L3_S0_TP4_GPU4_gate -> L3_S0_TP4_GPU4_expert0 [style=dashed]
	L3_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L3_S0_TP4_GPU4_gate -> L3_S0_TP4_GPU4_expert1 [style=dashed]
	L3_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP4_GPU4_moe_ar
	L3_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP4_GPU4_moe_ar -> L3_S0_TP4_GPU4_moe_res
	L3_S0_TP4_GPU4_mha_res -> L3_S0_TP4_GPU4_moe_res
	L3_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP4_GPU4_moe_res -> L3_S0_TP4_GPU4_norm
	L3_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L3_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L3_S0_TP5_GPU5_qkv_linear -> L3_S0_TP5_GPU5_qkv_slice
	L3_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L3_S0_TP5_GPU5_qkv_slice -> L3_S0_TP5_GPU5_qk_matmul
	L3_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L3_S0_TP5_GPU5_qk_matmul -> L3_S0_TP5_GPU5_softmax
	L3_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L3_S0_TP5_GPU5_softmax -> L3_S0_TP5_GPU5_attn_v
	L3_S0_TP5_GPU5_qkv_slice -> L3_S0_TP5_GPU5_attn_v
	L3_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP5_GPU5_mha_ar
	L3_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP5_GPU5_mha_ar -> L3_S0_TP5_GPU5_mha_res
	L3_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L3_S0_TP5_GPU5_mha_res -> L3_S0_TP5_GPU5_gate
	L3_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L3_S0_TP5_GPU5_gate -> L3_S0_TP5_GPU5_expert0 [style=dashed]
	L3_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L3_S0_TP5_GPU5_gate -> L3_S0_TP5_GPU5_expert1 [style=dashed]
	L3_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP5_GPU5_moe_ar
	L3_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP5_GPU5_moe_ar -> L3_S0_TP5_GPU5_moe_res
	L3_S0_TP5_GPU5_mha_res -> L3_S0_TP5_GPU5_moe_res
	L3_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP5_GPU5_moe_res -> L3_S0_TP5_GPU5_norm
	L3_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L3_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L3_S0_TP6_GPU6_qkv_linear -> L3_S0_TP6_GPU6_qkv_slice
	L3_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L3_S0_TP6_GPU6_qkv_slice -> L3_S0_TP6_GPU6_qk_matmul
	L3_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L3_S0_TP6_GPU6_qk_matmul -> L3_S0_TP6_GPU6_softmax
	L3_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L3_S0_TP6_GPU6_softmax -> L3_S0_TP6_GPU6_attn_v
	L3_S0_TP6_GPU6_qkv_slice -> L3_S0_TP6_GPU6_attn_v
	L3_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP6_GPU6_mha_ar
	L3_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP6_GPU6_mha_ar -> L3_S0_TP6_GPU6_mha_res
	L3_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L3_S0_TP6_GPU6_mha_res -> L3_S0_TP6_GPU6_gate
	L3_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L3_S0_TP6_GPU6_gate -> L3_S0_TP6_GPU6_expert0 [style=dashed]
	L3_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L3_S0_TP6_GPU6_gate -> L3_S0_TP6_GPU6_expert1 [style=dashed]
	L3_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP6_GPU6_moe_ar
	L3_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP6_GPU6_moe_ar -> L3_S0_TP6_GPU6_moe_res
	L3_S0_TP6_GPU6_mha_res -> L3_S0_TP6_GPU6_moe_res
	L3_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP6_GPU6_moe_res -> L3_S0_TP6_GPU6_norm
	L3_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L3_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L3_S0_TP7_GPU7_qkv_linear -> L3_S0_TP7_GPU7_qkv_slice
	L3_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L3_S0_TP7_GPU7_qkv_slice -> L3_S0_TP7_GPU7_qk_matmul
	L3_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L3_S0_TP7_GPU7_qk_matmul -> L3_S0_TP7_GPU7_softmax
	L3_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L3_S0_TP7_GPU7_softmax -> L3_S0_TP7_GPU7_attn_v
	L3_S0_TP7_GPU7_qkv_slice -> L3_S0_TP7_GPU7_attn_v
	L3_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP1_GPU1_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP2_GPU2_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP3_GPU3_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP4_GPU4_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP5_GPU5_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP6_GPU6_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP7_GPU7_attn_v -> L3_S0_TP7_GPU7_mha_ar
	L3_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP7_GPU7_mha_ar -> L3_S0_TP7_GPU7_mha_res
	L3_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L3_S0_TP7_GPU7_mha_res -> L3_S0_TP7_GPU7_gate
	L3_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L3_S0_TP7_GPU7_gate -> L3_S0_TP7_GPU7_expert0 [style=dashed]
	L3_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L3_S0_TP7_GPU7_gate -> L3_S0_TP7_GPU7_expert1 [style=dashed]
	L3_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L3_S0_TP0_GPU0_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP0_GPU0_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP1_GPU1_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP1_GPU1_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP2_GPU2_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP2_GPU2_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP3_GPU3_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP3_GPU3_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP4_GPU4_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP4_GPU4_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP5_GPU5_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP5_GPU5_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP6_GPU6_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP6_GPU6_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP7_GPU7_expert0 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP7_GPU7_expert1 -> L3_S0_TP7_GPU7_moe_ar
	L3_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP7_GPU7_moe_ar -> L3_S0_TP7_GPU7_moe_res
	L3_S0_TP7_GPU7_mha_res -> L3_S0_TP7_GPU7_moe_res
	L3_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L3_S0_TP7_GPU7_moe_res -> L3_S0_TP7_GPU7_norm
	L4_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L4_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L4_S0_TP0_GPU0_qkv_linear -> L4_S0_TP0_GPU0_qkv_slice
	L4_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L4_S0_TP0_GPU0_qkv_slice -> L4_S0_TP0_GPU0_qk_matmul
	L4_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L4_S0_TP0_GPU0_qk_matmul -> L4_S0_TP0_GPU0_softmax
	L4_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L4_S0_TP0_GPU0_softmax -> L4_S0_TP0_GPU0_attn_v
	L4_S0_TP0_GPU0_qkv_slice -> L4_S0_TP0_GPU0_attn_v
	L4_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP0_GPU0_mha_ar
	L4_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP0_GPU0_mha_ar -> L4_S0_TP0_GPU0_mha_res
	L4_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L4_S0_TP0_GPU0_mha_res -> L4_S0_TP0_GPU0_gate
	L4_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L4_S0_TP0_GPU0_gate -> L4_S0_TP0_GPU0_expert0 [style=dashed]
	L4_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L4_S0_TP0_GPU0_gate -> L4_S0_TP0_GPU0_expert1 [style=dashed]
	L4_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP0_GPU0_moe_ar
	L4_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP0_GPU0_moe_ar -> L4_S0_TP0_GPU0_moe_res
	L4_S0_TP0_GPU0_mha_res -> L4_S0_TP0_GPU0_moe_res
	L4_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP0_GPU0_moe_res -> L4_S0_TP0_GPU0_norm
	L4_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L4_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L4_S0_TP1_GPU1_qkv_linear -> L4_S0_TP1_GPU1_qkv_slice
	L4_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L4_S0_TP1_GPU1_qkv_slice -> L4_S0_TP1_GPU1_qk_matmul
	L4_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L4_S0_TP1_GPU1_qk_matmul -> L4_S0_TP1_GPU1_softmax
	L4_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L4_S0_TP1_GPU1_softmax -> L4_S0_TP1_GPU1_attn_v
	L4_S0_TP1_GPU1_qkv_slice -> L4_S0_TP1_GPU1_attn_v
	L4_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP1_GPU1_mha_ar
	L4_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP1_GPU1_mha_ar -> L4_S0_TP1_GPU1_mha_res
	L4_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L4_S0_TP1_GPU1_mha_res -> L4_S0_TP1_GPU1_gate
	L4_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L4_S0_TP1_GPU1_gate -> L4_S0_TP1_GPU1_expert0 [style=dashed]
	L4_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L4_S0_TP1_GPU1_gate -> L4_S0_TP1_GPU1_expert1 [style=dashed]
	L4_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP1_GPU1_moe_ar
	L4_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP1_GPU1_moe_ar -> L4_S0_TP1_GPU1_moe_res
	L4_S0_TP1_GPU1_mha_res -> L4_S0_TP1_GPU1_moe_res
	L4_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP1_GPU1_moe_res -> L4_S0_TP1_GPU1_norm
	L4_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L4_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L4_S0_TP2_GPU2_qkv_linear -> L4_S0_TP2_GPU2_qkv_slice
	L4_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L4_S0_TP2_GPU2_qkv_slice -> L4_S0_TP2_GPU2_qk_matmul
	L4_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L4_S0_TP2_GPU2_qk_matmul -> L4_S0_TP2_GPU2_softmax
	L4_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L4_S0_TP2_GPU2_softmax -> L4_S0_TP2_GPU2_attn_v
	L4_S0_TP2_GPU2_qkv_slice -> L4_S0_TP2_GPU2_attn_v
	L4_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP2_GPU2_mha_ar
	L4_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP2_GPU2_mha_ar -> L4_S0_TP2_GPU2_mha_res
	L4_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L4_S0_TP2_GPU2_mha_res -> L4_S0_TP2_GPU2_gate
	L4_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L4_S0_TP2_GPU2_gate -> L4_S0_TP2_GPU2_expert0 [style=dashed]
	L4_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L4_S0_TP2_GPU2_gate -> L4_S0_TP2_GPU2_expert1 [style=dashed]
	L4_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP2_GPU2_moe_ar
	L4_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP2_GPU2_moe_ar -> L4_S0_TP2_GPU2_moe_res
	L4_S0_TP2_GPU2_mha_res -> L4_S0_TP2_GPU2_moe_res
	L4_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP2_GPU2_moe_res -> L4_S0_TP2_GPU2_norm
	L4_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L4_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L4_S0_TP3_GPU3_qkv_linear -> L4_S0_TP3_GPU3_qkv_slice
	L4_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L4_S0_TP3_GPU3_qkv_slice -> L4_S0_TP3_GPU3_qk_matmul
	L4_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L4_S0_TP3_GPU3_qk_matmul -> L4_S0_TP3_GPU3_softmax
	L4_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L4_S0_TP3_GPU3_softmax -> L4_S0_TP3_GPU3_attn_v
	L4_S0_TP3_GPU3_qkv_slice -> L4_S0_TP3_GPU3_attn_v
	L4_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP3_GPU3_mha_ar
	L4_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP3_GPU3_mha_ar -> L4_S0_TP3_GPU3_mha_res
	L4_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L4_S0_TP3_GPU3_mha_res -> L4_S0_TP3_GPU3_gate
	L4_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L4_S0_TP3_GPU3_gate -> L4_S0_TP3_GPU3_expert0 [style=dashed]
	L4_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L4_S0_TP3_GPU3_gate -> L4_S0_TP3_GPU3_expert1 [style=dashed]
	L4_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP3_GPU3_moe_ar
	L4_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP3_GPU3_moe_ar -> L4_S0_TP3_GPU3_moe_res
	L4_S0_TP3_GPU3_mha_res -> L4_S0_TP3_GPU3_moe_res
	L4_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP3_GPU3_moe_res -> L4_S0_TP3_GPU3_norm
	L4_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L4_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L4_S0_TP4_GPU4_qkv_linear -> L4_S0_TP4_GPU4_qkv_slice
	L4_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L4_S0_TP4_GPU4_qkv_slice -> L4_S0_TP4_GPU4_qk_matmul
	L4_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L4_S0_TP4_GPU4_qk_matmul -> L4_S0_TP4_GPU4_softmax
	L4_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L4_S0_TP4_GPU4_softmax -> L4_S0_TP4_GPU4_attn_v
	L4_S0_TP4_GPU4_qkv_slice -> L4_S0_TP4_GPU4_attn_v
	L4_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP4_GPU4_mha_ar
	L4_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP4_GPU4_mha_ar -> L4_S0_TP4_GPU4_mha_res
	L4_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L4_S0_TP4_GPU4_mha_res -> L4_S0_TP4_GPU4_gate
	L4_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L4_S0_TP4_GPU4_gate -> L4_S0_TP4_GPU4_expert0 [style=dashed]
	L4_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L4_S0_TP4_GPU4_gate -> L4_S0_TP4_GPU4_expert1 [style=dashed]
	L4_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP4_GPU4_moe_ar
	L4_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP4_GPU4_moe_ar -> L4_S0_TP4_GPU4_moe_res
	L4_S0_TP4_GPU4_mha_res -> L4_S0_TP4_GPU4_moe_res
	L4_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP4_GPU4_moe_res -> L4_S0_TP4_GPU4_norm
	L4_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L4_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L4_S0_TP5_GPU5_qkv_linear -> L4_S0_TP5_GPU5_qkv_slice
	L4_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L4_S0_TP5_GPU5_qkv_slice -> L4_S0_TP5_GPU5_qk_matmul
	L4_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L4_S0_TP5_GPU5_qk_matmul -> L4_S0_TP5_GPU5_softmax
	L4_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L4_S0_TP5_GPU5_softmax -> L4_S0_TP5_GPU5_attn_v
	L4_S0_TP5_GPU5_qkv_slice -> L4_S0_TP5_GPU5_attn_v
	L4_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP5_GPU5_mha_ar
	L4_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP5_GPU5_mha_ar -> L4_S0_TP5_GPU5_mha_res
	L4_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L4_S0_TP5_GPU5_mha_res -> L4_S0_TP5_GPU5_gate
	L4_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L4_S0_TP5_GPU5_gate -> L4_S0_TP5_GPU5_expert0 [style=dashed]
	L4_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L4_S0_TP5_GPU5_gate -> L4_S0_TP5_GPU5_expert1 [style=dashed]
	L4_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP5_GPU5_moe_ar
	L4_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP5_GPU5_moe_ar -> L4_S0_TP5_GPU5_moe_res
	L4_S0_TP5_GPU5_mha_res -> L4_S0_TP5_GPU5_moe_res
	L4_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP5_GPU5_moe_res -> L4_S0_TP5_GPU5_norm
	L4_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L4_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L4_S0_TP6_GPU6_qkv_linear -> L4_S0_TP6_GPU6_qkv_slice
	L4_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L4_S0_TP6_GPU6_qkv_slice -> L4_S0_TP6_GPU6_qk_matmul
	L4_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L4_S0_TP6_GPU6_qk_matmul -> L4_S0_TP6_GPU6_softmax
	L4_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L4_S0_TP6_GPU6_softmax -> L4_S0_TP6_GPU6_attn_v
	L4_S0_TP6_GPU6_qkv_slice -> L4_S0_TP6_GPU6_attn_v
	L4_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP6_GPU6_mha_ar
	L4_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP6_GPU6_mha_ar -> L4_S0_TP6_GPU6_mha_res
	L4_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L4_S0_TP6_GPU6_mha_res -> L4_S0_TP6_GPU6_gate
	L4_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L4_S0_TP6_GPU6_gate -> L4_S0_TP6_GPU6_expert0 [style=dashed]
	L4_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L4_S0_TP6_GPU6_gate -> L4_S0_TP6_GPU6_expert1 [style=dashed]
	L4_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP6_GPU6_moe_ar
	L4_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP6_GPU6_moe_ar -> L4_S0_TP6_GPU6_moe_res
	L4_S0_TP6_GPU6_mha_res -> L4_S0_TP6_GPU6_moe_res
	L4_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP6_GPU6_moe_res -> L4_S0_TP6_GPU6_norm
	L4_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L4_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L4_S0_TP7_GPU7_qkv_linear -> L4_S0_TP7_GPU7_qkv_slice
	L4_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L4_S0_TP7_GPU7_qkv_slice -> L4_S0_TP7_GPU7_qk_matmul
	L4_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L4_S0_TP7_GPU7_qk_matmul -> L4_S0_TP7_GPU7_softmax
	L4_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L4_S0_TP7_GPU7_softmax -> L4_S0_TP7_GPU7_attn_v
	L4_S0_TP7_GPU7_qkv_slice -> L4_S0_TP7_GPU7_attn_v
	L4_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP1_GPU1_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP2_GPU2_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP3_GPU3_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP4_GPU4_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP5_GPU5_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP6_GPU6_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP7_GPU7_attn_v -> L4_S0_TP7_GPU7_mha_ar
	L4_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP7_GPU7_mha_ar -> L4_S0_TP7_GPU7_mha_res
	L4_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L4_S0_TP7_GPU7_mha_res -> L4_S0_TP7_GPU7_gate
	L4_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L4_S0_TP7_GPU7_gate -> L4_S0_TP7_GPU7_expert0 [style=dashed]
	L4_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L4_S0_TP7_GPU7_gate -> L4_S0_TP7_GPU7_expert1 [style=dashed]
	L4_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L4_S0_TP0_GPU0_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP0_GPU0_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP1_GPU1_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP1_GPU1_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP2_GPU2_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP2_GPU2_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP3_GPU3_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP3_GPU3_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP4_GPU4_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP4_GPU4_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP5_GPU5_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP5_GPU5_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP6_GPU6_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP6_GPU6_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP7_GPU7_expert0 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP7_GPU7_expert1 -> L4_S0_TP7_GPU7_moe_ar
	L4_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP7_GPU7_moe_ar -> L4_S0_TP7_GPU7_moe_res
	L4_S0_TP7_GPU7_mha_res -> L4_S0_TP7_GPU7_moe_res
	L4_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L4_S0_TP7_GPU7_moe_res -> L4_S0_TP7_GPU7_norm
	L5_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L5_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L5_S0_TP0_GPU0_qkv_linear -> L5_S0_TP0_GPU0_qkv_slice
	L5_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L5_S0_TP0_GPU0_qkv_slice -> L5_S0_TP0_GPU0_qk_matmul
	L5_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L5_S0_TP0_GPU0_qk_matmul -> L5_S0_TP0_GPU0_softmax
	L5_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L5_S0_TP0_GPU0_softmax -> L5_S0_TP0_GPU0_attn_v
	L5_S0_TP0_GPU0_qkv_slice -> L5_S0_TP0_GPU0_attn_v
	L5_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP0_GPU0_mha_ar
	L5_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP0_GPU0_mha_ar -> L5_S0_TP0_GPU0_mha_res
	L5_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L5_S0_TP0_GPU0_mha_res -> L5_S0_TP0_GPU0_gate
	L5_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L5_S0_TP0_GPU0_gate -> L5_S0_TP0_GPU0_expert0 [style=dashed]
	L5_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L5_S0_TP0_GPU0_gate -> L5_S0_TP0_GPU0_expert1 [style=dashed]
	L5_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP0_GPU0_moe_ar
	L5_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP0_GPU0_moe_ar -> L5_S0_TP0_GPU0_moe_res
	L5_S0_TP0_GPU0_mha_res -> L5_S0_TP0_GPU0_moe_res
	L5_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP0_GPU0_moe_res -> L5_S0_TP0_GPU0_norm
	L5_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L5_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L5_S0_TP1_GPU1_qkv_linear -> L5_S0_TP1_GPU1_qkv_slice
	L5_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L5_S0_TP1_GPU1_qkv_slice -> L5_S0_TP1_GPU1_qk_matmul
	L5_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L5_S0_TP1_GPU1_qk_matmul -> L5_S0_TP1_GPU1_softmax
	L5_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L5_S0_TP1_GPU1_softmax -> L5_S0_TP1_GPU1_attn_v
	L5_S0_TP1_GPU1_qkv_slice -> L5_S0_TP1_GPU1_attn_v
	L5_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP1_GPU1_mha_ar
	L5_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP1_GPU1_mha_ar -> L5_S0_TP1_GPU1_mha_res
	L5_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L5_S0_TP1_GPU1_mha_res -> L5_S0_TP1_GPU1_gate
	L5_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L5_S0_TP1_GPU1_gate -> L5_S0_TP1_GPU1_expert0 [style=dashed]
	L5_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L5_S0_TP1_GPU1_gate -> L5_S0_TP1_GPU1_expert1 [style=dashed]
	L5_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP1_GPU1_moe_ar
	L5_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP1_GPU1_moe_ar -> L5_S0_TP1_GPU1_moe_res
	L5_S0_TP1_GPU1_mha_res -> L5_S0_TP1_GPU1_moe_res
	L5_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP1_GPU1_moe_res -> L5_S0_TP1_GPU1_norm
	L5_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L5_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L5_S0_TP2_GPU2_qkv_linear -> L5_S0_TP2_GPU2_qkv_slice
	L5_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L5_S0_TP2_GPU2_qkv_slice -> L5_S0_TP2_GPU2_qk_matmul
	L5_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L5_S0_TP2_GPU2_qk_matmul -> L5_S0_TP2_GPU2_softmax
	L5_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L5_S0_TP2_GPU2_softmax -> L5_S0_TP2_GPU2_attn_v
	L5_S0_TP2_GPU2_qkv_slice -> L5_S0_TP2_GPU2_attn_v
	L5_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP2_GPU2_mha_ar
	L5_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP2_GPU2_mha_ar -> L5_S0_TP2_GPU2_mha_res
	L5_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L5_S0_TP2_GPU2_mha_res -> L5_S0_TP2_GPU2_gate
	L5_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L5_S0_TP2_GPU2_gate -> L5_S0_TP2_GPU2_expert0 [style=dashed]
	L5_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L5_S0_TP2_GPU2_gate -> L5_S0_TP2_GPU2_expert1 [style=dashed]
	L5_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP2_GPU2_moe_ar
	L5_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP2_GPU2_moe_ar -> L5_S0_TP2_GPU2_moe_res
	L5_S0_TP2_GPU2_mha_res -> L5_S0_TP2_GPU2_moe_res
	L5_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP2_GPU2_moe_res -> L5_S0_TP2_GPU2_norm
	L5_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L5_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L5_S0_TP3_GPU3_qkv_linear -> L5_S0_TP3_GPU3_qkv_slice
	L5_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L5_S0_TP3_GPU3_qkv_slice -> L5_S0_TP3_GPU3_qk_matmul
	L5_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L5_S0_TP3_GPU3_qk_matmul -> L5_S0_TP3_GPU3_softmax
	L5_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L5_S0_TP3_GPU3_softmax -> L5_S0_TP3_GPU3_attn_v
	L5_S0_TP3_GPU3_qkv_slice -> L5_S0_TP3_GPU3_attn_v
	L5_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP3_GPU3_mha_ar
	L5_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP3_GPU3_mha_ar -> L5_S0_TP3_GPU3_mha_res
	L5_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L5_S0_TP3_GPU3_mha_res -> L5_S0_TP3_GPU3_gate
	L5_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L5_S0_TP3_GPU3_gate -> L5_S0_TP3_GPU3_expert0 [style=dashed]
	L5_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L5_S0_TP3_GPU3_gate -> L5_S0_TP3_GPU3_expert1 [style=dashed]
	L5_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP3_GPU3_moe_ar
	L5_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP3_GPU3_moe_ar -> L5_S0_TP3_GPU3_moe_res
	L5_S0_TP3_GPU3_mha_res -> L5_S0_TP3_GPU3_moe_res
	L5_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP3_GPU3_moe_res -> L5_S0_TP3_GPU3_norm
	L5_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L5_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L5_S0_TP4_GPU4_qkv_linear -> L5_S0_TP4_GPU4_qkv_slice
	L5_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L5_S0_TP4_GPU4_qkv_slice -> L5_S0_TP4_GPU4_qk_matmul
	L5_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L5_S0_TP4_GPU4_qk_matmul -> L5_S0_TP4_GPU4_softmax
	L5_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L5_S0_TP4_GPU4_softmax -> L5_S0_TP4_GPU4_attn_v
	L5_S0_TP4_GPU4_qkv_slice -> L5_S0_TP4_GPU4_attn_v
	L5_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP4_GPU4_mha_ar
	L5_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP4_GPU4_mha_ar -> L5_S0_TP4_GPU4_mha_res
	L5_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L5_S0_TP4_GPU4_mha_res -> L5_S0_TP4_GPU4_gate
	L5_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L5_S0_TP4_GPU4_gate -> L5_S0_TP4_GPU4_expert0 [style=dashed]
	L5_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L5_S0_TP4_GPU4_gate -> L5_S0_TP4_GPU4_expert1 [style=dashed]
	L5_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP4_GPU4_moe_ar
	L5_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP4_GPU4_moe_ar -> L5_S0_TP4_GPU4_moe_res
	L5_S0_TP4_GPU4_mha_res -> L5_S0_TP4_GPU4_moe_res
	L5_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP4_GPU4_moe_res -> L5_S0_TP4_GPU4_norm
	L5_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L5_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L5_S0_TP5_GPU5_qkv_linear -> L5_S0_TP5_GPU5_qkv_slice
	L5_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L5_S0_TP5_GPU5_qkv_slice -> L5_S0_TP5_GPU5_qk_matmul
	L5_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L5_S0_TP5_GPU5_qk_matmul -> L5_S0_TP5_GPU5_softmax
	L5_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L5_S0_TP5_GPU5_softmax -> L5_S0_TP5_GPU5_attn_v
	L5_S0_TP5_GPU5_qkv_slice -> L5_S0_TP5_GPU5_attn_v
	L5_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP5_GPU5_mha_ar
	L5_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP5_GPU5_mha_ar -> L5_S0_TP5_GPU5_mha_res
	L5_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L5_S0_TP5_GPU5_mha_res -> L5_S0_TP5_GPU5_gate
	L5_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L5_S0_TP5_GPU5_gate -> L5_S0_TP5_GPU5_expert0 [style=dashed]
	L5_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L5_S0_TP5_GPU5_gate -> L5_S0_TP5_GPU5_expert1 [style=dashed]
	L5_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP5_GPU5_moe_ar
	L5_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP5_GPU5_moe_ar -> L5_S0_TP5_GPU5_moe_res
	L5_S0_TP5_GPU5_mha_res -> L5_S0_TP5_GPU5_moe_res
	L5_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP5_GPU5_moe_res -> L5_S0_TP5_GPU5_norm
	L5_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L5_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L5_S0_TP6_GPU6_qkv_linear -> L5_S0_TP6_GPU6_qkv_slice
	L5_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L5_S0_TP6_GPU6_qkv_slice -> L5_S0_TP6_GPU6_qk_matmul
	L5_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L5_S0_TP6_GPU6_qk_matmul -> L5_S0_TP6_GPU6_softmax
	L5_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L5_S0_TP6_GPU6_softmax -> L5_S0_TP6_GPU6_attn_v
	L5_S0_TP6_GPU6_qkv_slice -> L5_S0_TP6_GPU6_attn_v
	L5_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP6_GPU6_mha_ar
	L5_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP6_GPU6_mha_ar -> L5_S0_TP6_GPU6_mha_res
	L5_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L5_S0_TP6_GPU6_mha_res -> L5_S0_TP6_GPU6_gate
	L5_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L5_S0_TP6_GPU6_gate -> L5_S0_TP6_GPU6_expert0 [style=dashed]
	L5_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L5_S0_TP6_GPU6_gate -> L5_S0_TP6_GPU6_expert1 [style=dashed]
	L5_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP6_GPU6_moe_ar
	L5_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP6_GPU6_moe_ar -> L5_S0_TP6_GPU6_moe_res
	L5_S0_TP6_GPU6_mha_res -> L5_S0_TP6_GPU6_moe_res
	L5_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP6_GPU6_moe_res -> L5_S0_TP6_GPU6_norm
	L5_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L5_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L5_S0_TP7_GPU7_qkv_linear -> L5_S0_TP7_GPU7_qkv_slice
	L5_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L5_S0_TP7_GPU7_qkv_slice -> L5_S0_TP7_GPU7_qk_matmul
	L5_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L5_S0_TP7_GPU7_qk_matmul -> L5_S0_TP7_GPU7_softmax
	L5_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L5_S0_TP7_GPU7_softmax -> L5_S0_TP7_GPU7_attn_v
	L5_S0_TP7_GPU7_qkv_slice -> L5_S0_TP7_GPU7_attn_v
	L5_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP1_GPU1_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP2_GPU2_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP3_GPU3_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP4_GPU4_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP5_GPU5_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP6_GPU6_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP7_GPU7_attn_v -> L5_S0_TP7_GPU7_mha_ar
	L5_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP7_GPU7_mha_ar -> L5_S0_TP7_GPU7_mha_res
	L5_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L5_S0_TP7_GPU7_mha_res -> L5_S0_TP7_GPU7_gate
	L5_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L5_S0_TP7_GPU7_gate -> L5_S0_TP7_GPU7_expert0 [style=dashed]
	L5_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L5_S0_TP7_GPU7_gate -> L5_S0_TP7_GPU7_expert1 [style=dashed]
	L5_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L5_S0_TP0_GPU0_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP0_GPU0_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP1_GPU1_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP1_GPU1_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP2_GPU2_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP2_GPU2_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP3_GPU3_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP3_GPU3_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP4_GPU4_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP4_GPU4_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP5_GPU5_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP5_GPU5_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP6_GPU6_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP6_GPU6_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP7_GPU7_expert0 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP7_GPU7_expert1 -> L5_S0_TP7_GPU7_moe_ar
	L5_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP7_GPU7_moe_ar -> L5_S0_TP7_GPU7_moe_res
	L5_S0_TP7_GPU7_mha_res -> L5_S0_TP7_GPU7_moe_res
	L5_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L5_S0_TP7_GPU7_moe_res -> L5_S0_TP7_GPU7_norm
	L6_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L6_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L6_S0_TP0_GPU0_qkv_linear -> L6_S0_TP0_GPU0_qkv_slice
	L6_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L6_S0_TP0_GPU0_qkv_slice -> L6_S0_TP0_GPU0_qk_matmul
	L6_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L6_S0_TP0_GPU0_qk_matmul -> L6_S0_TP0_GPU0_softmax
	L6_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L6_S0_TP0_GPU0_softmax -> L6_S0_TP0_GPU0_attn_v
	L6_S0_TP0_GPU0_qkv_slice -> L6_S0_TP0_GPU0_attn_v
	L6_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP0_GPU0_mha_ar
	L6_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP0_GPU0_mha_ar -> L6_S0_TP0_GPU0_mha_res
	L6_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L6_S0_TP0_GPU0_mha_res -> L6_S0_TP0_GPU0_gate
	L6_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L6_S0_TP0_GPU0_gate -> L6_S0_TP0_GPU0_expert0 [style=dashed]
	L6_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L6_S0_TP0_GPU0_gate -> L6_S0_TP0_GPU0_expert1 [style=dashed]
	L6_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP0_GPU0_moe_ar
	L6_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP0_GPU0_moe_ar -> L6_S0_TP0_GPU0_moe_res
	L6_S0_TP0_GPU0_mha_res -> L6_S0_TP0_GPU0_moe_res
	L6_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP0_GPU0_moe_res -> L6_S0_TP0_GPU0_norm
	L6_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L6_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L6_S0_TP1_GPU1_qkv_linear -> L6_S0_TP1_GPU1_qkv_slice
	L6_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L6_S0_TP1_GPU1_qkv_slice -> L6_S0_TP1_GPU1_qk_matmul
	L6_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L6_S0_TP1_GPU1_qk_matmul -> L6_S0_TP1_GPU1_softmax
	L6_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L6_S0_TP1_GPU1_softmax -> L6_S0_TP1_GPU1_attn_v
	L6_S0_TP1_GPU1_qkv_slice -> L6_S0_TP1_GPU1_attn_v
	L6_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP1_GPU1_mha_ar
	L6_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP1_GPU1_mha_ar -> L6_S0_TP1_GPU1_mha_res
	L6_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L6_S0_TP1_GPU1_mha_res -> L6_S0_TP1_GPU1_gate
	L6_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L6_S0_TP1_GPU1_gate -> L6_S0_TP1_GPU1_expert0 [style=dashed]
	L6_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L6_S0_TP1_GPU1_gate -> L6_S0_TP1_GPU1_expert1 [style=dashed]
	L6_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP1_GPU1_moe_ar
	L6_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP1_GPU1_moe_ar -> L6_S0_TP1_GPU1_moe_res
	L6_S0_TP1_GPU1_mha_res -> L6_S0_TP1_GPU1_moe_res
	L6_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP1_GPU1_moe_res -> L6_S0_TP1_GPU1_norm
	L6_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L6_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L6_S0_TP2_GPU2_qkv_linear -> L6_S0_TP2_GPU2_qkv_slice
	L6_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L6_S0_TP2_GPU2_qkv_slice -> L6_S0_TP2_GPU2_qk_matmul
	L6_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L6_S0_TP2_GPU2_qk_matmul -> L6_S0_TP2_GPU2_softmax
	L6_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L6_S0_TP2_GPU2_softmax -> L6_S0_TP2_GPU2_attn_v
	L6_S0_TP2_GPU2_qkv_slice -> L6_S0_TP2_GPU2_attn_v
	L6_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP2_GPU2_mha_ar
	L6_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP2_GPU2_mha_ar -> L6_S0_TP2_GPU2_mha_res
	L6_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L6_S0_TP2_GPU2_mha_res -> L6_S0_TP2_GPU2_gate
	L6_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L6_S0_TP2_GPU2_gate -> L6_S0_TP2_GPU2_expert0 [style=dashed]
	L6_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L6_S0_TP2_GPU2_gate -> L6_S0_TP2_GPU2_expert1 [style=dashed]
	L6_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP2_GPU2_moe_ar
	L6_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP2_GPU2_moe_ar -> L6_S0_TP2_GPU2_moe_res
	L6_S0_TP2_GPU2_mha_res -> L6_S0_TP2_GPU2_moe_res
	L6_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP2_GPU2_moe_res -> L6_S0_TP2_GPU2_norm
	L6_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L6_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L6_S0_TP3_GPU3_qkv_linear -> L6_S0_TP3_GPU3_qkv_slice
	L6_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L6_S0_TP3_GPU3_qkv_slice -> L6_S0_TP3_GPU3_qk_matmul
	L6_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L6_S0_TP3_GPU3_qk_matmul -> L6_S0_TP3_GPU3_softmax
	L6_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L6_S0_TP3_GPU3_softmax -> L6_S0_TP3_GPU3_attn_v
	L6_S0_TP3_GPU3_qkv_slice -> L6_S0_TP3_GPU3_attn_v
	L6_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP3_GPU3_mha_ar
	L6_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP3_GPU3_mha_ar -> L6_S0_TP3_GPU3_mha_res
	L6_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L6_S0_TP3_GPU3_mha_res -> L6_S0_TP3_GPU3_gate
	L6_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L6_S0_TP3_GPU3_gate -> L6_S0_TP3_GPU3_expert0 [style=dashed]
	L6_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L6_S0_TP3_GPU3_gate -> L6_S0_TP3_GPU3_expert1 [style=dashed]
	L6_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP3_GPU3_moe_ar
	L6_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP3_GPU3_moe_ar -> L6_S0_TP3_GPU3_moe_res
	L6_S0_TP3_GPU3_mha_res -> L6_S0_TP3_GPU3_moe_res
	L6_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP3_GPU3_moe_res -> L6_S0_TP3_GPU3_norm
	L6_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L6_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L6_S0_TP4_GPU4_qkv_linear -> L6_S0_TP4_GPU4_qkv_slice
	L6_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L6_S0_TP4_GPU4_qkv_slice -> L6_S0_TP4_GPU4_qk_matmul
	L6_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L6_S0_TP4_GPU4_qk_matmul -> L6_S0_TP4_GPU4_softmax
	L6_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L6_S0_TP4_GPU4_softmax -> L6_S0_TP4_GPU4_attn_v
	L6_S0_TP4_GPU4_qkv_slice -> L6_S0_TP4_GPU4_attn_v
	L6_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP4_GPU4_mha_ar
	L6_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP4_GPU4_mha_ar -> L6_S0_TP4_GPU4_mha_res
	L6_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L6_S0_TP4_GPU4_mha_res -> L6_S0_TP4_GPU4_gate
	L6_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L6_S0_TP4_GPU4_gate -> L6_S0_TP4_GPU4_expert0 [style=dashed]
	L6_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L6_S0_TP4_GPU4_gate -> L6_S0_TP4_GPU4_expert1 [style=dashed]
	L6_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP4_GPU4_moe_ar
	L6_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP4_GPU4_moe_ar -> L6_S0_TP4_GPU4_moe_res
	L6_S0_TP4_GPU4_mha_res -> L6_S0_TP4_GPU4_moe_res
	L6_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP4_GPU4_moe_res -> L6_S0_TP4_GPU4_norm
	L6_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L6_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L6_S0_TP5_GPU5_qkv_linear -> L6_S0_TP5_GPU5_qkv_slice
	L6_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L6_S0_TP5_GPU5_qkv_slice -> L6_S0_TP5_GPU5_qk_matmul
	L6_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L6_S0_TP5_GPU5_qk_matmul -> L6_S0_TP5_GPU5_softmax
	L6_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L6_S0_TP5_GPU5_softmax -> L6_S0_TP5_GPU5_attn_v
	L6_S0_TP5_GPU5_qkv_slice -> L6_S0_TP5_GPU5_attn_v
	L6_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP5_GPU5_mha_ar
	L6_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP5_GPU5_mha_ar -> L6_S0_TP5_GPU5_mha_res
	L6_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L6_S0_TP5_GPU5_mha_res -> L6_S0_TP5_GPU5_gate
	L6_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L6_S0_TP5_GPU5_gate -> L6_S0_TP5_GPU5_expert0 [style=dashed]
	L6_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L6_S0_TP5_GPU5_gate -> L6_S0_TP5_GPU5_expert1 [style=dashed]
	L6_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP5_GPU5_moe_ar
	L6_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP5_GPU5_moe_ar -> L6_S0_TP5_GPU5_moe_res
	L6_S0_TP5_GPU5_mha_res -> L6_S0_TP5_GPU5_moe_res
	L6_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP5_GPU5_moe_res -> L6_S0_TP5_GPU5_norm
	L6_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L6_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L6_S0_TP6_GPU6_qkv_linear -> L6_S0_TP6_GPU6_qkv_slice
	L6_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L6_S0_TP6_GPU6_qkv_slice -> L6_S0_TP6_GPU6_qk_matmul
	L6_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L6_S0_TP6_GPU6_qk_matmul -> L6_S0_TP6_GPU6_softmax
	L6_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L6_S0_TP6_GPU6_softmax -> L6_S0_TP6_GPU6_attn_v
	L6_S0_TP6_GPU6_qkv_slice -> L6_S0_TP6_GPU6_attn_v
	L6_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP6_GPU6_mha_ar
	L6_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP6_GPU6_mha_ar -> L6_S0_TP6_GPU6_mha_res
	L6_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L6_S0_TP6_GPU6_mha_res -> L6_S0_TP6_GPU6_gate
	L6_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L6_S0_TP6_GPU6_gate -> L6_S0_TP6_GPU6_expert0 [style=dashed]
	L6_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L6_S0_TP6_GPU6_gate -> L6_S0_TP6_GPU6_expert1 [style=dashed]
	L6_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP6_GPU6_moe_ar
	L6_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP6_GPU6_moe_ar -> L6_S0_TP6_GPU6_moe_res
	L6_S0_TP6_GPU6_mha_res -> L6_S0_TP6_GPU6_moe_res
	L6_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP6_GPU6_moe_res -> L6_S0_TP6_GPU6_norm
	L6_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L6_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L6_S0_TP7_GPU7_qkv_linear -> L6_S0_TP7_GPU7_qkv_slice
	L6_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L6_S0_TP7_GPU7_qkv_slice -> L6_S0_TP7_GPU7_qk_matmul
	L6_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L6_S0_TP7_GPU7_qk_matmul -> L6_S0_TP7_GPU7_softmax
	L6_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L6_S0_TP7_GPU7_softmax -> L6_S0_TP7_GPU7_attn_v
	L6_S0_TP7_GPU7_qkv_slice -> L6_S0_TP7_GPU7_attn_v
	L6_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP1_GPU1_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP2_GPU2_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP3_GPU3_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP4_GPU4_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP5_GPU5_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP6_GPU6_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP7_GPU7_attn_v -> L6_S0_TP7_GPU7_mha_ar
	L6_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP7_GPU7_mha_ar -> L6_S0_TP7_GPU7_mha_res
	L6_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L6_S0_TP7_GPU7_mha_res -> L6_S0_TP7_GPU7_gate
	L6_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L6_S0_TP7_GPU7_gate -> L6_S0_TP7_GPU7_expert0 [style=dashed]
	L6_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L6_S0_TP7_GPU7_gate -> L6_S0_TP7_GPU7_expert1 [style=dashed]
	L6_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L6_S0_TP0_GPU0_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP0_GPU0_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP1_GPU1_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP1_GPU1_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP2_GPU2_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP2_GPU2_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP3_GPU3_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP3_GPU3_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP4_GPU4_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP4_GPU4_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP5_GPU5_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP5_GPU5_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP6_GPU6_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP6_GPU6_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP7_GPU7_expert0 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP7_GPU7_expert1 -> L6_S0_TP7_GPU7_moe_ar
	L6_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP7_GPU7_moe_ar -> L6_S0_TP7_GPU7_moe_res
	L6_S0_TP7_GPU7_mha_res -> L6_S0_TP7_GPU7_moe_res
	L6_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L6_S0_TP7_GPU7_moe_res -> L6_S0_TP7_GPU7_norm
	L7_S0_TP0_GPU0_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 0" shape=rectangle]
	L7_S0_TP0_GPU0_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L7_S0_TP0_GPU0_qkv_linear -> L7_S0_TP0_GPU0_qkv_slice
	L7_S0_TP0_GPU0_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L7_S0_TP0_GPU0_qkv_slice -> L7_S0_TP0_GPU0_qk_matmul
	L7_S0_TP0_GPU0_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 0" shape=rectangle]
	L7_S0_TP0_GPU0_qk_matmul -> L7_S0_TP0_GPU0_softmax
	L7_S0_TP0_GPU0_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 0" shape=rectangle]
	L7_S0_TP0_GPU0_softmax -> L7_S0_TP0_GPU0_attn_v
	L7_S0_TP0_GPU0_qkv_slice -> L7_S0_TP0_GPU0_attn_v
	L7_S0_TP0_GPU0_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP0_GPU0_mha_ar
	L7_S0_TP0_GPU0_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP0_GPU0_mha_ar -> L7_S0_TP0_GPU0_mha_res
	L7_S0_TP0_GPU0_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 0" shape=parallelogram]
	L7_S0_TP0_GPU0_mha_res -> L7_S0_TP0_GPU0_gate
	L7_S0_TP0_GPU0_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L7_S0_TP0_GPU0_gate -> L7_S0_TP0_GPU0_expert0 [style=dashed]
	L7_S0_TP0_GPU0_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 0" shape=rectangle]
	L7_S0_TP0_GPU0_gate -> L7_S0_TP0_GPU0_expert1 [style=dashed]
	L7_S0_TP0_GPU0_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP0_GPU0_moe_ar
	L7_S0_TP0_GPU0_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP0_GPU0_moe_ar -> L7_S0_TP0_GPU0_moe_res
	L7_S0_TP0_GPU0_mha_res -> L7_S0_TP0_GPU0_moe_res
	L7_S0_TP0_GPU0_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP0_GPU0_moe_res -> L7_S0_TP0_GPU0_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L7_S0_TP1_GPU1_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 1" shape=rectangle]
	L7_S0_TP1_GPU1_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L7_S0_TP1_GPU1_qkv_linear -> L7_S0_TP1_GPU1_qkv_slice
	L7_S0_TP1_GPU1_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L7_S0_TP1_GPU1_qkv_slice -> L7_S0_TP1_GPU1_qk_matmul
	L7_S0_TP1_GPU1_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 1" shape=rectangle]
	L7_S0_TP1_GPU1_qk_matmul -> L7_S0_TP1_GPU1_softmax
	L7_S0_TP1_GPU1_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 1" shape=rectangle]
	L7_S0_TP1_GPU1_softmax -> L7_S0_TP1_GPU1_attn_v
	L7_S0_TP1_GPU1_qkv_slice -> L7_S0_TP1_GPU1_attn_v
	L7_S0_TP1_GPU1_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP1_GPU1_mha_ar
	L7_S0_TP1_GPU1_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP1_GPU1_mha_ar -> L7_S0_TP1_GPU1_mha_res
	L7_S0_TP1_GPU1_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 1" shape=parallelogram]
	L7_S0_TP1_GPU1_mha_res -> L7_S0_TP1_GPU1_gate
	L7_S0_TP1_GPU1_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L7_S0_TP1_GPU1_gate -> L7_S0_TP1_GPU1_expert0 [style=dashed]
	L7_S0_TP1_GPU1_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 1" shape=rectangle]
	L7_S0_TP1_GPU1_gate -> L7_S0_TP1_GPU1_expert1 [style=dashed]
	L7_S0_TP1_GPU1_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP1_GPU1_moe_ar
	L7_S0_TP1_GPU1_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP1_GPU1_moe_ar -> L7_S0_TP1_GPU1_moe_res
	L7_S0_TP1_GPU1_mha_res -> L7_S0_TP1_GPU1_moe_res
	L7_S0_TP1_GPU1_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP1_GPU1_moe_res -> L7_S0_TP1_GPU1_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L7_S0_TP2_GPU2_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 2" shape=rectangle]
	L7_S0_TP2_GPU2_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L7_S0_TP2_GPU2_qkv_linear -> L7_S0_TP2_GPU2_qkv_slice
	L7_S0_TP2_GPU2_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L7_S0_TP2_GPU2_qkv_slice -> L7_S0_TP2_GPU2_qk_matmul
	L7_S0_TP2_GPU2_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 2" shape=rectangle]
	L7_S0_TP2_GPU2_qk_matmul -> L7_S0_TP2_GPU2_softmax
	L7_S0_TP2_GPU2_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 2" shape=rectangle]
	L7_S0_TP2_GPU2_softmax -> L7_S0_TP2_GPU2_attn_v
	L7_S0_TP2_GPU2_qkv_slice -> L7_S0_TP2_GPU2_attn_v
	L7_S0_TP2_GPU2_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP2_GPU2_mha_ar
	L7_S0_TP2_GPU2_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP2_GPU2_mha_ar -> L7_S0_TP2_GPU2_mha_res
	L7_S0_TP2_GPU2_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 2" shape=parallelogram]
	L7_S0_TP2_GPU2_mha_res -> L7_S0_TP2_GPU2_gate
	L7_S0_TP2_GPU2_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L7_S0_TP2_GPU2_gate -> L7_S0_TP2_GPU2_expert0 [style=dashed]
	L7_S0_TP2_GPU2_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 2" shape=rectangle]
	L7_S0_TP2_GPU2_gate -> L7_S0_TP2_GPU2_expert1 [style=dashed]
	L7_S0_TP2_GPU2_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP2_GPU2_moe_ar
	L7_S0_TP2_GPU2_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP2_GPU2_moe_ar -> L7_S0_TP2_GPU2_moe_res
	L7_S0_TP2_GPU2_mha_res -> L7_S0_TP2_GPU2_moe_res
	L7_S0_TP2_GPU2_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP2_GPU2_moe_res -> L7_S0_TP2_GPU2_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L7_S0_TP3_GPU3_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 3" shape=rectangle]
	L7_S0_TP3_GPU3_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L7_S0_TP3_GPU3_qkv_linear -> L7_S0_TP3_GPU3_qkv_slice
	L7_S0_TP3_GPU3_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L7_S0_TP3_GPU3_qkv_slice -> L7_S0_TP3_GPU3_qk_matmul
	L7_S0_TP3_GPU3_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 3" shape=rectangle]
	L7_S0_TP3_GPU3_qk_matmul -> L7_S0_TP3_GPU3_softmax
	L7_S0_TP3_GPU3_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 3" shape=rectangle]
	L7_S0_TP3_GPU3_softmax -> L7_S0_TP3_GPU3_attn_v
	L7_S0_TP3_GPU3_qkv_slice -> L7_S0_TP3_GPU3_attn_v
	L7_S0_TP3_GPU3_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP3_GPU3_mha_ar
	L7_S0_TP3_GPU3_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP3_GPU3_mha_ar -> L7_S0_TP3_GPU3_mha_res
	L7_S0_TP3_GPU3_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 3" shape=parallelogram]
	L7_S0_TP3_GPU3_mha_res -> L7_S0_TP3_GPU3_gate
	L7_S0_TP3_GPU3_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L7_S0_TP3_GPU3_gate -> L7_S0_TP3_GPU3_expert0 [style=dashed]
	L7_S0_TP3_GPU3_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 3" shape=rectangle]
	L7_S0_TP3_GPU3_gate -> L7_S0_TP3_GPU3_expert1 [style=dashed]
	L7_S0_TP3_GPU3_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP3_GPU3_moe_ar
	L7_S0_TP3_GPU3_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP3_GPU3_moe_ar -> L7_S0_TP3_GPU3_moe_res
	L7_S0_TP3_GPU3_mha_res -> L7_S0_TP3_GPU3_moe_res
	L7_S0_TP3_GPU3_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP3_GPU3_moe_res -> L7_S0_TP3_GPU3_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L7_S0_TP4_GPU4_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 4" shape=rectangle]
	L7_S0_TP4_GPU4_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L7_S0_TP4_GPU4_qkv_linear -> L7_S0_TP4_GPU4_qkv_slice
	L7_S0_TP4_GPU4_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L7_S0_TP4_GPU4_qkv_slice -> L7_S0_TP4_GPU4_qk_matmul
	L7_S0_TP4_GPU4_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 4" shape=rectangle]
	L7_S0_TP4_GPU4_qk_matmul -> L7_S0_TP4_GPU4_softmax
	L7_S0_TP4_GPU4_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 4" shape=rectangle]
	L7_S0_TP4_GPU4_softmax -> L7_S0_TP4_GPU4_attn_v
	L7_S0_TP4_GPU4_qkv_slice -> L7_S0_TP4_GPU4_attn_v
	L7_S0_TP4_GPU4_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP4_GPU4_mha_ar
	L7_S0_TP4_GPU4_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP4_GPU4_mha_ar -> L7_S0_TP4_GPU4_mha_res
	L7_S0_TP4_GPU4_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 4" shape=parallelogram]
	L7_S0_TP4_GPU4_mha_res -> L7_S0_TP4_GPU4_gate
	L7_S0_TP4_GPU4_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L7_S0_TP4_GPU4_gate -> L7_S0_TP4_GPU4_expert0 [style=dashed]
	L7_S0_TP4_GPU4_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 4" shape=rectangle]
	L7_S0_TP4_GPU4_gate -> L7_S0_TP4_GPU4_expert1 [style=dashed]
	L7_S0_TP4_GPU4_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP4_GPU4_moe_ar
	L7_S0_TP4_GPU4_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP4_GPU4_moe_ar -> L7_S0_TP4_GPU4_moe_res
	L7_S0_TP4_GPU4_mha_res -> L7_S0_TP4_GPU4_moe_res
	L7_S0_TP4_GPU4_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP4_GPU4_moe_res -> L7_S0_TP4_GPU4_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L7_S0_TP5_GPU5_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 5" shape=rectangle]
	L7_S0_TP5_GPU5_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L7_S0_TP5_GPU5_qkv_linear -> L7_S0_TP5_GPU5_qkv_slice
	L7_S0_TP5_GPU5_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L7_S0_TP5_GPU5_qkv_slice -> L7_S0_TP5_GPU5_qk_matmul
	L7_S0_TP5_GPU5_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 5" shape=rectangle]
	L7_S0_TP5_GPU5_qk_matmul -> L7_S0_TP5_GPU5_softmax
	L7_S0_TP5_GPU5_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 5" shape=rectangle]
	L7_S0_TP5_GPU5_softmax -> L7_S0_TP5_GPU5_attn_v
	L7_S0_TP5_GPU5_qkv_slice -> L7_S0_TP5_GPU5_attn_v
	L7_S0_TP5_GPU5_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP5_GPU5_mha_ar
	L7_S0_TP5_GPU5_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP5_GPU5_mha_ar -> L7_S0_TP5_GPU5_mha_res
	L7_S0_TP5_GPU5_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 5" shape=parallelogram]
	L7_S0_TP5_GPU5_mha_res -> L7_S0_TP5_GPU5_gate
	L7_S0_TP5_GPU5_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L7_S0_TP5_GPU5_gate -> L7_S0_TP5_GPU5_expert0 [style=dashed]
	L7_S0_TP5_GPU5_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 5" shape=rectangle]
	L7_S0_TP5_GPU5_gate -> L7_S0_TP5_GPU5_expert1 [style=dashed]
	L7_S0_TP5_GPU5_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP5_GPU5_moe_ar
	L7_S0_TP5_GPU5_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP5_GPU5_moe_ar -> L7_S0_TP5_GPU5_moe_res
	L7_S0_TP5_GPU5_mha_res -> L7_S0_TP5_GPU5_moe_res
	L7_S0_TP5_GPU5_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP5_GPU5_moe_res -> L7_S0_TP5_GPU5_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L7_S0_TP6_GPU6_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 6" shape=rectangle]
	L7_S0_TP6_GPU6_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L7_S0_TP6_GPU6_qkv_linear -> L7_S0_TP6_GPU6_qkv_slice
	L7_S0_TP6_GPU6_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L7_S0_TP6_GPU6_qkv_slice -> L7_S0_TP6_GPU6_qk_matmul
	L7_S0_TP6_GPU6_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 6" shape=rectangle]
	L7_S0_TP6_GPU6_qk_matmul -> L7_S0_TP6_GPU6_softmax
	L7_S0_TP6_GPU6_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 6" shape=rectangle]
	L7_S0_TP6_GPU6_softmax -> L7_S0_TP6_GPU6_attn_v
	L7_S0_TP6_GPU6_qkv_slice -> L7_S0_TP6_GPU6_attn_v
	L7_S0_TP6_GPU6_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP6_GPU6_mha_ar
	L7_S0_TP6_GPU6_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP6_GPU6_mha_ar -> L7_S0_TP6_GPU6_mha_res
	L7_S0_TP6_GPU6_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 6" shape=parallelogram]
	L7_S0_TP6_GPU6_mha_res -> L7_S0_TP6_GPU6_gate
	L7_S0_TP6_GPU6_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L7_S0_TP6_GPU6_gate -> L7_S0_TP6_GPU6_expert0 [style=dashed]
	L7_S0_TP6_GPU6_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 6" shape=rectangle]
	L7_S0_TP6_GPU6_gate -> L7_S0_TP6_GPU6_expert1 [style=dashed]
	L7_S0_TP6_GPU6_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP6_GPU6_moe_ar
	L7_S0_TP6_GPU6_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP6_GPU6_moe_ar -> L7_S0_TP6_GPU6_moe_res
	L7_S0_TP6_GPU6_mha_res -> L7_S0_TP6_GPU6_moe_res
	L7_S0_TP6_GPU6_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP6_GPU6_moe_res -> L7_S0_TP6_GPU6_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L7_S0_TP7_GPU7_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 7" shape=rectangle]
	L7_S0_TP7_GPU7_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L7_S0_TP7_GPU7_qkv_linear -> L7_S0_TP7_GPU7_qkv_slice
	L7_S0_TP7_GPU7_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L7_S0_TP7_GPU7_qkv_slice -> L7_S0_TP7_GPU7_qk_matmul
	L7_S0_TP7_GPU7_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 7" shape=rectangle]
	L7_S0_TP7_GPU7_qk_matmul -> L7_S0_TP7_GPU7_softmax
	L7_S0_TP7_GPU7_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 7" shape=rectangle]
	L7_S0_TP7_GPU7_softmax -> L7_S0_TP7_GPU7_attn_v
	L7_S0_TP7_GPU7_qkv_slice -> L7_S0_TP7_GPU7_attn_v
	L7_S0_TP7_GPU7_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP1_GPU1_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP2_GPU2_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP3_GPU3_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP4_GPU4_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP5_GPU5_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP6_GPU6_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP7_GPU7_attn_v -> L7_S0_TP7_GPU7_mha_ar
	L7_S0_TP7_GPU7_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP7_GPU7_mha_ar -> L7_S0_TP7_GPU7_mha_res
	L7_S0_TP7_GPU7_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 7" shape=parallelogram]
	L7_S0_TP7_GPU7_mha_res -> L7_S0_TP7_GPU7_gate
	L7_S0_TP7_GPU7_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L7_S0_TP7_GPU7_gate -> L7_S0_TP7_GPU7_expert0 [style=dashed]
	L7_S0_TP7_GPU7_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 7" shape=rectangle]
	L7_S0_TP7_GPU7_gate -> L7_S0_TP7_GPU7_expert1 [style=dashed]
	L7_S0_TP7_GPU7_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP0_GPU0_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP1_GPU1_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP1_GPU1_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP2_GPU2_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP2_GPU2_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP3_GPU3_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP3_GPU3_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP4_GPU4_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP4_GPU4_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP5_GPU5_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP5_GPU5_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP6_GPU6_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP6_GPU6_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP7_GPU7_expert0 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP7_GPU7_expert1 -> L7_S0_TP7_GPU7_moe_ar
	L7_S0_TP7_GPU7_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP7_GPU7_moe_ar -> L7_S0_TP7_GPU7_moe_res
	L7_S0_TP7_GPU7_mha_res -> L7_S0_TP7_GPU7_moe_res
	L7_S0_TP7_GPU7_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=rectangle]
	L7_S0_TP7_GPU7_moe_res -> L7_S0_TP7_GPU7_norm
	L7_send_stage1 [label="Send to Stage1\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 0" shape=ellipse]
	L7_S0_TP0_GPU0_norm -> L7_send_stage1
	L7_S0_TP1_GPU1_norm -> L7_send_stage1
	L7_S0_TP2_GPU2_norm -> L7_send_stage1
	L7_S0_TP3_GPU3_norm -> L7_send_stage1
	L7_S0_TP4_GPU4_norm -> L7_send_stage1
	L7_S0_TP5_GPU5_norm -> L7_send_stage1
	L7_S0_TP6_GPU6_norm -> L7_send_stage1
	L7_S0_TP7_GPU7_norm -> L7_send_stage1
	L8_recv_from_stage0_TP0 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP0
	L8_recv_from_stage0_TP1 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP1
	L8_recv_from_stage0_TP2 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP2
	L8_recv_from_stage0_TP3 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP3
	L8_recv_from_stage0_TP4 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP4
	L8_recv_from_stage0_TP5 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP5
	L8_recv_from_stage0_TP6 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP6
	L8_recv_from_stage0_TP7 [label="Recv from Stage0\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=ellipse]
	L7_send_stage1 -> L8_recv_from_stage0_TP7
	L8_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L8_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L8_S1_TP0_GPU8_qkv_linear -> L8_S1_TP0_GPU8_qkv_slice
	L8_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L8_S1_TP0_GPU8_qkv_slice -> L8_S1_TP0_GPU8_qk_matmul
	L8_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L8_S1_TP0_GPU8_qk_matmul -> L8_S1_TP0_GPU8_softmax
	L8_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L8_S1_TP0_GPU8_softmax -> L8_S1_TP0_GPU8_attn_v
	L8_S1_TP0_GPU8_qkv_slice -> L8_S1_TP0_GPU8_attn_v
	L8_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP0_GPU8_mha_ar
	L8_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP0_GPU8_mha_ar -> L8_S1_TP0_GPU8_mha_res
	L8_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L8_S1_TP0_GPU8_mha_res -> L8_S1_TP0_GPU8_gate
	L8_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L8_S1_TP0_GPU8_gate -> L8_S1_TP0_GPU8_expert0 [style=dashed]
	L8_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L8_S1_TP0_GPU8_gate -> L8_S1_TP0_GPU8_expert1 [style=dashed]
	L8_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP0_GPU8_moe_ar
	L8_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP0_GPU8_moe_ar -> L8_S1_TP0_GPU8_moe_res
	L8_S1_TP0_GPU8_mha_res -> L8_S1_TP0_GPU8_moe_res
	L8_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP0_GPU8_moe_res -> L8_S1_TP0_GPU8_norm
	L8_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L8_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L8_S1_TP1_GPU9_qkv_linear -> L8_S1_TP1_GPU9_qkv_slice
	L8_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L8_S1_TP1_GPU9_qkv_slice -> L8_S1_TP1_GPU9_qk_matmul
	L8_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L8_S1_TP1_GPU9_qk_matmul -> L8_S1_TP1_GPU9_softmax
	L8_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L8_S1_TP1_GPU9_softmax -> L8_S1_TP1_GPU9_attn_v
	L8_S1_TP1_GPU9_qkv_slice -> L8_S1_TP1_GPU9_attn_v
	L8_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP1_GPU9_mha_ar
	L8_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP1_GPU9_mha_ar -> L8_S1_TP1_GPU9_mha_res
	L8_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L8_S1_TP1_GPU9_mha_res -> L8_S1_TP1_GPU9_gate
	L8_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L8_S1_TP1_GPU9_gate -> L8_S1_TP1_GPU9_expert0 [style=dashed]
	L8_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L8_S1_TP1_GPU9_gate -> L8_S1_TP1_GPU9_expert1 [style=dashed]
	L8_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP1_GPU9_moe_ar
	L8_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP1_GPU9_moe_ar -> L8_S1_TP1_GPU9_moe_res
	L8_S1_TP1_GPU9_mha_res -> L8_S1_TP1_GPU9_moe_res
	L8_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP1_GPU9_moe_res -> L8_S1_TP1_GPU9_norm
	L8_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L8_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L8_S1_TP2_GPU10_qkv_linear -> L8_S1_TP2_GPU10_qkv_slice
	L8_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L8_S1_TP2_GPU10_qkv_slice -> L8_S1_TP2_GPU10_qk_matmul
	L8_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L8_S1_TP2_GPU10_qk_matmul -> L8_S1_TP2_GPU10_softmax
	L8_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L8_S1_TP2_GPU10_softmax -> L8_S1_TP2_GPU10_attn_v
	L8_S1_TP2_GPU10_qkv_slice -> L8_S1_TP2_GPU10_attn_v
	L8_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP2_GPU10_mha_ar
	L8_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP2_GPU10_mha_ar -> L8_S1_TP2_GPU10_mha_res
	L8_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L8_S1_TP2_GPU10_mha_res -> L8_S1_TP2_GPU10_gate
	L8_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L8_S1_TP2_GPU10_gate -> L8_S1_TP2_GPU10_expert0 [style=dashed]
	L8_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L8_S1_TP2_GPU10_gate -> L8_S1_TP2_GPU10_expert1 [style=dashed]
	L8_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP2_GPU10_moe_ar
	L8_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP2_GPU10_moe_ar -> L8_S1_TP2_GPU10_moe_res
	L8_S1_TP2_GPU10_mha_res -> L8_S1_TP2_GPU10_moe_res
	L8_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP2_GPU10_moe_res -> L8_S1_TP2_GPU10_norm
	L8_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L8_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L8_S1_TP3_GPU11_qkv_linear -> L8_S1_TP3_GPU11_qkv_slice
	L8_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L8_S1_TP3_GPU11_qkv_slice -> L8_S1_TP3_GPU11_qk_matmul
	L8_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L8_S1_TP3_GPU11_qk_matmul -> L8_S1_TP3_GPU11_softmax
	L8_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L8_S1_TP3_GPU11_softmax -> L8_S1_TP3_GPU11_attn_v
	L8_S1_TP3_GPU11_qkv_slice -> L8_S1_TP3_GPU11_attn_v
	L8_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP3_GPU11_mha_ar
	L8_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP3_GPU11_mha_ar -> L8_S1_TP3_GPU11_mha_res
	L8_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L8_S1_TP3_GPU11_mha_res -> L8_S1_TP3_GPU11_gate
	L8_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L8_S1_TP3_GPU11_gate -> L8_S1_TP3_GPU11_expert0 [style=dashed]
	L8_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L8_S1_TP3_GPU11_gate -> L8_S1_TP3_GPU11_expert1 [style=dashed]
	L8_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP3_GPU11_moe_ar
	L8_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP3_GPU11_moe_ar -> L8_S1_TP3_GPU11_moe_res
	L8_S1_TP3_GPU11_mha_res -> L8_S1_TP3_GPU11_moe_res
	L8_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP3_GPU11_moe_res -> L8_S1_TP3_GPU11_norm
	L8_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L8_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L8_S1_TP4_GPU12_qkv_linear -> L8_S1_TP4_GPU12_qkv_slice
	L8_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L8_S1_TP4_GPU12_qkv_slice -> L8_S1_TP4_GPU12_qk_matmul
	L8_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L8_S1_TP4_GPU12_qk_matmul -> L8_S1_TP4_GPU12_softmax
	L8_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L8_S1_TP4_GPU12_softmax -> L8_S1_TP4_GPU12_attn_v
	L8_S1_TP4_GPU12_qkv_slice -> L8_S1_TP4_GPU12_attn_v
	L8_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP4_GPU12_mha_ar
	L8_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP4_GPU12_mha_ar -> L8_S1_TP4_GPU12_mha_res
	L8_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L8_S1_TP4_GPU12_mha_res -> L8_S1_TP4_GPU12_gate
	L8_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L8_S1_TP4_GPU12_gate -> L8_S1_TP4_GPU12_expert0 [style=dashed]
	L8_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L8_S1_TP4_GPU12_gate -> L8_S1_TP4_GPU12_expert1 [style=dashed]
	L8_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP4_GPU12_moe_ar
	L8_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP4_GPU12_moe_ar -> L8_S1_TP4_GPU12_moe_res
	L8_S1_TP4_GPU12_mha_res -> L8_S1_TP4_GPU12_moe_res
	L8_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP4_GPU12_moe_res -> L8_S1_TP4_GPU12_norm
	L8_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L8_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L8_S1_TP5_GPU13_qkv_linear -> L8_S1_TP5_GPU13_qkv_slice
	L8_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L8_S1_TP5_GPU13_qkv_slice -> L8_S1_TP5_GPU13_qk_matmul
	L8_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L8_S1_TP5_GPU13_qk_matmul -> L8_S1_TP5_GPU13_softmax
	L8_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L8_S1_TP5_GPU13_softmax -> L8_S1_TP5_GPU13_attn_v
	L8_S1_TP5_GPU13_qkv_slice -> L8_S1_TP5_GPU13_attn_v
	L8_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP5_GPU13_mha_ar
	L8_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP5_GPU13_mha_ar -> L8_S1_TP5_GPU13_mha_res
	L8_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L8_S1_TP5_GPU13_mha_res -> L8_S1_TP5_GPU13_gate
	L8_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L8_S1_TP5_GPU13_gate -> L8_S1_TP5_GPU13_expert0 [style=dashed]
	L8_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L8_S1_TP5_GPU13_gate -> L8_S1_TP5_GPU13_expert1 [style=dashed]
	L8_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP5_GPU13_moe_ar
	L8_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP5_GPU13_moe_ar -> L8_S1_TP5_GPU13_moe_res
	L8_S1_TP5_GPU13_mha_res -> L8_S1_TP5_GPU13_moe_res
	L8_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP5_GPU13_moe_res -> L8_S1_TP5_GPU13_norm
	L8_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L8_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L8_S1_TP6_GPU14_qkv_linear -> L8_S1_TP6_GPU14_qkv_slice
	L8_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L8_S1_TP6_GPU14_qkv_slice -> L8_S1_TP6_GPU14_qk_matmul
	L8_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L8_S1_TP6_GPU14_qk_matmul -> L8_S1_TP6_GPU14_softmax
	L8_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L8_S1_TP6_GPU14_softmax -> L8_S1_TP6_GPU14_attn_v
	L8_S1_TP6_GPU14_qkv_slice -> L8_S1_TP6_GPU14_attn_v
	L8_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP6_GPU14_mha_ar
	L8_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP6_GPU14_mha_ar -> L8_S1_TP6_GPU14_mha_res
	L8_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L8_S1_TP6_GPU14_mha_res -> L8_S1_TP6_GPU14_gate
	L8_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L8_S1_TP6_GPU14_gate -> L8_S1_TP6_GPU14_expert0 [style=dashed]
	L8_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L8_S1_TP6_GPU14_gate -> L8_S1_TP6_GPU14_expert1 [style=dashed]
	L8_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP6_GPU14_moe_ar
	L8_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP6_GPU14_moe_ar -> L8_S1_TP6_GPU14_moe_res
	L8_S1_TP6_GPU14_mha_res -> L8_S1_TP6_GPU14_moe_res
	L8_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP6_GPU14_moe_res -> L8_S1_TP6_GPU14_norm
	L8_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L8_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L8_S1_TP7_GPU15_qkv_linear -> L8_S1_TP7_GPU15_qkv_slice
	L8_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L8_S1_TP7_GPU15_qkv_slice -> L8_S1_TP7_GPU15_qk_matmul
	L8_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L8_S1_TP7_GPU15_qk_matmul -> L8_S1_TP7_GPU15_softmax
	L8_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L8_S1_TP7_GPU15_softmax -> L8_S1_TP7_GPU15_attn_v
	L8_S1_TP7_GPU15_qkv_slice -> L8_S1_TP7_GPU15_attn_v
	L8_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP1_GPU9_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP2_GPU10_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP3_GPU11_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP4_GPU12_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP5_GPU13_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP6_GPU14_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP7_GPU15_attn_v -> L8_S1_TP7_GPU15_mha_ar
	L8_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP7_GPU15_mha_ar -> L8_S1_TP7_GPU15_mha_res
	L8_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L8_S1_TP7_GPU15_mha_res -> L8_S1_TP7_GPU15_gate
	L8_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L8_S1_TP7_GPU15_gate -> L8_S1_TP7_GPU15_expert0 [style=dashed]
	L8_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L8_S1_TP7_GPU15_gate -> L8_S1_TP7_GPU15_expert1 [style=dashed]
	L8_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L8_S1_TP0_GPU8_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP0_GPU8_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP1_GPU9_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP1_GPU9_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP2_GPU10_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP2_GPU10_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP3_GPU11_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP3_GPU11_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP4_GPU12_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP4_GPU12_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP5_GPU13_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP5_GPU13_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP6_GPU14_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP6_GPU14_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP7_GPU15_expert0 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP7_GPU15_expert1 -> L8_S1_TP7_GPU15_moe_ar
	L8_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP7_GPU15_moe_ar -> L8_S1_TP7_GPU15_moe_res
	L8_S1_TP7_GPU15_mha_res -> L8_S1_TP7_GPU15_moe_res
	L8_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L8_S1_TP7_GPU15_moe_res -> L8_S1_TP7_GPU15_norm
	L9_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L9_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L9_S1_TP0_GPU8_qkv_linear -> L9_S1_TP0_GPU8_qkv_slice
	L9_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L9_S1_TP0_GPU8_qkv_slice -> L9_S1_TP0_GPU8_qk_matmul
	L9_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L9_S1_TP0_GPU8_qk_matmul -> L9_S1_TP0_GPU8_softmax
	L9_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L9_S1_TP0_GPU8_softmax -> L9_S1_TP0_GPU8_attn_v
	L9_S1_TP0_GPU8_qkv_slice -> L9_S1_TP0_GPU8_attn_v
	L9_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP0_GPU8_mha_ar
	L9_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP0_GPU8_mha_ar -> L9_S1_TP0_GPU8_mha_res
	L9_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L9_S1_TP0_GPU8_mha_res -> L9_S1_TP0_GPU8_gate
	L9_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L9_S1_TP0_GPU8_gate -> L9_S1_TP0_GPU8_expert0 [style=dashed]
	L9_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L9_S1_TP0_GPU8_gate -> L9_S1_TP0_GPU8_expert1 [style=dashed]
	L9_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP0_GPU8_moe_ar
	L9_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP0_GPU8_moe_ar -> L9_S1_TP0_GPU8_moe_res
	L9_S1_TP0_GPU8_mha_res -> L9_S1_TP0_GPU8_moe_res
	L9_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP0_GPU8_moe_res -> L9_S1_TP0_GPU8_norm
	L9_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L9_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L9_S1_TP1_GPU9_qkv_linear -> L9_S1_TP1_GPU9_qkv_slice
	L9_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L9_S1_TP1_GPU9_qkv_slice -> L9_S1_TP1_GPU9_qk_matmul
	L9_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L9_S1_TP1_GPU9_qk_matmul -> L9_S1_TP1_GPU9_softmax
	L9_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L9_S1_TP1_GPU9_softmax -> L9_S1_TP1_GPU9_attn_v
	L9_S1_TP1_GPU9_qkv_slice -> L9_S1_TP1_GPU9_attn_v
	L9_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP1_GPU9_mha_ar
	L9_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP1_GPU9_mha_ar -> L9_S1_TP1_GPU9_mha_res
	L9_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L9_S1_TP1_GPU9_mha_res -> L9_S1_TP1_GPU9_gate
	L9_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L9_S1_TP1_GPU9_gate -> L9_S1_TP1_GPU9_expert0 [style=dashed]
	L9_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L9_S1_TP1_GPU9_gate -> L9_S1_TP1_GPU9_expert1 [style=dashed]
	L9_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP1_GPU9_moe_ar
	L9_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP1_GPU9_moe_ar -> L9_S1_TP1_GPU9_moe_res
	L9_S1_TP1_GPU9_mha_res -> L9_S1_TP1_GPU9_moe_res
	L9_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP1_GPU9_moe_res -> L9_S1_TP1_GPU9_norm
	L9_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L9_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L9_S1_TP2_GPU10_qkv_linear -> L9_S1_TP2_GPU10_qkv_slice
	L9_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L9_S1_TP2_GPU10_qkv_slice -> L9_S1_TP2_GPU10_qk_matmul
	L9_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L9_S1_TP2_GPU10_qk_matmul -> L9_S1_TP2_GPU10_softmax
	L9_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L9_S1_TP2_GPU10_softmax -> L9_S1_TP2_GPU10_attn_v
	L9_S1_TP2_GPU10_qkv_slice -> L9_S1_TP2_GPU10_attn_v
	L9_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP2_GPU10_mha_ar
	L9_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP2_GPU10_mha_ar -> L9_S1_TP2_GPU10_mha_res
	L9_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L9_S1_TP2_GPU10_mha_res -> L9_S1_TP2_GPU10_gate
	L9_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L9_S1_TP2_GPU10_gate -> L9_S1_TP2_GPU10_expert0 [style=dashed]
	L9_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L9_S1_TP2_GPU10_gate -> L9_S1_TP2_GPU10_expert1 [style=dashed]
	L9_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP2_GPU10_moe_ar
	L9_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP2_GPU10_moe_ar -> L9_S1_TP2_GPU10_moe_res
	L9_S1_TP2_GPU10_mha_res -> L9_S1_TP2_GPU10_moe_res
	L9_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP2_GPU10_moe_res -> L9_S1_TP2_GPU10_norm
	L9_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L9_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L9_S1_TP3_GPU11_qkv_linear -> L9_S1_TP3_GPU11_qkv_slice
	L9_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L9_S1_TP3_GPU11_qkv_slice -> L9_S1_TP3_GPU11_qk_matmul
	L9_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L9_S1_TP3_GPU11_qk_matmul -> L9_S1_TP3_GPU11_softmax
	L9_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L9_S1_TP3_GPU11_softmax -> L9_S1_TP3_GPU11_attn_v
	L9_S1_TP3_GPU11_qkv_slice -> L9_S1_TP3_GPU11_attn_v
	L9_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP3_GPU11_mha_ar
	L9_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP3_GPU11_mha_ar -> L9_S1_TP3_GPU11_mha_res
	L9_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L9_S1_TP3_GPU11_mha_res -> L9_S1_TP3_GPU11_gate
	L9_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L9_S1_TP3_GPU11_gate -> L9_S1_TP3_GPU11_expert0 [style=dashed]
	L9_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L9_S1_TP3_GPU11_gate -> L9_S1_TP3_GPU11_expert1 [style=dashed]
	L9_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP3_GPU11_moe_ar
	L9_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP3_GPU11_moe_ar -> L9_S1_TP3_GPU11_moe_res
	L9_S1_TP3_GPU11_mha_res -> L9_S1_TP3_GPU11_moe_res
	L9_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP3_GPU11_moe_res -> L9_S1_TP3_GPU11_norm
	L9_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L9_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L9_S1_TP4_GPU12_qkv_linear -> L9_S1_TP4_GPU12_qkv_slice
	L9_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L9_S1_TP4_GPU12_qkv_slice -> L9_S1_TP4_GPU12_qk_matmul
	L9_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L9_S1_TP4_GPU12_qk_matmul -> L9_S1_TP4_GPU12_softmax
	L9_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L9_S1_TP4_GPU12_softmax -> L9_S1_TP4_GPU12_attn_v
	L9_S1_TP4_GPU12_qkv_slice -> L9_S1_TP4_GPU12_attn_v
	L9_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP4_GPU12_mha_ar
	L9_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP4_GPU12_mha_ar -> L9_S1_TP4_GPU12_mha_res
	L9_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L9_S1_TP4_GPU12_mha_res -> L9_S1_TP4_GPU12_gate
	L9_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L9_S1_TP4_GPU12_gate -> L9_S1_TP4_GPU12_expert0 [style=dashed]
	L9_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L9_S1_TP4_GPU12_gate -> L9_S1_TP4_GPU12_expert1 [style=dashed]
	L9_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP4_GPU12_moe_ar
	L9_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP4_GPU12_moe_ar -> L9_S1_TP4_GPU12_moe_res
	L9_S1_TP4_GPU12_mha_res -> L9_S1_TP4_GPU12_moe_res
	L9_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP4_GPU12_moe_res -> L9_S1_TP4_GPU12_norm
	L9_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L9_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L9_S1_TP5_GPU13_qkv_linear -> L9_S1_TP5_GPU13_qkv_slice
	L9_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L9_S1_TP5_GPU13_qkv_slice -> L9_S1_TP5_GPU13_qk_matmul
	L9_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L9_S1_TP5_GPU13_qk_matmul -> L9_S1_TP5_GPU13_softmax
	L9_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L9_S1_TP5_GPU13_softmax -> L9_S1_TP5_GPU13_attn_v
	L9_S1_TP5_GPU13_qkv_slice -> L9_S1_TP5_GPU13_attn_v
	L9_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP5_GPU13_mha_ar
	L9_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP5_GPU13_mha_ar -> L9_S1_TP5_GPU13_mha_res
	L9_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L9_S1_TP5_GPU13_mha_res -> L9_S1_TP5_GPU13_gate
	L9_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L9_S1_TP5_GPU13_gate -> L9_S1_TP5_GPU13_expert0 [style=dashed]
	L9_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L9_S1_TP5_GPU13_gate -> L9_S1_TP5_GPU13_expert1 [style=dashed]
	L9_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP5_GPU13_moe_ar
	L9_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP5_GPU13_moe_ar -> L9_S1_TP5_GPU13_moe_res
	L9_S1_TP5_GPU13_mha_res -> L9_S1_TP5_GPU13_moe_res
	L9_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP5_GPU13_moe_res -> L9_S1_TP5_GPU13_norm
	L9_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L9_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L9_S1_TP6_GPU14_qkv_linear -> L9_S1_TP6_GPU14_qkv_slice
	L9_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L9_S1_TP6_GPU14_qkv_slice -> L9_S1_TP6_GPU14_qk_matmul
	L9_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L9_S1_TP6_GPU14_qk_matmul -> L9_S1_TP6_GPU14_softmax
	L9_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L9_S1_TP6_GPU14_softmax -> L9_S1_TP6_GPU14_attn_v
	L9_S1_TP6_GPU14_qkv_slice -> L9_S1_TP6_GPU14_attn_v
	L9_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP6_GPU14_mha_ar
	L9_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP6_GPU14_mha_ar -> L9_S1_TP6_GPU14_mha_res
	L9_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L9_S1_TP6_GPU14_mha_res -> L9_S1_TP6_GPU14_gate
	L9_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L9_S1_TP6_GPU14_gate -> L9_S1_TP6_GPU14_expert0 [style=dashed]
	L9_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L9_S1_TP6_GPU14_gate -> L9_S1_TP6_GPU14_expert1 [style=dashed]
	L9_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP6_GPU14_moe_ar
	L9_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP6_GPU14_moe_ar -> L9_S1_TP6_GPU14_moe_res
	L9_S1_TP6_GPU14_mha_res -> L9_S1_TP6_GPU14_moe_res
	L9_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP6_GPU14_moe_res -> L9_S1_TP6_GPU14_norm
	L9_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L9_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L9_S1_TP7_GPU15_qkv_linear -> L9_S1_TP7_GPU15_qkv_slice
	L9_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L9_S1_TP7_GPU15_qkv_slice -> L9_S1_TP7_GPU15_qk_matmul
	L9_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L9_S1_TP7_GPU15_qk_matmul -> L9_S1_TP7_GPU15_softmax
	L9_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L9_S1_TP7_GPU15_softmax -> L9_S1_TP7_GPU15_attn_v
	L9_S1_TP7_GPU15_qkv_slice -> L9_S1_TP7_GPU15_attn_v
	L9_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP1_GPU9_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP2_GPU10_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP3_GPU11_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP4_GPU12_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP5_GPU13_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP6_GPU14_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP7_GPU15_attn_v -> L9_S1_TP7_GPU15_mha_ar
	L9_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP7_GPU15_mha_ar -> L9_S1_TP7_GPU15_mha_res
	L9_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L9_S1_TP7_GPU15_mha_res -> L9_S1_TP7_GPU15_gate
	L9_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L9_S1_TP7_GPU15_gate -> L9_S1_TP7_GPU15_expert0 [style=dashed]
	L9_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L9_S1_TP7_GPU15_gate -> L9_S1_TP7_GPU15_expert1 [style=dashed]
	L9_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L9_S1_TP0_GPU8_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP0_GPU8_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP1_GPU9_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP1_GPU9_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP2_GPU10_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP2_GPU10_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP3_GPU11_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP3_GPU11_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP4_GPU12_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP4_GPU12_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP5_GPU13_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP5_GPU13_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP6_GPU14_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP6_GPU14_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP7_GPU15_expert0 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP7_GPU15_expert1 -> L9_S1_TP7_GPU15_moe_ar
	L9_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP7_GPU15_moe_ar -> L9_S1_TP7_GPU15_moe_res
	L9_S1_TP7_GPU15_mha_res -> L9_S1_TP7_GPU15_moe_res
	L9_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L9_S1_TP7_GPU15_moe_res -> L9_S1_TP7_GPU15_norm
	L10_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L10_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L10_S1_TP0_GPU8_qkv_linear -> L10_S1_TP0_GPU8_qkv_slice
	L10_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L10_S1_TP0_GPU8_qkv_slice -> L10_S1_TP0_GPU8_qk_matmul
	L10_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L10_S1_TP0_GPU8_qk_matmul -> L10_S1_TP0_GPU8_softmax
	L10_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L10_S1_TP0_GPU8_softmax -> L10_S1_TP0_GPU8_attn_v
	L10_S1_TP0_GPU8_qkv_slice -> L10_S1_TP0_GPU8_attn_v
	L10_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP0_GPU8_mha_ar
	L10_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP0_GPU8_mha_ar -> L10_S1_TP0_GPU8_mha_res
	L10_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L10_S1_TP0_GPU8_mha_res -> L10_S1_TP0_GPU8_gate
	L10_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L10_S1_TP0_GPU8_gate -> L10_S1_TP0_GPU8_expert0 [style=dashed]
	L10_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L10_S1_TP0_GPU8_gate -> L10_S1_TP0_GPU8_expert1 [style=dashed]
	L10_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP0_GPU8_moe_ar
	L10_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP0_GPU8_moe_ar -> L10_S1_TP0_GPU8_moe_res
	L10_S1_TP0_GPU8_mha_res -> L10_S1_TP0_GPU8_moe_res
	L10_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP0_GPU8_moe_res -> L10_S1_TP0_GPU8_norm
	L10_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L10_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L10_S1_TP1_GPU9_qkv_linear -> L10_S1_TP1_GPU9_qkv_slice
	L10_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L10_S1_TP1_GPU9_qkv_slice -> L10_S1_TP1_GPU9_qk_matmul
	L10_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L10_S1_TP1_GPU9_qk_matmul -> L10_S1_TP1_GPU9_softmax
	L10_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L10_S1_TP1_GPU9_softmax -> L10_S1_TP1_GPU9_attn_v
	L10_S1_TP1_GPU9_qkv_slice -> L10_S1_TP1_GPU9_attn_v
	L10_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP1_GPU9_mha_ar
	L10_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP1_GPU9_mha_ar -> L10_S1_TP1_GPU9_mha_res
	L10_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L10_S1_TP1_GPU9_mha_res -> L10_S1_TP1_GPU9_gate
	L10_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L10_S1_TP1_GPU9_gate -> L10_S1_TP1_GPU9_expert0 [style=dashed]
	L10_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L10_S1_TP1_GPU9_gate -> L10_S1_TP1_GPU9_expert1 [style=dashed]
	L10_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP1_GPU9_moe_ar
	L10_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP1_GPU9_moe_ar -> L10_S1_TP1_GPU9_moe_res
	L10_S1_TP1_GPU9_mha_res -> L10_S1_TP1_GPU9_moe_res
	L10_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP1_GPU9_moe_res -> L10_S1_TP1_GPU9_norm
	L10_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L10_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L10_S1_TP2_GPU10_qkv_linear -> L10_S1_TP2_GPU10_qkv_slice
	L10_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L10_S1_TP2_GPU10_qkv_slice -> L10_S1_TP2_GPU10_qk_matmul
	L10_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L10_S1_TP2_GPU10_qk_matmul -> L10_S1_TP2_GPU10_softmax
	L10_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L10_S1_TP2_GPU10_softmax -> L10_S1_TP2_GPU10_attn_v
	L10_S1_TP2_GPU10_qkv_slice -> L10_S1_TP2_GPU10_attn_v
	L10_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP2_GPU10_mha_ar
	L10_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP2_GPU10_mha_ar -> L10_S1_TP2_GPU10_mha_res
	L10_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L10_S1_TP2_GPU10_mha_res -> L10_S1_TP2_GPU10_gate
	L10_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L10_S1_TP2_GPU10_gate -> L10_S1_TP2_GPU10_expert0 [style=dashed]
	L10_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L10_S1_TP2_GPU10_gate -> L10_S1_TP2_GPU10_expert1 [style=dashed]
	L10_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP2_GPU10_moe_ar
	L10_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP2_GPU10_moe_ar -> L10_S1_TP2_GPU10_moe_res
	L10_S1_TP2_GPU10_mha_res -> L10_S1_TP2_GPU10_moe_res
	L10_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP2_GPU10_moe_res -> L10_S1_TP2_GPU10_norm
	L10_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L10_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L10_S1_TP3_GPU11_qkv_linear -> L10_S1_TP3_GPU11_qkv_slice
	L10_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L10_S1_TP3_GPU11_qkv_slice -> L10_S1_TP3_GPU11_qk_matmul
	L10_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L10_S1_TP3_GPU11_qk_matmul -> L10_S1_TP3_GPU11_softmax
	L10_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L10_S1_TP3_GPU11_softmax -> L10_S1_TP3_GPU11_attn_v
	L10_S1_TP3_GPU11_qkv_slice -> L10_S1_TP3_GPU11_attn_v
	L10_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP3_GPU11_mha_ar
	L10_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP3_GPU11_mha_ar -> L10_S1_TP3_GPU11_mha_res
	L10_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L10_S1_TP3_GPU11_mha_res -> L10_S1_TP3_GPU11_gate
	L10_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L10_S1_TP3_GPU11_gate -> L10_S1_TP3_GPU11_expert0 [style=dashed]
	L10_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L10_S1_TP3_GPU11_gate -> L10_S1_TP3_GPU11_expert1 [style=dashed]
	L10_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP3_GPU11_moe_ar
	L10_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP3_GPU11_moe_ar -> L10_S1_TP3_GPU11_moe_res
	L10_S1_TP3_GPU11_mha_res -> L10_S1_TP3_GPU11_moe_res
	L10_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP3_GPU11_moe_res -> L10_S1_TP3_GPU11_norm
	L10_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L10_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L10_S1_TP4_GPU12_qkv_linear -> L10_S1_TP4_GPU12_qkv_slice
	L10_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L10_S1_TP4_GPU12_qkv_slice -> L10_S1_TP4_GPU12_qk_matmul
	L10_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L10_S1_TP4_GPU12_qk_matmul -> L10_S1_TP4_GPU12_softmax
	L10_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L10_S1_TP4_GPU12_softmax -> L10_S1_TP4_GPU12_attn_v
	L10_S1_TP4_GPU12_qkv_slice -> L10_S1_TP4_GPU12_attn_v
	L10_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP4_GPU12_mha_ar
	L10_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP4_GPU12_mha_ar -> L10_S1_TP4_GPU12_mha_res
	L10_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L10_S1_TP4_GPU12_mha_res -> L10_S1_TP4_GPU12_gate
	L10_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L10_S1_TP4_GPU12_gate -> L10_S1_TP4_GPU12_expert0 [style=dashed]
	L10_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L10_S1_TP4_GPU12_gate -> L10_S1_TP4_GPU12_expert1 [style=dashed]
	L10_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP4_GPU12_moe_ar
	L10_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP4_GPU12_moe_ar -> L10_S1_TP4_GPU12_moe_res
	L10_S1_TP4_GPU12_mha_res -> L10_S1_TP4_GPU12_moe_res
	L10_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP4_GPU12_moe_res -> L10_S1_TP4_GPU12_norm
	L10_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L10_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L10_S1_TP5_GPU13_qkv_linear -> L10_S1_TP5_GPU13_qkv_slice
	L10_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L10_S1_TP5_GPU13_qkv_slice -> L10_S1_TP5_GPU13_qk_matmul
	L10_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L10_S1_TP5_GPU13_qk_matmul -> L10_S1_TP5_GPU13_softmax
	L10_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L10_S1_TP5_GPU13_softmax -> L10_S1_TP5_GPU13_attn_v
	L10_S1_TP5_GPU13_qkv_slice -> L10_S1_TP5_GPU13_attn_v
	L10_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP5_GPU13_mha_ar
	L10_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP5_GPU13_mha_ar -> L10_S1_TP5_GPU13_mha_res
	L10_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L10_S1_TP5_GPU13_mha_res -> L10_S1_TP5_GPU13_gate
	L10_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L10_S1_TP5_GPU13_gate -> L10_S1_TP5_GPU13_expert0 [style=dashed]
	L10_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L10_S1_TP5_GPU13_gate -> L10_S1_TP5_GPU13_expert1 [style=dashed]
	L10_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP5_GPU13_moe_ar
	L10_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP5_GPU13_moe_ar -> L10_S1_TP5_GPU13_moe_res
	L10_S1_TP5_GPU13_mha_res -> L10_S1_TP5_GPU13_moe_res
	L10_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP5_GPU13_moe_res -> L10_S1_TP5_GPU13_norm
	L10_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L10_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L10_S1_TP6_GPU14_qkv_linear -> L10_S1_TP6_GPU14_qkv_slice
	L10_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L10_S1_TP6_GPU14_qkv_slice -> L10_S1_TP6_GPU14_qk_matmul
	L10_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L10_S1_TP6_GPU14_qk_matmul -> L10_S1_TP6_GPU14_softmax
	L10_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L10_S1_TP6_GPU14_softmax -> L10_S1_TP6_GPU14_attn_v
	L10_S1_TP6_GPU14_qkv_slice -> L10_S1_TP6_GPU14_attn_v
	L10_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP6_GPU14_mha_ar
	L10_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP6_GPU14_mha_ar -> L10_S1_TP6_GPU14_mha_res
	L10_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L10_S1_TP6_GPU14_mha_res -> L10_S1_TP6_GPU14_gate
	L10_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L10_S1_TP6_GPU14_gate -> L10_S1_TP6_GPU14_expert0 [style=dashed]
	L10_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L10_S1_TP6_GPU14_gate -> L10_S1_TP6_GPU14_expert1 [style=dashed]
	L10_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP6_GPU14_moe_ar
	L10_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP6_GPU14_moe_ar -> L10_S1_TP6_GPU14_moe_res
	L10_S1_TP6_GPU14_mha_res -> L10_S1_TP6_GPU14_moe_res
	L10_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP6_GPU14_moe_res -> L10_S1_TP6_GPU14_norm
	L10_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L10_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L10_S1_TP7_GPU15_qkv_linear -> L10_S1_TP7_GPU15_qkv_slice
	L10_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L10_S1_TP7_GPU15_qkv_slice -> L10_S1_TP7_GPU15_qk_matmul
	L10_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L10_S1_TP7_GPU15_qk_matmul -> L10_S1_TP7_GPU15_softmax
	L10_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L10_S1_TP7_GPU15_softmax -> L10_S1_TP7_GPU15_attn_v
	L10_S1_TP7_GPU15_qkv_slice -> L10_S1_TP7_GPU15_attn_v
	L10_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP1_GPU9_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP2_GPU10_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP3_GPU11_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP4_GPU12_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP5_GPU13_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP6_GPU14_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP7_GPU15_attn_v -> L10_S1_TP7_GPU15_mha_ar
	L10_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP7_GPU15_mha_ar -> L10_S1_TP7_GPU15_mha_res
	L10_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L10_S1_TP7_GPU15_mha_res -> L10_S1_TP7_GPU15_gate
	L10_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L10_S1_TP7_GPU15_gate -> L10_S1_TP7_GPU15_expert0 [style=dashed]
	L10_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L10_S1_TP7_GPU15_gate -> L10_S1_TP7_GPU15_expert1 [style=dashed]
	L10_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L10_S1_TP0_GPU8_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP0_GPU8_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP1_GPU9_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP1_GPU9_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP2_GPU10_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP2_GPU10_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP3_GPU11_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP3_GPU11_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP4_GPU12_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP4_GPU12_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP5_GPU13_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP5_GPU13_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP6_GPU14_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP6_GPU14_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP7_GPU15_expert0 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP7_GPU15_expert1 -> L10_S1_TP7_GPU15_moe_ar
	L10_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP7_GPU15_moe_ar -> L10_S1_TP7_GPU15_moe_res
	L10_S1_TP7_GPU15_mha_res -> L10_S1_TP7_GPU15_moe_res
	L10_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L10_S1_TP7_GPU15_moe_res -> L10_S1_TP7_GPU15_norm
	L11_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L11_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L11_S1_TP0_GPU8_qkv_linear -> L11_S1_TP0_GPU8_qkv_slice
	L11_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L11_S1_TP0_GPU8_qkv_slice -> L11_S1_TP0_GPU8_qk_matmul
	L11_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L11_S1_TP0_GPU8_qk_matmul -> L11_S1_TP0_GPU8_softmax
	L11_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L11_S1_TP0_GPU8_softmax -> L11_S1_TP0_GPU8_attn_v
	L11_S1_TP0_GPU8_qkv_slice -> L11_S1_TP0_GPU8_attn_v
	L11_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP0_GPU8_mha_ar
	L11_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP0_GPU8_mha_ar -> L11_S1_TP0_GPU8_mha_res
	L11_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L11_S1_TP0_GPU8_mha_res -> L11_S1_TP0_GPU8_gate
	L11_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L11_S1_TP0_GPU8_gate -> L11_S1_TP0_GPU8_expert0 [style=dashed]
	L11_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L11_S1_TP0_GPU8_gate -> L11_S1_TP0_GPU8_expert1 [style=dashed]
	L11_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP0_GPU8_moe_ar
	L11_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP0_GPU8_moe_ar -> L11_S1_TP0_GPU8_moe_res
	L11_S1_TP0_GPU8_mha_res -> L11_S1_TP0_GPU8_moe_res
	L11_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP0_GPU8_moe_res -> L11_S1_TP0_GPU8_norm
	L11_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L11_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L11_S1_TP1_GPU9_qkv_linear -> L11_S1_TP1_GPU9_qkv_slice
	L11_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L11_S1_TP1_GPU9_qkv_slice -> L11_S1_TP1_GPU9_qk_matmul
	L11_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L11_S1_TP1_GPU9_qk_matmul -> L11_S1_TP1_GPU9_softmax
	L11_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L11_S1_TP1_GPU9_softmax -> L11_S1_TP1_GPU9_attn_v
	L11_S1_TP1_GPU9_qkv_slice -> L11_S1_TP1_GPU9_attn_v
	L11_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP1_GPU9_mha_ar
	L11_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP1_GPU9_mha_ar -> L11_S1_TP1_GPU9_mha_res
	L11_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L11_S1_TP1_GPU9_mha_res -> L11_S1_TP1_GPU9_gate
	L11_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L11_S1_TP1_GPU9_gate -> L11_S1_TP1_GPU9_expert0 [style=dashed]
	L11_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L11_S1_TP1_GPU9_gate -> L11_S1_TP1_GPU9_expert1 [style=dashed]
	L11_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP1_GPU9_moe_ar
	L11_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP1_GPU9_moe_ar -> L11_S1_TP1_GPU9_moe_res
	L11_S1_TP1_GPU9_mha_res -> L11_S1_TP1_GPU9_moe_res
	L11_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP1_GPU9_moe_res -> L11_S1_TP1_GPU9_norm
	L11_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L11_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L11_S1_TP2_GPU10_qkv_linear -> L11_S1_TP2_GPU10_qkv_slice
	L11_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L11_S1_TP2_GPU10_qkv_slice -> L11_S1_TP2_GPU10_qk_matmul
	L11_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L11_S1_TP2_GPU10_qk_matmul -> L11_S1_TP2_GPU10_softmax
	L11_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L11_S1_TP2_GPU10_softmax -> L11_S1_TP2_GPU10_attn_v
	L11_S1_TP2_GPU10_qkv_slice -> L11_S1_TP2_GPU10_attn_v
	L11_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP2_GPU10_mha_ar
	L11_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP2_GPU10_mha_ar -> L11_S1_TP2_GPU10_mha_res
	L11_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L11_S1_TP2_GPU10_mha_res -> L11_S1_TP2_GPU10_gate
	L11_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L11_S1_TP2_GPU10_gate -> L11_S1_TP2_GPU10_expert0 [style=dashed]
	L11_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L11_S1_TP2_GPU10_gate -> L11_S1_TP2_GPU10_expert1 [style=dashed]
	L11_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP2_GPU10_moe_ar
	L11_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP2_GPU10_moe_ar -> L11_S1_TP2_GPU10_moe_res
	L11_S1_TP2_GPU10_mha_res -> L11_S1_TP2_GPU10_moe_res
	L11_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP2_GPU10_moe_res -> L11_S1_TP2_GPU10_norm
	L11_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L11_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L11_S1_TP3_GPU11_qkv_linear -> L11_S1_TP3_GPU11_qkv_slice
	L11_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L11_S1_TP3_GPU11_qkv_slice -> L11_S1_TP3_GPU11_qk_matmul
	L11_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L11_S1_TP3_GPU11_qk_matmul -> L11_S1_TP3_GPU11_softmax
	L11_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L11_S1_TP3_GPU11_softmax -> L11_S1_TP3_GPU11_attn_v
	L11_S1_TP3_GPU11_qkv_slice -> L11_S1_TP3_GPU11_attn_v
	L11_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP3_GPU11_mha_ar
	L11_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP3_GPU11_mha_ar -> L11_S1_TP3_GPU11_mha_res
	L11_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L11_S1_TP3_GPU11_mha_res -> L11_S1_TP3_GPU11_gate
	L11_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L11_S1_TP3_GPU11_gate -> L11_S1_TP3_GPU11_expert0 [style=dashed]
	L11_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L11_S1_TP3_GPU11_gate -> L11_S1_TP3_GPU11_expert1 [style=dashed]
	L11_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP3_GPU11_moe_ar
	L11_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP3_GPU11_moe_ar -> L11_S1_TP3_GPU11_moe_res
	L11_S1_TP3_GPU11_mha_res -> L11_S1_TP3_GPU11_moe_res
	L11_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP3_GPU11_moe_res -> L11_S1_TP3_GPU11_norm
	L11_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L11_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L11_S1_TP4_GPU12_qkv_linear -> L11_S1_TP4_GPU12_qkv_slice
	L11_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L11_S1_TP4_GPU12_qkv_slice -> L11_S1_TP4_GPU12_qk_matmul
	L11_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L11_S1_TP4_GPU12_qk_matmul -> L11_S1_TP4_GPU12_softmax
	L11_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L11_S1_TP4_GPU12_softmax -> L11_S1_TP4_GPU12_attn_v
	L11_S1_TP4_GPU12_qkv_slice -> L11_S1_TP4_GPU12_attn_v
	L11_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP4_GPU12_mha_ar
	L11_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP4_GPU12_mha_ar -> L11_S1_TP4_GPU12_mha_res
	L11_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L11_S1_TP4_GPU12_mha_res -> L11_S1_TP4_GPU12_gate
	L11_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L11_S1_TP4_GPU12_gate -> L11_S1_TP4_GPU12_expert0 [style=dashed]
	L11_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L11_S1_TP4_GPU12_gate -> L11_S1_TP4_GPU12_expert1 [style=dashed]
	L11_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP4_GPU12_moe_ar
	L11_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP4_GPU12_moe_ar -> L11_S1_TP4_GPU12_moe_res
	L11_S1_TP4_GPU12_mha_res -> L11_S1_TP4_GPU12_moe_res
	L11_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP4_GPU12_moe_res -> L11_S1_TP4_GPU12_norm
	L11_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L11_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L11_S1_TP5_GPU13_qkv_linear -> L11_S1_TP5_GPU13_qkv_slice
	L11_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L11_S1_TP5_GPU13_qkv_slice -> L11_S1_TP5_GPU13_qk_matmul
	L11_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L11_S1_TP5_GPU13_qk_matmul -> L11_S1_TP5_GPU13_softmax
	L11_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L11_S1_TP5_GPU13_softmax -> L11_S1_TP5_GPU13_attn_v
	L11_S1_TP5_GPU13_qkv_slice -> L11_S1_TP5_GPU13_attn_v
	L11_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP5_GPU13_mha_ar
	L11_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP5_GPU13_mha_ar -> L11_S1_TP5_GPU13_mha_res
	L11_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L11_S1_TP5_GPU13_mha_res -> L11_S1_TP5_GPU13_gate
	L11_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L11_S1_TP5_GPU13_gate -> L11_S1_TP5_GPU13_expert0 [style=dashed]
	L11_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L11_S1_TP5_GPU13_gate -> L11_S1_TP5_GPU13_expert1 [style=dashed]
	L11_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP5_GPU13_moe_ar
	L11_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP5_GPU13_moe_ar -> L11_S1_TP5_GPU13_moe_res
	L11_S1_TP5_GPU13_mha_res -> L11_S1_TP5_GPU13_moe_res
	L11_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP5_GPU13_moe_res -> L11_S1_TP5_GPU13_norm
	L11_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L11_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L11_S1_TP6_GPU14_qkv_linear -> L11_S1_TP6_GPU14_qkv_slice
	L11_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L11_S1_TP6_GPU14_qkv_slice -> L11_S1_TP6_GPU14_qk_matmul
	L11_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L11_S1_TP6_GPU14_qk_matmul -> L11_S1_TP6_GPU14_softmax
	L11_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L11_S1_TP6_GPU14_softmax -> L11_S1_TP6_GPU14_attn_v
	L11_S1_TP6_GPU14_qkv_slice -> L11_S1_TP6_GPU14_attn_v
	L11_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP6_GPU14_mha_ar
	L11_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP6_GPU14_mha_ar -> L11_S1_TP6_GPU14_mha_res
	L11_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L11_S1_TP6_GPU14_mha_res -> L11_S1_TP6_GPU14_gate
	L11_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L11_S1_TP6_GPU14_gate -> L11_S1_TP6_GPU14_expert0 [style=dashed]
	L11_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L11_S1_TP6_GPU14_gate -> L11_S1_TP6_GPU14_expert1 [style=dashed]
	L11_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP6_GPU14_moe_ar
	L11_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP6_GPU14_moe_ar -> L11_S1_TP6_GPU14_moe_res
	L11_S1_TP6_GPU14_mha_res -> L11_S1_TP6_GPU14_moe_res
	L11_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP6_GPU14_moe_res -> L11_S1_TP6_GPU14_norm
	L11_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L11_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L11_S1_TP7_GPU15_qkv_linear -> L11_S1_TP7_GPU15_qkv_slice
	L11_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L11_S1_TP7_GPU15_qkv_slice -> L11_S1_TP7_GPU15_qk_matmul
	L11_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L11_S1_TP7_GPU15_qk_matmul -> L11_S1_TP7_GPU15_softmax
	L11_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L11_S1_TP7_GPU15_softmax -> L11_S1_TP7_GPU15_attn_v
	L11_S1_TP7_GPU15_qkv_slice -> L11_S1_TP7_GPU15_attn_v
	L11_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP1_GPU9_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP2_GPU10_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP3_GPU11_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP4_GPU12_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP5_GPU13_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP6_GPU14_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP7_GPU15_attn_v -> L11_S1_TP7_GPU15_mha_ar
	L11_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP7_GPU15_mha_ar -> L11_S1_TP7_GPU15_mha_res
	L11_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L11_S1_TP7_GPU15_mha_res -> L11_S1_TP7_GPU15_gate
	L11_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L11_S1_TP7_GPU15_gate -> L11_S1_TP7_GPU15_expert0 [style=dashed]
	L11_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L11_S1_TP7_GPU15_gate -> L11_S1_TP7_GPU15_expert1 [style=dashed]
	L11_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L11_S1_TP0_GPU8_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP0_GPU8_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP1_GPU9_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP1_GPU9_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP2_GPU10_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP2_GPU10_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP3_GPU11_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP3_GPU11_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP4_GPU12_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP4_GPU12_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP5_GPU13_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP5_GPU13_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP6_GPU14_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP6_GPU14_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP7_GPU15_expert0 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP7_GPU15_expert1 -> L11_S1_TP7_GPU15_moe_ar
	L11_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP7_GPU15_moe_ar -> L11_S1_TP7_GPU15_moe_res
	L11_S1_TP7_GPU15_mha_res -> L11_S1_TP7_GPU15_moe_res
	L11_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L11_S1_TP7_GPU15_moe_res -> L11_S1_TP7_GPU15_norm
	L12_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L12_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L12_S1_TP0_GPU8_qkv_linear -> L12_S1_TP0_GPU8_qkv_slice
	L12_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L12_S1_TP0_GPU8_qkv_slice -> L12_S1_TP0_GPU8_qk_matmul
	L12_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L12_S1_TP0_GPU8_qk_matmul -> L12_S1_TP0_GPU8_softmax
	L12_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L12_S1_TP0_GPU8_softmax -> L12_S1_TP0_GPU8_attn_v
	L12_S1_TP0_GPU8_qkv_slice -> L12_S1_TP0_GPU8_attn_v
	L12_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP0_GPU8_mha_ar
	L12_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP0_GPU8_mha_ar -> L12_S1_TP0_GPU8_mha_res
	L12_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L12_S1_TP0_GPU8_mha_res -> L12_S1_TP0_GPU8_gate
	L12_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L12_S1_TP0_GPU8_gate -> L12_S1_TP0_GPU8_expert0 [style=dashed]
	L12_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L12_S1_TP0_GPU8_gate -> L12_S1_TP0_GPU8_expert1 [style=dashed]
	L12_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP0_GPU8_moe_ar
	L12_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP0_GPU8_moe_ar -> L12_S1_TP0_GPU8_moe_res
	L12_S1_TP0_GPU8_mha_res -> L12_S1_TP0_GPU8_moe_res
	L12_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP0_GPU8_moe_res -> L12_S1_TP0_GPU8_norm
	L12_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L12_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L12_S1_TP1_GPU9_qkv_linear -> L12_S1_TP1_GPU9_qkv_slice
	L12_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L12_S1_TP1_GPU9_qkv_slice -> L12_S1_TP1_GPU9_qk_matmul
	L12_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L12_S1_TP1_GPU9_qk_matmul -> L12_S1_TP1_GPU9_softmax
	L12_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L12_S1_TP1_GPU9_softmax -> L12_S1_TP1_GPU9_attn_v
	L12_S1_TP1_GPU9_qkv_slice -> L12_S1_TP1_GPU9_attn_v
	L12_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP1_GPU9_mha_ar
	L12_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP1_GPU9_mha_ar -> L12_S1_TP1_GPU9_mha_res
	L12_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L12_S1_TP1_GPU9_mha_res -> L12_S1_TP1_GPU9_gate
	L12_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L12_S1_TP1_GPU9_gate -> L12_S1_TP1_GPU9_expert0 [style=dashed]
	L12_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L12_S1_TP1_GPU9_gate -> L12_S1_TP1_GPU9_expert1 [style=dashed]
	L12_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP1_GPU9_moe_ar
	L12_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP1_GPU9_moe_ar -> L12_S1_TP1_GPU9_moe_res
	L12_S1_TP1_GPU9_mha_res -> L12_S1_TP1_GPU9_moe_res
	L12_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP1_GPU9_moe_res -> L12_S1_TP1_GPU9_norm
	L12_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L12_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L12_S1_TP2_GPU10_qkv_linear -> L12_S1_TP2_GPU10_qkv_slice
	L12_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L12_S1_TP2_GPU10_qkv_slice -> L12_S1_TP2_GPU10_qk_matmul
	L12_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L12_S1_TP2_GPU10_qk_matmul -> L12_S1_TP2_GPU10_softmax
	L12_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L12_S1_TP2_GPU10_softmax -> L12_S1_TP2_GPU10_attn_v
	L12_S1_TP2_GPU10_qkv_slice -> L12_S1_TP2_GPU10_attn_v
	L12_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP2_GPU10_mha_ar
	L12_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP2_GPU10_mha_ar -> L12_S1_TP2_GPU10_mha_res
	L12_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L12_S1_TP2_GPU10_mha_res -> L12_S1_TP2_GPU10_gate
	L12_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L12_S1_TP2_GPU10_gate -> L12_S1_TP2_GPU10_expert0 [style=dashed]
	L12_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L12_S1_TP2_GPU10_gate -> L12_S1_TP2_GPU10_expert1 [style=dashed]
	L12_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP2_GPU10_moe_ar
	L12_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP2_GPU10_moe_ar -> L12_S1_TP2_GPU10_moe_res
	L12_S1_TP2_GPU10_mha_res -> L12_S1_TP2_GPU10_moe_res
	L12_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP2_GPU10_moe_res -> L12_S1_TP2_GPU10_norm
	L12_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L12_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L12_S1_TP3_GPU11_qkv_linear -> L12_S1_TP3_GPU11_qkv_slice
	L12_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L12_S1_TP3_GPU11_qkv_slice -> L12_S1_TP3_GPU11_qk_matmul
	L12_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L12_S1_TP3_GPU11_qk_matmul -> L12_S1_TP3_GPU11_softmax
	L12_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L12_S1_TP3_GPU11_softmax -> L12_S1_TP3_GPU11_attn_v
	L12_S1_TP3_GPU11_qkv_slice -> L12_S1_TP3_GPU11_attn_v
	L12_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP3_GPU11_mha_ar
	L12_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP3_GPU11_mha_ar -> L12_S1_TP3_GPU11_mha_res
	L12_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L12_S1_TP3_GPU11_mha_res -> L12_S1_TP3_GPU11_gate
	L12_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L12_S1_TP3_GPU11_gate -> L12_S1_TP3_GPU11_expert0 [style=dashed]
	L12_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L12_S1_TP3_GPU11_gate -> L12_S1_TP3_GPU11_expert1 [style=dashed]
	L12_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP3_GPU11_moe_ar
	L12_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP3_GPU11_moe_ar -> L12_S1_TP3_GPU11_moe_res
	L12_S1_TP3_GPU11_mha_res -> L12_S1_TP3_GPU11_moe_res
	L12_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP3_GPU11_moe_res -> L12_S1_TP3_GPU11_norm
	L12_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L12_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L12_S1_TP4_GPU12_qkv_linear -> L12_S1_TP4_GPU12_qkv_slice
	L12_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L12_S1_TP4_GPU12_qkv_slice -> L12_S1_TP4_GPU12_qk_matmul
	L12_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L12_S1_TP4_GPU12_qk_matmul -> L12_S1_TP4_GPU12_softmax
	L12_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L12_S1_TP4_GPU12_softmax -> L12_S1_TP4_GPU12_attn_v
	L12_S1_TP4_GPU12_qkv_slice -> L12_S1_TP4_GPU12_attn_v
	L12_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP4_GPU12_mha_ar
	L12_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP4_GPU12_mha_ar -> L12_S1_TP4_GPU12_mha_res
	L12_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L12_S1_TP4_GPU12_mha_res -> L12_S1_TP4_GPU12_gate
	L12_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L12_S1_TP4_GPU12_gate -> L12_S1_TP4_GPU12_expert0 [style=dashed]
	L12_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L12_S1_TP4_GPU12_gate -> L12_S1_TP4_GPU12_expert1 [style=dashed]
	L12_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP4_GPU12_moe_ar
	L12_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP4_GPU12_moe_ar -> L12_S1_TP4_GPU12_moe_res
	L12_S1_TP4_GPU12_mha_res -> L12_S1_TP4_GPU12_moe_res
	L12_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP4_GPU12_moe_res -> L12_S1_TP4_GPU12_norm
	L12_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L12_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L12_S1_TP5_GPU13_qkv_linear -> L12_S1_TP5_GPU13_qkv_slice
	L12_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L12_S1_TP5_GPU13_qkv_slice -> L12_S1_TP5_GPU13_qk_matmul
	L12_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L12_S1_TP5_GPU13_qk_matmul -> L12_S1_TP5_GPU13_softmax
	L12_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L12_S1_TP5_GPU13_softmax -> L12_S1_TP5_GPU13_attn_v
	L12_S1_TP5_GPU13_qkv_slice -> L12_S1_TP5_GPU13_attn_v
	L12_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP5_GPU13_mha_ar
	L12_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP5_GPU13_mha_ar -> L12_S1_TP5_GPU13_mha_res
	L12_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L12_S1_TP5_GPU13_mha_res -> L12_S1_TP5_GPU13_gate
	L12_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L12_S1_TP5_GPU13_gate -> L12_S1_TP5_GPU13_expert0 [style=dashed]
	L12_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L12_S1_TP5_GPU13_gate -> L12_S1_TP5_GPU13_expert1 [style=dashed]
	L12_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP5_GPU13_moe_ar
	L12_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP5_GPU13_moe_ar -> L12_S1_TP5_GPU13_moe_res
	L12_S1_TP5_GPU13_mha_res -> L12_S1_TP5_GPU13_moe_res
	L12_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP5_GPU13_moe_res -> L12_S1_TP5_GPU13_norm
	L12_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L12_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L12_S1_TP6_GPU14_qkv_linear -> L12_S1_TP6_GPU14_qkv_slice
	L12_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L12_S1_TP6_GPU14_qkv_slice -> L12_S1_TP6_GPU14_qk_matmul
	L12_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L12_S1_TP6_GPU14_qk_matmul -> L12_S1_TP6_GPU14_softmax
	L12_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L12_S1_TP6_GPU14_softmax -> L12_S1_TP6_GPU14_attn_v
	L12_S1_TP6_GPU14_qkv_slice -> L12_S1_TP6_GPU14_attn_v
	L12_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP6_GPU14_mha_ar
	L12_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP6_GPU14_mha_ar -> L12_S1_TP6_GPU14_mha_res
	L12_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L12_S1_TP6_GPU14_mha_res -> L12_S1_TP6_GPU14_gate
	L12_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L12_S1_TP6_GPU14_gate -> L12_S1_TP6_GPU14_expert0 [style=dashed]
	L12_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L12_S1_TP6_GPU14_gate -> L12_S1_TP6_GPU14_expert1 [style=dashed]
	L12_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP6_GPU14_moe_ar
	L12_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP6_GPU14_moe_ar -> L12_S1_TP6_GPU14_moe_res
	L12_S1_TP6_GPU14_mha_res -> L12_S1_TP6_GPU14_moe_res
	L12_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP6_GPU14_moe_res -> L12_S1_TP6_GPU14_norm
	L12_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L12_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L12_S1_TP7_GPU15_qkv_linear -> L12_S1_TP7_GPU15_qkv_slice
	L12_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L12_S1_TP7_GPU15_qkv_slice -> L12_S1_TP7_GPU15_qk_matmul
	L12_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L12_S1_TP7_GPU15_qk_matmul -> L12_S1_TP7_GPU15_softmax
	L12_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L12_S1_TP7_GPU15_softmax -> L12_S1_TP7_GPU15_attn_v
	L12_S1_TP7_GPU15_qkv_slice -> L12_S1_TP7_GPU15_attn_v
	L12_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP1_GPU9_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP2_GPU10_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP3_GPU11_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP4_GPU12_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP5_GPU13_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP6_GPU14_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP7_GPU15_attn_v -> L12_S1_TP7_GPU15_mha_ar
	L12_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP7_GPU15_mha_ar -> L12_S1_TP7_GPU15_mha_res
	L12_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L12_S1_TP7_GPU15_mha_res -> L12_S1_TP7_GPU15_gate
	L12_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L12_S1_TP7_GPU15_gate -> L12_S1_TP7_GPU15_expert0 [style=dashed]
	L12_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L12_S1_TP7_GPU15_gate -> L12_S1_TP7_GPU15_expert1 [style=dashed]
	L12_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L12_S1_TP0_GPU8_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP0_GPU8_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP1_GPU9_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP1_GPU9_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP2_GPU10_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP2_GPU10_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP3_GPU11_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP3_GPU11_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP4_GPU12_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP4_GPU12_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP5_GPU13_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP5_GPU13_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP6_GPU14_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP6_GPU14_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP7_GPU15_expert0 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP7_GPU15_expert1 -> L12_S1_TP7_GPU15_moe_ar
	L12_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP7_GPU15_moe_ar -> L12_S1_TP7_GPU15_moe_res
	L12_S1_TP7_GPU15_mha_res -> L12_S1_TP7_GPU15_moe_res
	L12_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L12_S1_TP7_GPU15_moe_res -> L12_S1_TP7_GPU15_norm
	L13_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L13_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L13_S1_TP0_GPU8_qkv_linear -> L13_S1_TP0_GPU8_qkv_slice
	L13_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L13_S1_TP0_GPU8_qkv_slice -> L13_S1_TP0_GPU8_qk_matmul
	L13_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L13_S1_TP0_GPU8_qk_matmul -> L13_S1_TP0_GPU8_softmax
	L13_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L13_S1_TP0_GPU8_softmax -> L13_S1_TP0_GPU8_attn_v
	L13_S1_TP0_GPU8_qkv_slice -> L13_S1_TP0_GPU8_attn_v
	L13_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP0_GPU8_mha_ar
	L13_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP0_GPU8_mha_ar -> L13_S1_TP0_GPU8_mha_res
	L13_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L13_S1_TP0_GPU8_mha_res -> L13_S1_TP0_GPU8_gate
	L13_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L13_S1_TP0_GPU8_gate -> L13_S1_TP0_GPU8_expert0 [style=dashed]
	L13_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L13_S1_TP0_GPU8_gate -> L13_S1_TP0_GPU8_expert1 [style=dashed]
	L13_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP0_GPU8_moe_ar
	L13_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP0_GPU8_moe_ar -> L13_S1_TP0_GPU8_moe_res
	L13_S1_TP0_GPU8_mha_res -> L13_S1_TP0_GPU8_moe_res
	L13_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP0_GPU8_moe_res -> L13_S1_TP0_GPU8_norm
	L13_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L13_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L13_S1_TP1_GPU9_qkv_linear -> L13_S1_TP1_GPU9_qkv_slice
	L13_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L13_S1_TP1_GPU9_qkv_slice -> L13_S1_TP1_GPU9_qk_matmul
	L13_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L13_S1_TP1_GPU9_qk_matmul -> L13_S1_TP1_GPU9_softmax
	L13_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L13_S1_TP1_GPU9_softmax -> L13_S1_TP1_GPU9_attn_v
	L13_S1_TP1_GPU9_qkv_slice -> L13_S1_TP1_GPU9_attn_v
	L13_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP1_GPU9_mha_ar
	L13_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP1_GPU9_mha_ar -> L13_S1_TP1_GPU9_mha_res
	L13_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L13_S1_TP1_GPU9_mha_res -> L13_S1_TP1_GPU9_gate
	L13_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L13_S1_TP1_GPU9_gate -> L13_S1_TP1_GPU9_expert0 [style=dashed]
	L13_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L13_S1_TP1_GPU9_gate -> L13_S1_TP1_GPU9_expert1 [style=dashed]
	L13_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP1_GPU9_moe_ar
	L13_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP1_GPU9_moe_ar -> L13_S1_TP1_GPU9_moe_res
	L13_S1_TP1_GPU9_mha_res -> L13_S1_TP1_GPU9_moe_res
	L13_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP1_GPU9_moe_res -> L13_S1_TP1_GPU9_norm
	L13_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L13_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L13_S1_TP2_GPU10_qkv_linear -> L13_S1_TP2_GPU10_qkv_slice
	L13_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L13_S1_TP2_GPU10_qkv_slice -> L13_S1_TP2_GPU10_qk_matmul
	L13_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L13_S1_TP2_GPU10_qk_matmul -> L13_S1_TP2_GPU10_softmax
	L13_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L13_S1_TP2_GPU10_softmax -> L13_S1_TP2_GPU10_attn_v
	L13_S1_TP2_GPU10_qkv_slice -> L13_S1_TP2_GPU10_attn_v
	L13_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP2_GPU10_mha_ar
	L13_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP2_GPU10_mha_ar -> L13_S1_TP2_GPU10_mha_res
	L13_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L13_S1_TP2_GPU10_mha_res -> L13_S1_TP2_GPU10_gate
	L13_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L13_S1_TP2_GPU10_gate -> L13_S1_TP2_GPU10_expert0 [style=dashed]
	L13_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L13_S1_TP2_GPU10_gate -> L13_S1_TP2_GPU10_expert1 [style=dashed]
	L13_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP2_GPU10_moe_ar
	L13_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP2_GPU10_moe_ar -> L13_S1_TP2_GPU10_moe_res
	L13_S1_TP2_GPU10_mha_res -> L13_S1_TP2_GPU10_moe_res
	L13_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP2_GPU10_moe_res -> L13_S1_TP2_GPU10_norm
	L13_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L13_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L13_S1_TP3_GPU11_qkv_linear -> L13_S1_TP3_GPU11_qkv_slice
	L13_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L13_S1_TP3_GPU11_qkv_slice -> L13_S1_TP3_GPU11_qk_matmul
	L13_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L13_S1_TP3_GPU11_qk_matmul -> L13_S1_TP3_GPU11_softmax
	L13_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L13_S1_TP3_GPU11_softmax -> L13_S1_TP3_GPU11_attn_v
	L13_S1_TP3_GPU11_qkv_slice -> L13_S1_TP3_GPU11_attn_v
	L13_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP3_GPU11_mha_ar
	L13_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP3_GPU11_mha_ar -> L13_S1_TP3_GPU11_mha_res
	L13_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L13_S1_TP3_GPU11_mha_res -> L13_S1_TP3_GPU11_gate
	L13_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L13_S1_TP3_GPU11_gate -> L13_S1_TP3_GPU11_expert0 [style=dashed]
	L13_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L13_S1_TP3_GPU11_gate -> L13_S1_TP3_GPU11_expert1 [style=dashed]
	L13_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP3_GPU11_moe_ar
	L13_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP3_GPU11_moe_ar -> L13_S1_TP3_GPU11_moe_res
	L13_S1_TP3_GPU11_mha_res -> L13_S1_TP3_GPU11_moe_res
	L13_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP3_GPU11_moe_res -> L13_S1_TP3_GPU11_norm
	L13_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L13_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L13_S1_TP4_GPU12_qkv_linear -> L13_S1_TP4_GPU12_qkv_slice
	L13_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L13_S1_TP4_GPU12_qkv_slice -> L13_S1_TP4_GPU12_qk_matmul
	L13_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L13_S1_TP4_GPU12_qk_matmul -> L13_S1_TP4_GPU12_softmax
	L13_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L13_S1_TP4_GPU12_softmax -> L13_S1_TP4_GPU12_attn_v
	L13_S1_TP4_GPU12_qkv_slice -> L13_S1_TP4_GPU12_attn_v
	L13_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP4_GPU12_mha_ar
	L13_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP4_GPU12_mha_ar -> L13_S1_TP4_GPU12_mha_res
	L13_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L13_S1_TP4_GPU12_mha_res -> L13_S1_TP4_GPU12_gate
	L13_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L13_S1_TP4_GPU12_gate -> L13_S1_TP4_GPU12_expert0 [style=dashed]
	L13_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L13_S1_TP4_GPU12_gate -> L13_S1_TP4_GPU12_expert1 [style=dashed]
	L13_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP4_GPU12_moe_ar
	L13_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP4_GPU12_moe_ar -> L13_S1_TP4_GPU12_moe_res
	L13_S1_TP4_GPU12_mha_res -> L13_S1_TP4_GPU12_moe_res
	L13_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP4_GPU12_moe_res -> L13_S1_TP4_GPU12_norm
	L13_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L13_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L13_S1_TP5_GPU13_qkv_linear -> L13_S1_TP5_GPU13_qkv_slice
	L13_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L13_S1_TP5_GPU13_qkv_slice -> L13_S1_TP5_GPU13_qk_matmul
	L13_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L13_S1_TP5_GPU13_qk_matmul -> L13_S1_TP5_GPU13_softmax
	L13_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L13_S1_TP5_GPU13_softmax -> L13_S1_TP5_GPU13_attn_v
	L13_S1_TP5_GPU13_qkv_slice -> L13_S1_TP5_GPU13_attn_v
	L13_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP5_GPU13_mha_ar
	L13_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP5_GPU13_mha_ar -> L13_S1_TP5_GPU13_mha_res
	L13_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L13_S1_TP5_GPU13_mha_res -> L13_S1_TP5_GPU13_gate
	L13_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L13_S1_TP5_GPU13_gate -> L13_S1_TP5_GPU13_expert0 [style=dashed]
	L13_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L13_S1_TP5_GPU13_gate -> L13_S1_TP5_GPU13_expert1 [style=dashed]
	L13_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP5_GPU13_moe_ar
	L13_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP5_GPU13_moe_ar -> L13_S1_TP5_GPU13_moe_res
	L13_S1_TP5_GPU13_mha_res -> L13_S1_TP5_GPU13_moe_res
	L13_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP5_GPU13_moe_res -> L13_S1_TP5_GPU13_norm
	L13_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L13_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L13_S1_TP6_GPU14_qkv_linear -> L13_S1_TP6_GPU14_qkv_slice
	L13_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L13_S1_TP6_GPU14_qkv_slice -> L13_S1_TP6_GPU14_qk_matmul
	L13_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L13_S1_TP6_GPU14_qk_matmul -> L13_S1_TP6_GPU14_softmax
	L13_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L13_S1_TP6_GPU14_softmax -> L13_S1_TP6_GPU14_attn_v
	L13_S1_TP6_GPU14_qkv_slice -> L13_S1_TP6_GPU14_attn_v
	L13_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP6_GPU14_mha_ar
	L13_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP6_GPU14_mha_ar -> L13_S1_TP6_GPU14_mha_res
	L13_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L13_S1_TP6_GPU14_mha_res -> L13_S1_TP6_GPU14_gate
	L13_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L13_S1_TP6_GPU14_gate -> L13_S1_TP6_GPU14_expert0 [style=dashed]
	L13_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L13_S1_TP6_GPU14_gate -> L13_S1_TP6_GPU14_expert1 [style=dashed]
	L13_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP6_GPU14_moe_ar
	L13_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP6_GPU14_moe_ar -> L13_S1_TP6_GPU14_moe_res
	L13_S1_TP6_GPU14_mha_res -> L13_S1_TP6_GPU14_moe_res
	L13_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP6_GPU14_moe_res -> L13_S1_TP6_GPU14_norm
	L13_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L13_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L13_S1_TP7_GPU15_qkv_linear -> L13_S1_TP7_GPU15_qkv_slice
	L13_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L13_S1_TP7_GPU15_qkv_slice -> L13_S1_TP7_GPU15_qk_matmul
	L13_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L13_S1_TP7_GPU15_qk_matmul -> L13_S1_TP7_GPU15_softmax
	L13_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L13_S1_TP7_GPU15_softmax -> L13_S1_TP7_GPU15_attn_v
	L13_S1_TP7_GPU15_qkv_slice -> L13_S1_TP7_GPU15_attn_v
	L13_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP1_GPU9_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP2_GPU10_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP3_GPU11_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP4_GPU12_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP5_GPU13_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP6_GPU14_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP7_GPU15_attn_v -> L13_S1_TP7_GPU15_mha_ar
	L13_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP7_GPU15_mha_ar -> L13_S1_TP7_GPU15_mha_res
	L13_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L13_S1_TP7_GPU15_mha_res -> L13_S1_TP7_GPU15_gate
	L13_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L13_S1_TP7_GPU15_gate -> L13_S1_TP7_GPU15_expert0 [style=dashed]
	L13_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L13_S1_TP7_GPU15_gate -> L13_S1_TP7_GPU15_expert1 [style=dashed]
	L13_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L13_S1_TP0_GPU8_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP0_GPU8_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP1_GPU9_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP1_GPU9_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP2_GPU10_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP2_GPU10_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP3_GPU11_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP3_GPU11_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP4_GPU12_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP4_GPU12_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP5_GPU13_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP5_GPU13_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP6_GPU14_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP6_GPU14_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP7_GPU15_expert0 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP7_GPU15_expert1 -> L13_S1_TP7_GPU15_moe_ar
	L13_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP7_GPU15_moe_ar -> L13_S1_TP7_GPU15_moe_res
	L13_S1_TP7_GPU15_mha_res -> L13_S1_TP7_GPU15_moe_res
	L13_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L13_S1_TP7_GPU15_moe_res -> L13_S1_TP7_GPU15_norm
	L14_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L14_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L14_S1_TP0_GPU8_qkv_linear -> L14_S1_TP0_GPU8_qkv_slice
	L14_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L14_S1_TP0_GPU8_qkv_slice -> L14_S1_TP0_GPU8_qk_matmul
	L14_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L14_S1_TP0_GPU8_qk_matmul -> L14_S1_TP0_GPU8_softmax
	L14_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L14_S1_TP0_GPU8_softmax -> L14_S1_TP0_GPU8_attn_v
	L14_S1_TP0_GPU8_qkv_slice -> L14_S1_TP0_GPU8_attn_v
	L14_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP0_GPU8_mha_ar
	L14_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP0_GPU8_mha_ar -> L14_S1_TP0_GPU8_mha_res
	L14_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L14_S1_TP0_GPU8_mha_res -> L14_S1_TP0_GPU8_gate
	L14_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L14_S1_TP0_GPU8_gate -> L14_S1_TP0_GPU8_expert0 [style=dashed]
	L14_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L14_S1_TP0_GPU8_gate -> L14_S1_TP0_GPU8_expert1 [style=dashed]
	L14_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP0_GPU8_moe_ar
	L14_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP0_GPU8_moe_ar -> L14_S1_TP0_GPU8_moe_res
	L14_S1_TP0_GPU8_mha_res -> L14_S1_TP0_GPU8_moe_res
	L14_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP0_GPU8_moe_res -> L14_S1_TP0_GPU8_norm
	L14_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L14_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L14_S1_TP1_GPU9_qkv_linear -> L14_S1_TP1_GPU9_qkv_slice
	L14_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L14_S1_TP1_GPU9_qkv_slice -> L14_S1_TP1_GPU9_qk_matmul
	L14_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L14_S1_TP1_GPU9_qk_matmul -> L14_S1_TP1_GPU9_softmax
	L14_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L14_S1_TP1_GPU9_softmax -> L14_S1_TP1_GPU9_attn_v
	L14_S1_TP1_GPU9_qkv_slice -> L14_S1_TP1_GPU9_attn_v
	L14_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP1_GPU9_mha_ar
	L14_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP1_GPU9_mha_ar -> L14_S1_TP1_GPU9_mha_res
	L14_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L14_S1_TP1_GPU9_mha_res -> L14_S1_TP1_GPU9_gate
	L14_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L14_S1_TP1_GPU9_gate -> L14_S1_TP1_GPU9_expert0 [style=dashed]
	L14_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L14_S1_TP1_GPU9_gate -> L14_S1_TP1_GPU9_expert1 [style=dashed]
	L14_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP1_GPU9_moe_ar
	L14_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP1_GPU9_moe_ar -> L14_S1_TP1_GPU9_moe_res
	L14_S1_TP1_GPU9_mha_res -> L14_S1_TP1_GPU9_moe_res
	L14_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP1_GPU9_moe_res -> L14_S1_TP1_GPU9_norm
	L14_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L14_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L14_S1_TP2_GPU10_qkv_linear -> L14_S1_TP2_GPU10_qkv_slice
	L14_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L14_S1_TP2_GPU10_qkv_slice -> L14_S1_TP2_GPU10_qk_matmul
	L14_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L14_S1_TP2_GPU10_qk_matmul -> L14_S1_TP2_GPU10_softmax
	L14_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L14_S1_TP2_GPU10_softmax -> L14_S1_TP2_GPU10_attn_v
	L14_S1_TP2_GPU10_qkv_slice -> L14_S1_TP2_GPU10_attn_v
	L14_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP2_GPU10_mha_ar
	L14_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP2_GPU10_mha_ar -> L14_S1_TP2_GPU10_mha_res
	L14_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L14_S1_TP2_GPU10_mha_res -> L14_S1_TP2_GPU10_gate
	L14_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L14_S1_TP2_GPU10_gate -> L14_S1_TP2_GPU10_expert0 [style=dashed]
	L14_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L14_S1_TP2_GPU10_gate -> L14_S1_TP2_GPU10_expert1 [style=dashed]
	L14_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP2_GPU10_moe_ar
	L14_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP2_GPU10_moe_ar -> L14_S1_TP2_GPU10_moe_res
	L14_S1_TP2_GPU10_mha_res -> L14_S1_TP2_GPU10_moe_res
	L14_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP2_GPU10_moe_res -> L14_S1_TP2_GPU10_norm
	L14_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L14_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L14_S1_TP3_GPU11_qkv_linear -> L14_S1_TP3_GPU11_qkv_slice
	L14_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L14_S1_TP3_GPU11_qkv_slice -> L14_S1_TP3_GPU11_qk_matmul
	L14_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L14_S1_TP3_GPU11_qk_matmul -> L14_S1_TP3_GPU11_softmax
	L14_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L14_S1_TP3_GPU11_softmax -> L14_S1_TP3_GPU11_attn_v
	L14_S1_TP3_GPU11_qkv_slice -> L14_S1_TP3_GPU11_attn_v
	L14_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP3_GPU11_mha_ar
	L14_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP3_GPU11_mha_ar -> L14_S1_TP3_GPU11_mha_res
	L14_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L14_S1_TP3_GPU11_mha_res -> L14_S1_TP3_GPU11_gate
	L14_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L14_S1_TP3_GPU11_gate -> L14_S1_TP3_GPU11_expert0 [style=dashed]
	L14_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L14_S1_TP3_GPU11_gate -> L14_S1_TP3_GPU11_expert1 [style=dashed]
	L14_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP3_GPU11_moe_ar
	L14_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP3_GPU11_moe_ar -> L14_S1_TP3_GPU11_moe_res
	L14_S1_TP3_GPU11_mha_res -> L14_S1_TP3_GPU11_moe_res
	L14_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP3_GPU11_moe_res -> L14_S1_TP3_GPU11_norm
	L14_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L14_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L14_S1_TP4_GPU12_qkv_linear -> L14_S1_TP4_GPU12_qkv_slice
	L14_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L14_S1_TP4_GPU12_qkv_slice -> L14_S1_TP4_GPU12_qk_matmul
	L14_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L14_S1_TP4_GPU12_qk_matmul -> L14_S1_TP4_GPU12_softmax
	L14_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L14_S1_TP4_GPU12_softmax -> L14_S1_TP4_GPU12_attn_v
	L14_S1_TP4_GPU12_qkv_slice -> L14_S1_TP4_GPU12_attn_v
	L14_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP4_GPU12_mha_ar
	L14_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP4_GPU12_mha_ar -> L14_S1_TP4_GPU12_mha_res
	L14_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L14_S1_TP4_GPU12_mha_res -> L14_S1_TP4_GPU12_gate
	L14_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L14_S1_TP4_GPU12_gate -> L14_S1_TP4_GPU12_expert0 [style=dashed]
	L14_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L14_S1_TP4_GPU12_gate -> L14_S1_TP4_GPU12_expert1 [style=dashed]
	L14_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP4_GPU12_moe_ar
	L14_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP4_GPU12_moe_ar -> L14_S1_TP4_GPU12_moe_res
	L14_S1_TP4_GPU12_mha_res -> L14_S1_TP4_GPU12_moe_res
	L14_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP4_GPU12_moe_res -> L14_S1_TP4_GPU12_norm
	L14_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L14_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L14_S1_TP5_GPU13_qkv_linear -> L14_S1_TP5_GPU13_qkv_slice
	L14_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L14_S1_TP5_GPU13_qkv_slice -> L14_S1_TP5_GPU13_qk_matmul
	L14_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L14_S1_TP5_GPU13_qk_matmul -> L14_S1_TP5_GPU13_softmax
	L14_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L14_S1_TP5_GPU13_softmax -> L14_S1_TP5_GPU13_attn_v
	L14_S1_TP5_GPU13_qkv_slice -> L14_S1_TP5_GPU13_attn_v
	L14_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP5_GPU13_mha_ar
	L14_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP5_GPU13_mha_ar -> L14_S1_TP5_GPU13_mha_res
	L14_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L14_S1_TP5_GPU13_mha_res -> L14_S1_TP5_GPU13_gate
	L14_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L14_S1_TP5_GPU13_gate -> L14_S1_TP5_GPU13_expert0 [style=dashed]
	L14_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L14_S1_TP5_GPU13_gate -> L14_S1_TP5_GPU13_expert1 [style=dashed]
	L14_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP5_GPU13_moe_ar
	L14_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP5_GPU13_moe_ar -> L14_S1_TP5_GPU13_moe_res
	L14_S1_TP5_GPU13_mha_res -> L14_S1_TP5_GPU13_moe_res
	L14_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP5_GPU13_moe_res -> L14_S1_TP5_GPU13_norm
	L14_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L14_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L14_S1_TP6_GPU14_qkv_linear -> L14_S1_TP6_GPU14_qkv_slice
	L14_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L14_S1_TP6_GPU14_qkv_slice -> L14_S1_TP6_GPU14_qk_matmul
	L14_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L14_S1_TP6_GPU14_qk_matmul -> L14_S1_TP6_GPU14_softmax
	L14_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L14_S1_TP6_GPU14_softmax -> L14_S1_TP6_GPU14_attn_v
	L14_S1_TP6_GPU14_qkv_slice -> L14_S1_TP6_GPU14_attn_v
	L14_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP6_GPU14_mha_ar
	L14_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP6_GPU14_mha_ar -> L14_S1_TP6_GPU14_mha_res
	L14_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L14_S1_TP6_GPU14_mha_res -> L14_S1_TP6_GPU14_gate
	L14_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L14_S1_TP6_GPU14_gate -> L14_S1_TP6_GPU14_expert0 [style=dashed]
	L14_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L14_S1_TP6_GPU14_gate -> L14_S1_TP6_GPU14_expert1 [style=dashed]
	L14_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP6_GPU14_moe_ar
	L14_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP6_GPU14_moe_ar -> L14_S1_TP6_GPU14_moe_res
	L14_S1_TP6_GPU14_mha_res -> L14_S1_TP6_GPU14_moe_res
	L14_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP6_GPU14_moe_res -> L14_S1_TP6_GPU14_norm
	L14_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L14_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L14_S1_TP7_GPU15_qkv_linear -> L14_S1_TP7_GPU15_qkv_slice
	L14_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L14_S1_TP7_GPU15_qkv_slice -> L14_S1_TP7_GPU15_qk_matmul
	L14_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L14_S1_TP7_GPU15_qk_matmul -> L14_S1_TP7_GPU15_softmax
	L14_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L14_S1_TP7_GPU15_softmax -> L14_S1_TP7_GPU15_attn_v
	L14_S1_TP7_GPU15_qkv_slice -> L14_S1_TP7_GPU15_attn_v
	L14_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP1_GPU9_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP2_GPU10_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP3_GPU11_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP4_GPU12_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP5_GPU13_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP6_GPU14_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP7_GPU15_attn_v -> L14_S1_TP7_GPU15_mha_ar
	L14_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP7_GPU15_mha_ar -> L14_S1_TP7_GPU15_mha_res
	L14_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L14_S1_TP7_GPU15_mha_res -> L14_S1_TP7_GPU15_gate
	L14_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L14_S1_TP7_GPU15_gate -> L14_S1_TP7_GPU15_expert0 [style=dashed]
	L14_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L14_S1_TP7_GPU15_gate -> L14_S1_TP7_GPU15_expert1 [style=dashed]
	L14_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L14_S1_TP0_GPU8_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP0_GPU8_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP1_GPU9_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP1_GPU9_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP2_GPU10_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP2_GPU10_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP3_GPU11_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP3_GPU11_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP4_GPU12_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP4_GPU12_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP5_GPU13_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP5_GPU13_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP6_GPU14_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP6_GPU14_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP7_GPU15_expert0 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP7_GPU15_expert1 -> L14_S1_TP7_GPU15_moe_ar
	L14_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP7_GPU15_moe_ar -> L14_S1_TP7_GPU15_moe_res
	L14_S1_TP7_GPU15_mha_res -> L14_S1_TP7_GPU15_moe_res
	L14_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L14_S1_TP7_GPU15_moe_res -> L14_S1_TP7_GPU15_norm
	L15_S1_TP0_GPU8_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 8" shape=rectangle]
	L15_S1_TP0_GPU8_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L15_S1_TP0_GPU8_qkv_linear -> L15_S1_TP0_GPU8_qkv_slice
	L15_S1_TP0_GPU8_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L15_S1_TP0_GPU8_qkv_slice -> L15_S1_TP0_GPU8_qk_matmul
	L15_S1_TP0_GPU8_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 8" shape=rectangle]
	L15_S1_TP0_GPU8_qk_matmul -> L15_S1_TP0_GPU8_softmax
	L15_S1_TP0_GPU8_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 8" shape=rectangle]
	L15_S1_TP0_GPU8_softmax -> L15_S1_TP0_GPU8_attn_v
	L15_S1_TP0_GPU8_qkv_slice -> L15_S1_TP0_GPU8_attn_v
	L15_S1_TP0_GPU8_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP0_GPU8_mha_ar
	L15_S1_TP0_GPU8_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP0_GPU8_mha_ar -> L15_S1_TP0_GPU8_mha_res
	L15_S1_TP0_GPU8_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 8" shape=parallelogram]
	L15_S1_TP0_GPU8_mha_res -> L15_S1_TP0_GPU8_gate
	L15_S1_TP0_GPU8_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L15_S1_TP0_GPU8_gate -> L15_S1_TP0_GPU8_expert0 [style=dashed]
	L15_S1_TP0_GPU8_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 8" shape=rectangle]
	L15_S1_TP0_GPU8_gate -> L15_S1_TP0_GPU8_expert1 [style=dashed]
	L15_S1_TP0_GPU8_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP0_GPU8_moe_ar
	L15_S1_TP0_GPU8_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP0_GPU8_moe_ar -> L15_S1_TP0_GPU8_moe_res
	L15_S1_TP0_GPU8_mha_res -> L15_S1_TP0_GPU8_moe_res
	L15_S1_TP0_GPU8_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP0_GPU8_moe_res -> L15_S1_TP0_GPU8_norm
	L15_S1_TP1_GPU9_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 9" shape=rectangle]
	L15_S1_TP1_GPU9_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L15_S1_TP1_GPU9_qkv_linear -> L15_S1_TP1_GPU9_qkv_slice
	L15_S1_TP1_GPU9_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L15_S1_TP1_GPU9_qkv_slice -> L15_S1_TP1_GPU9_qk_matmul
	L15_S1_TP1_GPU9_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 9" shape=rectangle]
	L15_S1_TP1_GPU9_qk_matmul -> L15_S1_TP1_GPU9_softmax
	L15_S1_TP1_GPU9_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 9" shape=rectangle]
	L15_S1_TP1_GPU9_softmax -> L15_S1_TP1_GPU9_attn_v
	L15_S1_TP1_GPU9_qkv_slice -> L15_S1_TP1_GPU9_attn_v
	L15_S1_TP1_GPU9_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP1_GPU9_mha_ar
	L15_S1_TP1_GPU9_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP1_GPU9_mha_ar -> L15_S1_TP1_GPU9_mha_res
	L15_S1_TP1_GPU9_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 9" shape=parallelogram]
	L15_S1_TP1_GPU9_mha_res -> L15_S1_TP1_GPU9_gate
	L15_S1_TP1_GPU9_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L15_S1_TP1_GPU9_gate -> L15_S1_TP1_GPU9_expert0 [style=dashed]
	L15_S1_TP1_GPU9_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 9" shape=rectangle]
	L15_S1_TP1_GPU9_gate -> L15_S1_TP1_GPU9_expert1 [style=dashed]
	L15_S1_TP1_GPU9_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP1_GPU9_moe_ar
	L15_S1_TP1_GPU9_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP1_GPU9_moe_ar -> L15_S1_TP1_GPU9_moe_res
	L15_S1_TP1_GPU9_mha_res -> L15_S1_TP1_GPU9_moe_res
	L15_S1_TP1_GPU9_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP1_GPU9_moe_res -> L15_S1_TP1_GPU9_norm
	L15_S1_TP2_GPU10_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 10" shape=rectangle]
	L15_S1_TP2_GPU10_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L15_S1_TP2_GPU10_qkv_linear -> L15_S1_TP2_GPU10_qkv_slice
	L15_S1_TP2_GPU10_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L15_S1_TP2_GPU10_qkv_slice -> L15_S1_TP2_GPU10_qk_matmul
	L15_S1_TP2_GPU10_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 10" shape=rectangle]
	L15_S1_TP2_GPU10_qk_matmul -> L15_S1_TP2_GPU10_softmax
	L15_S1_TP2_GPU10_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 10" shape=rectangle]
	L15_S1_TP2_GPU10_softmax -> L15_S1_TP2_GPU10_attn_v
	L15_S1_TP2_GPU10_qkv_slice -> L15_S1_TP2_GPU10_attn_v
	L15_S1_TP2_GPU10_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP2_GPU10_mha_ar
	L15_S1_TP2_GPU10_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP2_GPU10_mha_ar -> L15_S1_TP2_GPU10_mha_res
	L15_S1_TP2_GPU10_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 10" shape=parallelogram]
	L15_S1_TP2_GPU10_mha_res -> L15_S1_TP2_GPU10_gate
	L15_S1_TP2_GPU10_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L15_S1_TP2_GPU10_gate -> L15_S1_TP2_GPU10_expert0 [style=dashed]
	L15_S1_TP2_GPU10_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 10" shape=rectangle]
	L15_S1_TP2_GPU10_gate -> L15_S1_TP2_GPU10_expert1 [style=dashed]
	L15_S1_TP2_GPU10_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP2_GPU10_moe_ar
	L15_S1_TP2_GPU10_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP2_GPU10_moe_ar -> L15_S1_TP2_GPU10_moe_res
	L15_S1_TP2_GPU10_mha_res -> L15_S1_TP2_GPU10_moe_res
	L15_S1_TP2_GPU10_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP2_GPU10_moe_res -> L15_S1_TP2_GPU10_norm
	L15_S1_TP3_GPU11_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 11" shape=rectangle]
	L15_S1_TP3_GPU11_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L15_S1_TP3_GPU11_qkv_linear -> L15_S1_TP3_GPU11_qkv_slice
	L15_S1_TP3_GPU11_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L15_S1_TP3_GPU11_qkv_slice -> L15_S1_TP3_GPU11_qk_matmul
	L15_S1_TP3_GPU11_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 11" shape=rectangle]
	L15_S1_TP3_GPU11_qk_matmul -> L15_S1_TP3_GPU11_softmax
	L15_S1_TP3_GPU11_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 11" shape=rectangle]
	L15_S1_TP3_GPU11_softmax -> L15_S1_TP3_GPU11_attn_v
	L15_S1_TP3_GPU11_qkv_slice -> L15_S1_TP3_GPU11_attn_v
	L15_S1_TP3_GPU11_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP3_GPU11_mha_ar
	L15_S1_TP3_GPU11_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP3_GPU11_mha_ar -> L15_S1_TP3_GPU11_mha_res
	L15_S1_TP3_GPU11_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 11" shape=parallelogram]
	L15_S1_TP3_GPU11_mha_res -> L15_S1_TP3_GPU11_gate
	L15_S1_TP3_GPU11_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L15_S1_TP3_GPU11_gate -> L15_S1_TP3_GPU11_expert0 [style=dashed]
	L15_S1_TP3_GPU11_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 11" shape=rectangle]
	L15_S1_TP3_GPU11_gate -> L15_S1_TP3_GPU11_expert1 [style=dashed]
	L15_S1_TP3_GPU11_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP3_GPU11_moe_ar
	L15_S1_TP3_GPU11_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP3_GPU11_moe_ar -> L15_S1_TP3_GPU11_moe_res
	L15_S1_TP3_GPU11_mha_res -> L15_S1_TP3_GPU11_moe_res
	L15_S1_TP3_GPU11_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP3_GPU11_moe_res -> L15_S1_TP3_GPU11_norm
	L15_S1_TP4_GPU12_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 12" shape=rectangle]
	L15_S1_TP4_GPU12_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L15_S1_TP4_GPU12_qkv_linear -> L15_S1_TP4_GPU12_qkv_slice
	L15_S1_TP4_GPU12_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L15_S1_TP4_GPU12_qkv_slice -> L15_S1_TP4_GPU12_qk_matmul
	L15_S1_TP4_GPU12_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 12" shape=rectangle]
	L15_S1_TP4_GPU12_qk_matmul -> L15_S1_TP4_GPU12_softmax
	L15_S1_TP4_GPU12_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 12" shape=rectangle]
	L15_S1_TP4_GPU12_softmax -> L15_S1_TP4_GPU12_attn_v
	L15_S1_TP4_GPU12_qkv_slice -> L15_S1_TP4_GPU12_attn_v
	L15_S1_TP4_GPU12_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP4_GPU12_mha_ar
	L15_S1_TP4_GPU12_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP4_GPU12_mha_ar -> L15_S1_TP4_GPU12_mha_res
	L15_S1_TP4_GPU12_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 12" shape=parallelogram]
	L15_S1_TP4_GPU12_mha_res -> L15_S1_TP4_GPU12_gate
	L15_S1_TP4_GPU12_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L15_S1_TP4_GPU12_gate -> L15_S1_TP4_GPU12_expert0 [style=dashed]
	L15_S1_TP4_GPU12_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 12" shape=rectangle]
	L15_S1_TP4_GPU12_gate -> L15_S1_TP4_GPU12_expert1 [style=dashed]
	L15_S1_TP4_GPU12_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP4_GPU12_moe_ar
	L15_S1_TP4_GPU12_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP4_GPU12_moe_ar -> L15_S1_TP4_GPU12_moe_res
	L15_S1_TP4_GPU12_mha_res -> L15_S1_TP4_GPU12_moe_res
	L15_S1_TP4_GPU12_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP4_GPU12_moe_res -> L15_S1_TP4_GPU12_norm
	L15_S1_TP5_GPU13_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 13" shape=rectangle]
	L15_S1_TP5_GPU13_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L15_S1_TP5_GPU13_qkv_linear -> L15_S1_TP5_GPU13_qkv_slice
	L15_S1_TP5_GPU13_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L15_S1_TP5_GPU13_qkv_slice -> L15_S1_TP5_GPU13_qk_matmul
	L15_S1_TP5_GPU13_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 13" shape=rectangle]
	L15_S1_TP5_GPU13_qk_matmul -> L15_S1_TP5_GPU13_softmax
	L15_S1_TP5_GPU13_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 13" shape=rectangle]
	L15_S1_TP5_GPU13_softmax -> L15_S1_TP5_GPU13_attn_v
	L15_S1_TP5_GPU13_qkv_slice -> L15_S1_TP5_GPU13_attn_v
	L15_S1_TP5_GPU13_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP5_GPU13_mha_ar
	L15_S1_TP5_GPU13_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP5_GPU13_mha_ar -> L15_S1_TP5_GPU13_mha_res
	L15_S1_TP5_GPU13_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 13" shape=parallelogram]
	L15_S1_TP5_GPU13_mha_res -> L15_S1_TP5_GPU13_gate
	L15_S1_TP5_GPU13_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L15_S1_TP5_GPU13_gate -> L15_S1_TP5_GPU13_expert0 [style=dashed]
	L15_S1_TP5_GPU13_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 13" shape=rectangle]
	L15_S1_TP5_GPU13_gate -> L15_S1_TP5_GPU13_expert1 [style=dashed]
	L15_S1_TP5_GPU13_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP5_GPU13_moe_ar
	L15_S1_TP5_GPU13_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP5_GPU13_moe_ar -> L15_S1_TP5_GPU13_moe_res
	L15_S1_TP5_GPU13_mha_res -> L15_S1_TP5_GPU13_moe_res
	L15_S1_TP5_GPU13_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP5_GPU13_moe_res -> L15_S1_TP5_GPU13_norm
	L15_S1_TP6_GPU14_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 14" shape=rectangle]
	L15_S1_TP6_GPU14_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L15_S1_TP6_GPU14_qkv_linear -> L15_S1_TP6_GPU14_qkv_slice
	L15_S1_TP6_GPU14_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L15_S1_TP6_GPU14_qkv_slice -> L15_S1_TP6_GPU14_qk_matmul
	L15_S1_TP6_GPU14_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 14" shape=rectangle]
	L15_S1_TP6_GPU14_qk_matmul -> L15_S1_TP6_GPU14_softmax
	L15_S1_TP6_GPU14_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 14" shape=rectangle]
	L15_S1_TP6_GPU14_softmax -> L15_S1_TP6_GPU14_attn_v
	L15_S1_TP6_GPU14_qkv_slice -> L15_S1_TP6_GPU14_attn_v
	L15_S1_TP6_GPU14_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP6_GPU14_mha_ar
	L15_S1_TP6_GPU14_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP6_GPU14_mha_ar -> L15_S1_TP6_GPU14_mha_res
	L15_S1_TP6_GPU14_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 14" shape=parallelogram]
	L15_S1_TP6_GPU14_mha_res -> L15_S1_TP6_GPU14_gate
	L15_S1_TP6_GPU14_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L15_S1_TP6_GPU14_gate -> L15_S1_TP6_GPU14_expert0 [style=dashed]
	L15_S1_TP6_GPU14_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 14" shape=rectangle]
	L15_S1_TP6_GPU14_gate -> L15_S1_TP6_GPU14_expert1 [style=dashed]
	L15_S1_TP6_GPU14_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP6_GPU14_moe_ar
	L15_S1_TP6_GPU14_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP6_GPU14_moe_ar -> L15_S1_TP6_GPU14_moe_res
	L15_S1_TP6_GPU14_mha_res -> L15_S1_TP6_GPU14_moe_res
	L15_S1_TP6_GPU14_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP6_GPU14_moe_res -> L15_S1_TP6_GPU14_norm
	L15_S1_TP7_GPU15_qkv_linear [label="QKV Linear\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nGPU: 15" shape=rectangle]
	L15_S1_TP7_GPU15_qkv_slice [label="QKV Slice (TP)\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128, tp_shard=1]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L15_S1_TP7_GPU15_qkv_linear -> L15_S1_TP7_GPU15_qkv_slice
	L15_S1_TP7_GPU15_qk_matmul [label="QK Matmul\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L15_S1_TP7_GPU15_qkv_slice -> L15_S1_TP7_GPU15_qk_matmul
	L15_S1_TP7_GPU15_softmax [label="Softmax\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nOutput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000]\nGPU: 15" shape=rectangle]
	L15_S1_TP7_GPU15_qk_matmul -> L15_S1_TP7_GPU15_softmax
	L15_S1_TP7_GPU15_attn_v [label="Attn@V\nInput: [batch_size=128, heads=4, seq_len=10000, seq_len=10000], [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nOutput: [batch_size=128, seq_len=10000, heads=4, head_dim=128]\nGPU: 15" shape=rectangle]
	L15_S1_TP7_GPU15_softmax -> L15_S1_TP7_GPU15_attn_v
	L15_S1_TP7_GPU15_qkv_slice -> L15_S1_TP7_GPU15_attn_v
	L15_S1_TP7_GPU15_mha_ar [label="MHA AllReduce\nInput: [batch_size=128, seq_len=10000, heads=4, head_dim=128] (8×)\nOutput: [batch_size=128, seq_len=10000, heads=32, head_dim=128]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP1_GPU9_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP2_GPU10_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP3_GPU11_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP4_GPU12_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP5_GPU13_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP6_GPU14_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP7_GPU15_attn_v -> L15_S1_TP7_GPU15_mha_ar
	L15_S1_TP7_GPU15_mha_res [label="MHA Residual Add\nInput: [batch_size=128, seq_len=10000, heads=32, head_dim=128], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP7_GPU15_mha_ar -> L15_S1_TP7_GPU15_mha_res
	L15_S1_TP7_GPU15_gate [label="MoE Gate (Top2)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, top_k=2, expert_id=0..15]\nGPU: 15" shape=parallelogram]
	L15_S1_TP7_GPU15_mha_res -> L15_S1_TP7_GPU15_gate
	L15_S1_TP7_GPU15_expert0 [label="Expert0 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L15_S1_TP7_GPU15_gate -> L15_S1_TP7_GPU15_expert0 [style=dashed]
	L15_S1_TP7_GPU15_expert1 [label="Expert1 MLP (4096->16384->4096)\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: 15" shape=rectangle]
	L15_S1_TP7_GPU15_gate -> L15_S1_TP7_GPU15_expert1 [style=dashed]
	L15_S1_TP7_GPU15_moe_ar [label="MoE AllReduce\nInput: [batch_size=128, seq_len=10000, token_dim=4096] (8×)\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP0_GPU8_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP1_GPU9_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP1_GPU9_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP2_GPU10_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP2_GPU10_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP3_GPU11_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP3_GPU11_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP4_GPU12_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP4_GPU12_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP5_GPU13_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP5_GPU13_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP6_GPU14_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP6_GPU14_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP7_GPU15_expert0 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP7_GPU15_expert1 -> L15_S1_TP7_GPU15_moe_ar
	L15_S1_TP7_GPU15_moe_res [label="MoE Residual Add\nInput: [batch_size=128, seq_len=10000, token_dim=4096], [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP7_GPU15_moe_ar -> L15_S1_TP7_GPU15_moe_res
	L15_S1_TP7_GPU15_mha_res -> L15_S1_TP7_GPU15_moe_res
	L15_S1_TP7_GPU15_norm [label="LayerNorm\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=rectangle]
	L15_S1_TP7_GPU15_moe_res -> L15_S1_TP7_GPU15_norm
	output [label="Output\nInput: [batch_size=128, seq_len=10000, token_dim=4096]\nOutput: [batch_size=128, seq_len=10000, token_dim=4096]\nGPU: all stage 1" shape=ellipse]
	L15_S1_TP0_GPU8_norm -> output
	L15_S1_TP1_GPU9_norm -> output
	L15_S1_TP2_GPU10_norm -> output
	L15_S1_TP3_GPU11_norm -> output
	L15_S1_TP4_GPU12_norm -> output
	L15_S1_TP5_GPU13_norm -> output
	L15_S1_TP6_GPU14_norm -> output
	L15_S1_TP7_GPU15_norm -> output
}
