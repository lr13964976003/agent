{
  "deployment_configurations": {
    "baseline_method": {
      "name": "Tensor Parallelism + Pipeline Parallelism",
      "parallel_strategy": {
        "type": "hybrid",
        "tensor_parallelism": {
          "degree": 8,
          "strategy": "row_and_column_parallel",
          "parameters": {
            "attention_layer": {
              "query_key_value": "column_parallel",
              "dense": "row_parallel",
              "hidden_size": 8192,
              "num_heads": 16,
              "head_dim": 512
            },
            "mlp_layer": {
              "gate_proj": "column_parallel",
              "up_proj": "column_parallel", 
              "down_proj": "row_parallel",
              "ffn_hidden_size": 32768,
              "hidden_size": 8192
            }
          }
        },
        "pipeline_parallelism": {
          "degree": 2,
          "strategy": "gpipe",
          "num_layers": 16,
          "layers_per_stage": 8
        }
      },
      "device_mapping": {
        "stage_0": {
          "tensor_parallel_group": [0, 1, 2, 3, 4, 5, 6, 7],
          "layers": [0, 1, 2, 3, 4, 5, 6, 7]
        },
        "stage_1": {
          "tensor_parallel_group": [8, 9, 10, 11, 12, 13, 14, 15],
          "layers": [8, 9, 10, 11, 12, 13, 14, 15]
        }
      },
      "memory_requirements": {
        "per_device_memory": "shared_across_tensor_group",
        "activation_memory": "distributed_via_tensor_parallel",
        "weight_memory": "sharded_across_tensor_group"
      }
    },
    "proposed_method": {
      "name": "Layer-wise Cache-aware Deployment",
      "parallel_strategy": {
        "type": "layer_parallelism",
        "strategy": "greedy_layer_aggregation",
        "degree": 16,
        "cache_constraint": "SRAM_L2_cache_per_device",
        "partitioning_algorithm": {
          "type": "greedy",
          "memory_estimation": {
            "formula": "size(layer) = weight_size + activation_size + buffer_size",
            "parameters": {
              "batch_size": 1024,
              "sequence_length": 10000,
              "precision": "FP16",
              "datatype_size": 2
            }
          },
          "cache_capacity_per_device": "H100_SRAM_L2_size",
          "contiguous_assignment": true
        }
      },
      "device_mapping": {
        "device_0": {
          "layers": [0],
          "memory_allocation": {
            "weights": "layer_0_weights",
            "activations": "layer_0_activations",
            "buffers": "layer_0_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_1": {
          "layers": [1],
          "memory_allocation": {
            "weights": "layer_1_weights",
            "activations": "layer_1_activations", 
            "buffers": "layer_1_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_2": {
          "layers": [2],
          "memory_allocation": {
            "weights": "layer_2_weights",
            "activations": "layer_2_activations",
            "buffers": "layer_2_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_3": {
          "layers": [3],
          "memory_allocation": {
            "weights": "layer_3_weights",
            "activations": "layer_3_activations",
            "buffers": "layer_3_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_4": {
          "layers": [4],
          "memory_allocation": {
            "weights": "layer_4_weights",
            "activations": "layer_4_activations",
            "buffers": "layer_4_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_5": {
          "layers": [5],
          "memory_allocation": {
            "weights": "layer_5_weights",
            "activations": "layer_5_activations",
            "buffers": "layer_5_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_6": {
          "layers": [6],
          "memory_allocation": {
            "weights": "layer_6_weights",
            "activations": "layer_6_activations",
            "buffers": "layer_6_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_7": {
          "layers": [7],
          "memory_allocation": {
            "weights": "layer_7_weights",
            "activations": "layer_7_activations",
            "buffers": "layer_7_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_8": {
          "layers": [8],
          "memory_allocation": {
            "weights": "layer_8_weights",
            "activations": "layer_8_activations",
            "buffers": "layer_8_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_9": {
          "layers": [9],
          "memory_allocation": {
            "weights": "layer_9_weights",
            "activations": "layer_9_activations",
            "buffers": "layer_9_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_10": {
          "layers": [10],
          "memory_allocation": {
            "weights": "layer_10_weights",
            "activations": "layer_10_activations",
            "buffers": "layer_10_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_11": {
          "layers": [11],
          "memory_allocation": {
            "weights": "layer_11_weights",
            "activations": "layer_11_activations",
            "buffers": "layer_11_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_12": {
          "layers": [12],
          "memory_allocation": {
            "weights": "layer_12_weights",
            "activations": "layer_12_activations",
            "buffers": "layer_12_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_13": {
          "layers": [13],
          "memory_allocation": {
            "weights": "layer_13_weights",
            "activations": "layer_13_activations",
            "buffers": "layer_13_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_14": {
          "layers": [14],
          "memory_allocation": {
            "weights": "layer_14_weights",
            "activations": "layer_14_activations",
            "buffers": "layer_14_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        },
        "device_15": {
          "layers": [15],
          "memory_allocation": {
            "weights": "layer_15_weights",
            "activations": "layer_15_activations",
            "buffers": "layer_15_workspace",
            "cache_constraint": "must_fit_in_SRAM_L2"
          }
        }
      },
      "communication_pattern": {
        "type": "layer_to_layer_transfer",
        "transfer_points": [
          {"from_device": 0, "to_device": 1, "transfer_layer": "activation_0_to_1"},
          {"from_device": 1, "to_device": 2, "transfer_layer": "activation_1_to_2"},
          {"from_device": 2, "to_device": 3, "transfer_layer": "activation_2_to_3"},
          {"from_device": 3, "to_device": 4, "transfer_layer": "activation_3_to_4"},
          {"from_device": 4, "to_device": 5, "transfer_layer": "activation_4_to_5"},
          {"from_device": 5, "to_device": 6, "transfer_layer": "activation_5_to_6"},
          {"from_device": 6, "to_device": 7, "transfer_layer": "activation_6_to_7"},
          {"from_device": 7, "to_device": 8, "transfer_layer": "activation_7_to_8"},
          {"from_device": 8, "to_device": 9, "transfer_layer": "activation_8_to_9"},
          {"from_device": 9, "to_device": 10, "transfer_layer": "activation_9_to_10"},
          {"from_device": 10, "to_device": 11, "transfer_layer": "activation_10_to_11"},
          {"from_device": 11, "to_device": 12, "transfer_layer": "activation_11_to_12"},
          {"from_device": 12, "to_device": 13, "transfer_layer": "activation_12_to_13"},
          {"from_device": 13, "to_device": 14, "transfer_layer": "activation_13_to_14"},
          {"from_device": 14, "to_device": 15, "transfer_layer": "activation_14_to_15"}
        ]
      }
    }
  },
  "model_specifications": {
    "dense_model": {
      "layers": 16,
      "architecture": "fully_connected_dense",
      "parameters": {
        "hidden_size": 8192,
        "num_heads": 16,
        "head_dim": 512,
        "ffn_hidden_size": 32768,
        "sequence_length": 10000,
        "batch_size": 1024,
        "precision": "FP16"
      }
    }
  }
}