// Layer 1 Detailed DAG
digraph Layer_1_Detailed {
	graph [compound=true rankdir=TB]
	layer_input [label="Layer 1 Input\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=ellipse style=filled]
	ln1 [label="LayerNorm\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_0 [label="QKV Projection\nGPU: 0\nHeads: 0-3\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_1 [label="QKV Projection\nGPU: 1\nHeads: 4-7\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_2 [label="QKV Projection\nGPU: 2\nHeads: 8-11\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_3 [label="QKV Projection\nGPU: 3\nHeads: 12-15\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_4 [label="QKV Projection\nGPU: 4\nHeads: 16-19\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_5 [label="QKV Projection\nGPU: 5\nHeads: 20-23\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_6 [label="QKV Projection\nGPU: 6\nHeads: 24-27\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	qkv_gpu_7 [label="QKV Projection\nGPU: 7\nHeads: 28-31\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_0 [label="Multi-Head Attention\nGPU: 0\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_1 [label="Multi-Head Attention\nGPU: 1\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_2 [label="Multi-Head Attention\nGPU: 2\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_3 [label="Multi-Head Attention\nGPU: 3\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_4 [label="Multi-Head Attention\nGPU: 4\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_5 [label="Multi-Head Attention\nGPU: 5\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_6 [label="Multi-Head Attention\nGPU: 6\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_7 [label="Multi-Head Attention\nGPU: 7\n[batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_allreduce [label="All-Reduce Attention\nCombine 8Ã—4 heads\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	attn_proj_0 [label="Output Projection\nGPU: 0\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_proj_1 [label="Output Projection\nGPU: 1\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_proj_2 [label="Output Projection\nGPU: 2\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_proj_3 [label="Output Projection\nGPU: 3\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_proj_4 [label="Output Projection\nGPU: 4\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_proj_5 [label="Output Projection\nGPU: 5\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_proj_6 [label="Output Projection\nGPU: 6\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	attn_proj_7 [label="Output Projection\nGPU: 7\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	broadcast_to_moe [label="Broadcast to MoE GPUs\nFrom GPUs 0-7 to GPUs 8-15\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	residual1 [label="Residual Add 1\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=rectangle style=filled]
	ln2 [label="LayerNorm\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	gate [label="Gate (Top-2)\n[batch_size=B, seq_len=2048, top_k=2]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	expert_group_8 [label="Expert Group\nGPU: 8\nExperts: 0-1\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_group_9 [label="Expert Group\nGPU: 9\nExperts: 2-3\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_group_10 [label="Expert Group\nGPU: 10\nExperts: 4-5\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_group_11 [label="Expert Group\nGPU: 11\nExperts: 6-7\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_group_12 [label="Expert Group\nGPU: 12\nExperts: 8-9\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_group_13 [label="Expert Group\nGPU: 13\nExperts: 10-11\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_group_14 [label="Expert Group\nGPU: 14\nExperts: 12-13\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_group_15 [label="Expert Group\nGPU: 15\nExperts: 14-15\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	expert_agg [label="Expert Aggregation\nAggregate 16 experts\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	residual2 [label="Residual Add 2\n[batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_input -> ln1
	ln1 -> qkv_gpu_0
	ln1 -> qkv_gpu_1
	ln1 -> qkv_gpu_2
	ln1 -> qkv_gpu_3
	ln1 -> qkv_gpu_4
	ln1 -> qkv_gpu_5
	ln1 -> qkv_gpu_6
	ln1 -> qkv_gpu_7
	qkv_gpu_0 -> attn_0
	qkv_gpu_1 -> attn_1
	qkv_gpu_2 -> attn_2
	qkv_gpu_3 -> attn_3
	qkv_gpu_4 -> attn_4
	qkv_gpu_5 -> attn_5
	qkv_gpu_6 -> attn_6
	qkv_gpu_7 -> attn_7
	attn_0 -> attn_allreduce
	attn_1 -> attn_allreduce
	attn_2 -> attn_allreduce
	attn_3 -> attn_allreduce
	attn_4 -> attn_allreduce
	attn_5 -> attn_allreduce
	attn_6 -> attn_allreduce
	attn_7 -> attn_allreduce
	attn_allreduce -> attn_proj_0
	attn_allreduce -> attn_proj_1
	attn_allreduce -> attn_proj_2
	attn_allreduce -> attn_proj_3
	attn_allreduce -> attn_proj_4
	attn_allreduce -> attn_proj_5
	attn_allreduce -> attn_proj_6
	attn_allreduce -> attn_proj_7
	attn_proj_0 -> broadcast_to_moe
	attn_proj_1 -> broadcast_to_moe
	attn_proj_2 -> broadcast_to_moe
	attn_proj_3 -> broadcast_to_moe
	attn_proj_4 -> broadcast_to_moe
	attn_proj_5 -> broadcast_to_moe
	attn_proj_6 -> broadcast_to_moe
	attn_proj_7 -> broadcast_to_moe
	broadcast_to_moe -> residual1
	layer_input -> residual1
	residual1 -> ln2
	ln2 -> gate
	gate -> expert_group_8 [style=dashed]
	gate -> expert_group_9 [style=dashed]
	gate -> expert_group_10 [style=dashed]
	gate -> expert_group_11 [style=dashed]
	gate -> expert_group_12 [style=dashed]
	gate -> expert_group_13 [style=dashed]
	gate -> expert_group_14 [style=dashed]
	gate -> expert_group_15 [style=dashed]
	expert_group_8 -> expert_agg
	expert_group_9 -> expert_agg
	expert_group_10 -> expert_agg
	expert_group_11 -> expert_agg
	expert_group_12 -> expert_agg
	expert_group_13 -> expert_agg
	expert_group_14 -> expert_agg
	expert_group_15 -> expert_agg
	expert_agg -> residual2
	residual1 -> residual2
}
