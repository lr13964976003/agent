// MA Separation: 4-layer MoE with 16 GPUs
digraph MA_Separation_Deployment {
	graph [compound=true rankdir=TB ranksep=1.5 splines=ortho]
	node [fontname=monospace shape=ellipse]
	input [label="Input\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#E6FFE6" shape=ellipse style=filled]
	layer_0_ln1 [label="LayerNorm1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_0 [label="QKV Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_1 [label="QKV Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_2 [label="QKV Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_3 [label="QKV Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_4 [label="QKV Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_5 [label="QKV Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_6 [label="QKV Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_qkv_gpu_7 [label="QKV Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_0 [label="Multi-Head Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_1 [label="Multi-Head Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_2 [label="Multi-Head Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_3 [label="Multi-Head Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_4 [label="Multi-Head Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_5 [label="Multi-Head Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_6 [label="Multi-Head Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_gpu_7 [label="Multi-Head Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_allreduce_0 [label="All-Reduce Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_allreduce_1 [label="All-Reduce Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_allreduce_2 [label="All-Reduce Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_allreduce_3 [label="All-Reduce Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_allreduce_4 [label="All-Reduce Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_allreduce_5 [label="All-Reduce Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_allreduce_6 [label="All-Reduce Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_allreduce_7 [label="All-Reduce Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_attn_proj_0 [label="Attention Output Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_proj_1 [label="Attention Output Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_proj_2 [label="Attention Output Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_proj_3 [label="Attention Output Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_proj_4 [label="Attention Output Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_proj_5 [label="Attention Output Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_proj_6 [label="Attention Output Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_attn_proj_7 [label="Attention Output Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_0_recv_attn_8 [label="Receive Attention Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_recv_attn_9 [label="Receive Attention Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_recv_attn_10 [label="Receive Attention Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_recv_attn_11 [label="Receive Attention Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_recv_attn_12 [label="Receive Attention Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_recv_attn_13 [label="Receive Attention Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_recv_attn_14 [label="Receive Attention Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_recv_attn_15 [label="Receive Attention Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_residual1 [label="Residual Add 1\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_0_ln2 [label="LayerNorm2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_gate [label="Gate (Top-2 routing)\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, top_k=2]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_0_gpu_8 [label="Expert 0\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_1_gpu_8 [label="Expert 1\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_2_gpu_9 [label="Expert 2\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_3_gpu_9 [label="Expert 3\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_4_gpu_10 [label="Expert 4\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_5_gpu_10 [label="Expert 5\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_6_gpu_11 [label="Expert 6\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_7_gpu_11 [label="Expert 7\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_8_gpu_12 [label="Expert 8\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_9_gpu_12 [label="Expert 9\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_10_gpu_13 [label="Expert 10\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_11_gpu_13 [label="Expert 11\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_12_gpu_14 [label="Expert 12\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_13_gpu_14 [label="Expert 13\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_14_gpu_15 [label="Expert 14\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_15_gpu_15 [label="Expert 15\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_0_expert_select_8 [label="Expert Selection\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_select_9 [label="Expert Selection\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_select_10 [label="Expert Selection\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_select_11 [label="Expert Selection\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_select_12 [label="Expert Selection\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_select_13 [label="Expert Selection\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_select_14 [label="Expert Selection\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_select_15 [label="Expert Selection\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_0_expert_agg_8 [label="Expert Aggregation\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_agg_9 [label="Expert Aggregation\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_agg_10 [label="Expert Aggregation\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_agg_11 [label="Expert Aggregation\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_agg_12 [label="Expert Aggregation\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_agg_13 [label="Expert Aggregation\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_agg_14 [label="Expert Aggregation\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_agg_15 [label="Expert Aggregation\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_0 [label="Broadcast Expert Output\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_1 [label="Broadcast Expert Output\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_2 [label="Broadcast Expert Output\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_3 [label="Broadcast Expert Output\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_4 [label="Broadcast Expert Output\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_5 [label="Broadcast Expert Output\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_6 [label="Broadcast Expert Output\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_7 [label="Broadcast Expert Output\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_8 [label="Broadcast Expert Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_9 [label="Broadcast Expert Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_10 [label="Broadcast Expert Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_11 [label="Broadcast Expert Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_12 [label="Broadcast Expert Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_13 [label="Broadcast Expert Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_14 [label="Broadcast Expert Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_expert_broadcast_15 [label="Broadcast Expert Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_0_residual2 [label="Residual Add 2\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_1_ln1 [label="LayerNorm1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_0 [label="QKV Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_1 [label="QKV Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_2 [label="QKV Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_3 [label="QKV Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_4 [label="QKV Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_5 [label="QKV Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_6 [label="QKV Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_qkv_gpu_7 [label="QKV Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_0 [label="Multi-Head Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_1 [label="Multi-Head Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_2 [label="Multi-Head Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_3 [label="Multi-Head Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_4 [label="Multi-Head Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_5 [label="Multi-Head Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_6 [label="Multi-Head Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_gpu_7 [label="Multi-Head Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_allreduce_0 [label="All-Reduce Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_allreduce_1 [label="All-Reduce Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_allreduce_2 [label="All-Reduce Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_allreduce_3 [label="All-Reduce Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_allreduce_4 [label="All-Reduce Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_allreduce_5 [label="All-Reduce Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_allreduce_6 [label="All-Reduce Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_allreduce_7 [label="All-Reduce Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_attn_proj_0 [label="Attention Output Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_proj_1 [label="Attention Output Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_proj_2 [label="Attention Output Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_proj_3 [label="Attention Output Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_proj_4 [label="Attention Output Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_proj_5 [label="Attention Output Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_proj_6 [label="Attention Output Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_attn_proj_7 [label="Attention Output Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_1_recv_attn_8 [label="Receive Attention Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_recv_attn_9 [label="Receive Attention Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_recv_attn_10 [label="Receive Attention Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_recv_attn_11 [label="Receive Attention Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_recv_attn_12 [label="Receive Attention Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_recv_attn_13 [label="Receive Attention Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_recv_attn_14 [label="Receive Attention Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_recv_attn_15 [label="Receive Attention Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_residual1 [label="Residual Add 1\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_1_ln2 [label="LayerNorm2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_gate [label="Gate (Top-2 routing)\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, top_k=2]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_0_gpu_8 [label="Expert 0\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_1_gpu_8 [label="Expert 1\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_2_gpu_9 [label="Expert 2\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_3_gpu_9 [label="Expert 3\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_4_gpu_10 [label="Expert 4\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_5_gpu_10 [label="Expert 5\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_6_gpu_11 [label="Expert 6\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_7_gpu_11 [label="Expert 7\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_8_gpu_12 [label="Expert 8\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_9_gpu_12 [label="Expert 9\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_10_gpu_13 [label="Expert 10\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_11_gpu_13 [label="Expert 11\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_12_gpu_14 [label="Expert 12\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_13_gpu_14 [label="Expert 13\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_14_gpu_15 [label="Expert 14\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_15_gpu_15 [label="Expert 15\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_1_expert_select_8 [label="Expert Selection\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_select_9 [label="Expert Selection\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_select_10 [label="Expert Selection\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_select_11 [label="Expert Selection\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_select_12 [label="Expert Selection\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_select_13 [label="Expert Selection\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_select_14 [label="Expert Selection\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_select_15 [label="Expert Selection\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_1_expert_agg_8 [label="Expert Aggregation\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_agg_9 [label="Expert Aggregation\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_agg_10 [label="Expert Aggregation\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_agg_11 [label="Expert Aggregation\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_agg_12 [label="Expert Aggregation\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_agg_13 [label="Expert Aggregation\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_agg_14 [label="Expert Aggregation\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_agg_15 [label="Expert Aggregation\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_0 [label="Broadcast Expert Output\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_1 [label="Broadcast Expert Output\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_2 [label="Broadcast Expert Output\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_3 [label="Broadcast Expert Output\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_4 [label="Broadcast Expert Output\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_5 [label="Broadcast Expert Output\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_6 [label="Broadcast Expert Output\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_7 [label="Broadcast Expert Output\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_8 [label="Broadcast Expert Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_9 [label="Broadcast Expert Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_10 [label="Broadcast Expert Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_11 [label="Broadcast Expert Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_12 [label="Broadcast Expert Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_13 [label="Broadcast Expert Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_14 [label="Broadcast Expert Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_expert_broadcast_15 [label="Broadcast Expert Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_1_residual2 [label="Residual Add 2\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_2_ln1 [label="LayerNorm1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_0 [label="QKV Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_1 [label="QKV Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_2 [label="QKV Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_3 [label="QKV Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_4 [label="QKV Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_5 [label="QKV Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_6 [label="QKV Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_qkv_gpu_7 [label="QKV Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_0 [label="Multi-Head Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_1 [label="Multi-Head Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_2 [label="Multi-Head Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_3 [label="Multi-Head Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_4 [label="Multi-Head Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_5 [label="Multi-Head Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_6 [label="Multi-Head Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_gpu_7 [label="Multi-Head Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_allreduce_0 [label="All-Reduce Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_allreduce_1 [label="All-Reduce Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_allreduce_2 [label="All-Reduce Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_allreduce_3 [label="All-Reduce Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_allreduce_4 [label="All-Reduce Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_allreduce_5 [label="All-Reduce Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_allreduce_6 [label="All-Reduce Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_allreduce_7 [label="All-Reduce Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_attn_proj_0 [label="Attention Output Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_proj_1 [label="Attention Output Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_proj_2 [label="Attention Output Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_proj_3 [label="Attention Output Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_proj_4 [label="Attention Output Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_proj_5 [label="Attention Output Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_proj_6 [label="Attention Output Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_attn_proj_7 [label="Attention Output Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_2_recv_attn_8 [label="Receive Attention Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_recv_attn_9 [label="Receive Attention Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_recv_attn_10 [label="Receive Attention Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_recv_attn_11 [label="Receive Attention Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_recv_attn_12 [label="Receive Attention Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_recv_attn_13 [label="Receive Attention Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_recv_attn_14 [label="Receive Attention Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_recv_attn_15 [label="Receive Attention Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_residual1 [label="Residual Add 1\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_2_ln2 [label="LayerNorm2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_gate [label="Gate (Top-2 routing)\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, top_k=2]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_0_gpu_8 [label="Expert 0\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_1_gpu_8 [label="Expert 1\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_2_gpu_9 [label="Expert 2\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_3_gpu_9 [label="Expert 3\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_4_gpu_10 [label="Expert 4\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_5_gpu_10 [label="Expert 5\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_6_gpu_11 [label="Expert 6\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_7_gpu_11 [label="Expert 7\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_8_gpu_12 [label="Expert 8\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_9_gpu_12 [label="Expert 9\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_10_gpu_13 [label="Expert 10\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_11_gpu_13 [label="Expert 11\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_12_gpu_14 [label="Expert 12\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_13_gpu_14 [label="Expert 13\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_14_gpu_15 [label="Expert 14\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_15_gpu_15 [label="Expert 15\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_2_expert_select_8 [label="Expert Selection\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_select_9 [label="Expert Selection\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_select_10 [label="Expert Selection\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_select_11 [label="Expert Selection\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_select_12 [label="Expert Selection\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_select_13 [label="Expert Selection\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_select_14 [label="Expert Selection\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_select_15 [label="Expert Selection\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_2_expert_agg_8 [label="Expert Aggregation\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_agg_9 [label="Expert Aggregation\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_agg_10 [label="Expert Aggregation\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_agg_11 [label="Expert Aggregation\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_agg_12 [label="Expert Aggregation\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_agg_13 [label="Expert Aggregation\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_agg_14 [label="Expert Aggregation\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_agg_15 [label="Expert Aggregation\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_0 [label="Broadcast Expert Output\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_1 [label="Broadcast Expert Output\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_2 [label="Broadcast Expert Output\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_3 [label="Broadcast Expert Output\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_4 [label="Broadcast Expert Output\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_5 [label="Broadcast Expert Output\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_6 [label="Broadcast Expert Output\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_7 [label="Broadcast Expert Output\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_8 [label="Broadcast Expert Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_9 [label="Broadcast Expert Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_10 [label="Broadcast Expert Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_11 [label="Broadcast Expert Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_12 [label="Broadcast Expert Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_13 [label="Broadcast Expert Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_14 [label="Broadcast Expert Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_expert_broadcast_15 [label="Broadcast Expert Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_2_residual2 [label="Residual Add 2\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_3_ln1 [label="LayerNorm1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_0 [label="QKV Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_1 [label="QKV Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_2 [label="QKV Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_3 [label="QKV Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_4 [label="QKV Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_5 [label="QKV Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_6 [label="QKV Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_qkv_gpu_7 [label="QKV Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_0 [label="Multi-Head Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_1 [label="Multi-Head Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_2 [label="Multi-Head Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_3 [label="Multi-Head Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_4 [label="Multi-Head Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_5 [label="Multi-Head Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_6 [label="Multi-Head Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_gpu_7 [label="Multi-Head Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, heads=4, d_k=128]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_allreduce_0 [label="All-Reduce Attention\nGPU: 0\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_allreduce_1 [label="All-Reduce Attention\nGPU: 1\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_allreduce_2 [label="All-Reduce Attention\nGPU: 2\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_allreduce_3 [label="All-Reduce Attention\nGPU: 3\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_allreduce_4 [label="All-Reduce Attention\nGPU: 4\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_allreduce_5 [label="All-Reduce Attention\nGPU: 5\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_allreduce_6 [label="All-Reduce Attention\nGPU: 6\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_allreduce_7 [label="All-Reduce Attention\nGPU: 7\nInput: [batch_size=B, seq_len=2048, heads=4, d_k=128]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_attn_proj_0 [label="Attention Output Projection\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_proj_1 [label="Attention Output Projection\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_proj_2 [label="Attention Output Projection\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_proj_3 [label="Attention Output Projection\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_proj_4 [label="Attention Output Projection\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_proj_5 [label="Attention Output Projection\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_proj_6 [label="Attention Output Projection\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_attn_proj_7 [label="Attention Output Projection\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6F3FF" shape=rectangle style=filled]
	layer_3_recv_attn_8 [label="Receive Attention Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_recv_attn_9 [label="Receive Attention Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_recv_attn_10 [label="Receive Attention Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_recv_attn_11 [label="Receive Attention Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_recv_attn_12 [label="Receive Attention Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_recv_attn_13 [label="Receive Attention Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_recv_attn_14 [label="Receive Attention Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_recv_attn_15 [label="Receive Attention Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_residual1 [label="Residual Add 1\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	layer_3_ln2 [label="LayerNorm2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_gate [label="Gate (Top-2 routing)\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, top_k=2]\nGPU: all GPUs" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_0_gpu_8 [label="Expert 0\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_1_gpu_8 [label="Expert 1\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_2_gpu_9 [label="Expert 2\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_3_gpu_9 [label="Expert 3\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_4_gpu_10 [label="Expert 4\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_5_gpu_10 [label="Expert 5\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_6_gpu_11 [label="Expert 6\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_7_gpu_11 [label="Expert 7\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_8_gpu_12 [label="Expert 8\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_9_gpu_12 [label="Expert 9\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_10_gpu_13 [label="Expert 10\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_11_gpu_13 [label="Expert 11\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_12_gpu_14 [label="Expert 12\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_13_gpu_14 [label="Expert 13\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_14_gpu_15 [label="Expert 14\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_15_gpu_15 [label="Expert 15\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=rectangle style=filled]
	layer_3_expert_select_8 [label="Expert Selection\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_select_9 [label="Expert Selection\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_select_10 [label="Expert Selection\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_select_11 [label="Expert Selection\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_select_12 [label="Expert Selection\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_select_13 [label="Expert Selection\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_select_14 [label="Expert Selection\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_select_15 [label="Expert Selection\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFE6E6" shape=parallelogram style=filled]
	layer_3_expert_agg_8 [label="Expert Aggregation\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_agg_9 [label="Expert Aggregation\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_agg_10 [label="Expert Aggregation\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_agg_11 [label="Expert Aggregation\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_agg_12 [label="Expert Aggregation\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_agg_13 [label="Expert Aggregation\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_agg_14 [label="Expert Aggregation\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_agg_15 [label="Expert Aggregation\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#FFF0E6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_0 [label="Broadcast Expert Output\nGPU: 0\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_1 [label="Broadcast Expert Output\nGPU: 1\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_2 [label="Broadcast Expert Output\nGPU: 2\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_3 [label="Broadcast Expert Output\nGPU: 3\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_4 [label="Broadcast Expert Output\nGPU: 4\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_5 [label="Broadcast Expert Output\nGPU: 5\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_6 [label="Broadcast Expert Output\nGPU: 6\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_7 [label="Broadcast Expert Output\nGPU: 7\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_8 [label="Broadcast Expert Output\nGPU: 8\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_9 [label="Broadcast Expert Output\nGPU: 9\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_10 [label="Broadcast Expert Output\nGPU: 10\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_11 [label="Broadcast Expert Output\nGPU: 11\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_12 [label="Broadcast Expert Output\nGPU: 12\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_13 [label="Broadcast Expert Output\nGPU: 13\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_14 [label="Broadcast Expert Output\nGPU: 14\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_expert_broadcast_15 [label="Broadcast Expert Output\nGPU: 15\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]" fillcolor="#E6FFE6" shape=parallelogram style=filled]
	layer_3_residual2 [label="Residual Add 2\nInput1: [batch_size=B, seq_len=2048, hidden_dim=4096]\nInput2: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nGPU: all GPUs" fillcolor="#FFF0E6" shape=rectangle style=filled]
	output [label="Output\nInput: [batch_size=B, seq_len=2048, hidden_dim=4096]\nOutput: [batch_size=B, seq_len=2048, vocab_size=50265]\nGPU: all GPUs" fillcolor="#E6FFE6" shape=ellipse style=filled]
	input -> layer_0_ln1
	layer_0_ln1 -> layer_0_qkv_gpu_0
	layer_0_ln1 -> layer_0_qkv_gpu_1
	layer_0_ln1 -> layer_0_qkv_gpu_2
	layer_0_ln1 -> layer_0_qkv_gpu_3
	layer_0_ln1 -> layer_0_qkv_gpu_4
	layer_0_ln1 -> layer_0_qkv_gpu_5
	layer_0_ln1 -> layer_0_qkv_gpu_6
	layer_0_ln1 -> layer_0_qkv_gpu_7
	layer_0_qkv_gpu_0 -> layer_0_attn_gpu_0
	layer_0_qkv_gpu_1 -> layer_0_attn_gpu_1
	layer_0_qkv_gpu_2 -> layer_0_attn_gpu_2
	layer_0_qkv_gpu_3 -> layer_0_attn_gpu_3
	layer_0_qkv_gpu_4 -> layer_0_attn_gpu_4
	layer_0_qkv_gpu_5 -> layer_0_attn_gpu_5
	layer_0_qkv_gpu_6 -> layer_0_attn_gpu_6
	layer_0_qkv_gpu_7 -> layer_0_attn_gpu_7
	layer_0_attn_gpu_0 -> layer_0_attn_allreduce_0
	layer_0_attn_gpu_1 -> layer_0_attn_allreduce_1
	layer_0_attn_gpu_2 -> layer_0_attn_allreduce_2
	layer_0_attn_gpu_3 -> layer_0_attn_allreduce_3
	layer_0_attn_gpu_4 -> layer_0_attn_allreduce_4
	layer_0_attn_gpu_5 -> layer_0_attn_allreduce_5
	layer_0_attn_gpu_6 -> layer_0_attn_allreduce_6
	layer_0_attn_gpu_7 -> layer_0_attn_allreduce_7
	layer_0_attn_allreduce_0 -> layer_0_attn_proj_0
	layer_0_attn_allreduce_1 -> layer_0_attn_proj_1
	layer_0_attn_allreduce_2 -> layer_0_attn_proj_2
	layer_0_attn_allreduce_3 -> layer_0_attn_proj_3
	layer_0_attn_allreduce_4 -> layer_0_attn_proj_4
	layer_0_attn_allreduce_5 -> layer_0_attn_proj_5
	layer_0_attn_allreduce_6 -> layer_0_attn_proj_6
	layer_0_attn_allreduce_7 -> layer_0_attn_proj_7
	layer_0_attn_proj_0 -> layer_0_recv_attn_8
	layer_0_attn_proj_0 -> layer_0_recv_attn_9
	layer_0_attn_proj_0 -> layer_0_recv_attn_10
	layer_0_attn_proj_0 -> layer_0_recv_attn_11
	layer_0_attn_proj_0 -> layer_0_recv_attn_12
	layer_0_attn_proj_0 -> layer_0_recv_attn_13
	layer_0_attn_proj_0 -> layer_0_recv_attn_14
	layer_0_attn_proj_0 -> layer_0_recv_attn_15
	layer_0_attn_proj_1 -> layer_0_recv_attn_8
	layer_0_attn_proj_1 -> layer_0_recv_attn_9
	layer_0_attn_proj_1 -> layer_0_recv_attn_10
	layer_0_attn_proj_1 -> layer_0_recv_attn_11
	layer_0_attn_proj_1 -> layer_0_recv_attn_12
	layer_0_attn_proj_1 -> layer_0_recv_attn_13
	layer_0_attn_proj_1 -> layer_0_recv_attn_14
	layer_0_attn_proj_1 -> layer_0_recv_attn_15
	layer_0_attn_proj_2 -> layer_0_recv_attn_8
	layer_0_attn_proj_2 -> layer_0_recv_attn_9
	layer_0_attn_proj_2 -> layer_0_recv_attn_10
	layer_0_attn_proj_2 -> layer_0_recv_attn_11
	layer_0_attn_proj_2 -> layer_0_recv_attn_12
	layer_0_attn_proj_2 -> layer_0_recv_attn_13
	layer_0_attn_proj_2 -> layer_0_recv_attn_14
	layer_0_attn_proj_2 -> layer_0_recv_attn_15
	layer_0_attn_proj_3 -> layer_0_recv_attn_8
	layer_0_attn_proj_3 -> layer_0_recv_attn_9
	layer_0_attn_proj_3 -> layer_0_recv_attn_10
	layer_0_attn_proj_3 -> layer_0_recv_attn_11
	layer_0_attn_proj_3 -> layer_0_recv_attn_12
	layer_0_attn_proj_3 -> layer_0_recv_attn_13
	layer_0_attn_proj_3 -> layer_0_recv_attn_14
	layer_0_attn_proj_3 -> layer_0_recv_attn_15
	layer_0_attn_proj_4 -> layer_0_recv_attn_8
	layer_0_attn_proj_4 -> layer_0_recv_attn_9
	layer_0_attn_proj_4 -> layer_0_recv_attn_10
	layer_0_attn_proj_4 -> layer_0_recv_attn_11
	layer_0_attn_proj_4 -> layer_0_recv_attn_12
	layer_0_attn_proj_4 -> layer_0_recv_attn_13
	layer_0_attn_proj_4 -> layer_0_recv_attn_14
	layer_0_attn_proj_4 -> layer_0_recv_attn_15
	layer_0_attn_proj_5 -> layer_0_recv_attn_8
	layer_0_attn_proj_5 -> layer_0_recv_attn_9
	layer_0_attn_proj_5 -> layer_0_recv_attn_10
	layer_0_attn_proj_5 -> layer_0_recv_attn_11
	layer_0_attn_proj_5 -> layer_0_recv_attn_12
	layer_0_attn_proj_5 -> layer_0_recv_attn_13
	layer_0_attn_proj_5 -> layer_0_recv_attn_14
	layer_0_attn_proj_5 -> layer_0_recv_attn_15
	layer_0_attn_proj_6 -> layer_0_recv_attn_8
	layer_0_attn_proj_6 -> layer_0_recv_attn_9
	layer_0_attn_proj_6 -> layer_0_recv_attn_10
	layer_0_attn_proj_6 -> layer_0_recv_attn_11
	layer_0_attn_proj_6 -> layer_0_recv_attn_12
	layer_0_attn_proj_6 -> layer_0_recv_attn_13
	layer_0_attn_proj_6 -> layer_0_recv_attn_14
	layer_0_attn_proj_6 -> layer_0_recv_attn_15
	layer_0_attn_proj_7 -> layer_0_recv_attn_8
	layer_0_attn_proj_7 -> layer_0_recv_attn_9
	layer_0_attn_proj_7 -> layer_0_recv_attn_10
	layer_0_attn_proj_7 -> layer_0_recv_attn_11
	layer_0_attn_proj_7 -> layer_0_recv_attn_12
	layer_0_attn_proj_7 -> layer_0_recv_attn_13
	layer_0_attn_proj_7 -> layer_0_recv_attn_14
	layer_0_attn_proj_7 -> layer_0_recv_attn_15
	layer_0_recv_attn_8 -> layer_0_residual1
	layer_0_recv_attn_9 -> layer_0_residual1
	layer_0_recv_attn_10 -> layer_0_residual1
	layer_0_recv_attn_11 -> layer_0_residual1
	layer_0_recv_attn_12 -> layer_0_residual1
	layer_0_recv_attn_13 -> layer_0_residual1
	layer_0_recv_attn_14 -> layer_0_residual1
	layer_0_recv_attn_15 -> layer_0_residual1
	input -> layer_0_residual1
	layer_0_residual1 -> layer_0_ln2
	layer_0_ln2 -> layer_0_gate
	layer_0_gate -> layer_0_expert_select_8 [style=dashed]
	layer_0_gate -> layer_0_expert_select_9 [style=dashed]
	layer_0_gate -> layer_0_expert_select_10 [style=dashed]
	layer_0_gate -> layer_0_expert_select_11 [style=dashed]
	layer_0_gate -> layer_0_expert_select_12 [style=dashed]
	layer_0_gate -> layer_0_expert_select_13 [style=dashed]
	layer_0_gate -> layer_0_expert_select_14 [style=dashed]
	layer_0_gate -> layer_0_expert_select_15 [style=dashed]
	layer_0_expert_select_8 -> layer_0_expert_0_gpu_8
	layer_0_expert_select_8 -> layer_0_expert_1_gpu_8
	layer_0_expert_select_9 -> layer_0_expert_2_gpu_9
	layer_0_expert_select_9 -> layer_0_expert_3_gpu_9
	layer_0_expert_select_10 -> layer_0_expert_4_gpu_10
	layer_0_expert_select_10 -> layer_0_expert_5_gpu_10
	layer_0_expert_select_11 -> layer_0_expert_6_gpu_11
	layer_0_expert_select_11 -> layer_0_expert_7_gpu_11
	layer_0_expert_select_12 -> layer_0_expert_8_gpu_12
	layer_0_expert_select_12 -> layer_0_expert_9_gpu_12
	layer_0_expert_select_13 -> layer_0_expert_10_gpu_13
	layer_0_expert_select_13 -> layer_0_expert_11_gpu_13
	layer_0_expert_select_14 -> layer_0_expert_12_gpu_14
	layer_0_expert_select_14 -> layer_0_expert_13_gpu_14
	layer_0_expert_select_15 -> layer_0_expert_14_gpu_15
	layer_0_expert_select_15 -> layer_0_expert_15_gpu_15
	layer_0_expert_0_gpu_8 -> layer_0_expert_agg_8
	layer_0_expert_1_gpu_8 -> layer_0_expert_agg_8
	layer_0_expert_2_gpu_9 -> layer_0_expert_agg_9
	layer_0_expert_3_gpu_9 -> layer_0_expert_agg_9
	layer_0_expert_4_gpu_10 -> layer_0_expert_agg_10
	layer_0_expert_5_gpu_10 -> layer_0_expert_agg_10
	layer_0_expert_6_gpu_11 -> layer_0_expert_agg_11
	layer_0_expert_7_gpu_11 -> layer_0_expert_agg_11
	layer_0_expert_8_gpu_12 -> layer_0_expert_agg_12
	layer_0_expert_9_gpu_12 -> layer_0_expert_agg_12
	layer_0_expert_10_gpu_13 -> layer_0_expert_agg_13
	layer_0_expert_11_gpu_13 -> layer_0_expert_agg_13
	layer_0_expert_12_gpu_14 -> layer_0_expert_agg_14
	layer_0_expert_13_gpu_14 -> layer_0_expert_agg_14
	layer_0_expert_14_gpu_15 -> layer_0_expert_agg_15
	layer_0_expert_15_gpu_15 -> layer_0_expert_agg_15
	layer_0_expert_agg_8 -> layer_0_expert_broadcast_8
	layer_0_expert_agg_9 -> layer_0_expert_broadcast_9
	layer_0_expert_agg_10 -> layer_0_expert_broadcast_10
	layer_0_expert_agg_11 -> layer_0_expert_broadcast_11
	layer_0_expert_agg_12 -> layer_0_expert_broadcast_12
	layer_0_expert_agg_13 -> layer_0_expert_broadcast_13
	layer_0_expert_agg_14 -> layer_0_expert_broadcast_14
	layer_0_expert_agg_15 -> layer_0_expert_broadcast_15
	layer_0_expert_broadcast_0 -> layer_0_residual2
	layer_0_expert_broadcast_1 -> layer_0_residual2
	layer_0_expert_broadcast_2 -> layer_0_residual2
	layer_0_expert_broadcast_3 -> layer_0_residual2
	layer_0_expert_broadcast_4 -> layer_0_residual2
	layer_0_expert_broadcast_5 -> layer_0_residual2
	layer_0_expert_broadcast_6 -> layer_0_residual2
	layer_0_expert_broadcast_7 -> layer_0_residual2
	layer_0_expert_broadcast_8 -> layer_0_residual2
	layer_0_expert_broadcast_9 -> layer_0_residual2
	layer_0_expert_broadcast_10 -> layer_0_residual2
	layer_0_expert_broadcast_11 -> layer_0_residual2
	layer_0_expert_broadcast_12 -> layer_0_residual2
	layer_0_expert_broadcast_13 -> layer_0_residual2
	layer_0_expert_broadcast_14 -> layer_0_residual2
	layer_0_expert_broadcast_15 -> layer_0_residual2
	layer_0_residual2 -> layer_1_ln1
	layer_1_ln1 -> layer_1_qkv_gpu_0
	layer_1_ln1 -> layer_1_qkv_gpu_1
	layer_1_ln1 -> layer_1_qkv_gpu_2
	layer_1_ln1 -> layer_1_qkv_gpu_3
	layer_1_ln1 -> layer_1_qkv_gpu_4
	layer_1_ln1 -> layer_1_qkv_gpu_5
	layer_1_ln1 -> layer_1_qkv_gpu_6
	layer_1_ln1 -> layer_1_qkv_gpu_7
	layer_1_qkv_gpu_0 -> layer_1_attn_gpu_0
	layer_1_qkv_gpu_1 -> layer_1_attn_gpu_1
	layer_1_qkv_gpu_2 -> layer_1_attn_gpu_2
	layer_1_qkv_gpu_3 -> layer_1_attn_gpu_3
	layer_1_qkv_gpu_4 -> layer_1_attn_gpu_4
	layer_1_qkv_gpu_5 -> layer_1_attn_gpu_5
	layer_1_qkv_gpu_6 -> layer_1_attn_gpu_6
	layer_1_qkv_gpu_7 -> layer_1_attn_gpu_7
	layer_1_attn_gpu_0 -> layer_1_attn_allreduce_0
	layer_1_attn_gpu_1 -> layer_1_attn_allreduce_1
	layer_1_attn_gpu_2 -> layer_1_attn_allreduce_2
	layer_1_attn_gpu_3 -> layer_1_attn_allreduce_3
	layer_1_attn_gpu_4 -> layer_1_attn_allreduce_4
	layer_1_attn_gpu_5 -> layer_1_attn_allreduce_5
	layer_1_attn_gpu_6 -> layer_1_attn_allreduce_6
	layer_1_attn_gpu_7 -> layer_1_attn_allreduce_7
	layer_1_attn_allreduce_0 -> layer_1_attn_proj_0
	layer_1_attn_allreduce_1 -> layer_1_attn_proj_1
	layer_1_attn_allreduce_2 -> layer_1_attn_proj_2
	layer_1_attn_allreduce_3 -> layer_1_attn_proj_3
	layer_1_attn_allreduce_4 -> layer_1_attn_proj_4
	layer_1_attn_allreduce_5 -> layer_1_attn_proj_5
	layer_1_attn_allreduce_6 -> layer_1_attn_proj_6
	layer_1_attn_allreduce_7 -> layer_1_attn_proj_7
	layer_1_attn_proj_0 -> layer_1_recv_attn_8
	layer_1_attn_proj_0 -> layer_1_recv_attn_9
	layer_1_attn_proj_0 -> layer_1_recv_attn_10
	layer_1_attn_proj_0 -> layer_1_recv_attn_11
	layer_1_attn_proj_0 -> layer_1_recv_attn_12
	layer_1_attn_proj_0 -> layer_1_recv_attn_13
	layer_1_attn_proj_0 -> layer_1_recv_attn_14
	layer_1_attn_proj_0 -> layer_1_recv_attn_15
	layer_1_attn_proj_1 -> layer_1_recv_attn_8
	layer_1_attn_proj_1 -> layer_1_recv_attn_9
	layer_1_attn_proj_1 -> layer_1_recv_attn_10
	layer_1_attn_proj_1 -> layer_1_recv_attn_11
	layer_1_attn_proj_1 -> layer_1_recv_attn_12
	layer_1_attn_proj_1 -> layer_1_recv_attn_13
	layer_1_attn_proj_1 -> layer_1_recv_attn_14
	layer_1_attn_proj_1 -> layer_1_recv_attn_15
	layer_1_attn_proj_2 -> layer_1_recv_attn_8
	layer_1_attn_proj_2 -> layer_1_recv_attn_9
	layer_1_attn_proj_2 -> layer_1_recv_attn_10
	layer_1_attn_proj_2 -> layer_1_recv_attn_11
	layer_1_attn_proj_2 -> layer_1_recv_attn_12
	layer_1_attn_proj_2 -> layer_1_recv_attn_13
	layer_1_attn_proj_2 -> layer_1_recv_attn_14
	layer_1_attn_proj_2 -> layer_1_recv_attn_15
	layer_1_attn_proj_3 -> layer_1_recv_attn_8
	layer_1_attn_proj_3 -> layer_1_recv_attn_9
	layer_1_attn_proj_3 -> layer_1_recv_attn_10
	layer_1_attn_proj_3 -> layer_1_recv_attn_11
	layer_1_attn_proj_3 -> layer_1_recv_attn_12
	layer_1_attn_proj_3 -> layer_1_recv_attn_13
	layer_1_attn_proj_3 -> layer_1_recv_attn_14
	layer_1_attn_proj_3 -> layer_1_recv_attn_15
	layer_1_attn_proj_4 -> layer_1_recv_attn_8
	layer_1_attn_proj_4 -> layer_1_recv_attn_9
	layer_1_attn_proj_4 -> layer_1_recv_attn_10
	layer_1_attn_proj_4 -> layer_1_recv_attn_11
	layer_1_attn_proj_4 -> layer_1_recv_attn_12
	layer_1_attn_proj_4 -> layer_1_recv_attn_13
	layer_1_attn_proj_4 -> layer_1_recv_attn_14
	layer_1_attn_proj_4 -> layer_1_recv_attn_15
	layer_1_attn_proj_5 -> layer_1_recv_attn_8
	layer_1_attn_proj_5 -> layer_1_recv_attn_9
	layer_1_attn_proj_5 -> layer_1_recv_attn_10
	layer_1_attn_proj_5 -> layer_1_recv_attn_11
	layer_1_attn_proj_5 -> layer_1_recv_attn_12
	layer_1_attn_proj_5 -> layer_1_recv_attn_13
	layer_1_attn_proj_5 -> layer_1_recv_attn_14
	layer_1_attn_proj_5 -> layer_1_recv_attn_15
	layer_1_attn_proj_6 -> layer_1_recv_attn_8
	layer_1_attn_proj_6 -> layer_1_recv_attn_9
	layer_1_attn_proj_6 -> layer_1_recv_attn_10
	layer_1_attn_proj_6 -> layer_1_recv_attn_11
	layer_1_attn_proj_6 -> layer_1_recv_attn_12
	layer_1_attn_proj_6 -> layer_1_recv_attn_13
	layer_1_attn_proj_6 -> layer_1_recv_attn_14
	layer_1_attn_proj_6 -> layer_1_recv_attn_15
	layer_1_attn_proj_7 -> layer_1_recv_attn_8
	layer_1_attn_proj_7 -> layer_1_recv_attn_9
	layer_1_attn_proj_7 -> layer_1_recv_attn_10
	layer_1_attn_proj_7 -> layer_1_recv_attn_11
	layer_1_attn_proj_7 -> layer_1_recv_attn_12
	layer_1_attn_proj_7 -> layer_1_recv_attn_13
	layer_1_attn_proj_7 -> layer_1_recv_attn_14
	layer_1_attn_proj_7 -> layer_1_recv_attn_15
	layer_1_recv_attn_8 -> layer_1_residual1
	layer_1_recv_attn_9 -> layer_1_residual1
	layer_1_recv_attn_10 -> layer_1_residual1
	layer_1_recv_attn_11 -> layer_1_residual1
	layer_1_recv_attn_12 -> layer_1_residual1
	layer_1_recv_attn_13 -> layer_1_residual1
	layer_1_recv_attn_14 -> layer_1_residual1
	layer_1_recv_attn_15 -> layer_1_residual1
	layer_0_residual2 -> layer_1_residual1
	layer_1_residual1 -> layer_1_ln2
	layer_1_ln2 -> layer_1_gate
	layer_1_gate -> layer_1_expert_select_8 [style=dashed]
	layer_1_gate -> layer_1_expert_select_9 [style=dashed]
	layer_1_gate -> layer_1_expert_select_10 [style=dashed]
	layer_1_gate -> layer_1_expert_select_11 [style=dashed]
	layer_1_gate -> layer_1_expert_select_12 [style=dashed]
	layer_1_gate -> layer_1_expert_select_13 [style=dashed]
	layer_1_gate -> layer_1_expert_select_14 [style=dashed]
	layer_1_gate -> layer_1_expert_select_15 [style=dashed]
	layer_1_expert_select_8 -> layer_1_expert_0_gpu_8
	layer_1_expert_select_8 -> layer_1_expert_1_gpu_8
	layer_1_expert_select_9 -> layer_1_expert_2_gpu_9
	layer_1_expert_select_9 -> layer_1_expert_3_gpu_9
	layer_1_expert_select_10 -> layer_1_expert_4_gpu_10
	layer_1_expert_select_10 -> layer_1_expert_5_gpu_10
	layer_1_expert_select_11 -> layer_1_expert_6_gpu_11
	layer_1_expert_select_11 -> layer_1_expert_7_gpu_11
	layer_1_expert_select_12 -> layer_1_expert_8_gpu_12
	layer_1_expert_select_12 -> layer_1_expert_9_gpu_12
	layer_1_expert_select_13 -> layer_1_expert_10_gpu_13
	layer_1_expert_select_13 -> layer_1_expert_11_gpu_13
	layer_1_expert_select_14 -> layer_1_expert_12_gpu_14
	layer_1_expert_select_14 -> layer_1_expert_13_gpu_14
	layer_1_expert_select_15 -> layer_1_expert_14_gpu_15
	layer_1_expert_select_15 -> layer_1_expert_15_gpu_15
	layer_1_expert_0_gpu_8 -> layer_1_expert_agg_8
	layer_1_expert_1_gpu_8 -> layer_1_expert_agg_8
	layer_1_expert_2_gpu_9 -> layer_1_expert_agg_9
	layer_1_expert_3_gpu_9 -> layer_1_expert_agg_9
	layer_1_expert_4_gpu_10 -> layer_1_expert_agg_10
	layer_1_expert_5_gpu_10 -> layer_1_expert_agg_10
	layer_1_expert_6_gpu_11 -> layer_1_expert_agg_11
	layer_1_expert_7_gpu_11 -> layer_1_expert_agg_11
	layer_1_expert_8_gpu_12 -> layer_1_expert_agg_12
	layer_1_expert_9_gpu_12 -> layer_1_expert_agg_12
	layer_1_expert_10_gpu_13 -> layer_1_expert_agg_13
	layer_1_expert_11_gpu_13 -> layer_1_expert_agg_13
	layer_1_expert_12_gpu_14 -> layer_1_expert_agg_14
	layer_1_expert_13_gpu_14 -> layer_1_expert_agg_14
	layer_1_expert_14_gpu_15 -> layer_1_expert_agg_15
	layer_1_expert_15_gpu_15 -> layer_1_expert_agg_15
	layer_1_expert_agg_8 -> layer_1_expert_broadcast_8
	layer_1_expert_agg_9 -> layer_1_expert_broadcast_9
	layer_1_expert_agg_10 -> layer_1_expert_broadcast_10
	layer_1_expert_agg_11 -> layer_1_expert_broadcast_11
	layer_1_expert_agg_12 -> layer_1_expert_broadcast_12
	layer_1_expert_agg_13 -> layer_1_expert_broadcast_13
	layer_1_expert_agg_14 -> layer_1_expert_broadcast_14
	layer_1_expert_agg_15 -> layer_1_expert_broadcast_15
	layer_1_expert_broadcast_0 -> layer_1_residual2
	layer_1_expert_broadcast_1 -> layer_1_residual2
	layer_1_expert_broadcast_2 -> layer_1_residual2
	layer_1_expert_broadcast_3 -> layer_1_residual2
	layer_1_expert_broadcast_4 -> layer_1_residual2
	layer_1_expert_broadcast_5 -> layer_1_residual2
	layer_1_expert_broadcast_6 -> layer_1_residual2
	layer_1_expert_broadcast_7 -> layer_1_residual2
	layer_1_expert_broadcast_8 -> layer_1_residual2
	layer_1_expert_broadcast_9 -> layer_1_residual2
	layer_1_expert_broadcast_10 -> layer_1_residual2
	layer_1_expert_broadcast_11 -> layer_1_residual2
	layer_1_expert_broadcast_12 -> layer_1_residual2
	layer_1_expert_broadcast_13 -> layer_1_residual2
	layer_1_expert_broadcast_14 -> layer_1_residual2
	layer_1_expert_broadcast_15 -> layer_1_residual2
	layer_1_residual2 -> layer_2_ln1
	layer_2_ln1 -> layer_2_qkv_gpu_0
	layer_2_ln1 -> layer_2_qkv_gpu_1
	layer_2_ln1 -> layer_2_qkv_gpu_2
	layer_2_ln1 -> layer_2_qkv_gpu_3
	layer_2_ln1 -> layer_2_qkv_gpu_4
	layer_2_ln1 -> layer_2_qkv_gpu_5
	layer_2_ln1 -> layer_2_qkv_gpu_6
	layer_2_ln1 -> layer_2_qkv_gpu_7
	layer_2_qkv_gpu_0 -> layer_2_attn_gpu_0
	layer_2_qkv_gpu_1 -> layer_2_attn_gpu_1
	layer_2_qkv_gpu_2 -> layer_2_attn_gpu_2
	layer_2_qkv_gpu_3 -> layer_2_attn_gpu_3
	layer_2_qkv_gpu_4 -> layer_2_attn_gpu_4
	layer_2_qkv_gpu_5 -> layer_2_attn_gpu_5
	layer_2_qkv_gpu_6 -> layer_2_attn_gpu_6
	layer_2_qkv_gpu_7 -> layer_2_attn_gpu_7
	layer_2_attn_gpu_0 -> layer_2_attn_allreduce_0
	layer_2_attn_gpu_1 -> layer_2_attn_allreduce_1
	layer_2_attn_gpu_2 -> layer_2_attn_allreduce_2
	layer_2_attn_gpu_3 -> layer_2_attn_allreduce_3
	layer_2_attn_gpu_4 -> layer_2_attn_allreduce_4
	layer_2_attn_gpu_5 -> layer_2_attn_allreduce_5
	layer_2_attn_gpu_6 -> layer_2_attn_allreduce_6
	layer_2_attn_gpu_7 -> layer_2_attn_allreduce_7
	layer_2_attn_allreduce_0 -> layer_2_attn_proj_0
	layer_2_attn_allreduce_1 -> layer_2_attn_proj_1
	layer_2_attn_allreduce_2 -> layer_2_attn_proj_2
	layer_2_attn_allreduce_3 -> layer_2_attn_proj_3
	layer_2_attn_allreduce_4 -> layer_2_attn_proj_4
	layer_2_attn_allreduce_5 -> layer_2_attn_proj_5
	layer_2_attn_allreduce_6 -> layer_2_attn_proj_6
	layer_2_attn_allreduce_7 -> layer_2_attn_proj_7
	layer_2_attn_proj_0 -> layer_2_recv_attn_8
	layer_2_attn_proj_0 -> layer_2_recv_attn_9
	layer_2_attn_proj_0 -> layer_2_recv_attn_10
	layer_2_attn_proj_0 -> layer_2_recv_attn_11
	layer_2_attn_proj_0 -> layer_2_recv_attn_12
	layer_2_attn_proj_0 -> layer_2_recv_attn_13
	layer_2_attn_proj_0 -> layer_2_recv_attn_14
	layer_2_attn_proj_0 -> layer_2_recv_attn_15
	layer_2_attn_proj_1 -> layer_2_recv_attn_8
	layer_2_attn_proj_1 -> layer_2_recv_attn_9
	layer_2_attn_proj_1 -> layer_2_recv_attn_10
	layer_2_attn_proj_1 -> layer_2_recv_attn_11
	layer_2_attn_proj_1 -> layer_2_recv_attn_12
	layer_2_attn_proj_1 -> layer_2_recv_attn_13
	layer_2_attn_proj_1 -> layer_2_recv_attn_14
	layer_2_attn_proj_1 -> layer_2_recv_attn_15
	layer_2_attn_proj_2 -> layer_2_recv_attn_8
	layer_2_attn_proj_2 -> layer_2_recv_attn_9
	layer_2_attn_proj_2 -> layer_2_recv_attn_10
	layer_2_attn_proj_2 -> layer_2_recv_attn_11
	layer_2_attn_proj_2 -> layer_2_recv_attn_12
	layer_2_attn_proj_2 -> layer_2_recv_attn_13
	layer_2_attn_proj_2 -> layer_2_recv_attn_14
	layer_2_attn_proj_2 -> layer_2_recv_attn_15
	layer_2_attn_proj_3 -> layer_2_recv_attn_8
	layer_2_attn_proj_3 -> layer_2_recv_attn_9
	layer_2_attn_proj_3 -> layer_2_recv_attn_10
	layer_2_attn_proj_3 -> layer_2_recv_attn_11
	layer_2_attn_proj_3 -> layer_2_recv_attn_12
	layer_2_attn_proj_3 -> layer_2_recv_attn_13
	layer_2_attn_proj_3 -> layer_2_recv_attn_14
	layer_2_attn_proj_3 -> layer_2_recv_attn_15
	layer_2_attn_proj_4 -> layer_2_recv_attn_8
	layer_2_attn_proj_4 -> layer_2_recv_attn_9
	layer_2_attn_proj_4 -> layer_2_recv_attn_10
	layer_2_attn_proj_4 -> layer_2_recv_attn_11
	layer_2_attn_proj_4 -> layer_2_recv_attn_12
	layer_2_attn_proj_4 -> layer_2_recv_attn_13
	layer_2_attn_proj_4 -> layer_2_recv_attn_14
	layer_2_attn_proj_4 -> layer_2_recv_attn_15
	layer_2_attn_proj_5 -> layer_2_recv_attn_8
	layer_2_attn_proj_5 -> layer_2_recv_attn_9
	layer_2_attn_proj_5 -> layer_2_recv_attn_10
	layer_2_attn_proj_5 -> layer_2_recv_attn_11
	layer_2_attn_proj_5 -> layer_2_recv_attn_12
	layer_2_attn_proj_5 -> layer_2_recv_attn_13
	layer_2_attn_proj_5 -> layer_2_recv_attn_14
	layer_2_attn_proj_5 -> layer_2_recv_attn_15
	layer_2_attn_proj_6 -> layer_2_recv_attn_8
	layer_2_attn_proj_6 -> layer_2_recv_attn_9
	layer_2_attn_proj_6 -> layer_2_recv_attn_10
	layer_2_attn_proj_6 -> layer_2_recv_attn_11
	layer_2_attn_proj_6 -> layer_2_recv_attn_12
	layer_2_attn_proj_6 -> layer_2_recv_attn_13
	layer_2_attn_proj_6 -> layer_2_recv_attn_14
	layer_2_attn_proj_6 -> layer_2_recv_attn_15
	layer_2_attn_proj_7 -> layer_2_recv_attn_8
	layer_2_attn_proj_7 -> layer_2_recv_attn_9
	layer_2_attn_proj_7 -> layer_2_recv_attn_10
	layer_2_attn_proj_7 -> layer_2_recv_attn_11
	layer_2_attn_proj_7 -> layer_2_recv_attn_12
	layer_2_attn_proj_7 -> layer_2_recv_attn_13
	layer_2_attn_proj_7 -> layer_2_recv_attn_14
	layer_2_attn_proj_7 -> layer_2_recv_attn_15
	layer_2_recv_attn_8 -> layer_2_residual1
	layer_2_recv_attn_9 -> layer_2_residual1
	layer_2_recv_attn_10 -> layer_2_residual1
	layer_2_recv_attn_11 -> layer_2_residual1
	layer_2_recv_attn_12 -> layer_2_residual1
	layer_2_recv_attn_13 -> layer_2_residual1
	layer_2_recv_attn_14 -> layer_2_residual1
	layer_2_recv_attn_15 -> layer_2_residual1
	layer_1_residual2 -> layer_2_residual1
	layer_2_residual1 -> layer_2_ln2
	layer_2_ln2 -> layer_2_gate
	layer_2_gate -> layer_2_expert_select_8 [style=dashed]
	layer_2_gate -> layer_2_expert_select_9 [style=dashed]
	layer_2_gate -> layer_2_expert_select_10 [style=dashed]
	layer_2_gate -> layer_2_expert_select_11 [style=dashed]
	layer_2_gate -> layer_2_expert_select_12 [style=dashed]
	layer_2_gate -> layer_2_expert_select_13 [style=dashed]
	layer_2_gate -> layer_2_expert_select_14 [style=dashed]
	layer_2_gate -> layer_2_expert_select_15 [style=dashed]
	layer_2_expert_select_8 -> layer_2_expert_0_gpu_8
	layer_2_expert_select_8 -> layer_2_expert_1_gpu_8
	layer_2_expert_select_9 -> layer_2_expert_2_gpu_9
	layer_2_expert_select_9 -> layer_2_expert_3_gpu_9
	layer_2_expert_select_10 -> layer_2_expert_4_gpu_10
	layer_2_expert_select_10 -> layer_2_expert_5_gpu_10
	layer_2_expert_select_11 -> layer_2_expert_6_gpu_11
	layer_2_expert_select_11 -> layer_2_expert_7_gpu_11
	layer_2_expert_select_12 -> layer_2_expert_8_gpu_12
	layer_2_expert_select_12 -> layer_2_expert_9_gpu_12
	layer_2_expert_select_13 -> layer_2_expert_10_gpu_13
	layer_2_expert_select_13 -> layer_2_expert_11_gpu_13
	layer_2_expert_select_14 -> layer_2_expert_12_gpu_14
	layer_2_expert_select_14 -> layer_2_expert_13_gpu_14
	layer_2_expert_select_15 -> layer_2_expert_14_gpu_15
	layer_2_expert_select_15 -> layer_2_expert_15_gpu_15
	layer_2_expert_0_gpu_8 -> layer_2_expert_agg_8
	layer_2_expert_1_gpu_8 -> layer_2_expert_agg_8
	layer_2_expert_2_gpu_9 -> layer_2_expert_agg_9
	layer_2_expert_3_gpu_9 -> layer_2_expert_agg_9
	layer_2_expert_4_gpu_10 -> layer_2_expert_agg_10
	layer_2_expert_5_gpu_10 -> layer_2_expert_agg_10
	layer_2_expert_6_gpu_11 -> layer_2_expert_agg_11
	layer_2_expert_7_gpu_11 -> layer_2_expert_agg_11
	layer_2_expert_8_gpu_12 -> layer_2_expert_agg_12
	layer_2_expert_9_gpu_12 -> layer_2_expert_agg_12
	layer_2_expert_10_gpu_13 -> layer_2_expert_agg_13
	layer_2_expert_11_gpu_13 -> layer_2_expert_agg_13
	layer_2_expert_12_gpu_14 -> layer_2_expert_agg_14
	layer_2_expert_13_gpu_14 -> layer_2_expert_agg_14
	layer_2_expert_14_gpu_15 -> layer_2_expert_agg_15
	layer_2_expert_15_gpu_15 -> layer_2_expert_agg_15
	layer_2_expert_agg_8 -> layer_2_expert_broadcast_8
	layer_2_expert_agg_9 -> layer_2_expert_broadcast_9
	layer_2_expert_agg_10 -> layer_2_expert_broadcast_10
	layer_2_expert_agg_11 -> layer_2_expert_broadcast_11
	layer_2_expert_agg_12 -> layer_2_expert_broadcast_12
	layer_2_expert_agg_13 -> layer_2_expert_broadcast_13
	layer_2_expert_agg_14 -> layer_2_expert_broadcast_14
	layer_2_expert_agg_15 -> layer_2_expert_broadcast_15
	layer_2_expert_broadcast_0 -> layer_2_residual2
	layer_2_expert_broadcast_1 -> layer_2_residual2
	layer_2_expert_broadcast_2 -> layer_2_residual2
	layer_2_expert_broadcast_3 -> layer_2_residual2
	layer_2_expert_broadcast_4 -> layer_2_residual2
	layer_2_expert_broadcast_5 -> layer_2_residual2
	layer_2_expert_broadcast_6 -> layer_2_residual2
	layer_2_expert_broadcast_7 -> layer_2_residual2
	layer_2_expert_broadcast_8 -> layer_2_residual2
	layer_2_expert_broadcast_9 -> layer_2_residual2
	layer_2_expert_broadcast_10 -> layer_2_residual2
	layer_2_expert_broadcast_11 -> layer_2_residual2
	layer_2_expert_broadcast_12 -> layer_2_residual2
	layer_2_expert_broadcast_13 -> layer_2_residual2
	layer_2_expert_broadcast_14 -> layer_2_residual2
	layer_2_expert_broadcast_15 -> layer_2_residual2
	layer_2_residual2 -> layer_3_ln1
	layer_3_ln1 -> layer_3_qkv_gpu_0
	layer_3_ln1 -> layer_3_qkv_gpu_1
	layer_3_ln1 -> layer_3_qkv_gpu_2
	layer_3_ln1 -> layer_3_qkv_gpu_3
	layer_3_ln1 -> layer_3_qkv_gpu_4
	layer_3_ln1 -> layer_3_qkv_gpu_5
	layer_3_ln1 -> layer_3_qkv_gpu_6
	layer_3_ln1 -> layer_3_qkv_gpu_7
	layer_3_qkv_gpu_0 -> layer_3_attn_gpu_0
	layer_3_qkv_gpu_1 -> layer_3_attn_gpu_1
	layer_3_qkv_gpu_2 -> layer_3_attn_gpu_2
	layer_3_qkv_gpu_3 -> layer_3_attn_gpu_3
	layer_3_qkv_gpu_4 -> layer_3_attn_gpu_4
	layer_3_qkv_gpu_5 -> layer_3_attn_gpu_5
	layer_3_qkv_gpu_6 -> layer_3_attn_gpu_6
	layer_3_qkv_gpu_7 -> layer_3_attn_gpu_7
	layer_3_attn_gpu_0 -> layer_3_attn_allreduce_0
	layer_3_attn_gpu_1 -> layer_3_attn_allreduce_1
	layer_3_attn_gpu_2 -> layer_3_attn_allreduce_2
	layer_3_attn_gpu_3 -> layer_3_attn_allreduce_3
	layer_3_attn_gpu_4 -> layer_3_attn_allreduce_4
	layer_3_attn_gpu_5 -> layer_3_attn_allreduce_5
	layer_3_attn_gpu_6 -> layer_3_attn_allreduce_6
	layer_3_attn_gpu_7 -> layer_3_attn_allreduce_7
	layer_3_attn_allreduce_0 -> layer_3_attn_proj_0
	layer_3_attn_allreduce_1 -> layer_3_attn_proj_1
	layer_3_attn_allreduce_2 -> layer_3_attn_proj_2
	layer_3_attn_allreduce_3 -> layer_3_attn_proj_3
	layer_3_attn_allreduce_4 -> layer_3_attn_proj_4
	layer_3_attn_allreduce_5 -> layer_3_attn_proj_5
	layer_3_attn_allreduce_6 -> layer_3_attn_proj_6
	layer_3_attn_allreduce_7 -> layer_3_attn_proj_7
	layer_3_attn_proj_0 -> layer_3_recv_attn_8
	layer_3_attn_proj_0 -> layer_3_recv_attn_9
	layer_3_attn_proj_0 -> layer_3_recv_attn_10
	layer_3_attn_proj_0 -> layer_3_recv_attn_11
	layer_3_attn_proj_0 -> layer_3_recv_attn_12
	layer_3_attn_proj_0 -> layer_3_recv_attn_13
	layer_3_attn_proj_0 -> layer_3_recv_attn_14
	layer_3_attn_proj_0 -> layer_3_recv_attn_15
	layer_3_attn_proj_1 -> layer_3_recv_attn_8
	layer_3_attn_proj_1 -> layer_3_recv_attn_9
	layer_3_attn_proj_1 -> layer_3_recv_attn_10
	layer_3_attn_proj_1 -> layer_3_recv_attn_11
	layer_3_attn_proj_1 -> layer_3_recv_attn_12
	layer_3_attn_proj_1 -> layer_3_recv_attn_13
	layer_3_attn_proj_1 -> layer_3_recv_attn_14
	layer_3_attn_proj_1 -> layer_3_recv_attn_15
	layer_3_attn_proj_2 -> layer_3_recv_attn_8
	layer_3_attn_proj_2 -> layer_3_recv_attn_9
	layer_3_attn_proj_2 -> layer_3_recv_attn_10
	layer_3_attn_proj_2 -> layer_3_recv_attn_11
	layer_3_attn_proj_2 -> layer_3_recv_attn_12
	layer_3_attn_proj_2 -> layer_3_recv_attn_13
	layer_3_attn_proj_2 -> layer_3_recv_attn_14
	layer_3_attn_proj_2 -> layer_3_recv_attn_15
	layer_3_attn_proj_3 -> layer_3_recv_attn_8
	layer_3_attn_proj_3 -> layer_3_recv_attn_9
	layer_3_attn_proj_3 -> layer_3_recv_attn_10
	layer_3_attn_proj_3 -> layer_3_recv_attn_11
	layer_3_attn_proj_3 -> layer_3_recv_attn_12
	layer_3_attn_proj_3 -> layer_3_recv_attn_13
	layer_3_attn_proj_3 -> layer_3_recv_attn_14
	layer_3_attn_proj_3 -> layer_3_recv_attn_15
	layer_3_attn_proj_4 -> layer_3_recv_attn_8
	layer_3_attn_proj_4 -> layer_3_recv_attn_9
	layer_3_attn_proj_4 -> layer_3_recv_attn_10
	layer_3_attn_proj_4 -> layer_3_recv_attn_11
	layer_3_attn_proj_4 -> layer_3_recv_attn_12
	layer_3_attn_proj_4 -> layer_3_recv_attn_13
	layer_3_attn_proj_4 -> layer_3_recv_attn_14
	layer_3_attn_proj_4 -> layer_3_recv_attn_15
	layer_3_attn_proj_5 -> layer_3_recv_attn_8
	layer_3_attn_proj_5 -> layer_3_recv_attn_9
	layer_3_attn_proj_5 -> layer_3_recv_attn_10
	layer_3_attn_proj_5 -> layer_3_recv_attn_11
	layer_3_attn_proj_5 -> layer_3_recv_attn_12
	layer_3_attn_proj_5 -> layer_3_recv_attn_13
	layer_3_attn_proj_5 -> layer_3_recv_attn_14
	layer_3_attn_proj_5 -> layer_3_recv_attn_15
	layer_3_attn_proj_6 -> layer_3_recv_attn_8
	layer_3_attn_proj_6 -> layer_3_recv_attn_9
	layer_3_attn_proj_6 -> layer_3_recv_attn_10
	layer_3_attn_proj_6 -> layer_3_recv_attn_11
	layer_3_attn_proj_6 -> layer_3_recv_attn_12
	layer_3_attn_proj_6 -> layer_3_recv_attn_13
	layer_3_attn_proj_6 -> layer_3_recv_attn_14
	layer_3_attn_proj_6 -> layer_3_recv_attn_15
	layer_3_attn_proj_7 -> layer_3_recv_attn_8
	layer_3_attn_proj_7 -> layer_3_recv_attn_9
	layer_3_attn_proj_7 -> layer_3_recv_attn_10
	layer_3_attn_proj_7 -> layer_3_recv_attn_11
	layer_3_attn_proj_7 -> layer_3_recv_attn_12
	layer_3_attn_proj_7 -> layer_3_recv_attn_13
	layer_3_attn_proj_7 -> layer_3_recv_attn_14
	layer_3_attn_proj_7 -> layer_3_recv_attn_15
	layer_3_recv_attn_8 -> layer_3_residual1
	layer_3_recv_attn_9 -> layer_3_residual1
	layer_3_recv_attn_10 -> layer_3_residual1
	layer_3_recv_attn_11 -> layer_3_residual1
	layer_3_recv_attn_12 -> layer_3_residual1
	layer_3_recv_attn_13 -> layer_3_residual1
	layer_3_recv_attn_14 -> layer_3_residual1
	layer_3_recv_attn_15 -> layer_3_residual1
	layer_2_residual2 -> layer_3_residual1
	layer_3_residual1 -> layer_3_ln2
	layer_3_ln2 -> layer_3_gate
	layer_3_gate -> layer_3_expert_select_8 [style=dashed]
	layer_3_gate -> layer_3_expert_select_9 [style=dashed]
	layer_3_gate -> layer_3_expert_select_10 [style=dashed]
	layer_3_gate -> layer_3_expert_select_11 [style=dashed]
	layer_3_gate -> layer_3_expert_select_12 [style=dashed]
	layer_3_gate -> layer_3_expert_select_13 [style=dashed]
	layer_3_gate -> layer_3_expert_select_14 [style=dashed]
	layer_3_gate -> layer_3_expert_select_15 [style=dashed]
	layer_3_expert_select_8 -> layer_3_expert_0_gpu_8
	layer_3_expert_select_8 -> layer_3_expert_1_gpu_8
	layer_3_expert_select_9 -> layer_3_expert_2_gpu_9
	layer_3_expert_select_9 -> layer_3_expert_3_gpu_9
	layer_3_expert_select_10 -> layer_3_expert_4_gpu_10
	layer_3_expert_select_10 -> layer_3_expert_5_gpu_10
	layer_3_expert_select_11 -> layer_3_expert_6_gpu_11
	layer_3_expert_select_11 -> layer_3_expert_7_gpu_11
	layer_3_expert_select_12 -> layer_3_expert_8_gpu_12
	layer_3_expert_select_12 -> layer_3_expert_9_gpu_12
	layer_3_expert_select_13 -> layer_3_expert_10_gpu_13
	layer_3_expert_select_13 -> layer_3_expert_11_gpu_13
	layer_3_expert_select_14 -> layer_3_expert_12_gpu_14
	layer_3_expert_select_14 -> layer_3_expert_13_gpu_14
	layer_3_expert_select_15 -> layer_3_expert_14_gpu_15
	layer_3_expert_select_15 -> layer_3_expert_15_gpu_15
	layer_3_expert_0_gpu_8 -> layer_3_expert_agg_8
	layer_3_expert_1_gpu_8 -> layer_3_expert_agg_8
	layer_3_expert_2_gpu_9 -> layer_3_expert_agg_9
	layer_3_expert_3_gpu_9 -> layer_3_expert_agg_9
	layer_3_expert_4_gpu_10 -> layer_3_expert_agg_10
	layer_3_expert_5_gpu_10 -> layer_3_expert_agg_10
	layer_3_expert_6_gpu_11 -> layer_3_expert_agg_11
	layer_3_expert_7_gpu_11 -> layer_3_expert_agg_11
	layer_3_expert_8_gpu_12 -> layer_3_expert_agg_12
	layer_3_expert_9_gpu_12 -> layer_3_expert_agg_12
	layer_3_expert_10_gpu_13 -> layer_3_expert_agg_13
	layer_3_expert_11_gpu_13 -> layer_3_expert_agg_13
	layer_3_expert_12_gpu_14 -> layer_3_expert_agg_14
	layer_3_expert_13_gpu_14 -> layer_3_expert_agg_14
	layer_3_expert_14_gpu_15 -> layer_3_expert_agg_15
	layer_3_expert_15_gpu_15 -> layer_3_expert_agg_15
	layer_3_expert_agg_8 -> layer_3_expert_broadcast_8
	layer_3_expert_agg_9 -> layer_3_expert_broadcast_9
	layer_3_expert_agg_10 -> layer_3_expert_broadcast_10
	layer_3_expert_agg_11 -> layer_3_expert_broadcast_11
	layer_3_expert_agg_12 -> layer_3_expert_broadcast_12
	layer_3_expert_agg_13 -> layer_3_expert_broadcast_13
	layer_3_expert_agg_14 -> layer_3_expert_broadcast_14
	layer_3_expert_agg_15 -> layer_3_expert_broadcast_15
	layer_3_expert_broadcast_0 -> layer_3_residual2
	layer_3_expert_broadcast_1 -> layer_3_residual2
	layer_3_expert_broadcast_2 -> layer_3_residual2
	layer_3_expert_broadcast_3 -> layer_3_residual2
	layer_3_expert_broadcast_4 -> layer_3_residual2
	layer_3_expert_broadcast_5 -> layer_3_residual2
	layer_3_expert_broadcast_6 -> layer_3_residual2
	layer_3_expert_broadcast_7 -> layer_3_residual2
	layer_3_expert_broadcast_8 -> layer_3_residual2
	layer_3_expert_broadcast_9 -> layer_3_residual2
	layer_3_expert_broadcast_10 -> layer_3_residual2
	layer_3_expert_broadcast_11 -> layer_3_residual2
	layer_3_expert_broadcast_12 -> layer_3_residual2
	layer_3_expert_broadcast_13 -> layer_3_residual2
	layer_3_expert_broadcast_14 -> layer_3_residual2
	layer_3_expert_broadcast_15 -> layer_3_residual2
	layer_3_residual2 -> output
}
