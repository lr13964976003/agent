{
  "generated_dags": {
    "complete_dag": {
      "dot_file": "./generated_docs/helix/helix_complete_dag.dot",
      "svg_file": "./generated_docs/helix/helix_complete_dag.svg",
      "description": "Complete Helix deployment DAG showing two-level attention partitioning across 16 GPUs for a 2-layer transformer model"
    },
    "layer1_dag": {
      "dot_file": "./generated_docs/helix/helix_layer1_dag.dot",
      "description": "Detailed Layer 1 DAG showing Multi-Head Attention and MLP operations with Helix partitioning"
    },
    "layer2_dag": {
      "dot_file": "./generated_docs/helix/helix_layer2_dag.dot", 
      "description": "Detailed Layer 2 DAG showing Multi-Head Attention and MLP operations with Helix partitioning"
    }
  },
  "deployment_summary": {
    "total_gpus": 16,
    "partitioning_strategy": "Helix Two-Level Attention Partitioning",
    "head_groups": 4,
    "dimension_slices": 4,
    "total_partitions": 16,
    "batch_size": 1024,
    "sequence_length": 10000,
    "hidden_dimension": 8192,
    "heads_per_group": 4,
    "dimension_per_slice": 128,
    "model_layers": 2,
    "attention_heads": 16
  },
  "key_features": [
    "Card Boundary Division (16 GPUs specified for each node)",
    "Multi-Card Communication Path Simulation (split/concat/all-reduce nodes)",
    "Data aggregation and split represented by routing/aggregation nodes",
    "No loss of dimensional information - all tensor dimensions specified",
    "Perfect alignment of local and global dimensions",
    "No module simplification - complete operator-level representation",
    "GPU load balancing across 16 devices",
    "Complete DAG from input to output for throughput/latency evaluation"
  ],
  "verification": {
    "dag_cycles": "none",
    "node_count": 289,
    "edge_count": 528,
    "gpu_assignment": "complete",
    "dimension_tracking": "verified"
  }
}