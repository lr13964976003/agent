{
  "deployment_summary": {
    "model_architecture": {
      "layers": 4,
      "experts_per_layer": 16,
      "total_experts": 64,
      "total_gpus": 16,
      "expert_parallelism": 16,
      "experts_per_gpu": 4
    },
    "gpu_assignment_strategy": {
      "description": "Round-robin placement ensuring one expert per layer per GPU",
      "assignment_pattern": "GPU ID = expert_idx + (layer_idx * 16) % 16",
      "gpu_usage": "Each GPU hosts 4 experts (1 per layer)"
    },
    "expert_distribution_by_gpu": {
      "GPU_0": ["layer_0_expert_0", "layer_1_expert_0", "layer_2_expert_0", "layer_3_expert_0"],
      "GPU_1": ["layer_0_expert_1", "layer_1_expert_1", "layer_2_expert_1", "layer_3_expert_1"],
      "GPU_2": ["layer_0_expert_2", "layer_1_expert_2", "layer_2_expert_2", "layer_3_expert_2"],
      "GPU_3": ["layer_0_expert_3", "layer_1_expert_3", "layer_2_expert_3", "layer_3_expert_3"],
      "GPU_4": ["layer_0_expert_4", "layer_1_expert_4", "layer_2_expert_4", "layer_3_expert_4"],
      "GPU_5": ["layer_0_expert_5", "layer_1_expert_5", "layer_2_expert_5", "layer_3_expert_5"],
      "GPU_6": ["layer_0_expert_6", "layer_1_expert_6", "layer_2_expert_6", "layer_3_expert_6"],
      "GPU_7": ["layer_0_expert_7", "layer_1_expert_7", "layer_2_expert_7", "layer_3_expert_7"],
      "GPU_8": ["layer_0_expert_8", "layer_1_expert_8", "layer_2_expert_8", "layer_3_expert_8"],
      "GPU_9": ["layer_0_expert_9", "layer_1_expert_9", "layer_2_expert_9", "layer_3_expert_9"],
      "GPU_10": ["layer_0_expert_10", "layer_1_expert_10", "layer_2_expert_10", "layer_3_expert_10"],
      "GPU_11": ["layer_0_expert_11", "layer_1_expert_11", "layer_2_expert_11", "layer_3_expert_11"],
      "GPU_12": ["layer_0_expert_12", "layer_1_expert_12", "layer_2_expert_12", "layer_3_expert_12"],
      "GPU_13": ["layer_0_expert_13", "layer_1_expert_13", "layer_2_expert_13", "layer_3_expert_13"],
      "GPU_14": ["layer_0_expert_14", "layer_1_expert_14", "layer_2_expert_14", "layer_3_expert_14"],
      "GPU_15": ["layer_0_expert_15", "layer_1_expert_15", "layer_2_expert_15", "layer_3_expert_15"]
    },
    "tensor_dimensions": {
      "model_input": "[batch_size=1024, seq_len=10000, hidden_size=8192]",
      "mha_qkv_output": "[batch_size=1024, seq_len=10000, heads=16, d_k=512]",
      "mha_final_output": "[batch_size=1024, seq_len=10000, hidden_size=8192]",
      "expert_gate_output": "[batch_size=variable, ffn_hidden=32768]",
      "expert_up_output": "[batch_size=variable, ffn_hidden=32768]",
      "expert_down_output": "[batch_size=variable, hidden_size=8192]",
      "model_output": "[batch_size=1024, seq_len=10000, hidden_size=8192]"
    },
    "communication_patterns": {
      "inter_gpu": "Token routing between GPUs for expert processing",
      "intra_gpu": "Sequential execution of expert components on same GPU",
      "load_balancing": "Dynamic gating ensures balanced expert utilization"
    },
    "verification_metrics": {
      "total_nodes": 324,
      "total_edges": 483,
      "cycle_check": "passed",
      "gpu_load_balance": "even (4 experts per GPU across 4 layers)",
      "expert_parallelism": "16 (one expert per GPU per layer)"
    }
  }
}