digraph {
	graph [bb="0,0,78839,20877",
		nodesep=0.8,
		rankdir=TB,
		ranksep=1.0,
		splines=ortho
	];
	node [label="\N",
		style=filled
	];
	Input_Total	[fillcolor=lightblue,
		height=1.041,
		label="Input\nInput: [batch_size=1024, seq_len=10000, d_model=8192]\nOutput: [batch_size=1024, seq_len=10000, d_model=8192]",
		pos="4787,20839",
		shape=ellipse,
		width=8.721];
	Sequence_Split	[fillcolor=lightyellow,
		height=1.4722,
		label="Sequence Split\nInput: [batch_size=1024, seq_len=10000, d_model=8192]\nOutput: 16Ã—[batch_size=1024, seq_len=625, d_model=8192]",
		pos="4787,20676",
		shape=parallelogram,
		width=13.132];
	Input_Total -> Sequence_Split	[pos="e,4787,20729 4787,20802 4787,20802 4787,20739 4787,20739"];
	Layer0_Device0_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4787,20512",
		shape=ellipse,
		width=8.3674];
	Sequence_Split -> Layer0_Device0_Input	[pos="e,4787,20550 4787,20623 4787,20623 4787,20560 4787,20560"];
	Layer0_Device0_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4398,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device0_Input -> Layer0_Device0_LayerNorm1	[pos="e,4548.4,20402 4548.4,20489 4548.4,20489 4548.4,20412 4548.4,20412"];
	Layer0_Device0_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="4846,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device0_Input -> Layer0_Device0_Residual1	[pos="e,4937.4,16558 4937.4,20479 4937.4,20479 4937.4,16568 4937.4,16568"];
	Layer0_Device0_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3808,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device0_LayerNorm1 -> Layer0_Device0_QKVProj	[pos="e,4321.2,20276 4321.2,20348 4321.2,20348 4321.2,20286 4321.2,20286"];
	Layer0_Device0_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2562,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage0_RecvKV	[label="Local K,V",
		lp="3062.5,20179",
		pos="e,3320.1,20136 3320.1,20222 3320.1,20222 3320.1,20146 3320.1,20146"];
	Layer0_Device0_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3352,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage0_Attention	[label=Q_local,
		lp="3624,20083",
		pos="e,3532.6,19917 3532.6,20222 3532.6,20222 3532.6,19927 3532.6,19927"];
	Layer0_Device0_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1022,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage1_Attention	[label=Q_local,
		lp="1086,19986",
		pos="e,1006.9,19723 3158.3,20231 2330.9,20231 1006.9,20231 1006.9,20231 1006.9,20231 1006.9,19733 1006.9,19733"];
	Layer0_Device0_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1633,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage2_Attention	[label=Q_local,
		lp="3764,19890",
		pos="e,1803.4,19530 3627.6,20222 3627.6,20123 3627.6,19778 3627.6,19778 3627.6,19778 1803.4,19778 1803.4,19778 1803.4,19778 1803.4,19540 \
1803.4,19540"];
	Layer0_Device0_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3648,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage3_Attention	[label=Q_local,
		lp="3904,19793",
		pos="e,3820.6,19337 3820.6,20223 3820.6,20223 3820.6,19347 3820.6,19347"];
	Layer0_Device0_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="854,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage4_Attention	[label=Q_local,
		lp="706,19697",
		pos="e,706.5,19144 3158.3,20240 2248.9,20240 706.5,20240 706.5,20240 706.5,20240 706.5,19154 706.5,19154"];
	Layer0_Device0_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3772,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage5_Attention	[label=Q_local,
		lp="4008,19600",
		pos="e,3941.5,18951 3941.5,20222 3941.5,20222 3941.5,18961 3941.5,18961"];
	Layer0_Device0_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3804,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage6_Attention	[label=Q_local,
		lp="4140,19504",
		pos="e,4019.5,18758 4019.5,20222 4019.5,20222 4019.5,18768 4019.5,18768"];
	Layer0_Device0_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="865,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage7_Attention	[label=Q_local,
		lp="560,19407",
		pos="e,633.45,18538 3158.4,20249 2210.4,20249 559.5,20249 559.5,20249 559.5,20249 559.5,18538 559.5,18538 559.5,18538 623.45,18538 623.45,\
18538"];
	Layer0_Device0_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3604,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage8_Attention	[label=Q_local,
		lp="4264,19311",
		pos="e,3835.7,18345 4081.3,20223 4081.3,19993 4081.3,18345 4081.3,18345 4081.3,18345 3845.7,18345 3845.7,18345"];
	Layer0_Device0_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="665,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage9_Attention	[label=Q_local,
		lp="442,19214",
		pos="e,496.5,18179 3158.2,20257 2193.9,20257 496.5,20257 496.5,20257 496.5,20257 496.5,18189 496.5,18189"];
	Layer0_Device0_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="795,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage10_Attention	[label=Q_local,
		lp="302,19118",
		pos="e,563.48,17959 3158.4,20266 2168.3,20266 395.5,20266 395.5,20266 395.5,20266 395.5,17959 395.5,17959 395.5,17959 553.48,17959 553.48,\
17959"];
	Layer0_Device0_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2136,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage11_Attention	[label=Q_local,
		lp="4385,19021",
		pos="e,2213.2,17794 4173,20222 4173,19960 4173,17840 4173,17840 4173,17840 2213.2,17840 2213.2,17840 2213.2,17840 2213.2,17804 2213.2,\
17804"];
	Layer0_Device0_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2425,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage12_Attention	[label=Q_local,
		lp="4528,18925",
		pos="e,2396.4,17600 4218.8,20222 4218.8,19959 4218.8,17838 4218.8,17838 4218.8,17838 2396.4,17838 2396.4,17838 2396.4,17838 2396.4,17610 \
2396.4,17610"];
	Layer0_Device0_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4440,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage13_Attention	[label=Q_local,
		lp="4668,18828",
		pos="e,4582.3,17408 4457.9,20235 4532.4,20235 4582.3,20235 4582.3,20235 4582.3,20235 4582.3,17418 4582.3,17418"];
	Layer0_Device0_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2022,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage14_Attention	[label=Q_local,
		lp="156,18732",
		pos="e,1790.3,17187 4127.1,20222 4127.1,19962 4127.1,17896 4127.1,17896 4127.1,17896 1638.9,17896 1638.9,17896 1638.9,17896 1638.9,17187 \
1638.9,17187 1638.9,17187 1780.3,17187 1780.3,17187"];
	Layer0_Device0_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4155,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_QKVProj -> Layer0_Device0_Stage15_Attention	[label=Q_local,
		lp="4786,18635",
		pos="e,4386.8,17043 4457.7,20249 4610.1,20249 4729.1,20249 4729.1,20249 4729.1,20249 4729.1,17043 4729.1,17043 4729.1,17043 4396.8,17043 \
4396.8,17043"];
	Layer0_Device0_Stage0_RecvKV -> Layer0_Device0_Stage0_Attention	[pos="e,3301.1,19917 3301.1,20082 3301.1,20082 3301.1,19927 3301.1,19927"];
	Layer0_Device0_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2143,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage0_RecvKV -> Layer0_Device0_Stage1_RecvKV	[label="Ring transfer",
		lp="2420,19986",
		pos="e,2352.5,19943 2352.5,20029 2352.5,20029 2352.5,19953 2352.5,19953"];
	Layer0_Device0_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="1543,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage0_Attention -> Layer0_Device0_Stage0_Accumulate	[pos="e,1543,19723 3352,19863 3352,19831 3352,19781 3352,19781 3352,19781 1543,19781 1543,19781 1543,19781 1543,19733 1543,19733"];
	Layer0_Device0_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1112,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage0_Accumulate -> Layer0_Device0_Stage1_Accumulate	[pos="e,1327.5,19530 1327.5,19670 1327.5,19670 1327.5,19540 1327.5,19540"];
	Layer0_Device0_Stage1_RecvKV -> Layer0_Device0_Stage1_Attention	[pos="e,1238.4,19724 1238.4,19889 1238.4,19889 1238.4,19734 1238.4,19734"];
	Layer0_Device0_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2752,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage1_RecvKV -> Layer0_Device0_Stage2_RecvKV	[label="Ring transfer",
		lp="2450,19793",
		pos="e,2447.5,19750 2447.5,19836 2447.5,19836 2447.5,19760 2447.5,19760"];
	Layer0_Device0_Stage1_Attention -> Layer0_Device0_Stage1_Accumulate	[pos="e,1067,19530 1067,19670 1067,19670 1067,19540 1067,19540"];
	Layer0_Device0_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1230,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage1_Accumulate -> Layer0_Device0_Stage2_Accumulate	[pos="e,1171,19337 1171,19477 1171,19477 1171,19347 1171,19347"];
	Layer0_Device0_Stage2_RecvKV -> Layer0_Device0_Stage2_Attention	[pos="e,1848.4,19531 1848.4,19696 1848.4,19696 1848.4,19541 1848.4,19541"];
	Layer0_Device0_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2842,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage2_RecvKV -> Layer0_Device0_Stage3_RecvKV	[label="Ring transfer",
		lp="2845,19600",
		pos="e,2797,19557 2797,19643 2797,19643 2797,19567 2797,19567"];
	Layer0_Device0_Stage2_Attention -> Layer0_Device0_Stage2_Accumulate	[pos="e,1431.5,19337 1431.5,19477 1431.5,19477 1431.5,19347 1431.5,19347"];
	Layer0_Device0_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1375,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage2_Accumulate -> Layer0_Device0_Stage3_Accumulate	[pos="e,1302.5,19144 1302.5,19284 1302.5,19284 1302.5,19154 1302.5,19154"];
	Layer0_Device0_Stage3_RecvKV -> Layer0_Device0_Stage3_Attention	[pos="e,3589.1,19338 3589.1,19503 3589.1,19503 3589.1,19348 3589.1,19348"];
	Layer0_Device0_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2439,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage3_RecvKV -> Layer0_Device0_Stage4_RecvKV	[label="Ring transfer",
		lp="2623,19407",
		pos="e,2640.5,19364 2640.5,19450 2640.5,19450 2640.5,19374 2640.5,19374"];
	Layer0_Device0_Stage3_Attention -> Layer0_Device0_Stage3_Accumulate	[pos="e,1562.9,19144 3460.1,19284 3460.1,19244 3460.1,19175 3460.1,19175 3460.1,19175 1562.9,19175 1562.9,19175 1562.9,19175 1562.9,19154 \
1562.9,19154"];
	Layer0_Device0_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1354,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage3_Accumulate -> Layer0_Device0_Stage4_Accumulate	[pos="e,1364.5,18951 1364.5,19091 1364.5,19091 1364.5,18961 1364.5,18961"];
	Layer0_Device0_Stage4_RecvKV -> Layer0_Device0_Stage4_Attention	[pos="e,1042,19144 1563.2,19270 1282.1,19270 1042,19270 1042,19270 1042,19270 1042,19154 1042,19154"];
	Layer0_Device0_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2584,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage4_RecvKV -> Layer0_Device0_Stage5_RecvKV	[label="Ring transfer",
		lp="2559,19214",
		pos="e,2511.5,19171 2511.5,19257 2511.5,19257 2511.5,19181 2511.5,19181"];
	Layer0_Device0_Stage4_Attention -> Layer0_Device0_Stage4_Accumulate	[pos="e,1133,18951 1085.7,19117 1113.9,19117 1133,19117 1133,19117 1133,19117 1133,18961 1133,18961"];
	Layer0_Device0_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3283,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage4_Accumulate -> Layer0_Device0_Stage5_Accumulate	[pos="e,3267.1,18758 1369.9,18898 1369.9,18872 1369.9,18837 1369.9,18837 1369.9,18837 3267.1,18837 3267.1,18837 3267.1,18837 3267.1,18768 \
3267.1,18768"];
	Layer0_Device0_Stage5_RecvKV -> Layer0_Device0_Stage5_Attention	[pos="e,3540.4,18933 3493.1,19117 3493.1,19112 3493.1,18933 3493.1,18933 3493.1,18933 3530.4,18933 3530.4,18933"];
	Layer0_Device0_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2563,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage5_RecvKV -> Layer0_Device0_Stage6_RecvKV	[label="Ring transfer",
		lp="2613,19021",
		pos="e,2573.5,18978 2573.5,19064 2573.5,19064 2573.5,18988 2573.5,18988"];
	Layer0_Device0_Stage5_Attention -> Layer0_Device0_Stage5_Accumulate	[pos="e,3509.1,18758 3540.2,18915 3521.2,18915 3509.1,18915 3509.1,18915 3509.1,18915 3509.1,18768 3509.1,18768"];
	Layer0_Device0_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3283,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage5_Accumulate -> Layer0_Device0_Stage6_Accumulate	[pos="e,3283,18565 3283,18705 3283,18705 3283,18575 3283,18575"];
	Layer0_Device0_Stage6_RecvKV -> Layer0_Device0_Stage6_Attention	[pos="e,3788,18758 3150.4,18884 3464.8,18884 3788,18884 3788,18884 3788,18884 3788,18768 3788,18768"];
	Layer0_Device0_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2074,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage6_RecvKV -> Layer0_Device0_Stage7_RecvKV	[label="Ring transfer",
		lp="2301,18828",
		pos="e,2318.5,18785 2318.5,18871 2318.5,18871 2318.5,18795 2318.5,18795"];
	Layer0_Device0_Stage6_Attention -> Layer0_Device0_Stage6_Accumulate	[pos="e,3514.7,18529 3704,18705 3704,18650 3704,18529 3704,18529 3704,18529 3524.7,18529 3524.7,18529"];
	Layer0_Device0_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3083,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage6_Accumulate -> Layer0_Device0_Stage7_Accumulate	[pos="e,3183,18372 3183,18512 3183,18512 3183,18382 3183,18382"];
	Layer0_Device0_Stage7_RecvKV -> Layer0_Device0_Stage7_Attention	[pos="e,1091,18565 1340,18731 1197.3,18731 1091,18731 1091,18731 1091,18731 1091,18575 1091,18575"];
	Layer0_Device0_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2074,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage7_RecvKV -> Layer0_Device0_Stage8_RecvKV	[label="Ring transfer",
		lp="2122,18635",
		pos="e,2074,18592 2074,18678 2074,18678 2074,18602 2074,18602"];
	Layer0_Device0_Stage7_Attention -> Layer0_Device0_Stage7_Accumulate	[pos="e,2922.6,18372 1025.4,18512 1025.4,18478 1025.4,18424 1025.4,18424 1025.4,18424 2922.6,18424 2922.6,18424 2922.6,18424 2922.6,18382 \
2922.6,18382"];
	Layer0_Device0_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3083,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage7_Accumulate -> Layer0_Device0_Stage8_Accumulate	[pos="e,3083,18179 3083,18319 3083,18319 3083,18189 3083,18189"];
	Layer0_Device0_Stage8_RecvKV -> Layer0_Device0_Stage8_Attention	[pos="e,3443.5,18372 2661.7,18498 3033.2,18498 3443.5,18498 3443.5,18498 3443.5,18498 3443.5,18382 3443.5,18382"];
	Layer0_Device0_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="1874,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage8_RecvKV -> Layer0_Device0_Stage9_RecvKV	[label="Ring transfer",
		lp="1956,18442",
		pos="e,1974,18399 1974,18485 1974,18485 1974,18409 1974,18409"];
	Layer0_Device0_Stage8_Attention -> Layer0_Device0_Stage8_Accumulate	[pos="e,3314.8,18152 3408.6,18319 3408.6,18266 3408.6,18152 3408.6,18152 3408.6,18152 3324.8,18152 3324.8,18152"];
	Layer0_Device0_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1316,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage8_Accumulate -> Layer0_Device0_Stage9_Accumulate	[pos="e,1393.2,17986 3083,18126 3083,18102 3083,18071 3083,18071 3083,18071 1393.2,18071 1393.2,18071 1393.2,18071 1393.2,17996 1393.2,\
17996"];
	Layer0_Device0_Stage9_RecvKV -> Layer0_Device0_Stage9_Attention	[pos="e,765,18179 1140.1,18345 934.01,18345 765,18345 765,18345 765,18345 765,18189 765,18189"];
	Layer0_Device0_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="1874,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage9_RecvKV -> Layer0_Device0_Stage10_RecvKV	[label="Ring transfer",
		lp="1922,18249",
		pos="e,1874,18206 1874,18292 1874,18292 1874,18216 1874,18216"];
	Layer0_Device0_Stage9_Attention -> Layer0_Device0_Stage9_Accumulate	[pos="e,1238.8,17987 730,18126 730,18082 730,17999 730,17999 730,17999 1238.8,17999 1238.8,17999 1238.8,17999 1238.8,17997 1238.8,17997"];
	Layer0_Device0_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1316,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage9_Accumulate -> Layer0_Device0_Stage10_Accumulate	[pos="e,1316,17793 1316,17933 1316,17933 1316,17803 1316,17803"];
	Layer0_Device0_Stage10_RecvKV -> Layer0_Device0_Stage10_Attention	[pos="e,990.45,17987 990.45,18152 990.45,18152 990.45,17997 990.45,17997"];
	Layer0_Device0_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2525,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage10_RecvKV -> Layer0_Device0_Stage11_RecvKV	[label="Ring transfer",
		lp="2290,18056",
		pos="e,2199.5,18013 2199.5,18099 2199.5,18099 2199.5,18023 2199.5,18023"];
	Layer0_Device0_Stage10_Attention -> Layer0_Device0_Stage10_Accumulate	[pos="e,1084.2,17766 843.5,17933 843.5,17880 843.5,17766 843.5,17766 843.5,17766 1074.2,17766 1074.2,17766"];
	Layer0_Device0_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1904,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage10_Accumulate -> Layer0_Device0_Stage11_Accumulate	[pos="e,1672.4,17573 1529.9,17740 1529.9,17687 1529.9,17573 1529.9,17573 1529.9,17573 1662.4,17573 1662.4,17573"];
	Layer0_Device0_Stage11_RecvKV -> Layer0_Device0_Stage11_Attention	[pos="e,2058.8,17793 2058.8,17906 2058.8,17906 2058.8,17803 2058.8,17803"];
	Layer0_Device0_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3345,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage11_RecvKV -> Layer0_Device0_Stage12_RecvKV	[label="Ring transfer",
		lp="3030,17863",
		pos="e,2935,17820 2935,17906 2935,17906 2935,17830 2935,17830"];
	Layer0_Device0_Stage11_Attention -> Layer0_Device0_Stage11_Accumulate	[pos="e,2020,17600 2020,17740 2020,17740 2020,17610 2020,17610"];
	Layer0_Device0_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2022,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage11_Accumulate -> Layer0_Device0_Stage12_Accumulate	[pos="e,1963,17407 1963,17547 1963,17547 1963,17417 1963,17417"];
	Layer0_Device0_Stage12_RecvKV -> Layer0_Device0_Stage12_Attention	[pos="e,2540.9,17600 2540.9,17713 2540.9,17713 2540.9,17610 2540.9,17610"];
	Layer0_Device0_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3634,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage12_RecvKV -> Layer0_Device0_Stage13_RecvKV	[label="Ring transfer",
		lp="3538,17670",
		pos="e,3489.5,17627 3489.5,17713 3489.5,17713 3489.5,17637 3489.5,17637"];
	Layer0_Device0_Stage12_Attention -> Layer0_Device0_Stage12_Accumulate	[pos="e,2223.5,17407 2223.5,17547 2223.5,17547 2223.5,17417 2223.5,17417"];
	Layer0_Device0_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2543,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage12_Accumulate -> Layer0_Device0_Stage13_Accumulate	[pos="e,2311.3,17187 2253.6,17380 2271.3,17380 2282.4,17380 2282.4,17380 2282.4,17380 2282.4,17187 2282.4,17187 2282.4,17187 2301.3,17187 \
2301.3,17187"];
	Layer0_Device0_Stage13_RecvKV -> Layer0_Device0_Stage13_Attention	[pos="e,4381.1,17408 4381.1,17573 4381.1,17573 4381.1,17418 4381.1,17418"];
	Layer0_Device0_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3231,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage13_RecvKV -> Layer0_Device0_Stage14_RecvKV	[label="Ring transfer",
		lp="3415,17477",
		pos="e,3432.5,17434 3432.5,17520 3432.5,17520 3432.5,17444 3432.5,17444"];
	Layer0_Device0_Stage13_Attention -> Layer0_Device0_Stage13_Accumulate	[pos="e,2543,17214 4440,17354 4440,17314 4440,17245 4440,17245 4440,17245 2543,17245 2543,17245 2543,17245 2543,17224 2543,17224"];
	Layer0_Device0_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2543,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage13_Accumulate -> Layer0_Device0_Stage14_Accumulate	[pos="e,2543,17062 2543,17161 2543,17161 2543,17072 2543,17072"];
	Layer0_Device0_Stage14_RecvKV -> Layer0_Device0_Stage14_Attention	[pos="e,2022,17214 2355.2,17340 2166.3,17340 2022,17340 2022,17340 2022,17340 2022,17224 2022,17224"];
	Layer0_Device0_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3752,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device0_Stage14_RecvKV -> Layer0_Device0_Stage15_RecvKV	[label="Ring transfer",
		lp="3539,17284",
		pos="e,3491.5,17241 3491.5,17327 3491.5,17327 3491.5,17251 3491.5,17251"];
	Layer0_Device0_Stage14_Attention -> Layer0_Device0_Stage14_Accumulate	[pos="e,2311.4,17035 2022,17161 2022,17117 2022,17035 2022,17035 2022,17035 2301.4,17035 2301.4,17035"];
	Layer0_Device0_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="4155,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device0_Stage14_Accumulate -> Layer0_Device0_Stage15_Accumulate	[pos="e,3923.2,16909 2543,17008 2543,16971 2543,16909 2543,16909 2543,16909 3913.2,16909 3913.2,16909"];
	Layer0_Device0_Stage15_RecvKV -> Layer0_Device0_Stage15_Attention	[pos="e,4155,17062 4155,17134 4155,17134 4155,17072 4155,17072"];
	Layer0_Device0_Stage15_Attention -> Layer0_Device0_Stage15_Accumulate	[pos="e,4155,16936 4155,17008 4155,17008 4155,16946 4155,16946"];
	Layer0_Device0_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4561,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device0_Stage15_Accumulate -> Layer0_Device0_ConcatHeads	[pos="e,4361,16810 4361,16882 4361,16882 4361,16820 4361,16820"];
	Layer0_Device0_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4569,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device0_ConcatHeads -> Layer0_Device0_OutputProj	[pos="e,4569,16684 4569,16756 4569,16756 4569,16694 4569,16694"];
	Layer0_Device0_OutputProj -> Layer0_Device0_Residual1	[pos="e,4620.8,16558 4620.8,16630 4620.8,16630 4620.8,16568 4620.8,16568"];
	Layer0_Device0_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4662,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device0_Residual1 -> Layer0_Device0_LayerNorm2	[pos="e,4667.2,16432 4667.2,16504 4667.2,16504 4667.2,16442 4667.2,16442"];
	Layer0_Device0_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="4731,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device0_Residual1 -> Layer0_Device0_Residual2	[pos="e,5012.8,15750 5012.8,16504 5012.8,16504 5012.8,15760 5012.8,15760"];
	Layer0_Device0_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4370,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device0_LayerNorm2 -> Layer0_Device0_GateProj	[pos="e,4522,16306 4522,16378 4522,16378 4522,16316 4522,16316"];
	Layer0_Device0_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4683,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device0_LayerNorm2 -> Layer0_Device0_UpProj	[pos="e,4735,16217 4735,16378 4735,16378 4735,16227 4735,16227"];
	Layer0_Device0_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4370,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device0_GateProj -> Layer0_Device0_Activation	[pos="e,4301.5,16128 4301.5,16252 4301.5,16252 4301.5,16138 4301.5,16138"];
	Layer0_Device0_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4465,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device0_UpProj -> Layer0_Device0_ElemMul	[pos="e,4735.2,16002 4735.2,16163 4735.2,16163 4735.2,16012 4735.2,16012"];
	Layer0_Device0_Activation -> Layer0_Device0_ElemMul	[pos="e,4370,16002 4370,16074 4370,16074 4370,16012 4370,16012"];
	Layer0_Device0_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4587,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device0_ElemMul -> Layer0_Device0_DownProj	[pos="e,4587,15876 4587,15948 4587,15948 4587,15886 4587,15886"];
	Layer0_Device0_DownProj -> Layer0_Device0_Residual2	[pos="e,4587,15750 4587,15822 4587,15822 4587,15760 4587,15760"];
	Layer0_Device0_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4731,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device0_Residual2 -> Layer0_Device0_Output	[pos="e,4731,15624 4731,15696 4731,15696 4731,15634 4731,15634"];
	Layer1_Device0_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4731,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device0_Output -> Layer1_Device0_Input	[pos="e,4731,15476 4731,15548 4731,15548 4731,15486 4731,15486"];
	Layer0_Device1_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9729,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device1_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9103,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device1_Input -> Layer0_Device1_LayerNorm1	[pos="e,9316.1,20375 9461.7,20495 9461.7,20458 9461.7,20375 9461.7,20375 9461.7,20375 9326.1,20375 9326.1,20375"];
	Layer0_Device1_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="9719,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device1_Input -> Layer0_Device1_Residual1	[pos="e,9874.6,16558 9874.6,20479 9874.6,20479 9874.6,16568 9874.6,16568"];
	Layer0_Device1_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8749,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device1_LayerNorm1 -> Layer0_Device1_QKVProj	[pos="e,9103,20276 9103,20348 9103,20348 9103,20286 9103,20286"];
	Layer0_Device1_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7503,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage0_RecvKV	[label="Local K,V",
		lp="8003.5,20179",
		pos="e,8261.1,20136 8261.1,20222 8261.1,20222 8261.1,20146 8261.1,20146"];
	Layer0_Device1_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8293,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage0_Attention	[label=Q_local,
		lp="8565,20083",
		pos="e,8473.6,19917 8473.6,20222 8473.6,20222 8473.6,19927 8473.6,19927"];
	Layer0_Device1_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5963,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage1_Attention	[label=Q_local,
		lp="6027,19986",
		pos="e,5947.9,19723 8099.3,20229 7271.9,20229 5947.9,20229 5947.9,20229 5947.9,20229 5947.9,19733 5947.9,19733"];
	Layer0_Device1_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6574,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage2_Attention	[label=Q_local,
		lp="8705,19890",
		pos="e,6744.4,19530 8568.6,20222 8568.6,20123 8568.6,19783 8568.6,19783 8568.6,19783 6744.4,19783 6744.4,19783 6744.4,19783 6744.4,19540 \
6744.4,19540"];
	Layer0_Device1_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8589,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage3_Attention	[label=Q_local,
		lp="8845,19793",
		pos="e,8761.6,19337 8761.6,20223 8761.6,20223 8761.6,19347 8761.6,19347"];
	Layer0_Device1_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5795,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage4_Attention	[label=Q_local,
		lp="5647,19697",
		pos="e,5647.5,19144 8099.3,20242 7189.9,20242 5647.5,20242 5647.5,20242 5647.5,20242 5647.5,19154 5647.5,19154"];
	Layer0_Device1_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8714,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage5_Attention	[label=Q_local,
		lp="8949,19600",
		pos="e,8883,18951 8883,20222 8883,20222 8883,18961 8883,18961"];
	Layer0_Device1_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8782,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage6_Attention	[label=Q_local,
		lp="9081,19504",
		pos="e,8979.5,18758 8979.5,20222 8979.5,20222 8979.5,18768 8979.5,18768"];
	Layer0_Device1_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5843,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage7_Attention	[label=Q_local,
		lp="5501,19407",
		pos="e,5611.2,18547 8099.4,20255 7158.8,20255 5529.2,20255 5529.2,20255 5529.2,20255 5529.2,18547 5529.2,18547 5529.2,18547 5601.2,18547 \
5601.2,18547"];
	Layer0_Device1_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8631,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage8_Attention	[label=Q_local,
		lp="9206,19311",
		pos="e,8862.7,18345 9050.3,20223 9050.3,19993 9050.3,18345 9050.3,18345 9050.3,18345 8872.7,18345 8872.7,18345"];
	Layer0_Device1_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5692,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage9_Attention	[label=Q_local,
		lp="5383,19214",
		pos="e,5494.8,18179 8099.3,20262 7149.8,20262 5494.8,20262 5494.8,20262 5494.8,20262 5494.8,18189 5494.8,18189"];
	Layer0_Device1_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5739,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage10_Attention	[label=Q_local,
		lp="5243,19118",
		pos="e,5507.5,17959 8099.3,20269 7133.9,20269 5433,20269 5433,20269 5433,20269 5433,17959 5433,17959 5433,17959 5497.5,17959 5497.5,17959"];
	Layer0_Device1_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7069,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage11_Attention	[label=Q_local,
		lp="9327,19021",
		pos="e,7146.2,17793 9124,20223 9124,19961 9124,17849 9124,17849 9124,17849 7146.2,17849 7146.2,17849 7146.2,17849 7146.2,17803 7146.2,\
17803"];
	Layer0_Device1_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7367,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage12_Attention	[label=Q_local,
		lp="9470,18925",
		pos="e,7329.4,17600 9160.8,20223 9160.8,19960 9160.8,17847 9160.8,17847 9160.8,17847 7329.4,17847 7329.4,17847 7329.4,17847 7329.4,17610 \
7329.4,17610"];
	Layer0_Device1_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9382,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage13_Attention	[label=Q_local,
		lp="9610,18828",
		pos="e,9554.6,17408 9398.8,20237 9490.6,20237 9554.6,20237 9554.6,20237 9554.6,20237 9554.6,17418 9554.6,17418"];
	Layer0_Device1_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6964,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage14_Attention	[label=Q_local,
		lp="5097,18732",
		pos="e,6732.4,17187 9087.1,20222 9087.1,19962 9087.1,17893 9087.1,17893 9087.1,17893 6581.9,17893 6581.9,17893 6581.9,17893 6581.9,17187 \
6581.9,17187 6581.9,17187 6722.4,17187 6722.4,17187"];
	Layer0_Device1_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8733,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_QKVProj -> Layer0_Device1_Stage15_Attention	[label=Q_local,
		lp="9728,18635",
		pos="e,8964.6,17043 9398.5,20252 9549.1,20252 9666.3,20252 9666.3,20252 9666.3,20252 9666.3,17043 9666.3,17043 9666.3,17043 8974.6,17043 \
8974.6,17043"];
	Layer0_Device1_Stage0_RecvKV -> Layer0_Device1_Stage0_Attention	[pos="e,8242.1,19917 8242.1,20082 8242.1,20082 8242.1,19927 8242.1,19927"];
	Layer0_Device1_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7084,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage0_RecvKV -> Layer0_Device1_Stage1_RecvKV	[label="Ring transfer",
		lp="7423,19986",
		pos="e,7293.5,19943 7293.5,20029 7293.5,20029 7293.5,19953 7293.5,19953"];
	Layer0_Device1_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="6484,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage0_Attention -> Layer0_Device1_Stage0_Accumulate	[pos="e,6484,19723 8293,19863 8293,19832 8293,19786 8293,19786 8293,19786 6484,19786 6484,19786 6484,19786 6484,19733 6484,19733"];
	Layer0_Device1_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6053,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage0_Accumulate -> Layer0_Device1_Stage1_Accumulate	[pos="e,6268.5,19530 6268.5,19670 6268.5,19670 6268.5,19540 6268.5,19540"];
	Layer0_Device1_Stage1_RecvKV -> Layer0_Device1_Stage1_Attention	[pos="e,6179.4,19724 6179.4,19889 6179.4,19889 6179.4,19734 6179.4,19734"];
	Layer0_Device1_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7693,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage1_RecvKV -> Layer0_Device1_Stage2_RecvKV	[label="Ring transfer",
		lp="7391,19793",
		pos="e,7388.5,19750 7388.5,19836 7388.5,19836 7388.5,19760 7388.5,19760"];
	Layer0_Device1_Stage1_Attention -> Layer0_Device1_Stage1_Accumulate	[pos="e,6008,19530 6008,19670 6008,19670 6008,19540 6008,19540"];
	Layer0_Device1_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6171,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage1_Accumulate -> Layer0_Device1_Stage2_Accumulate	[pos="e,6112,19337 6112,19477 6112,19477 6112,19347 6112,19347"];
	Layer0_Device1_Stage2_RecvKV -> Layer0_Device1_Stage2_Attention	[pos="e,6789.4,19531 6789.4,19696 6789.4,19696 6789.4,19541 6789.4,19541"];
	Layer0_Device1_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7783,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage2_RecvKV -> Layer0_Device1_Stage3_RecvKV	[label="Ring transfer",
		lp="7786,19600",
		pos="e,7738,19557 7738,19643 7738,19643 7738,19567 7738,19567"];
	Layer0_Device1_Stage2_Attention -> Layer0_Device1_Stage2_Accumulate	[pos="e,6372.5,19337 6372.5,19477 6372.5,19477 6372.5,19347 6372.5,19347"];
	Layer0_Device1_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6316,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage2_Accumulate -> Layer0_Device1_Stage3_Accumulate	[pos="e,6243.5,19144 6243.5,19284 6243.5,19284 6243.5,19154 6243.5,19154"];
	Layer0_Device1_Stage3_RecvKV -> Layer0_Device1_Stage3_Attention	[pos="e,8530.1,19338 8530.1,19503 8530.1,19503 8530.1,19348 8530.1,19348"];
	Layer0_Device1_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7380,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage3_RecvKV -> Layer0_Device1_Stage4_RecvKV	[label="Ring transfer",
		lp="7697,19407",
		pos="e,7581.5,19364 7581.5,19450 7581.5,19450 7581.5,19374 7581.5,19374"];
	Layer0_Device1_Stage3_Attention -> Layer0_Device1_Stage3_Accumulate	[pos="e,6503.9,19144 8401.1,19284 8401.1,19246 8401.1,19180 8401.1,19180 8401.1,19180 6503.9,19180 6503.9,19180 6503.9,19180 6503.9,19154 \
6503.9,19154"];
	Layer0_Device1_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6296,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage3_Accumulate -> Layer0_Device1_Stage4_Accumulate	[pos="e,6306,18951 6306,19091 6306,19091 6306,18961 6306,18961"];
	Layer0_Device1_Stage4_RecvKV -> Layer0_Device1_Stage4_Attention	[pos="e,5983,19144 6504.2,19270 6223.1,19270 5983,19270 5983,19270 5983,19270 5983,19154 5983,19154"];
	Layer0_Device1_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7525,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage4_RecvKV -> Layer0_Device1_Stage5_RecvKV	[label="Ring transfer",
		lp="7500,19214",
		pos="e,7452.5,19171 7452.5,19257 7452.5,19257 7452.5,19181 7452.5,19181"];
	Layer0_Device1_Stage4_Attention -> Layer0_Device1_Stage4_Accumulate	[pos="e,6074.5,18951 6026.8,19117 6055.2,19117 6074.5,19117 6074.5,19117 6074.5,19117 6074.5,18961 6074.5,18961"];
	Layer0_Device1_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8261,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage4_Accumulate -> Layer0_Device1_Stage5_Accumulate	[pos="e,8227.1,18758 6329.9,18898 6329.9,18877 6329.9,18852 6329.9,18852 6329.9,18852 8227.1,18852 8227.1,18852 8227.1,18852 8227.1,18768 \
8227.1,18768"];
	Layer0_Device1_Stage5_RecvKV -> Layer0_Device1_Stage5_Attention	[pos="e,8482.3,18924 8434.6,19117 8434.6,19112 8434.6,18924 8434.6,18924 8434.6,18924 8472.3,18924 8472.3,18924"];
	Layer0_Device1_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7505,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage5_RecvKV -> Layer0_Device1_Stage6_RecvKV	[label="Ring transfer",
		lp="7563,19021",
		pos="e,7515,18978 7515,19064 7515,19064 7515,18988 7515,18988"];
	Layer0_Device1_Stage5_Attention -> Layer0_Device1_Stage5_Accumulate	[pos="e,8487.5,18758 8487.5,18898 8487.5,18898 8487.5,18768 8487.5,18768"];
	Layer0_Device1_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8261,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage5_Accumulate -> Layer0_Device1_Stage6_Accumulate	[pos="e,8261,18565 8261,18705 8261,18705 8261,18575 8261,18575"];
	Layer0_Device1_Stage6_RecvKV -> Layer0_Device1_Stage6_Attention	[pos="e,8748,18758 8092.4,18884 8414.1,18884 8748,18884 8748,18884 8748,18884 8748,18768 8748,18768"];
	Layer0_Device1_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7052,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage6_RecvKV -> Layer0_Device1_Stage7_RecvKV	[label="Ring transfer",
		lp="7366,18828",
		pos="e,7278.5,18785 7278.5,18871 7278.5,18871 7278.5,18795 7278.5,18795"];
	Layer0_Device1_Stage6_Attention -> Layer0_Device1_Stage6_Accumulate	[pos="e,8492.6,18529 8706.5,18705 8706.5,18650 8706.5,18529 8706.5,18529 8706.5,18529 8502.6,18529 8502.6,18529"];
	Layer0_Device1_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8110,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage6_Accumulate -> Layer0_Device1_Stage7_Accumulate	[pos="e,8185.5,18372 8185.5,18512 8185.5,18512 8185.5,18382 8185.5,18382"];
	Layer0_Device1_Stage7_RecvKV -> Layer0_Device1_Stage7_Attention	[pos="e,6069.5,18565 6176,18691 6110.6,18691 6069.5,18691 6069.5,18691 6069.5,18691 6069.5,18575 6069.5,18575"];
	Layer0_Device1_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7052,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage7_RecvKV -> Layer0_Device1_Stage8_RecvKV	[label="Ring transfer",
		lp="7100,18635",
		pos="e,7052,18592 7052,18678 7052,18678 7052,18602 7052,18602"];
	Layer0_Device1_Stage7_Attention -> Layer0_Device1_Stage7_Accumulate	[pos="e,7925.1,18372 6027.9,18512 6027.9,18491 6027.9,18465 6027.9,18465 6027.9,18465 7925.1,18465 7925.1,18465 7925.1,18465 7925.1,18382 \
7925.1,18382"];
	Layer0_Device1_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8110,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage7_Accumulate -> Layer0_Device1_Stage8_Accumulate	[pos="e,8110,18179 8110,18319 8110,18319 8110,18189 8110,18189"];
	Layer0_Device1_Stage8_RecvKV -> Layer0_Device1_Stage8_Attention	[pos="e,8446,18372 7639.4,18498 8020.3,18498 8446,18498 8446,18498 8446,18498 8446,18382 8446,18382"];
	Layer0_Device1_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="6901,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage8_RecvKV -> Layer0_Device1_Stage9_RecvKV	[label="Ring transfer",
		lp="6992,18442",
		pos="e,6976.5,18399 6976.5,18485 6976.5,18485 6976.5,18409 6976.5,18409"];
	Layer0_Device1_Stage8_Attention -> Layer0_Device1_Stage8_Accumulate	[pos="e,8341.6,18152 8631,18319 8631,18266 8631,18152 8631,18152 8631,18152 8351.6,18152 8351.6,18152"];
	Layer0_Device1_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6260,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage8_Accumulate -> Layer0_Device1_Stage9_Accumulate	[pos="e,6260,17986 8110,18126 8110,18105 8110,18079 8110,18079 8110,18079 6260,18079 6260,18079 6260,18079 6260,17996 6260,17996"];
	Layer0_Device1_Stage9_RecvKV -> Layer0_Device1_Stage9_Attention	[pos="e,5767.5,18179 6025.1,18305 5875.6,18305 5767.5,18305 5767.5,18305 5767.5,18305 5767.5,18189 5767.5,18189"];
	Layer0_Device1_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="6901,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage9_RecvKV -> Layer0_Device1_Stage10_RecvKV	[label="Ring transfer",
		lp="6949,18249",
		pos="e,6901,18206 6901,18292 6901,18292 6901,18216 6901,18216"];
	Layer0_Device1_Stage9_Attention -> Layer0_Device1_Stage9_Accumulate	[pos="e,6028.4,17959 5923.5,18152 5954.6,18152 5975.9,18152 5975.9,18152 5975.9,18152 5975.9,17959 5975.9,17959 5975.9,17959 6018.4,17959 \
6018.4,17959"];
	Layer0_Device1_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6260,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage9_Accumulate -> Layer0_Device1_Stage10_Accumulate	[pos="e,6260,17793 6260,17933 6260,17933 6260,17803 6260,17803"];
	Layer0_Device1_Stage10_RecvKV -> Layer0_Device1_Stage10_Attention	[pos="e,5947,17986 6025.1,18112 5976.5,18112 5947,18112 5947,18112 5947,18112 5947,17996 5947,17996"];
	Layer0_Device1_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7469,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage10_RecvKV -> Layer0_Device1_Stage11_RecvKV	[label="Ring transfer",
		lp="7218,18056",
		pos="e,7185,18013 7185,18099 7185,18099 7185,18023 7185,18023"];
	Layer0_Device1_Stage10_Attention -> Layer0_Device1_Stage10_Accumulate	[pos="e,6028.3,17766 5867.5,17933 5867.5,17880 5867.5,17766 5867.5,17766 5867.5,17766 6018.3,17766 6018.3,17766"];
	Layer0_Device1_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6846,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage10_Accumulate -> Layer0_Device1_Stage11_Accumulate	[pos="e,6726,17600 6491.6,17766 6608.6,17766 6726,17766 6726,17766 6726,17766 6726,17610 6726,17610"];
	Layer0_Device1_Stage11_RecvKV -> Layer0_Device1_Stage11_Attention	[pos="e,6991.8,17793 6991.8,17906 6991.8,17906 6991.8,17803 6991.8,17803"];
	Layer0_Device1_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8278,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage11_RecvKV -> Layer0_Device1_Stage12_RecvKV	[label="Ring transfer",
		lp="7922,17863",
		pos="e,7873.5,17820 7873.5,17906 7873.5,17906 7873.5,17830 7873.5,17830"];
	Layer0_Device1_Stage11_Attention -> Layer0_Device1_Stage11_Accumulate	[pos="e,6957.5,17600 6957.5,17740 6957.5,17740 6957.5,17610 6957.5,17610"];
	Layer0_Device1_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6964,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage11_Accumulate -> Layer0_Device1_Stage12_Accumulate	[pos="e,6905,17407 6905,17547 6905,17547 6905,17417 6905,17417"];
	Layer0_Device1_Stage12_RecvKV -> Layer0_Device1_Stage12_Attention	[pos="e,7478.4,17600 7478.4,17713 7478.4,17713 7478.4,17610 7478.4,17610"];
	Layer0_Device1_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8576,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage12_RecvKV -> Layer0_Device1_Stage13_RecvKV	[label="Ring transfer",
		lp="8396,17670",
		pos="e,8427,17627 8427,17713 8427,17713 8427,17637 8427,17637"];
	Layer0_Device1_Stage12_Attention -> Layer0_Device1_Stage12_Accumulate	[pos="e,7165.5,17407 7165.5,17547 7165.5,17547 7165.5,17417 7165.5,17417"];
	Layer0_Device1_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7485,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage12_Accumulate -> Layer0_Device1_Stage13_Accumulate	[pos="e,7253.3,17187 7195.6,17380 7213.3,17380 7224.4,17380 7224.4,17380 7224.4,17380 7224.4,17187 7224.4,17187 7224.4,17187 7243.3,17187 \
7243.3,17187"];
	Layer0_Device1_Stage13_RecvKV -> Layer0_Device1_Stage13_Attention	[pos="e,9323.1,17408 9323.1,17573 9323.1,17573 9323.1,17418 9323.1,17418"];
	Layer0_Device1_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8173,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage13_RecvKV -> Layer0_Device1_Stage14_RecvKV	[label="Ring transfer",
		lp="8490,17477",
		pos="e,8374.5,17434 8374.5,17520 8374.5,17520 8374.5,17444 8374.5,17444"];
	Layer0_Device1_Stage13_Attention -> Layer0_Device1_Stage13_Accumulate	[pos="e,7485,17214 9382,17354 9382,17316 9382,17250 9382,17250 9382,17250 7485,17250 7485,17250 7485,17250 7485,17224 7485,17224"];
	Layer0_Device1_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7485,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage13_Accumulate -> Layer0_Device1_Stage14_Accumulate	[pos="e,7485,17062 7485,17161 7485,17161 7485,17072 7485,17072"];
	Layer0_Device1_Stage14_RecvKV -> Layer0_Device1_Stage14_Attention	[pos="e,6964,17214 7297.2,17340 7108.3,17340 6964,17340 6964,17340 6964,17340 6964,17224 6964,17224"];
	Layer0_Device1_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8694,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device1_Stage14_RecvKV -> Layer0_Device1_Stage15_RecvKV	[label="Ring transfer",
		lp="8481,17284",
		pos="e,8433.5,17241 8433.5,17327 8433.5,17327 8433.5,17251 8433.5,17251"];
	Layer0_Device1_Stage14_Attention -> Layer0_Device1_Stage14_Accumulate	[pos="e,7253.4,17026 6964,17161 6964,17115 6964,17026 6964,17026 6964,17026 7243.4,17026 7243.4,17026"];
	Layer0_Device1_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8733,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device1_Stage14_Accumulate -> Layer0_Device1_Stage15_Accumulate	[pos="e,8501.2,16909 7485,17008 7485,16971 7485,16909 7485,16909 7485,16909 8491.2,16909 8491.2,16909"];
	Layer0_Device1_Stage15_RecvKV -> Layer0_Device1_Stage15_Attention	[pos="e,8733,17062 8733,17134 8733,17134 8733,17072 8733,17072"];
	Layer0_Device1_Stage15_Attention -> Layer0_Device1_Stage15_Accumulate	[pos="e,8733,16936 8733,17008 8733,17008 8733,16946 8733,16946"];
	Layer0_Device1_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9042,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device1_Stage15_Accumulate -> Layer0_Device1_ConcatHeads	[pos="e,8890.5,16810 8890.5,16882 8890.5,16882 8890.5,16820 8890.5,16820"];
	Layer0_Device1_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9506,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device1_ConcatHeads -> Layer0_Device1_OutputProj	[pos="e,9292.9,16657 9129.8,16756 9129.8,16719 9129.8,16657 9129.8,16657 9129.8,16657 9282.9,16657 9282.9,16657"];
	Layer0_Device1_OutputProj -> Layer0_Device1_Residual1	[pos="e,9525.8,16558 9525.8,16630 9525.8,16630 9525.8,16568 9525.8,16568"];
	Layer0_Device1_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9509,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device1_Residual1 -> Layer0_Device1_LayerNorm2	[pos="e,9527.2,16432 9527.2,16504 9527.2,16504 9527.2,16442 9527.2,16442"];
	Layer0_Device1_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="9809,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device1_Residual1 -> Layer0_Device1_Residual2	[pos="e,9930.2,15750 9930.2,16504 9930.2,16504 9930.2,15760 9930.2,15760"];
	Layer0_Device1_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9217,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device1_LayerNorm2 -> Layer0_Device1_GateProj	[pos="e,9369,16306 9369,16378 9369,16378 9369,16316 9369,16316"];
	Layer0_Device1_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9530,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device1_LayerNorm2 -> Layer0_Device1_UpProj	[pos="e,9582,16217 9582,16378 9582,16378 9582,16227 9582,16227"];
	Layer0_Device1_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9217,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device1_GateProj -> Layer0_Device1_Activation	[pos="e,9148.5,16128 9148.5,16252 9148.5,16252 9148.5,16138 9148.5,16138"];
	Layer0_Device1_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9312,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device1_UpProj -> Layer0_Device1_ElemMul	[pos="e,9582.2,16002 9582.2,16163 9582.2,16163 9582.2,16012 9582.2,16012"];
	Layer0_Device1_Activation -> Layer0_Device1_ElemMul	[pos="e,9217,16002 9217,16074 9217,16074 9217,16012 9217,16012"];
	Layer0_Device1_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9455,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device1_ElemMul -> Layer0_Device1_DownProj	[pos="e,9455,15876 9455,15948 9455,15948 9455,15886 9455,15886"];
	Layer0_Device1_DownProj -> Layer0_Device1_Residual2	[pos="e,9548.2,15750 9548.2,15822 9548.2,15822 9548.2,15760 9548.2,15760"];
	Layer0_Device1_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9809,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device1_Residual2 -> Layer0_Device1_Output	[pos="e,9809,15624 9809,15696 9809,15696 9809,15634 9809,15634"];
	Layer1_Device1_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9809,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device1_Output -> Layer1_Device1_Input	[pos="e,9809,15476 9809,15548 9809,15548 9809,15486 9809,15486"];
	Layer0_Device2_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14412,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device2_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14088,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device2_Input -> Layer0_Device2_LayerNorm1	[pos="e,14206,20402 14206,20485 14206,20485 14206,20412 14206,20412"];
	Layer0_Device2_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="14730,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device2_Input -> Layer0_Device2_Residual1	[pos="e,14628,16558 14628,20486 14628,20486 14628,16568 14628,16568"];
	Layer0_Device2_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13667,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device2_LayerNorm1 -> Layer0_Device2_QKVProj	[pos="e,14088,20276 14088,20348 14088,20348 14088,20286 14088,20286"];
	Layer0_Device2_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12421,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage0_RecvKV	[label="Local K,V",
		lp="12922,20179",
		pos="e,13179,20136 13179,20222 13179,20222 13179,20146 13179,20146"];
	Layer0_Device2_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13211,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage0_Attention	[label=Q_local,
		lp="13483,20083",
		pos="e,13392,19917 13392,20222 13392,20222 13392,19927 13392,19927"];
	Layer0_Device2_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10881,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage1_Attention	[label=Q_local,
		lp="10945,19986",
		pos="e,10866,19723 13017,20230 12190,20230 10866,20230 10866,20230 10866,20230 10866,19733 10866,19733"];
	Layer0_Device2_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11492,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage2_Attention	[label=Q_local,
		lp="13623,19890",
		pos="e,11662,19530 13487,20222 13487,20124 13487,19789 13487,19789 13487,19789 11662,19789 11662,19789 11662,19789 11662,19540 11662,\
19540"];
	Layer0_Device2_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13507,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage3_Attention	[label=Q_local,
		lp="13763,19793",
		pos="e,13680,19337 13680,20223 13680,20223 13680,19347 13680,19347"];
	Layer0_Device2_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10713,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage4_Attention	[label=Q_local,
		lp="10565,19697",
		pos="e,10566,19144 13017,20245 12108,20245 10566,20245 10566,20245 10566,20245 10566,19154 10566,19154"];
	Layer0_Device2_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13629,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage5_Attention	[label=Q_local,
		lp="13867,19600",
		pos="e,13800,18951 13800,20222 13800,20222 13800,18961 13800,18961"];
	Layer0_Device2_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13652,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage6_Attention	[label=Q_local,
		lp="13999,19504",
		pos="e,13872,18758 13872,20222 13872,20222 13872,18768 13872,18768"];
	Layer0_Device2_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10713,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage7_Attention	[label=Q_local,
		lp="10419,19407",
		pos="e,10481,18547 13017,20260 12070,20260 10419,20260 10419,20260 10419,20260 10419,18547 10419,18547 10419,18547 10471,18547 10471,\
18547"];
	Layer0_Device2_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13464,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage8_Attention	[label=Q_local,
		lp="14121,19311",
		pos="e,13696,18345 13929,20223 13929,19993 13929,18345 13929,18345 13929,18345 13706,18345 13706,18345"];
	Layer0_Device2_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10525,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage9_Attention	[label=Q_local,
		lp="10301,19214",
		pos="e,10356,18179 13017,20268 12053,20268 10356,20268 10356,20268 10356,20268 10356,18189 10356,18189"];
	Layer0_Device2_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10873,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage10_Attention	[label=Q_local,
		lp="10161,19118",
		pos="e,11008,17986 13975,20222 13975,19974 13975,18094 13975,18094 13975,18094 11008,18094 11008,18094 11008,18094 11008,17996 11008,\
17996"];
	Layer0_Device2_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11984,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage11_Attention	[label=Q_local,
		lp="14245,19021",
		pos="e,12061,17793 14021,20222 14021,19959 14021,17857 14021,17857 14021,17857 12061,17857 12061,17857 12061,17857 12061,17803 12061,\
17803"];
	Layer0_Device2_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="12282,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage12_Attention	[label=Q_local,
		lp="14385,18925",
		pos="e,12244,17600 14067,20222 14067,19959 14067,17855 14067,17855 14067,17855 12244,17855 12244,17855 12244,17855 12244,17610 12244,\
17610"];
	Layer0_Device2_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14297,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage13_Attention	[label=Q_local,
		lp="14525,18828",
		pos="e,14470,17408 14317,20237 14407,20237 14470,20237 14470,20237 14470,20237 14470,17418 14470,17418"];
	Layer0_Device2_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11879,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage14_Attention	[label=Q_local,
		lp="10015,18732",
		pos="e,11647,17187 14215,20223 14215,19951 14215,17691 14215,17691 14215,17691 11519,17691 11519,17691 11519,17691 11519,17187 11519,\
17187 11519,17187 11637,17187 11637,17187"];
	Layer0_Device2_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14012,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_QKVProj -> Layer0_Device2_Stage15_Attention	[label=Q_local,
		lp="14643,18635",
		pos="e,14244,17043 14317,20252 14442,20252 14536,20252 14536,20252 14536,20252 14536,17043 14536,17043 14536,17043 14254,17043 14254,\
17043"];
	Layer0_Device2_Stage0_RecvKV -> Layer0_Device2_Stage0_Attention	[pos="e,13160,19917 13160,20082 13160,20082 13160,19927 13160,19927"];
	Layer0_Device2_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12002,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage0_RecvKV -> Layer0_Device2_Stage1_RecvKV	[label="Ring transfer",
		lp="12331,19986",
		pos="e,12212,19943 12212,20029 12212,20029 12212,19953 12212,19953"];
	Layer0_Device2_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="11402,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage0_Attention -> Layer0_Device2_Stage0_Accumulate	[pos="e,11402,19723 13211,19863 13211,19834 13211,19791 13211,19791 13211,19791 11402,19791 11402,19791 11402,19791 11402,19733 11402,\
19733"];
	Layer0_Device2_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="10971,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage0_Accumulate -> Layer0_Device2_Stage1_Accumulate	[pos="e,11186,19530 11186,19670 11186,19670 11186,19540 11186,19540"];
	Layer0_Device2_Stage1_RecvKV -> Layer0_Device2_Stage1_Attention	[pos="e,11097,19724 11097,19889 11097,19889 11097,19734 11097,19734"];
	Layer0_Device2_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12611,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage1_RecvKV -> Layer0_Device2_Stage2_RecvKV	[label="Ring transfer",
		lp="12309,19793",
		pos="e,12306,19750 12306,19836 12306,19836 12306,19760 12306,19760"];
	Layer0_Device2_Stage1_Attention -> Layer0_Device2_Stage1_Accumulate	[pos="e,10926,19530 10926,19670 10926,19670 10926,19540 10926,19540"];
	Layer0_Device2_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11089,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage1_Accumulate -> Layer0_Device2_Stage2_Accumulate	[pos="e,11030,19337 11030,19477 11030,19477 11030,19347 11030,19347"];
	Layer0_Device2_Stage2_RecvKV -> Layer0_Device2_Stage2_Attention	[pos="e,11707,19531 11707,19696 11707,19696 11707,19541 11707,19541"];
	Layer0_Device2_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12701,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage2_RecvKV -> Layer0_Device2_Stage3_RecvKV	[label="Ring transfer",
		lp="12704,19600",
		pos="e,12656,19557 12656,19643 12656,19643 12656,19567 12656,19567"];
	Layer0_Device2_Stage2_Attention -> Layer0_Device2_Stage2_Accumulate	[pos="e,11290,19337 11290,19477 11290,19477 11290,19347 11290,19347"];
	Layer0_Device2_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11234,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage2_Accumulate -> Layer0_Device2_Stage3_Accumulate	[pos="e,11162,19144 11162,19284 11162,19284 11162,19154 11162,19154"];
	Layer0_Device2_Stage3_RecvKV -> Layer0_Device2_Stage3_Attention	[pos="e,13448,19338 13448,19503 13448,19503 13448,19348 13448,19348"];
	Layer0_Device2_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12298,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage3_RecvKV -> Layer0_Device2_Stage4_RecvKV	[label="Ring transfer",
		lp="12547,19407",
		pos="e,12500,19364 12500,19450 12500,19450 12500,19374 12500,19374"];
	Layer0_Device2_Stage3_Attention -> Layer0_Device2_Stage3_Accumulate	[pos="e,11422,19144 13319,19284 13319,19247 13319,19185 13319,19185 13319,19185 11422,19185 11422,19185 11422,19185 11422,19154 11422,\
19154"];
	Layer0_Device2_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11211,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage3_Accumulate -> Layer0_Device2_Stage4_Accumulate	[pos="e,11222,18951 11222,19091 11222,19091 11222,18961 11222,18961"];
	Layer0_Device2_Stage4_RecvKV -> Layer0_Device2_Stage4_Attention	[pos="e,10901,19144 11422,19270 11141,19270 10901,19270 10901,19270 10901,19270 10901,19154 10901,19154"];
	Layer0_Device2_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12443,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage4_RecvKV -> Layer0_Device2_Stage5_RecvKV	[label="Ring transfer",
		lp="12419,19214",
		pos="e,12370,19171 12370,19257 12370,19257 12370,19181 12370,19181"];
	Layer0_Device2_Stage4_Attention -> Layer0_Device2_Stage4_Accumulate	[pos="e,10991,18951 10945,19117 10972,19117 10991,19117 10991,19117 10991,19117 10991,18961 10991,18961"];
	Layer0_Device2_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13131,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage4_Accumulate -> Layer0_Device2_Stage5_Accumulate	[pos="e,13120,18758 11222,18898 11222,18865 11222,18813 11222,18813 11222,18813 13120,18813 13120,18813 13120,18813 13120,18768 13120,\
18768"];
	Layer0_Device2_Stage5_RecvKV -> Layer0_Device2_Stage5_Attention	[pos="e,13397,18933 13347,19117 13347,19112 13347,18933 13347,18933 13347,18933 13387,18933 13387,18933"];
	Layer0_Device2_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12420,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage5_RecvKV -> Layer0_Device2_Stage6_RecvKV	[label="Ring transfer",
		lp="12478,19021",
		pos="e,12432,18978 12432,19064 12432,19064 12432,18988 12432,18988"];
	Layer0_Device2_Stage5_Attention -> Layer0_Device2_Stage5_Accumulate	[pos="e,13355,18758 13397,18915 13372,18915 13355,18915 13355,18915 13355,18915 13355,18768 13355,18768"];
	Layer0_Device2_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13131,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage5_Accumulate -> Layer0_Device2_Stage6_Accumulate	[pos="e,13131,18565 13131,18705 13131,18705 13131,18575 13131,18575"];
	Layer0_Device2_Stage6_RecvKV -> Layer0_Device2_Stage6_Attention	[pos="e,13640,18758 13007,18884 13320,18884 13640,18884 13640,18884 13640,18884 13640,18768 13640,18768"];
	Layer0_Device2_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11922,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage6_RecvKV -> Layer0_Device2_Stage7_RecvKV	[label="Ring transfer",
		lp="12210,18828",
		pos="e,12171,18785 12171,18871 12171,18871 12171,18795 12171,18795"];
	Layer0_Device2_Stage6_Attention -> Layer0_Device2_Stage6_Accumulate	[pos="e,13363,18529 13558,18705 13558,18650 13558,18529 13558,18529 13558,18529 13373,18529 13373,18529"];
	Layer0_Device2_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12943,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage6_Accumulate -> Layer0_Device2_Stage7_Accumulate	[pos="e,13037,18372 13037,18512 13037,18512 13037,18382 13037,18382"];
	Layer0_Device2_Stage7_RecvKV -> Layer0_Device2_Stage7_Attention	[pos="e,10713,18565 11046,18691 10857,18691 10713,18691 10713,18691 10713,18691 10713,18575 10713,18575"];
	Layer0_Device2_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11922,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage7_RecvKV -> Layer0_Device2_Stage8_RecvKV	[label="Ring transfer",
		lp="11970,18635",
		pos="e,11922,18592 11922,18678 11922,18678 11922,18602 11922,18602"];
	Layer0_Device2_Stage7_Attention -> Layer0_Device2_Stage7_Accumulate	[pos="e,12777,18372 10879,18512 10879,18489 10879,18460 10879,18460 10879,18460 12777,18460 12777,18460 12777,18460 12777,18382 12777,\
18382"];
	Layer0_Device2_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12943,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage7_Accumulate -> Layer0_Device2_Stage8_Accumulate	[pos="e,12943,18179 12943,18319 12943,18319 12943,18189 12943,18189"];
	Layer0_Device2_Stage8_RecvKV -> Layer0_Device2_Stage8_Attention	[pos="e,13298,18372 12509,18498 12883,18498 13298,18498 13298,18498 13298,18498 13298,18382 13298,18382"];
	Layer0_Device2_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="11734,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage8_RecvKV -> Layer0_Device2_Stage9_RecvKV	[label="Ring transfer",
		lp="11876,18442",
		pos="e,11828,18399 11828,18485 11828,18485 11828,18409 11828,18409"];
	Layer0_Device2_Stage8_Attention -> Layer0_Device2_Stage8_Accumulate	[pos="e,13175,18152 13378,18319 13378,18266 13378,18152 13378,18152 13378,18152 13185,18152 13185,18152"];
	Layer0_Device2_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11394,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage8_Accumulate -> Layer0_Device2_Stage9_Accumulate	[pos="e,11471,17986 12943,18126 12943,18092 12943,18037 12943,18037 12943,18037 11471,18037 11471,18037 11471,18037 11471,17996 11471,\
17996"];
	Layer0_Device2_Stage9_RecvKV -> Layer0_Device2_Stage9_Attention	[pos="e,10619,18179 10858,18305 10719,18305 10619,18305 10619,18305 10619,18305 10619,18189 10619,18189"];
	Layer0_Device2_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11734,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage9_RecvKV -> Layer0_Device2_Stage10_RecvKV	[label="Ring transfer",
		lp="11782,18249",
		pos="e,11734,18206 11734,18292 11734,18292 11734,18216 11734,18216"];
	Layer0_Device2_Stage9_Attention -> Layer0_Device2_Stage9_Accumulate	[pos="e,11317,17987 10699,18126 10699,18082 10699,17999 10699,17999 10699,17999 11317,17999 11317,17999 11317,17999 11317,17997 11317,\
17997"];
	Layer0_Device2_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11394,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage9_Accumulate -> Layer0_Device2_Stage10_Accumulate	[pos="e,11394,17793 11394,17933 11394,17933 11394,17803 11394,17803"];
	Layer0_Device2_Stage10_RecvKV -> Layer0_Device2_Stage10_Attention	[pos="e,10911,17986 10911,18099 10911,18099 10911,17996 10911,17996"];
	Layer0_Device2_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12603,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage10_RecvKV -> Layer0_Device2_Stage11_RecvKV	[label="Ring transfer",
		lp="12051,18056",
		pos="e,12168,18013 12168,18099 12168,18099 12168,18023 12168,18023"];
	Layer0_Device2_Stage10_Attention -> Layer0_Device2_Stage10_Accumulate	[pos="e,11162,17766 11079,17933 11079,17880 11079,17766 11079,17766 11079,17766 11152,17766 11152,17766"];
	Layer0_Device2_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11761,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage10_Accumulate -> Layer0_Device2_Stage11_Accumulate	[pos="e,11578,17600 11578,17740 11578,17740 11578,17610 11578,17610"];
	Layer0_Device2_Stage11_RecvKV -> Layer0_Device2_Stage11_Attention	[pos="e,11907,17793 11907,17906 11907,17906 11907,17803 11907,17803"];
	Layer0_Device2_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13193,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage11_RecvKV -> Layer0_Device2_Stage12_RecvKV	[label="Ring transfer",
		lp="13027,17863",
		pos="e,12898,17820 12898,17906 12898,17906 12898,17830 12898,17830"];
	Layer0_Device2_Stage11_Attention -> Layer0_Device2_Stage11_Accumulate	[pos="e,11872,17600 11872,17740 11872,17740 11872,17610 11872,17610"];
	Layer0_Device2_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11879,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage11_Accumulate -> Layer0_Device2_Stage12_Accumulate	[pos="e,11820,17407 11820,17547 11820,17547 11820,17417 11820,17417"];
	Layer0_Device2_Stage12_RecvKV -> Layer0_Device2_Stage12_Attention	[pos="e,12393,17600 12393,17713 12393,17713 12393,17610 12393,17610"];
	Layer0_Device2_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13491,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage12_RecvKV -> Layer0_Device2_Stage13_RecvKV	[label="Ring transfer",
		lp="13390,17670",
		pos="e,13342,17627 13342,17713 13342,17713 13342,17637 13342,17637"];
	Layer0_Device2_Stage12_Attention -> Layer0_Device2_Stage12_Accumulate	[pos="e,12080,17407 12080,17547 12080,17547 12080,17417 12080,17417"];
	Layer0_Device2_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12400,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage12_Accumulate -> Layer0_Device2_Stage13_Accumulate	[pos="e,12168,17187 12111,17380 12128,17380 12139,17380 12139,17380 12139,17380 12139,17187 12139,17187 12139,17187 12158,17187 12158,\
17187"];
	Layer0_Device2_Stage13_RecvKV -> Layer0_Device2_Stage13_Attention	[pos="e,14238,17408 14238,17573 14238,17573 14238,17418 14238,17418"];
	Layer0_Device2_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13088,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage13_RecvKV -> Layer0_Device2_Stage14_RecvKV	[label="Ring transfer",
		lp="13272,17477",
		pos="e,13290,17434 13290,17520 13290,17520 13290,17444 13290,17444"];
	Layer0_Device2_Stage13_Attention -> Layer0_Device2_Stage13_Accumulate	[pos="e,12400,17214 14297,17354 14297,17317 14297,17255 14297,17255 14297,17255 12400,17255 12400,17255 12400,17255 12400,17224 12400,\
17224"];
	Layer0_Device2_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12400,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage13_Accumulate -> Layer0_Device2_Stage14_Accumulate	[pos="e,12400,17062 12400,17161 12400,17161 12400,17072 12400,17072"];
	Layer0_Device2_Stage14_RecvKV -> Layer0_Device2_Stage14_Attention	[pos="e,11879,17214 12212,17340 12023,17340 11879,17340 11879,17340 11879,17340 11879,17224 11879,17224"];
	Layer0_Device2_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13609,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device2_Stage14_RecvKV -> Layer0_Device2_Stage15_RecvKV	[label="Ring transfer",
		lp="13396,17284",
		pos="e,13348,17241 13348,17327 13348,17327 13348,17251 13348,17251"];
	Layer0_Device2_Stage14_Attention -> Layer0_Device2_Stage14_Accumulate	[pos="e,12168,17026 11879,17161 11879,17115 11879,17026 11879,17026 11879,17026 12158,17026 12158,17026"];
	Layer0_Device2_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="14012,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device2_Stage14_Accumulate -> Layer0_Device2_Stage15_Accumulate	[pos="e,13780,16909 12400,17008 12400,16971 12400,16909 12400,16909 12400,16909 13770,16909 13770,16909"];
	Layer0_Device2_Stage15_RecvKV -> Layer0_Device2_Stage15_Attention	[pos="e,14012,17062 14012,17134 14012,17134 14012,17072 14012,17072"];
	Layer0_Device2_Stage15_Attention -> Layer0_Device2_Stage15_Accumulate	[pos="e,14012,16936 14012,17008 14012,17008 14012,16946 14012,16946"];
	Layer0_Device2_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14218,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device2_Stage15_Accumulate -> Layer0_Device2_ConcatHeads	[pos="e,14118,16810 14118,16882 14118,16882 14118,16820 14118,16820"];
	Layer0_Device2_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14330,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device2_ConcatHeads -> Layer0_Device2_OutputProj	[pos="e,14280,16684 14280,16756 14280,16756 14280,16694 14280,16694"];
	Layer0_Device2_OutputProj -> Layer0_Device2_Residual1	[pos="e,14443,16558 14443,16630 14443,16630 14443,16568 14443,16568"];
	Layer0_Device2_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14433,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device2_Residual1 -> Layer0_Device2_LayerNorm2	[pos="e,14495,16432 14495,16504 14495,16504 14495,16442 14495,16442"];
	Layer0_Device2_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="14732,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device2_Residual1 -> Layer0_Device2_Residual2	[pos="e,14897,15750 14897,16504 14897,16504 14897,15760 14897,15760"];
	Layer0_Device2_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14140,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device2_LayerNorm2 -> Layer0_Device2_GateProj	[pos="e,14292,16306 14292,16378 14292,16378 14292,16316 14292,16316"];
	Layer0_Device2_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14453,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device2_LayerNorm2 -> Layer0_Device2_UpProj	[pos="e,14506,16217 14506,16378 14506,16378 14506,16227 14506,16227"];
	Layer0_Device2_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14140,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device2_GateProj -> Layer0_Device2_Activation	[pos="e,14072,16128 14072,16252 14072,16252 14072,16138 14072,16138"];
	Layer0_Device2_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14236,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device2_UpProj -> Layer0_Device2_ElemMul	[pos="e,14506,16002 14506,16163 14506,16163 14506,16012 14506,16012"];
	Layer0_Device2_Activation -> Layer0_Device2_ElemMul	[pos="e,14140,16002 14140,16074 14140,16074 14140,16012 14140,16012"];
	Layer0_Device2_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14332,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device2_ElemMul -> Layer0_Device2_DownProj	[pos="e,14332,15876 14332,15948 14332,15948 14332,15886 14332,15886"];
	Layer0_Device2_DownProj -> Layer0_Device2_Residual2	[pos="e,14448,15750 14448,15822 14448,15822 14448,15760 14448,15760"];
	Layer0_Device2_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14732,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device2_Residual2 -> Layer0_Device2_Output	[pos="e,14732,15624 14732,15696 14732,15696 14732,15634 14732,15634"];
	Layer1_Device2_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14732,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device2_Output -> Layer1_Device2_Input	[pos="e,14732,15476 14732,15548 14732,15548 14732,15486 14732,15486"];
	Layer0_Device3_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19589,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device3_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19046,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device3_Input -> Layer0_Device3_LayerNorm1	[pos="e,19259,20375 19299,20502 19299,20470 19299,20375 19299,20375 19299,20375 19269,20375 19269,20375"];
	Layer0_Device3_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="19570,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device3_Input -> Layer0_Device3_Residual1	[pos="e,19712,16558 19712,20478 19712,20478 19712,16568 19712,16568"];
	Layer0_Device3_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18539,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device3_LayerNorm1 -> Layer0_Device3_QKVProj	[pos="e,19011,20276 19011,20348 19011,20348 19011,20286 19011,20286"];
	Layer0_Device3_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17318,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage0_RecvKV	[label="Local K,V",
		lp="17818,20179",
		pos="e,18064,20136 18064,20222 18064,20222 18064,20146 18064,20146"];
	Layer0_Device3_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18071,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage0_Attention	[label=Q_local,
		lp="18380,20083",
		pos="e,18270,19917 18270,20222 18270,20222 18270,19927 18270,19927"];
	Layer0_Device3_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15778,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage1_Attention	[label=Q_local,
		lp="15842,19986",
		pos="e,15744,19723 17889,20230 17063,20230 15744,20230 15744,20230 15744,20230 15744,19733 15744,19733"];
	Layer0_Device3_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16389,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage2_Attention	[label=Q_local,
		lp="18520,19890",
		pos="e,16559,19530 18365,20222 18365,20119 18365,19752 18365,19752 18365,19752 16559,19752 16559,19752 16559,19752 16559,19540 16559,\
19540"];
	Layer0_Device3_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18404,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage3_Attention	[label=Q_local,
		lp="18660,19793",
		pos="e,18577,19337 18577,20223 18577,20223 18577,19347 18577,19347"];
	Layer0_Device3_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15610,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage4_Attention	[label=Q_local,
		lp="15462,19697",
		pos="e,15462,19144 17889,20245 16987,20245 15462,20245 15462,20245 15462,20245 15462,19154 15462,19154"];
	Layer0_Device3_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18529,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage5_Attention	[label=Q_local,
		lp="18757,19600",
		pos="e,18698,18951 18698,20222 18698,20222 18698,18961 18698,18961"];
	Layer0_Device3_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18553,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage6_Attention	[label=Q_local,
		lp="18896,19504",
		pos="e,18772,18758 18772,20222 18772,20222 18772,18768 18772,18768"];
	Layer0_Device3_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15614,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage7_Attention	[label=Q_local,
		lp="15316,19407",
		pos="e,15382,18547 17889,20260 16942,20260 15294,20260 15294,20260 15294,20260 15294,18547 15294,18547 15294,18547 15372,18547 15372,\
18547"];
	Layer0_Device3_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18295,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage8_Attention	[label=Q_local,
		lp="19021,19311",
		pos="e,18527,18345 18843,20223 18843,19993 18843,18345 18843,18345 18843,18345 18537,18345 18537,18345"];
	Layer0_Device3_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15356,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage9_Attention	[label=Q_local,
		lp="15198,19214",
		pos="e,15209,18179 17889,20268 16920,20268 15209,20268 15209,20268 15209,20268 15209,18189 15209,18189"];
	Layer0_Device3_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15785,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage10_Attention	[label=Q_local,
		lp="15058,19118",
		pos="e,15893,17986 18901,20223 18901,19976 18901,18091 18901,18091 18901,18091 15893,18091 15893,18091 15893,18091 15893,17996 15893,\
17996"];
	Layer0_Device3_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16947,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage11_Attention	[label=Q_local,
		lp="19139,19021",
		pos="e,17024,17793 18959,20222 18959,19958 18959,17823 18959,17823 18959,17823 17024,17823 17024,17823 17024,17823 17024,17803 17024,\
17803"];
	Layer0_Device3_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="17182,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage12_Attention	[label=Q_local,
		lp="19285,18925",
		pos="e,17207,17601 19017,20222 19017,19957 19017,17821 19017,17821 19017,17821 17207,17821 17207,17821 17207,17821 17207,17611 17207,\
17611"];
	Layer0_Device3_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="19197,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage13_Attention	[label=Q_local,
		lp="19425,18828",
		pos="e,19370,17408 19189,20235 19294,20235 19370,20235 19370,20235 19370,20235 19370,17418 19370,17418"];
	Layer0_Device3_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16779,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage14_Attention	[label=Q_local,
		lp="14912,18732",
		pos="e,16547,17187 19132,20223 19132,19948 19132,17648 19132,17648 19132,17648 16414,17648 16414,17648 16414,17648 16414,17187 16414,\
17187 16414,17187 16537,17187 16537,17187"];
	Layer0_Device3_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18741,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_QKVProj -> Layer0_Device3_Stage15_Attention	[label=Q_local,
		lp="19543,18635",
		pos="e,18973,17043 19189,20249 19352,20249 19481,20249 19481,20249 19481,20249 19481,17043 19481,17043 19481,17043 18983,17043 18983,\
17043"];
	Layer0_Device3_Stage0_RecvKV -> Layer0_Device3_Stage0_Attention	[pos="e,18039,19916 18039,20079 18039,20079 18039,19926 18039,19926"];
	Layer0_Device3_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="16862,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage0_RecvKV -> Layer0_Device3_Stage1_RecvKV	[label="Ring transfer",
		lp="17138,19986",
		pos="e,17090,19943 17090,20029 17090,20029 17090,19953 17090,19953"];
	Layer0_Device3_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="16299,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage0_Attention -> Layer0_Device3_Stage0_Accumulate	[pos="e,16299,19723 18071,19863 18071,19823 18071,19754 18071,19754 18071,19754 16299,19754 16299,19754 16299,19754 16299,19733 16299,\
19733"];
	Layer0_Device3_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="15868,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage0_Accumulate -> Layer0_Device3_Stage1_Accumulate	[pos="e,16084,19530 16084,19670 16084,19670 16084,19540 16084,19540"];
	Layer0_Device3_Stage1_RecvKV -> Layer0_Device3_Stage1_Attention	[pos="e,15976,19724 15976,19889 15976,19889 15976,19734 15976,19734"];
	Layer0_Device3_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17508,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage1_RecvKV -> Layer0_Device3_Stage2_RecvKV	[label="Ring transfer",
		lp="17179,19793",
		pos="e,17185,19750 17185,19836 17185,19836 17185,19760 17185,19760"];
	Layer0_Device3_Stage1_Attention -> Layer0_Device3_Stage1_Accumulate	[pos="e,15823,19530 15823,19670 15823,19670 15823,19540 15823,19540"];
	Layer0_Device3_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="15986,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage1_Accumulate -> Layer0_Device3_Stage2_Accumulate	[pos="e,15927,19337 15927,19477 15927,19477 15927,19347 15927,19347"];
	Layer0_Device3_Stage2_RecvKV -> Layer0_Device3_Stage2_Attention	[pos="e,16604,19531 16604,19696 16604,19696 16604,19541 16604,19541"];
	Layer0_Device3_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17598,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage2_RecvKV -> Layer0_Device3_Stage3_RecvKV	[label="Ring transfer",
		lp="17601,19600",
		pos="e,17553,19557 17553,19643 17553,19643 17553,19567 17553,19567"];
	Layer0_Device3_Stage2_Attention -> Layer0_Device3_Stage2_Accumulate	[pos="e,16188,19337 16188,19477 16188,19477 16188,19347 16188,19347"];
	Layer0_Device3_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16131,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage2_Accumulate -> Layer0_Device3_Stage3_Accumulate	[pos="e,16058,19144 16058,19284 16058,19284 16058,19154 16058,19154"];
	Layer0_Device3_Stage3_RecvKV -> Layer0_Device3_Stage3_Attention	[pos="e,18345,19338 18345,19503 18345,19503 18345,19348 18345,19348"];
	Layer0_Device3_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17195,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage3_RecvKV -> Layer0_Device3_Stage4_RecvKV	[label="Ring transfer",
		lp="17512,19407",
		pos="e,17396,19364 17396,19450 17396,19450 17396,19374 17396,19374"];
	Layer0_Device3_Stage3_Attention -> Layer0_Device3_Stage3_Accumulate	[pos="e,16319,19144 18216,19284 18216,19248 18216,19189 18216,19189 18216,19189 16319,19189 16319,19189 16319,19189 16319,19154 16319,\
19154"];
	Layer0_Device3_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16111,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage3_Accumulate -> Layer0_Device3_Stage4_Accumulate	[pos="e,16121,18951 16121,19091 16121,19091 16121,18961 16121,18961"];
	Layer0_Device3_Stage4_RecvKV -> Layer0_Device3_Stage4_Attention	[pos="e,15798,19144 16319,19270 16038,19270 15798,19270 15798,19270 15798,19270 15798,19154 15798,19154"];
	Layer0_Device3_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17340,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage4_RecvKV -> Layer0_Device3_Stage5_RecvKV	[label="Ring transfer",
		lp="17315,19214",
		pos="e,17268,19171 17268,19257 17268,19257 17268,19181 17268,19181"];
	Layer0_Device3_Stage4_Attention -> Layer0_Device3_Stage4_Accumulate	[pos="e,15890,18951 15842,19117 15870,19117 15890,19117 15890,19117 15890,19117 15890,18961 15890,18961"];
	Layer0_Device3_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18032,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage4_Accumulate -> Layer0_Device3_Stage5_Accumulate	[pos="e,18020,18758 16123,18898 16123,18866 16123,18818 16123,18818 16123,18818 18020,18818 18020,18818 18020,18818 18020,18768 18020,\
18768"];
	Layer0_Device3_Stage5_RecvKV -> Layer0_Device3_Stage5_Attention	[pos="e,18297,18924 18250,19117 18250,19112 18250,18924 18250,18924 18250,18924 18287,18924 18287,18924"];
	Layer0_Device3_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17320,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage5_RecvKV -> Layer0_Device3_Stage6_RecvKV	[label="Ring transfer",
		lp="17378,19021",
		pos="e,17330,18978 17330,19064 17330,19064 17330,18988 17330,18988"];
	Layer0_Device3_Stage5_Attention -> Layer0_Device3_Stage5_Accumulate	[pos="e,18264,18731 18310,18898 18310,18845 18310,18731 18310,18731 18310,18731 18274,18731 18274,18731"];
	Layer0_Device3_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18032,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage5_Accumulate -> Layer0_Device3_Stage6_Accumulate	[pos="e,18032,18565 18032,18705 18032,18705 18032,18575 18032,18575"];
	Layer0_Device3_Stage6_RecvKV -> Layer0_Device3_Stage6_Attention	[pos="e,18541,18758 17908,18884 18220,18884 18541,18884 18541,18884 18541,18884 18541,18768 18541,18768"];
	Layer0_Device3_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16823,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage6_RecvKV -> Layer0_Device3_Stage7_RecvKV	[label="Ring transfer",
		lp="17115,18828",
		pos="e,17072,18785 17072,18871 17072,18871 17072,18795 17072,18795"];
	Layer0_Device3_Stage6_Attention -> Layer0_Device3_Stage6_Accumulate	[pos="e,18264,18529 18424,18705 18424,18650 18424,18529 18424,18529 18424,18529 18274,18529 18274,18529"];
	Layer0_Device3_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17774,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage6_Accumulate -> Layer0_Device3_Stage7_Accumulate	[pos="e,17903,18372 17903,18512 17903,18512 17903,18382 17903,18382"];
	Layer0_Device3_Stage7_RecvKV -> Layer0_Device3_Stage7_Attention	[pos="e,15612,18565 15947,18691 15757,18691 15612,18691 15612,18691 15612,18691 15612,18575 15612,18575"];
	Layer0_Device3_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16823,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage7_RecvKV -> Layer0_Device3_Stage8_RecvKV	[label="Ring transfer",
		lp="16871,18635",
		pos="e,16823,18592 16823,18678 16823,18678 16823,18602 16823,18602"];
	Layer0_Device3_Stage7_Attention -> Layer0_Device3_Stage7_Accumulate	[pos="e,17643,18372 15745,18512 15745,18474 15745,18408 15745,18408 15745,18408 17643,18408 17643,18408 17643,18408 17643,18382 17643,\
18382"];
	Layer0_Device3_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17774,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage7_Accumulate -> Layer0_Device3_Stage8_Accumulate	[pos="e,17774,18179 17774,18319 17774,18319 17774,18189 17774,18189"];
	Layer0_Device3_Stage8_RecvKV -> Layer0_Device3_Stage8_Attention	[pos="e,18164,18372 17410,18498 17771,18498 18164,18498 18164,18498 18164,18498 18164,18382 18164,18382"];
	Layer0_Device3_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16565,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage8_RecvKV -> Layer0_Device3_Stage9_RecvKV	[label="Ring transfer",
		lp="16615,18442",
		pos="e,16694,18399 16694,18485 16694,18485 16694,18409 16694,18409"];
	Layer0_Device3_Stage8_Attention -> Layer0_Device3_Stage8_Accumulate	[pos="e,18006,18152 18249,18319 18249,18266 18249,18152 18249,18152 18249,18152 18016,18152 18016,18152"];
	Layer0_Device3_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16306,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage8_Accumulate -> Layer0_Device3_Stage9_Accumulate	[pos="e,16383,17986 17774,18126 17774,18090 17774,18032 17774,18032 17774,18032 16383,18032 16383,18032 16383,18032 16383,17996 16383,\
17996"];
	Layer0_Device3_Stage9_RecvKV -> Layer0_Device3_Stage9_Attention	[pos="e,15485,18179 15689,18305 15569,18305 15485,18305 15485,18305 15485,18305 15485,18189 15485,18189"];
	Layer0_Device3_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16565,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage9_RecvKV -> Layer0_Device3_Stage10_RecvKV	[label="Ring transfer",
		lp="16613,18249",
		pos="e,16565,18206 16565,18292 16565,18292 16565,18216 16565,18216"];
	Layer0_Device3_Stage9_Attention -> Layer0_Device3_Stage9_Accumulate	[pos="e,16229,17986 15570,18126 15570,18090 15570,18029 15570,18029 15570,18029 16229,18029 16229,18029 16229,18029 16229,17996 16229,\
17996"];
	Layer0_Device3_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16306,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage9_Accumulate -> Layer0_Device3_Stage10_Accumulate	[pos="e,16306,17793 16306,17933 16306,17933 16306,17803 16306,17803"];
	Layer0_Device3_Stage10_RecvKV -> Layer0_Device3_Stage10_Attention	[pos="e,15769,17986 15769,18099 15769,18099 15769,17996 15769,17996"];
	Layer0_Device3_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17515,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage10_RecvKV -> Layer0_Device3_Stage11_RecvKV	[label="Ring transfer",
		lp="16882,18056",
		pos="e,17040,18013 17040,18099 17040,18099 17040,18023 17040,18023"];
	Layer0_Device3_Stage10_Attention -> Layer0_Device3_Stage10_Accumulate	[pos="e,16074,17766 15979,17933 15979,17880 15979,17766 15979,17766 15979,17766 16064,17766 16064,17766"];
	Layer0_Device3_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16661,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage10_Accumulate -> Layer0_Device3_Stage11_Accumulate	[pos="e,16484,17600 16484,17740 16484,17740 16484,17610 16484,17610"];
	Layer0_Device3_Stage11_RecvKV -> Layer0_Device3_Stage11_Attention	[pos="e,16870,17793 16870,17906 16870,17906 16870,17803 16870,17803"];
	Layer0_Device3_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18156,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage11_RecvKV -> Layer0_Device3_Stage12_RecvKV	[label="Ring transfer",
		lp="17717,17863",
		pos="e,17836,17820 17836,17906 17836,17906 17836,17830 17836,17830"];
	Layer0_Device3_Stage11_Attention -> Layer0_Device3_Stage11_Accumulate	[pos="e,16804,17600 16804,17740 16804,17740 16804,17610 16804,17610"];
	Layer0_Device3_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16779,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage11_Accumulate -> Layer0_Device3_Stage12_Accumulate	[pos="e,16720,17407 16720,17547 16720,17547 16720,17417 16720,17417"];
	Layer0_Device3_Stage12_RecvKV -> Layer0_Device3_Stage12_Attention	[pos="e,17325,17600 17325,17713 17325,17713 17325,17610 17325,17610"];
	Layer0_Device3_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18391,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage12_RecvKV -> Layer0_Device3_Stage13_RecvKV	[label="Ring transfer",
		lp="18262,17670",
		pos="e,18274,17627 18274,17713 18274,17713 18274,17637 18274,17637"];
	Layer0_Device3_Stage12_Attention -> Layer0_Device3_Stage12_Accumulate	[pos="e,16980,17407 16980,17547 16980,17547 16980,17417 16980,17417"];
	Layer0_Device3_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17300,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage12_Accumulate -> Layer0_Device3_Stage13_Accumulate	[pos="e,17068,17187 17011,17380 17028,17380 17039,17380 17039,17380 17039,17380 17039,17187 17039,17187 17039,17187 17058,17187 17058,\
17187"];
	Layer0_Device3_Stage13_RecvKV -> Layer0_Device3_Stage13_Attention	[pos="e,19138,17408 19138,17573 19138,17573 19138,17418 19138,17418"];
	Layer0_Device3_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17988,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage13_RecvKV -> Layer0_Device3_Stage14_RecvKV	[label="Ring transfer",
		lp="18172,17477",
		pos="e,18190,17434 18190,17520 18190,17520 18190,17444 18190,17444"];
	Layer0_Device3_Stage13_Attention -> Layer0_Device3_Stage13_Accumulate	[pos="e,17300,17215 19197,17354 19197,17319 19197,17261 19197,17261 19197,17261 17300,17261 17300,17261 17300,17261 17300,17225 17300,\
17225"];
	Layer0_Device3_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17300,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage13_Accumulate -> Layer0_Device3_Stage14_Accumulate	[pos="e,17300,17062 17300,17161 17300,17161 17300,17072 17300,17072"];
	Layer0_Device3_Stage14_RecvKV -> Layer0_Device3_Stage14_Attention	[pos="e,16779,17214 17112,17340 16923,17340 16779,17340 16779,17340 16779,17340 16779,17224 16779,17224"];
	Layer0_Device3_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18509,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device3_Stage14_RecvKV -> Layer0_Device3_Stage15_RecvKV	[label="Ring transfer",
		lp="18296,17284",
		pos="e,18248,17241 18248,17327 18248,17327 18248,17251 18248,17251"];
	Layer0_Device3_Stage14_Attention -> Layer0_Device3_Stage14_Accumulate	[pos="e,17068,17026 16779,17161 16779,17115 16779,17026 16779,17026 16779,17026 17058,17026 17058,17026"];
	Layer0_Device3_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18741,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device3_Stage14_Accumulate -> Layer0_Device3_Stage15_Accumulate	[pos="e,18509,16909 17300,17008 17300,16971 17300,16909 17300,16909 17300,16909 18499,16909 18499,16909"];
	Layer0_Device3_Stage15_RecvKV -> Layer0_Device3_Stage15_Attention	[pos="e,18741,17062 18741,17134 18741,17134 18741,17072 18741,17072"];
	Layer0_Device3_Stage15_Attention -> Layer0_Device3_Stage15_Accumulate	[pos="e,18741,16936 18741,17008 18741,17008 18741,16946 18741,16946"];
	Layer0_Device3_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="18857,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device3_Stage15_Accumulate -> Layer0_Device3_ConcatHeads	[pos="e,18802,16810 18802,16882 18802,16882 18802,16820 18802,16820"];
	Layer0_Device3_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19321,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device3_ConcatHeads -> Layer0_Device3_OutputProj	[pos="e,19108,16657 18979,16756 18979,16719 18979,16657 18979,16657 18979,16657 19098,16657 19098,16657"];
	Layer0_Device3_OutputProj -> Layer0_Device3_Residual1	[pos="e,19359,16558 19359,16630 19359,16630 19359,16568 19359,16568"];
	Layer0_Device3_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19394,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device3_Residual1 -> Layer0_Device3_LayerNorm2	[pos="e,19395,16432 19395,16504 19395,16504 19395,16442 19395,16442"];
	Layer0_Device3_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="19523,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device3_Residual1 -> Layer0_Device3_Residual2	[pos="e,19774,15750 19774,16504 19774,16504 19774,15760 19774,15760"];
	Layer0_Device3_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19101,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device3_LayerNorm2 -> Layer0_Device3_GateProj	[pos="e,19254,16306 19254,16378 19254,16378 19254,16316 19254,16316"];
	Layer0_Device3_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19414,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device3_LayerNorm2 -> Layer0_Device3_UpProj	[pos="e,19466,16217 19466,16378 19466,16378 19466,16227 19466,16227"];
	Layer0_Device3_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19101,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device3_GateProj -> Layer0_Device3_Activation	[pos="e,19032,16128 19032,16252 19032,16252 19032,16138 19032,16138"];
	Layer0_Device3_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19197,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device3_UpProj -> Layer0_Device3_ElemMul	[pos="e,19467,16002 19467,16163 19467,16163 19467,16012 19467,16012"];
	Layer0_Device3_Activation -> Layer0_Device3_ElemMul	[pos="e,19101,16002 19101,16074 19101,16074 19101,16012 19101,16012"];
	Layer0_Device3_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19326,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device3_ElemMul -> Layer0_Device3_DownProj	[pos="e,19326,15876 19326,15948 19326,15948 19326,15886 19326,15886"];
	Layer0_Device3_DownProj -> Layer0_Device3_Residual2	[pos="e,19341,15750 19341,15822 19341,15822 19341,15760 19341,15760"];
	Layer0_Device3_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19523,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device3_Residual2 -> Layer0_Device3_Output	[pos="e,19523,15624 19523,15696 19523,15696 19523,15634 19523,15634"];
	Layer1_Device3_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19523,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device3_Output -> Layer1_Device3_Input	[pos="e,19523,15476 19523,15548 19523,15548 19523,15486 19523,15486"];
	Layer0_Device4_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24371,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device4_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="23886,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device4_Input -> Layer0_Device4_LayerNorm1	[pos="e,24084,20402 24084,20500 24084,20500 24084,20412 24084,20412"];
	Layer0_Device4_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="24218,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device4_Input -> Layer0_Device4_Residual1	[pos="e,24524,16558 24524,20480 24524,20480 24524,16568 24524,16568"];
	Layer0_Device4_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23464,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device4_LayerNorm1 -> Layer0_Device4_QKVProj	[pos="e,23886,20276 23886,20348 23886,20348 23886,20286 23886,20286"];
	Layer0_Device4_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22218,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage0_RecvKV	[label="Local K,V",
		lp="22718,20179",
		pos="e,22976,20136 22976,20222 22976,20222 22976,20146 22976,20146"];
	Layer0_Device4_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23008,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage0_Attention	[label=Q_local,
		lp="23280,20083",
		pos="e,23189,19917 23189,20222 23189,20222 23189,19927 23189,19927"];
	Layer0_Device4_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20678,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage1_Attention	[label=Q_local,
		lp="20742,19986",
		pos="e,20663,19723 22814,20229 21987,20229 20663,20229 20663,20229 20663,20229 20663,19733 20663,19733"];
	Layer0_Device4_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21289,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage2_Attention	[label=Q_local,
		lp="23420,19890",
		pos="e,21459,19530 23284,20222 23284,20125 23284,19794 23284,19794 23284,19794 21459,19794 21459,19794 21459,19794 21459,19540 21459,\
19540"];
	Layer0_Device4_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23304,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage3_Attention	[label=Q_local,
		lp="23560,19793",
		pos="e,23477,19337 23477,20223 23477,20223 23477,19347 23477,19347"];
	Layer0_Device4_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20510,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage4_Attention	[label=Q_local,
		lp="20362,19697",
		pos="e,20362,19144 22814,20242 21905,20242 20362,20242 20362,20242 20362,20242 20362,19154 20362,19154"];
	Layer0_Device4_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23427,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage5_Attention	[label=Q_local,
		lp="23664,19600",
		pos="e,23597,18951 23597,20222 23597,20222 23597,18961 23597,18961"];
	Layer0_Device4_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23502,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage6_Attention	[label=Q_local,
		lp="23796,19504",
		pos="e,23696,18758 23696,20222 23696,20222 23696,18768 23696,18768"];
	Layer0_Device4_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20563,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage7_Attention	[label=Q_local,
		lp="20216,19407",
		pos="e,20331,18547 22814,20255 21869,20255 20227,20255 20227,20255 20227,20255 20227,18547 20227,18547 20227,18547 20321,18547 20321,\
18547"];
	Layer0_Device4_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23295,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage8_Attention	[label=Q_local,
		lp="23919,19311",
		pos="e,23527,18345 23769,20223 23769,19993 23769,18345 23769,18345 23769,18345 23537,18345 23537,18345"];
	Layer0_Device4_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20356,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage9_Attention	[label=Q_local,
		lp="20098,19214",
		pos="e,20176,18179 22814,20262 21856,20262 20176,20262 20176,20262 20176,20262 20176,18189 20176,18189"];
	Layer0_Device4_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20531,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage10_Attention	[label=Q_local,
		lp="19958,19118",
		pos="e,20299,17959 22814,20269 21829,20269 20068,20269 20068,20269 20068,20269 20068,17959 20068,17959 20068,17959 20289,17959 20289,\
17959"];
	Layer0_Device4_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21782,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage11_Attention	[label=Q_local,
		lp="24042,19021",
		pos="e,21859,17793 23840,20222 23840,19959 23840,17853 23840,17853 23840,17853 21859,17853 21859,17853 21859,17853 21859,17803 21859,\
17803"];
	Layer0_Device4_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="22080,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage12_Attention	[label=Q_local,
		lp="24183,18925",
		pos="e,22042,17600 23875,20223 23875,19961 23875,17851 23875,17851 23875,17851 22042,17851 22042,17851 22042,17851 22042,17610 22042,\
17610"];
	Layer0_Device4_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="24095,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage13_Attention	[label=Q_local,
		lp="24323,18828",
		pos="e,24268,17408 24114,20235 24204,20235 24268,20235 24268,20235 24268,20235 24268,17418 24268,17418"];
	Layer0_Device4_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21677,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage14_Attention	[label=Q_local,
		lp="19812,18732",
		pos="e,21445,17187 23804,20223 23804,19974 23804,18057 23804,18057 23804,18057 21307,18057 21307,18057 21307,18057 21307,17187 21307,\
17187 21307,17187 21435,17187 21435,17187"];
	Layer0_Device4_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="24212,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_QKVProj -> Layer0_Device4_Stage15_Attention	[label=Q_local,
		lp="24441,18635",
		pos="e,24385,17062 24114,20249 24266,20249 24385,20249 24385,20249 24385,20249 24385,17072 24385,17072"];
	Layer0_Device4_Stage0_RecvKV -> Layer0_Device4_Stage0_Attention	[pos="e,22957,19917 22957,20082 22957,20082 22957,19927 22957,19927"];
	Layer0_Device4_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="21799,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage0_RecvKV -> Layer0_Device4_Stage1_RecvKV	[label="Ring transfer",
		lp="22128,19986",
		pos="e,22008,19943 22008,20029 22008,20029 22008,19953 22008,19953"];
	Layer0_Device4_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="21199,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage0_Attention -> Layer0_Device4_Stage0_Accumulate	[pos="e,21199,19723 23008,19863 23008,19835 23008,19797 23008,19797 23008,19797 21199,19797 21199,19797 21199,19797 21199,19733 21199,\
19733"];
	Layer0_Device4_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20768,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage0_Accumulate -> Layer0_Device4_Stage1_Accumulate	[pos="e,20984,19530 20984,19670 20984,19670 20984,19540 20984,19540"];
	Layer0_Device4_Stage1_RecvKV -> Layer0_Device4_Stage1_Attention	[pos="e,20894,19724 20894,19889 20894,19889 20894,19734 20894,19734"];
	Layer0_Device4_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22408,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage1_RecvKV -> Layer0_Device4_Stage2_RecvKV	[label="Ring transfer",
		lp="22106,19793",
		pos="e,22104,19750 22104,19836 22104,19836 22104,19760 22104,19760"];
	Layer0_Device4_Stage1_Attention -> Layer0_Device4_Stage1_Accumulate	[pos="e,20723,19530 20723,19670 20723,19670 20723,19540 20723,19540"];
	Layer0_Device4_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20886,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage1_Accumulate -> Layer0_Device4_Stage2_Accumulate	[pos="e,20827,19337 20827,19477 20827,19477 20827,19347 20827,19347"];
	Layer0_Device4_Stage2_RecvKV -> Layer0_Device4_Stage2_Attention	[pos="e,21504,19531 21504,19696 21504,19696 21504,19541 21504,19541"];
	Layer0_Device4_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22498,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage2_RecvKV -> Layer0_Device4_Stage3_RecvKV	[label="Ring transfer",
		lp="22501,19600",
		pos="e,22453,19557 22453,19643 22453,19643 22453,19567 22453,19567"];
	Layer0_Device4_Stage2_Attention -> Layer0_Device4_Stage2_Accumulate	[pos="e,21088,19337 21088,19477 21088,19477 21088,19347 21088,19347"];
	Layer0_Device4_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21031,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage2_Accumulate -> Layer0_Device4_Stage3_Accumulate	[pos="e,20958,19144 20958,19284 20958,19284 20958,19154 20958,19154"];
	Layer0_Device4_Stage3_RecvKV -> Layer0_Device4_Stage3_Attention	[pos="e,23245,19338 23245,19503 23245,19503 23245,19348 23245,19348"];
	Layer0_Device4_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22095,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage3_RecvKV -> Layer0_Device4_Stage4_RecvKV	[label="Ring transfer",
		lp="22279,19407",
		pos="e,22296,19364 22296,19450 22296,19450 22296,19374 22296,19374"];
	Layer0_Device4_Stage3_Attention -> Layer0_Device4_Stage3_Accumulate	[pos="e,21219,19144 23116,19284 23116,19249 23116,19194 23116,19194 23116,19194 21219,19194 21219,19194 21219,19194 21219,19154 21219,\
19154"];
	Layer0_Device4_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21009,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage3_Accumulate -> Layer0_Device4_Stage4_Accumulate	[pos="e,21020,18951 21020,19091 21020,19091 21020,18961 21020,18961"];
	Layer0_Device4_Stage4_RecvKV -> Layer0_Device4_Stage4_Attention	[pos="e,20698,19144 21219,19270 20938,19270 20698,19270 20698,19270 20698,19270 20698,19154 20698,19154"];
	Layer0_Device4_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22240,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage4_RecvKV -> Layer0_Device4_Stage5_RecvKV	[label="Ring transfer",
		lp="22215,19214",
		pos="e,22168,19171 22168,19257 22168,19257 22168,19181 22168,19181"];
	Layer0_Device4_Stage4_Attention -> Layer0_Device4_Stage4_Accumulate	[pos="e,20788,18951 20742,19117 20770,19117 20788,19117 20788,19117 20788,19117 20788,18961 20788,18961"];
	Layer0_Device4_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22981,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage4_Accumulate -> Layer0_Device4_Stage5_Accumulate	[pos="e,22944,18758 21046,18898 21046,18879 21046,18857 21046,18857 21046,18857 22944,18857 22944,18857 22944,18857 22944,18768 22944,\
18768"];
	Layer0_Device4_Stage5_RecvKV -> Layer0_Device4_Stage5_Attention	[pos="e,23195,18924 23149,19117 23149,19112 23149,18924 23149,18924 23149,18924 23185,18924 23185,18924"];
	Layer0_Device4_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22218,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage5_RecvKV -> Layer0_Device4_Stage6_RecvKV	[label="Ring transfer",
		lp="22277,19021",
		pos="e,22229,18978 22229,19064 22229,19064 22229,18988 22229,18988"];
	Layer0_Device4_Stage5_Attention -> Layer0_Device4_Stage5_Accumulate	[pos="e,23204,18758 23204,18898 23204,18898 23204,18768 23204,18768"];
	Layer0_Device4_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22981,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage5_Accumulate -> Layer0_Device4_Stage6_Accumulate	[pos="e,22981,18565 22981,18705 22981,18705 22981,18575 22981,18575"];
	Layer0_Device4_Stage6_RecvKV -> Layer0_Device4_Stage6_Attention	[pos="e,23464,18758 22806,18884 23129,18884 23464,18884 23464,18884 23464,18884 23464,18768 23464,18768"];
	Layer0_Device4_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21772,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage6_RecvKV -> Layer0_Device4_Stage7_RecvKV	[label="Ring transfer",
		lp="21977,18828",
		pos="e,21995,18785 21995,18871 21995,18871 21995,18795 21995,18795"];
	Layer0_Device4_Stage6_Attention -> Layer0_Device4_Stage6_Accumulate	[pos="e,23213,18529 23398,18705 23398,18650 23398,18529 23398,18529 23398,18529 23223,18529 23223,18529"];
	Layer0_Device4_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22774,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage6_Accumulate -> Layer0_Device4_Stage7_Accumulate	[pos="e,22878,18372 22878,18512 22878,18512 22878,18382 22878,18382"];
	Layer0_Device4_Stage7_RecvKV -> Layer0_Device4_Stage7_Attention	[pos="e,20786,18565 20896,18691 20828,18691 20786,18691 20786,18691 20786,18691 20786,18575 20786,18575"];
	Layer0_Device4_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21772,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage7_RecvKV -> Layer0_Device4_Stage8_RecvKV	[label="Ring transfer",
		lp="21820,18635",
		pos="e,21772,18592 21772,18678 21772,18678 21772,18602 21772,18602"];
	Layer0_Device4_Stage7_Attention -> Layer0_Device4_Stage7_Accumulate	[pos="e,22617,18373 20720,18512 20720,18477 20720,18419 20720,18419 20720,18419 22617,18419 22617,18419 22617,18419 22617,18383 22617,\
18383"];
	Layer0_Device4_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22774,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage7_Accumulate -> Layer0_Device4_Stage8_Accumulate	[pos="e,22774,18179 22774,18319 22774,18319 22774,18189 22774,18189"];
	Layer0_Device4_Stage8_RecvKV -> Layer0_Device4_Stage8_Attention	[pos="e,23138,18372 22360,18498 22730,18498 23138,18498 23138,18498 23138,18498 23138,18382 23138,18382"];
	Layer0_Device4_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21565,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage8_RecvKV -> Layer0_Device4_Stage9_RecvKV	[label="Ring transfer",
		lp="21651,18442",
		pos="e,21668,18399 21668,18485 21668,18485 21668,18409 21668,18409"];
	Layer0_Device4_Stage8_Attention -> Layer0_Device4_Stage8_Accumulate	[pos="e,23006,18152 23122,18319 23122,18266 23122,18152 23122,18152 23122,18152 23016,18152 23016,18152"];
	Layer0_Device4_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21052,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage8_Accumulate -> Layer0_Device4_Stage9_Accumulate	[pos="e,21129,17986 22774,18126 22774,18098 22774,18060 22774,18060 22774,18060 21129,18060 21129,18060 21129,18060 21129,17996 21129,\
17996"];
	Layer0_Device4_Stage9_RecvKV -> Layer0_Device4_Stage9_Attention	[pos="e,20460,18179 20689,18305 20555,18305 20460,18305 20460,18305 20460,18305 20460,18189 20460,18189"];
	Layer0_Device4_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21565,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage9_RecvKV -> Layer0_Device4_Stage10_RecvKV	[label="Ring transfer",
		lp="21613,18249",
		pos="e,21565,18206 21565,18292 21565,18292 21565,18216 21565,18216"];
	Layer0_Device4_Stage9_Attention -> Layer0_Device4_Stage9_Accumulate	[pos="e,20975,17986 20444,18126 20444,18088 20444,18021 20444,18021 20444,18021 20975,18021 20975,18021 20975,18021 20975,17996 20975,\
17996"];
	Layer0_Device4_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21052,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage9_Accumulate -> Layer0_Device4_Stage10_Accumulate	[pos="e,21052,17793 21052,17933 21052,17933 21052,17803 21052,17803"];
	Layer0_Device4_Stage10_RecvKV -> Layer0_Device4_Stage10_Attention	[pos="e,20704,17987 20704,18152 20704,18152 20704,17997 20704,17997"];
	Layer0_Device4_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22261,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage10_RecvKV -> Layer0_Device4_Stage11_RecvKV	[label="Ring transfer",
		lp="21882,18056",
		pos="e,21913,18013 21913,18099 21913,18099 21913,18023 21913,18023"];
	Layer0_Device4_Stage10_Attention -> Layer0_Device4_Stage10_Accumulate	[pos="e,20820,17766 20598,17933 20598,17880 20598,17766 20598,17766 20598,17766 20810,17766 20810,17766"];
	Layer0_Device4_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21559,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage10_Accumulate -> Layer0_Device4_Stage11_Accumulate	[pos="e,21334,17600 21284,17766 21314,17766 21334,17766 21334,17766 21334,17766 21334,17610 21334,17610"];
	Layer0_Device4_Stage11_RecvKV -> Layer0_Device4_Stage11_Attention	[pos="e,21705,17793 21705,17906 21705,17906 21705,17803 21705,17803"];
	Layer0_Device4_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22991,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage11_RecvKV -> Layer0_Device4_Stage12_RecvKV	[label="Ring transfer",
		lp="22674,17863",
		pos="e,22626,17820 22626,17906 22626,17906 22626,17830 22626,17830"];
	Layer0_Device4_Stage11_Attention -> Layer0_Device4_Stage11_Accumulate	[pos="e,21670,17600 21670,17740 21670,17740 21670,17610 21670,17610"];
	Layer0_Device4_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21677,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage11_Accumulate -> Layer0_Device4_Stage12_Accumulate	[pos="e,21618,17407 21618,17547 21618,17547 21618,17417 21618,17417"];
	Layer0_Device4_Stage12_RecvKV -> Layer0_Device4_Stage12_Attention	[pos="e,22191,17600 22191,17713 22191,17713 22191,17610 22191,17610"];
	Layer0_Device4_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23289,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage12_RecvKV -> Layer0_Device4_Stage13_RecvKV	[label="Ring transfer",
		lp="23314,17670",
		pos="e,23140,17627 23140,17713 23140,17713 23140,17637 23140,17637"];
	Layer0_Device4_Stage12_Attention -> Layer0_Device4_Stage12_Accumulate	[pos="e,21878,17407 21878,17547 21878,17547 21878,17417 21878,17417"];
	Layer0_Device4_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22198,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage12_Accumulate -> Layer0_Device4_Stage13_Accumulate	[pos="e,21966,17187 21909,17380 21926,17380 21937,17380 21937,17380 21937,17380 21937,17187 21937,17187 21937,17187 21956,17187 21956,\
17187"];
	Layer0_Device4_Stage13_RecvKV -> Layer0_Device4_Stage13_Attention	[pos="e,24036,17408 24036,17573 24036,17573 24036,17418 24036,17418"];
	Layer0_Device4_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22886,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage13_RecvKV -> Layer0_Device4_Stage14_RecvKV	[label="Ring transfer",
		lp="23070,17477",
		pos="e,23088,17434 23088,17520 23088,17520 23088,17444 23088,17444"];
	Layer0_Device4_Stage13_Attention -> Layer0_Device4_Stage13_Accumulate	[pos="e,22198,17214 24095,17354 24095,17320 24095,17266 24095,17266 24095,17266 22198,17266 22198,17266 22198,17266 22198,17224 22198,\
17224"];
	Layer0_Device4_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22198,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage13_Accumulate -> Layer0_Device4_Stage14_Accumulate	[pos="e,22198,17062 22198,17161 22198,17161 22198,17072 22198,17072"];
	Layer0_Device4_Stage14_RecvKV -> Layer0_Device4_Stage14_Attention	[pos="e,21677,17214 22010,17340 21821,17340 21677,17340 21677,17340 21677,17340 21677,17224 21677,17224"];
	Layer0_Device4_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23407,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device4_Stage14_RecvKV -> Layer0_Device4_Stage15_RecvKV	[label="Ring transfer",
		lp="23194,17284",
		pos="e,23146,17241 23146,17327 23146,17327 23146,17251 23146,17251"];
	Layer0_Device4_Stage14_Attention -> Layer0_Device4_Stage14_Accumulate	[pos="e,21966,17026 21677,17161 21677,17115 21677,17026 21677,17026 21677,17026 21956,17026 21956,17026"];
	Layer0_Device4_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="24212,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device4_Stage14_Accumulate -> Layer0_Device4_Stage15_Accumulate	[pos="e,23980,16909 22198,17008 22198,16971 22198,16909 22198,16909 22198,16909 23970,16909 23970,16909"];
	Layer0_Device4_Stage15_RecvKV -> Layer0_Device4_Stage15_Attention	[pos="e,24154,17062 24154,17188 24154,17188 24154,17072 24154,17072"];
	Layer0_Device4_Stage15_Attention -> Layer0_Device4_Stage15_Accumulate	[pos="e,24212,16936 24212,17008 24212,17008 24212,16946 24212,16946"];
	Layer0_Device4_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24215,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device4_Stage15_Accumulate -> Layer0_Device4_ConcatHeads	[pos="e,24215,16810 24215,16882 24215,16882 24215,16820 24215,16820"];
	Layer0_Device4_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24217,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device4_ConcatHeads -> Layer0_Device4_OutputProj	[pos="e,24217,16684 24217,16756 24217,16756 24217,16694 24217,16694"];
	Layer0_Device4_OutputProj -> Layer0_Device4_Residual1	[pos="e,24217,16558 24217,16630 24217,16630 24217,16568 24217,16568"];
	Layer0_Device4_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="23918,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device4_Residual1 -> Layer0_Device4_LayerNorm2	[pos="e,23981,16432 23981,16504 23981,16504 23981,16442 23981,16442"];
	Layer0_Device4_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="24217,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device4_Residual1 -> Layer0_Device4_Residual2	[pos="e,24383,15750 24383,16504 24383,16504 24383,15760 24383,15760"];
	Layer0_Device4_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="23625,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device4_LayerNorm2 -> Layer0_Device4_GateProj	[pos="e,23778,16306 23778,16378 23778,16378 23778,16316 23778,16316"];
	Layer0_Device4_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="23938,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device4_LayerNorm2 -> Layer0_Device4_UpProj	[pos="e,23990,16217 23990,16378 23990,16378 23990,16227 23990,16227"];
	Layer0_Device4_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="23625,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device4_GateProj -> Layer0_Device4_Activation	[pos="e,23556,16128 23556,16252 23556,16252 23556,16138 23556,16138"];
	Layer0_Device4_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="23721,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device4_UpProj -> Layer0_Device4_ElemMul	[pos="e,23991,16002 23991,16163 23991,16163 23991,16012 23991,16012"];
	Layer0_Device4_Activation -> Layer0_Device4_ElemMul	[pos="e,23625,16002 23625,16074 23625,16074 23625,16012 23625,16012"];
	Layer0_Device4_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="23864,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device4_ElemMul -> Layer0_Device4_DownProj	[pos="e,23864,15876 23864,15948 23864,15948 23864,15886 23864,15886"];
	Layer0_Device4_DownProj -> Layer0_Device4_Residual2	[pos="e,23957,15750 23957,15822 23957,15822 23957,15760 23957,15760"];
	Layer0_Device4_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24217,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device4_Residual2 -> Layer0_Device4_Output	[pos="e,24217,15624 24217,15696 24217,15696 24217,15634 24217,15634"];
	Layer1_Device4_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24217,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device4_Output -> Layer1_Device4_Input	[pos="e,24217,15476 24217,15548 24217,15548 24217,15486 24217,15486"];
	Layer0_Device5_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29102,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device5_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="28788,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device5_Input -> Layer0_Device5_LayerNorm1	[pos="e,28901,20402 28901,20484 28901,20484 28901,20412 28901,20412"];
	Layer0_Device5_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="29340,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device5_Input -> Layer0_Device5_Residual1	[pos="e,29368,16558 29368,20494 29368,20494 29368,16568 29368,16568"];
	Layer0_Device5_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28361,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device5_LayerNorm1 -> Layer0_Device5_QKVProj	[pos="e,28788,20276 28788,20348 28788,20348 28788,20286 28788,20286"];
	Layer0_Device5_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27115,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage0_RecvKV	[label="Local K,V",
		lp="27616,20179",
		pos="e,27873,20136 27873,20222 27873,20222 27873,20146 27873,20146"];
	Layer0_Device5_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="27905,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage0_Attention	[label=Q_local,
		lp="28177,20083",
		pos="e,28086,19917 28086,20222 28086,20222 28086,19927 28086,19927"];
	Layer0_Device5_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25575,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage1_Attention	[label=Q_local,
		lp="25639,19986",
		pos="e,25560,19723 27711,20229 26884,20229 25560,20229 25560,20229 25560,20229 25560,19733 25560,19733"];
	Layer0_Device5_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26186,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage2_Attention	[label=Q_local,
		lp="28317,19890",
		pos="e,26356,19530 28181,20222 28181,20125 28181,19799 28181,19799 28181,19799 26356,19799 26356,19799 26356,19799 26356,19540 26356,\
19540"];
	Layer0_Device5_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28201,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage3_Attention	[label=Q_local,
		lp="28457,19793",
		pos="e,28374,19337 28374,20223 28374,20223 28374,19347 28374,19347"];
	Layer0_Device5_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25407,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage4_Attention	[label=Q_local,
		lp="25259,19697",
		pos="e,25260,19144 27711,20242 26802,20242 25260,20242 25260,20242 25260,20242 25260,19154 25260,19154"];
	Layer0_Device5_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28320,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage5_Attention	[label=Q_local,
		lp="28561,19600",
		pos="e,28492,18951 28492,20222 28492,20222 28492,18961 28492,18961"];
	Layer0_Device5_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28348,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage6_Attention	[label=Q_local,
		lp="28693,19504",
		pos="e,28566,18758 28566,20222 28566,20222 28566,18768 28566,18768"];
	Layer0_Device5_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25409,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage7_Attention	[label=Q_local,
		lp="25113,19407",
		pos="e,25177,18547 27711,20255 26763,20255 25111,20255 25111,20255 25111,20255 25111,18547 25111,18547 25111,18547 25167,18547 25167,\
18547"];
	Layer0_Device5_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28153,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage8_Attention	[label=Q_local,
		lp="28812,19311",
		pos="e,28385,18345 28618,20223 28618,19993 28618,18345 28618,18345 28618,18345 28395,18345 28395,18345"];
	Layer0_Device5_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25214,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage9_Attention	[label=Q_local,
		lp="24995,19214",
		pos="e,25047,18179 27711,20262 26746,20262 25047,20262 25047,20262 25047,20262 25047,18189 25047,18189"];
	Layer0_Device5_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25408,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage10_Attention	[label=Q_local,
		lp="24855,19118",
		pos="e,25176,17959 27711,20269 26728,20269 24974,20269 24974,20269 24974,20269 24974,17959 24974,17959 24974,17959 25166,17959 25166,\
17959"];
	Layer0_Device5_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26643,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage11_Attention	[label=Q_local,
		lp="28933,19021",
		pos="e,26720,17793 28695,20223 28695,19963 28695,17885 28695,17885 28695,17885 26720,17885 26720,17885 26720,17885 26720,17803 26720,\
17803"];
	Layer0_Device5_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26973,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage12_Attention	[label=Q_local,
		lp="29076,18925",
		pos="e,26903,17600 28733,20223 28733,19962 28733,17879 28733,17879 28733,17879 26903,17879 26903,17879 26903,17879 26903,17610 26903,\
17610"];
	Layer0_Device5_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28988,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage13_Attention	[label=Q_local,
		lp="29216,18828",
		pos="e,29161,17408 29011,20235 29099,20235 29161,20235 29161,20235 29161,20235 29161,17418 29161,17418"];
	Layer0_Device5_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26570,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage14_Attention	[label=Q_local,
		lp="24709,18732",
		pos="e,26338,17187 28656,20223 28656,19973 28656,18051 28656,18051 28656,18051 26201,18051 26201,18051 26201,18051 26201,17187 26201,\
17187 26201,17187 26328,17187 26328,17187"];
	Layer0_Device5_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28452,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_QKVProj -> Layer0_Device5_Stage15_Attention	[label=Q_local,
		lp="29334,18635",
		pos="e,28684,17043 29011,20249 29160,20249 29276,20249 29276,20249 29276,20249 29276,17043 29276,17043 29276,17043 28694,17043 28694,\
17043"];
	Layer0_Device5_Stage0_RecvKV -> Layer0_Device5_Stage0_Attention	[pos="e,27854,19917 27854,20082 27854,20082 27854,19927 27854,19927"];
	Layer0_Device5_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="26696,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage0_RecvKV -> Layer0_Device5_Stage1_RecvKV	[label="Ring transfer",
		lp="27016,19986",
		pos="e,26906,19943 26906,20029 26906,20029 26906,19953 26906,19953"];
	Layer0_Device5_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="26096,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage0_Attention -> Layer0_Device5_Stage0_Accumulate	[pos="e,26096,19723 27905,19863 27905,19837 27905,19802 27905,19802 27905,19802 26096,19802 26096,19802 26096,19802 26096,19733 26096,\
19733"];
	Layer0_Device5_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25665,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage0_Accumulate -> Layer0_Device5_Stage1_Accumulate	[pos="e,25880,19530 25880,19670 25880,19670 25880,19540 25880,19540"];
	Layer0_Device5_Stage1_RecvKV -> Layer0_Device5_Stage1_Attention	[pos="e,25791,19724 25791,19889 25791,19889 25791,19734 25791,19734"];
	Layer0_Device5_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27305,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage1_RecvKV -> Layer0_Device5_Stage2_RecvKV	[label="Ring transfer",
		lp="27003,19793",
		pos="e,27000,19750 27000,19836 27000,19836 27000,19760 27000,19760"];
	Layer0_Device5_Stage1_Attention -> Layer0_Device5_Stage1_Accumulate	[pos="e,25620,19530 25620,19670 25620,19670 25620,19540 25620,19540"];
	Layer0_Device5_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25783,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage1_Accumulate -> Layer0_Device5_Stage2_Accumulate	[pos="e,25724,19337 25724,19477 25724,19477 25724,19347 25724,19347"];
	Layer0_Device5_Stage2_RecvKV -> Layer0_Device5_Stage2_Attention	[pos="e,26401,19531 26401,19696 26401,19696 26401,19541 26401,19541"];
	Layer0_Device5_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27395,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage2_RecvKV -> Layer0_Device5_Stage3_RecvKV	[label="Ring transfer",
		lp="27398,19600",
		pos="e,27350,19557 27350,19643 27350,19643 27350,19567 27350,19567"];
	Layer0_Device5_Stage2_Attention -> Layer0_Device5_Stage2_Accumulate	[pos="e,25984,19337 25984,19477 25984,19477 25984,19347 25984,19347"];
	Layer0_Device5_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25928,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage2_Accumulate -> Layer0_Device5_Stage3_Accumulate	[pos="e,25856,19144 25856,19284 25856,19284 25856,19154 25856,19154"];
	Layer0_Device5_Stage3_RecvKV -> Layer0_Device5_Stage3_Attention	[pos="e,28142,19338 28142,19503 28142,19503 28142,19348 28142,19348"];
	Layer0_Device5_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="26992,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage3_RecvKV -> Layer0_Device5_Stage4_RecvKV	[label="Ring transfer",
		lp="27309,19407",
		pos="e,27194,19364 27194,19450 27194,19450 27194,19374 27194,19374"];
	Layer0_Device5_Stage3_Attention -> Layer0_Device5_Stage3_Accumulate	[pos="e,26116,19144 28013,19284 28013,19251 28013,19199 28013,19199 28013,19199 26116,19199 26116,19199 26116,19199 26116,19154 26116,\
19154"];
	Layer0_Device5_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25902,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage3_Accumulate -> Layer0_Device5_Stage4_Accumulate	[pos="e,25915,18951 25915,19091 25915,19091 25915,18961 25915,18961"];
	Layer0_Device5_Stage4_RecvKV -> Layer0_Device5_Stage4_Attention	[pos="e,25595,19144 26116,19270 25835,19270 25595,19270 25595,19270 25595,19270 25595,19154 25595,19154"];
	Layer0_Device5_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27137,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage4_RecvKV -> Layer0_Device5_Stage5_RecvKV	[label="Ring transfer",
		lp="27112,19214",
		pos="e,27064,19171 27064,19257 27064,19257 27064,19181 27064,19181"];
	Layer0_Device5_Stage4_Attention -> Layer0_Device5_Stage4_Accumulate	[pos="e,25684,18951 25639,19117 25666,19117 25684,19117 25684,19117 25684,19117 25684,18961 25684,18961"];
	Layer0_Device5_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27827,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage4_Accumulate -> Layer0_Device5_Stage5_Accumulate	[pos="e,27813,18758 25916,18898 25916,18870 25916,18832 25916,18832 25916,18832 27813,18832 27813,18832 27813,18832 27813,18768 27813,\
18768"];
	Layer0_Device5_Stage5_RecvKV -> Layer0_Device5_Stage5_Attention	[pos="e,28088,18933 28039,19117 28039,19112 28039,18933 28039,18933 28039,18933 28078,18933 28078,18933"];
	Layer0_Device5_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27111,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage5_RecvKV -> Layer0_Device5_Stage6_RecvKV	[label="Ring transfer",
		lp="27172,19021",
		pos="e,27124,18978 27124,19064 27124,19064 27124,18988 27124,18988"];
	Layer0_Device5_Stage5_Attention -> Layer0_Device5_Stage5_Accumulate	[pos="e,28048,18758 28088,18915 28064,18915 28048,18915 28048,18915 28048,18915 28048,18768 28048,18768"];
	Layer0_Device5_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27827,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage5_Accumulate -> Layer0_Device5_Stage6_Accumulate	[pos="e,27827,18565 27827,18705 27827,18705 27827,18575 27827,18575"];
	Layer0_Device5_Stage6_RecvKV -> Layer0_Device5_Stage6_Attention	[pos="e,28334,18758 27698,18884 28012,18884 28334,18884 28334,18884 28334,18884 28334,18768 28334,18768"];
	Layer0_Device5_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26618,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage6_RecvKV -> Layer0_Device5_Stage7_RecvKV	[label="Ring transfer",
		lp="26847,18828",
		pos="e,26864,18785 26864,18871 26864,18871 26864,18795 26864,18795"];
	Layer0_Device5_Stage6_Attention -> Layer0_Device5_Stage6_Accumulate	[pos="e,28059,18529 28250,18705 28250,18650 28250,18529 28250,18529 28250,18529 28069,18529 28069,18529"];
	Layer0_Device5_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27632,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage6_Accumulate -> Layer0_Device5_Stage7_Accumulate	[pos="e,27730,18372 27730,18512 27730,18512 27730,18382 27730,18382"];
	Layer0_Device5_Stage7_RecvKV -> Layer0_Device5_Stage7_Attention	[pos="e,25408,18565 25742,18691 25553,18691 25408,18691 25408,18691 25408,18691 25408,18575 25408,18575"];
	Layer0_Device5_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26618,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage7_RecvKV -> Layer0_Device5_Stage8_RecvKV	[label="Ring transfer",
		lp="26666,18635",
		pos="e,26618,18592 26618,18678 26618,18678 26618,18602 26618,18602"];
	Layer0_Device5_Stage7_Attention -> Layer0_Device5_Stage7_Accumulate	[pos="e,27469,18372 25572,18512 25572,18481 25572,18434 25572,18434 25572,18434 27469,18434 27469,18434 27469,18434 27469,18382 27469,\
18382"];
	Layer0_Device5_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27632,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage7_Accumulate -> Layer0_Device5_Stage8_Accumulate	[pos="e,27632,18179 27632,18319 27632,18319 27632,18189 27632,18189"];
	Layer0_Device5_Stage8_RecvKV -> Layer0_Device5_Stage8_Attention	[pos="e,27990,18372 27205,18498 27578,18498 27990,18498 27990,18498 27990,18498 27990,18382 27990,18382"];
	Layer0_Device5_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26423,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage8_RecvKV -> Layer0_Device5_Stage9_RecvKV	[label="Ring transfer",
		lp="26601,18442",
		pos="e,26520,18399 26520,18485 26520,18485 26520,18409 26520,18409"];
	Layer0_Device5_Stage8_Attention -> Layer0_Device5_Stage8_Accumulate	[pos="e,27864,18152 27990,18319 27990,18266 27990,18152 27990,18152 27990,18152 27874,18152 27874,18152"];
	Layer0_Device5_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25929,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage8_Accumulate -> Layer0_Device5_Stage9_Accumulate	[pos="e,26006,17986 27632,18126 27632,18097 27632,18054 27632,18054 27632,18054 26006,18054 26006,18054 26006,18054 26006,17996 26006,\
17996"];
	Layer0_Device5_Stage9_RecvKV -> Layer0_Device5_Stage9_Attention	[pos="e,25312,18179 25547,18305 25409,18305 25312,18305 25312,18305 25312,18305 25312,18189 25312,18189"];
	Layer0_Device5_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26423,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage9_RecvKV -> Layer0_Device5_Stage10_RecvKV	[label="Ring transfer",
		lp="26471,18249",
		pos="e,26423,18206 26423,18292 26423,18292 26423,18216 26423,18216"];
	Layer0_Device5_Stage9_Attention -> Layer0_Device5_Stage9_Accumulate	[pos="e,25852,17987 25311,18126 25311,18082 25311,17999 25311,17999 25311,17999 25852,17999 25852,17999 25852,17999 25852,17997 25852,\
17997"];
	Layer0_Device5_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25929,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage9_Accumulate -> Layer0_Device5_Stage10_Accumulate	[pos="e,25929,17793 25929,17933 25929,17933 25929,17803 25929,17803"];
	Layer0_Device5_Stage10_RecvKV -> Layer0_Device5_Stage10_Attention	[pos="e,25571,17987 25571,18152 25571,18152 25571,17997 25571,17997"];
	Layer0_Device5_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27138,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage10_RecvKV -> Layer0_Device5_Stage11_RecvKV	[label="Ring transfer",
		lp="26740,18056",
		pos="e,26780,18013 26780,18099 26780,18099 26780,18023 26780,18023"];
	Layer0_Device5_Stage10_Attention -> Layer0_Device5_Stage10_Accumulate	[pos="e,25697,17766 25486,17933 25486,17880 25486,17766 25486,17766 25486,17766 25687,17766 25687,17766"];
	Layer0_Device5_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26452,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage10_Accumulate -> Layer0_Device5_Stage11_Accumulate	[pos="e,26316,17600 26161,17766 26243,17766 26316,17766 26316,17766 26316,17766 26316,17610 26316,17610"];
	Layer0_Device5_Stage11_RecvKV -> Layer0_Device5_Stage11_Attention	[pos="e,26566,17793 26566,17906 26566,17906 26566,17803 26566,17803"];
	Layer0_Device5_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27852,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage11_RecvKV -> Layer0_Device5_Stage12_RecvKV	[label="Ring transfer",
		lp="27590,17863",
		pos="e,27495,17820 27495,17906 27495,17906 27495,17830 27495,17830"];
	Layer0_Device5_Stage11_Attention -> Layer0_Device5_Stage11_Accumulate	[pos="e,26548,17600 26548,17740 26548,17740 26548,17610 26548,17610"];
	Layer0_Device5_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26570,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage11_Accumulate -> Layer0_Device5_Stage12_Accumulate	[pos="e,26511,17407 26511,17547 26511,17547 26511,17417 26511,17417"];
	Layer0_Device5_Stage12_RecvKV -> Layer0_Device5_Stage12_Attention	[pos="e,27068,17600 27068,17713 27068,17713 27068,17610 27068,17610"];
	Layer0_Device5_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28182,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage12_RecvKV -> Layer0_Device5_Stage13_RecvKV	[label="Ring transfer",
		lp="28065,17670",
		pos="e,28017,17627 28017,17713 28017,17713 28017,17637 28017,17637"];
	Layer0_Device5_Stage12_Attention -> Layer0_Device5_Stage12_Accumulate	[pos="e,26772,17407 26772,17547 26772,17547 26772,17417 26772,17417"];
	Layer0_Device5_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27091,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage12_Accumulate -> Layer0_Device5_Stage13_Accumulate	[pos="e,26859,17187 26802,17380 26819,17380 26830,17380 26830,17380 26830,17380 26830,17187 26830,17187 26830,17187 26849,17187 26849,\
17187"];
	Layer0_Device5_Stage13_RecvKV -> Layer0_Device5_Stage13_Attention	[pos="e,28929,17408 28929,17573 28929,17573 28929,17418 28929,17418"];
	Layer0_Device5_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27779,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage13_RecvKV -> Layer0_Device5_Stage14_RecvKV	[label="Ring transfer",
		lp="28028,17477",
		pos="e,27980,17434 27980,17520 27980,17520 27980,17444 27980,17444"];
	Layer0_Device5_Stage13_Attention -> Layer0_Device5_Stage13_Accumulate	[pos="e,27091,17214 28988,17354 28988,17322 28988,17271 28988,17271 28988,17271 27091,17271 27091,17271 27091,17271 27091,17224 27091,\
17224"];
	Layer0_Device5_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27091,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage13_Accumulate -> Layer0_Device5_Stage14_Accumulate	[pos="e,27091,17062 27091,17161 27091,17161 27091,17072 27091,17072"];
	Layer0_Device5_Stage14_RecvKV -> Layer0_Device5_Stage14_Attention	[pos="e,26570,17214 26903,17340 26714,17340 26570,17340 26570,17340 26570,17340 26570,17224 26570,17224"];
	Layer0_Device5_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28300,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device5_Stage14_RecvKV -> Layer0_Device5_Stage15_RecvKV	[label="Ring transfer",
		lp="28087,17284",
		pos="e,28040,17241 28040,17327 28040,17327 28040,17251 28040,17251"];
	Layer0_Device5_Stage14_Attention -> Layer0_Device5_Stage14_Accumulate	[pos="e,26859,17035 26570,17161 26570,17117 26570,17035 26570,17035 26570,17035 26849,17035 26849,17035"];
	Layer0_Device5_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28452,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device5_Stage14_Accumulate -> Layer0_Device5_Stage15_Accumulate	[pos="e,28220,16909 27091,17008 27091,16971 27091,16909 27091,16909 27091,16909 28210,16909 28210,16909"];
	Layer0_Device5_Stage15_RecvKV -> Layer0_Device5_Stage15_Attention	[pos="e,28452,17062 28452,17134 28452,17134 28452,17072 28452,17072"];
	Layer0_Device5_Stage15_Attention -> Layer0_Device5_Stage15_Accumulate	[pos="e,28452,16936 28452,17008 28452,17008 28452,16946 28452,16946"];
	Layer0_Device5_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="28706,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device5_Stage15_Accumulate -> Layer0_Device5_ConcatHeads	[pos="e,28582,16810 28582,16882 28582,16882 28582,16820 28582,16820"];
	Layer0_Device5_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29119,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device5_ConcatHeads -> Layer0_Device5_OutputProj	[pos="e,28919,16684 28919,16756 28919,16756 28919,16694 28919,16694"];
	Layer0_Device5_OutputProj -> Layer0_Device5_Residual1	[pos="e,29143,16558 29143,16630 29143,16630 29143,16568 29143,16568"];
	Layer0_Device5_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29067,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device5_Residual1 -> Layer0_Device5_LayerNorm2	[pos="e,29117,16432 29117,16504 29117,16504 29117,16442 29117,16442"];
	Layer0_Device5_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="29295,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device5_Residual1 -> Layer0_Device5_Residual2	[pos="e,29497,15750 29497,16504 29497,16504 29497,15760 29497,15760"];
	Layer0_Device5_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="28774,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device5_LayerNorm2 -> Layer0_Device5_GateProj	[pos="e,28926,16306 28926,16378 28926,16378 28926,16316 28926,16316"];
	Layer0_Device5_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29087,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device5_LayerNorm2 -> Layer0_Device5_UpProj	[pos="e,29140,16217 29140,16378 29140,16378 29140,16227 29140,16227"];
	Layer0_Device5_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="28774,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device5_GateProj -> Layer0_Device5_Activation	[pos="e,28706,16128 28706,16252 28706,16252 28706,16138 28706,16138"];
	Layer0_Device5_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="28870,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device5_UpProj -> Layer0_Device5_ElemMul	[pos="e,29140,16002 29140,16163 29140,16163 29140,16012 29140,16012"];
	Layer0_Device5_Activation -> Layer0_Device5_ElemMul	[pos="e,28774,16002 28774,16074 28774,16074 28774,16012 28774,16012"];
	Layer0_Device5_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="28966,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device5_ElemMul -> Layer0_Device5_DownProj	[pos="e,28966,15876 28966,15948 28966,15948 28966,15886 28966,15886"];
	Layer0_Device5_DownProj -> Layer0_Device5_Residual2	[pos="e,29047,15750 29047,15822 29047,15822 29047,15760 29047,15760"];
	Layer0_Device5_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29295,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device5_Residual2 -> Layer0_Device5_Output	[pos="e,29295,15624 29295,15696 29295,15696 29295,15634 29295,15634"];
	Layer1_Device5_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29295,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device5_Output -> Layer1_Device5_Input	[pos="e,29295,15476 29295,15548 29295,15548 29295,15486 29295,15486"];
	Layer0_Device6_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34242,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device6_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33676,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device6_Input -> Layer0_Device6_LayerNorm1	[pos="e,33889,20375 33975,20494 33975,20457 33975,20375 33975,20375 33975,20375 33899,20375 33899,20375"];
	Layer0_Device6_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="34203,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device6_Input -> Layer0_Device6_Residual1	[pos="e,34393,16558 34393,20479 34393,20479 34393,16568 34393,16568"];
	Layer0_Device6_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33250,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device6_LayerNorm1 -> Layer0_Device6_QKVProj	[pos="e,33676,20276 33676,20348 33676,20348 33676,20286 33676,20286"];
	Layer0_Device6_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32004,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage0_RecvKV	[label="Local K,V",
		lp="32504,20179",
		pos="e,32762,20136 32762,20222 32762,20222 32762,20146 32762,20146"];
	Layer0_Device6_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="32794,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage0_Attention	[label=Q_local,
		lp="33066,20083",
		pos="e,32975,19917 32975,20222 32975,20222 32975,19927 32975,19927"];
	Layer0_Device6_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30464,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage1_Attention	[label=Q_local,
		lp="30528,19986",
		pos="e,30449,19723 32600,20229 31773,20229 30449,20229 30449,20229 30449,20229 30449,19733 30449,19733"];
	Layer0_Device6_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31075,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage2_Attention	[label=Q_local,
		lp="33206,19890",
		pos="e,31245,19530 33070,20223 33070,20127 33070,19804 33070,19804 33070,19804 31245,19804 31245,19804 31245,19804 31245,19540 31245,\
19540"];
	Layer0_Device6_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33090,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage3_Attention	[label=Q_local,
		lp="33346,19793",
		pos="e,33263,19337 33263,20223 33263,20223 33263,19347 33263,19347"];
	Layer0_Device6_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30296,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage4_Attention	[label=Q_local,
		lp="30148,19697",
		pos="e,30148,19144 32600,20242 31691,20242 30148,20242 30148,20242 30148,20242 30148,19154 30148,19154"];
	Layer0_Device6_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33228,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage5_Attention	[label=Q_local,
		lp="33450,19600",
		pos="e,33390,18951 33390,20222 33390,20222 33390,18961 33390,18961"];
	Layer0_Device6_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33246,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage6_Attention	[label=Q_local,
		lp="33582,19504",
		pos="e,33468,18758 33468,20222 33468,20222 33468,18768 33468,18768"];
	Layer0_Device6_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30307,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage7_Attention	[label=Q_local,
		lp="30002,19407",
		pos="e,30075,18547 32600,20255 31657,20255 30020,20255 30020,20255 30020,20255 30020,18547 30020,18547 30020,18547 30065,18547 30065,\
18547"];
	Layer0_Device6_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33102,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage8_Attention	[label=Q_local,
		lp="33720,19311",
		pos="e,33334,18345 33537,20223 33537,19993 33537,18345 33537,18345 33537,18345 33344,18345 33344,18345"];
	Layer0_Device6_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30163,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage9_Attention	[label=Q_local,
		lp="29884,19214",
		pos="e,29976,18179 32600,20262 31646,20262 29976,20262 29976,20262 29976,20262 29976,18189 29976,18189"];
	Layer0_Device6_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30364,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage10_Attention	[label=Q_local,
		lp="29744,19118",
		pos="e,30132,17959 32600,20269 31624,20269 29892,20269 29892,20269 29892,20269 29892,17959 29892,17959 29892,17959 30122,17959 30122,\
17959"];
	Layer0_Device6_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31646,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage11_Attention	[label=Q_local,
		lp="33841,19021",
		pos="e,31723,17793 33656,20222 33656,19959 33656,17832 33656,17832 33656,17832 31723,17832 31723,17832 31723,17832 31723,17803 31723,\
17803"];
	Layer0_Device6_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31881,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage12_Attention	[label=Q_local,
		lp="33984,18925",
		pos="e,31906,17601 33715,20222 33715,19958 33715,17830 33715,17830 33715,17830 31906,17830 31906,17830 31906,17830 31906,17611 31906,\
17611"];
	Layer0_Device6_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33896,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage13_Attention	[label=Q_local,
		lp="34124,18828",
		pos="e,34069,17408 33900,20235 33999,20235 34069,20235 34069,20235 34069,20235 34069,17418 34069,17418"];
	Layer0_Device6_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31478,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage14_Attention	[label=Q_local,
		lp="29598,18732",
		pos="e,31246,17187 33596,20222 33596,19972 33596,18046 33596,18046 33596,18046 31122,18046 31122,18046 31122,18046 31122,17187 31122,\
17187 31122,17187 31236,17187 31236,17187"];
	Layer0_Device6_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33611,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_QKVProj -> Layer0_Device6_Stage15_Attention	[label=Q_local,
		lp="34242,18635",
		pos="e,33843,17043 33900,20249 34059,20249 34185,20249 34185,20249 34185,20249 34185,17043 34185,17043 34185,17043 33853,17043 33853,\
17043"];
	Layer0_Device6_Stage0_RecvKV -> Layer0_Device6_Stage0_Attention	[pos="e,32743,19917 32743,20082 32743,20082 32743,19927 32743,19927"];
	Layer0_Device6_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31585,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage0_RecvKV -> Layer0_Device6_Stage1_RecvKV	[label="Ring transfer",
		lp="31842,19986",
		pos="e,31794,19943 31794,20029 31794,20029 31794,19953 31794,19953"];
	Layer0_Device6_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="30985,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage0_Attention -> Layer0_Device6_Stage0_Accumulate	[pos="e,30985,19723 32794,19863 32794,19839 32794,19807 32794,19807 32794,19807 30985,19807 30985,19807 30985,19807 30985,19733 30985,\
19733"];
	Layer0_Device6_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30554,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage0_Accumulate -> Layer0_Device6_Stage1_Accumulate	[pos="e,30770,19530 30770,19670 30770,19670 30770,19540 30770,19540"];
	Layer0_Device6_Stage1_RecvKV -> Layer0_Device6_Stage1_Attention	[pos="e,30680,19724 30680,19889 30680,19889 30680,19734 30680,19734"];
	Layer0_Device6_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32194,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage1_RecvKV -> Layer0_Device6_Stage2_RecvKV	[label="Ring transfer",
		lp="31902,19793",
		pos="e,31890,19750 31890,19836 31890,19836 31890,19760 31890,19760"];
	Layer0_Device6_Stage1_Attention -> Layer0_Device6_Stage1_Accumulate	[pos="e,30509,19530 30509,19670 30509,19670 30509,19540 30509,19540"];
	Layer0_Device6_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30672,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage1_Accumulate -> Layer0_Device6_Stage2_Accumulate	[pos="e,30613,19337 30613,19477 30613,19477 30613,19347 30613,19347"];
	Layer0_Device6_Stage2_RecvKV -> Layer0_Device6_Stage2_Attention	[pos="e,31290,19531 31290,19696 31290,19696 31290,19541 31290,19541"];
	Layer0_Device6_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32284,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage2_RecvKV -> Layer0_Device6_Stage3_RecvKV	[label="Ring transfer",
		lp="32287,19600",
		pos="e,32239,19557 32239,19643 32239,19643 32239,19567 32239,19567"];
	Layer0_Device6_Stage2_Attention -> Layer0_Device6_Stage2_Accumulate	[pos="e,30874,19337 30874,19477 30874,19477 30874,19347 30874,19347"];
	Layer0_Device6_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30817,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage2_Accumulate -> Layer0_Device6_Stage3_Accumulate	[pos="e,30744,19144 30744,19284 30744,19284 30744,19154 30744,19154"];
	Layer0_Device6_Stage3_RecvKV -> Layer0_Device6_Stage3_Attention	[pos="e,33031,19338 33031,19503 33031,19503 33031,19348 33031,19348"];
	Layer0_Device6_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31881,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage3_RecvKV -> Layer0_Device6_Stage4_RecvKV	[label="Ring transfer",
		lp="32198,19407",
		pos="e,32082,19364 32082,19450 32082,19450 32082,19374 32082,19374"];
	Layer0_Device6_Stage3_Attention -> Layer0_Device6_Stage3_Accumulate	[pos="e,31005,19144 32902,19284 32902,19252 32902,19204 32902,19204 32902,19204 31005,19204 31005,19204 31005,19204 31005,19154 31005,\
19154"];
	Layer0_Device6_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30810,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage3_Accumulate -> Layer0_Device6_Stage4_Accumulate	[pos="e,30814,18951 30814,19091 30814,19091 30814,18961 30814,18961"];
	Layer0_Device6_Stage4_RecvKV -> Layer0_Device6_Stage4_Attention	[pos="e,30484,19144 31005,19270 30724,19270 30484,19270 30484,19270 30484,19270 30484,19154 30484,19154"];
	Layer0_Device6_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32026,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage4_RecvKV -> Layer0_Device6_Stage5_RecvKV	[label="Ring transfer",
		lp="32001,19214",
		pos="e,31954,19171 31954,19257 31954,19257 31954,19181 31954,19181"];
	Layer0_Device6_Stage4_Attention -> Layer0_Device6_Stage4_Accumulate	[pos="e,30582,18951 30528,19117 30560,19117 30582,19117 30582,19117 30582,19117 30582,18961 30582,18961"];
	Layer0_Device6_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32725,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage4_Accumulate -> Layer0_Device6_Stage5_Accumulate	[pos="e,32716,18758 30819,18898 30819,18862 30819,18803 30819,18803 30819,18803 32716,18803 32716,18803 32716,18803 32716,18768 32716,\
18768"];
	Layer0_Device6_Stage5_RecvKV -> Layer0_Device6_Stage5_Attention	[pos="e,32996,18933 32942,19117 32942,19112 32942,18933 32942,18933 32942,18933 32986,18933 32986,18933"];
	Layer0_Device6_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32019,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage5_RecvKV -> Layer0_Device6_Stage6_RecvKV	[label="Ring transfer",
		lp="32070,19021",
		pos="e,32022,18978 32022,19064 32022,19064 32022,18988 32022,18988"];
	Layer0_Device6_Stage5_Attention -> Layer0_Device6_Stage5_Accumulate	[pos="e,32951,18758 32996,18915 32969,18915 32951,18915 32951,18915 32951,18915 32951,18768 32951,18768"];
	Layer0_Device6_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32725,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage5_Accumulate -> Layer0_Device6_Stage6_Accumulate	[pos="e,32725,18565 32725,18705 32725,18705 32725,18575 32725,18575"];
	Layer0_Device6_Stage6_RecvKV -> Layer0_Device6_Stage6_Attention	[pos="e,33237,18758 32607,18884 32918,18884 33237,18884 33237,18884 33237,18884 33237,18768 33237,18768"];
	Layer0_Device6_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31516,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage6_RecvKV -> Layer0_Device6_Stage7_RecvKV	[label="Ring transfer",
		lp="31815,18828",
		pos="e,31768,18785 31768,18871 31768,18871 31768,18795 31768,18795"];
	Layer0_Device6_Stage6_Attention -> Layer0_Device6_Stage6_Accumulate	[pos="e,32957,18529 33174,18705 33174,18650 33174,18529 33174,18529 33174,18529 32967,18529 32967,18529"];
	Layer0_Device6_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32581,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage6_Accumulate -> Layer0_Device6_Stage7_Accumulate	[pos="e,32653,18372 32653,18512 32653,18512 32653,18382 32653,18382"];
	Layer0_Device6_Stage7_RecvKV -> Layer0_Device6_Stage7_Attention	[pos="e,30533,18565 30640,18691 30574,18691 30533,18691 30533,18691 30533,18691 30533,18575 30533,18575"];
	Layer0_Device6_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31516,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage7_RecvKV -> Layer0_Device6_Stage8_RecvKV	[label="Ring transfer",
		lp="31564,18635",
		pos="e,31516,18592 31516,18678 31516,18678 31516,18602 31516,18602"];
	Layer0_Device6_Stage7_Attention -> Layer0_Device6_Stage7_Accumulate	[pos="e,32393,18372 30495,18512 30495,18493 30495,18470 30495,18470 30495,18470 32393,18470 32393,18470 32393,18470 32393,18382 32393,\
18382"];
	Layer0_Device6_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32581,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage7_Accumulate -> Layer0_Device6_Stage8_Accumulate	[pos="e,32581,18179 32581,18319 32581,18319 32581,18189 32581,18189"];
	Layer0_Device6_Stage8_RecvKV -> Layer0_Device6_Stage8_Attention	[pos="e,32914,18372 32103,18498 32486,18498 32914,18498 32914,18498 32914,18498 32914,18382 32914,18382"];
	Layer0_Device6_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31372,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage8_RecvKV -> Layer0_Device6_Stage9_RecvKV	[label="Ring transfer",
		lp="31492,18442",
		pos="e,31444,18399 31444,18485 31444,18485 31444,18409 31444,18409"];
	Layer0_Device6_Stage8_Attention -> Layer0_Device6_Stage8_Accumulate	[pos="e,32813,18152 32942,18319 32942,18266 32942,18152 32942,18152 32942,18152 32823,18152 32823,18152"];
	Layer0_Device6_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30885,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage8_Accumulate -> Layer0_Device6_Stage9_Accumulate	[pos="e,30962,17986 32581,18126 32581,18095 32581,18049 32581,18049 32581,18049 30962,18049 30962,18049 30962,18049 30962,17996 30962,\
17996"];
	Layer0_Device6_Stage9_RecvKV -> Layer0_Device6_Stage9_Attention	[pos="e,30235,18179 30496,18305 30345,18305 30235,18305 30235,18305 30235,18305 30235,18189 30235,18189"];
	Layer0_Device6_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31372,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage9_RecvKV -> Layer0_Device6_Stage10_RecvKV	[label="Ring transfer",
		lp="31420,18249",
		pos="e,31372,18206 31372,18292 31372,18292 31372,18216 31372,18216"];
	Layer0_Device6_Stage9_Attention -> Layer0_Device6_Stage9_Accumulate	[pos="e,30808,17986 30264,18126 30264,18088 30264,18023 30264,18023 30264,18023 30808,18023 30808,18023 30808,18023 30808,17996 30808,\
17996"];
	Layer0_Device6_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30885,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage9_Accumulate -> Layer0_Device6_Stage10_Accumulate	[pos="e,30885,17793 30885,17933 30885,17933 30885,17803 30885,17803"];
	Layer0_Device6_Stage10_RecvKV -> Layer0_Device6_Stage10_Attention	[pos="e,30524,17987 30524,18152 30524,18152 30524,17997 30524,17997"];
	Layer0_Device6_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32094,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage10_RecvKV -> Layer0_Device6_Stage11_RecvKV	[label="Ring transfer",
		lp="31901,18056",
		pos="e,31733,18013 31733,18099 31733,18099 31733,18023 31733,18023"];
	Layer0_Device6_Stage10_Attention -> Layer0_Device6_Stage10_Accumulate	[pos="e,30653,17766 30412,17933 30412,17880 30412,17766 30412,17766 30412,17766 30643,17766 30643,17766"];
	Layer0_Device6_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31360,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage10_Accumulate -> Layer0_Device6_Stage11_Accumulate	[pos="e,31128,17573 31098,17740 31098,17687 31098,17573 31098,17573 31098,17573 31118,17573 31118,17573"];
	Layer0_Device6_Stage11_RecvKV -> Layer0_Device6_Stage11_Attention	[pos="e,31569,17793 31569,17906 31569,17906 31569,17803 31569,17803"];
	Layer0_Device6_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32855,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage11_RecvKV -> Layer0_Device6_Stage12_RecvKV	[label="Ring transfer",
		lp="32337,17863",
		pos="e,32474,17820 32474,17906 32474,17906 32474,17830 32474,17830"];
	Layer0_Device6_Stage11_Attention -> Layer0_Device6_Stage11_Accumulate	[pos="e,31503,17600 31503,17740 31503,17740 31503,17610 31503,17610"];
	Layer0_Device6_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31478,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage11_Accumulate -> Layer0_Device6_Stage12_Accumulate	[pos="e,31419,17407 31419,17547 31419,17547 31419,17417 31419,17417"];
	Layer0_Device6_Stage12_RecvKV -> Layer0_Device6_Stage12_Attention	[pos="e,32024,17600 32024,17713 32024,17713 32024,17610 32024,17610"];
	Layer0_Device6_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33090,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage12_RecvKV -> Layer0_Device6_Stage13_RecvKV	[label="Ring transfer",
		lp="33021,17670",
		pos="e,32972,17627 32972,17713 32972,17713 32972,17637 32972,17637"];
	Layer0_Device6_Stage12_Attention -> Layer0_Device6_Stage12_Accumulate	[pos="e,31680,17407 31680,17547 31680,17547 31680,17417 31680,17417"];
	Layer0_Device6_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31999,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage12_Accumulate -> Layer0_Device6_Stage13_Accumulate	[pos="e,31767,17187 31710,17380 31727,17380 31738,17380 31738,17380 31738,17380 31738,17187 31738,17187 31738,17187 31757,17187 31757,\
17187"];
	Layer0_Device6_Stage13_RecvKV -> Layer0_Device6_Stage13_Attention	[pos="e,33837,17408 33837,17573 33837,17573 33837,17418 33837,17418"];
	Layer0_Device6_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32687,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage13_RecvKV -> Layer0_Device6_Stage14_RecvKV	[label="Ring transfer",
		lp="32871,17477",
		pos="e,32888,17434 32888,17520 32888,17520 32888,17444 32888,17444"];
	Layer0_Device6_Stage13_Attention -> Layer0_Device6_Stage13_Accumulate	[pos="e,31999,17214 33896,17354 33896,17323 33896,17276 33896,17276 33896,17276 31999,17276 31999,17276 31999,17276 31999,17224 31999,\
17224"];
	Layer0_Device6_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31999,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage13_Accumulate -> Layer0_Device6_Stage14_Accumulate	[pos="e,31999,17062 31999,17161 31999,17161 31999,17072 31999,17072"];
	Layer0_Device6_Stage14_RecvKV -> Layer0_Device6_Stage14_Attention	[pos="e,31478,17214 31811,17340 31622,17340 31478,17340 31478,17340 31478,17340 31478,17224 31478,17224"];
	Layer0_Device6_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33208,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device6_Stage14_RecvKV -> Layer0_Device6_Stage15_RecvKV	[label="Ring transfer",
		lp="32995,17284",
		pos="e,32948,17241 32948,17327 32948,17327 32948,17251 32948,17251"];
	Layer0_Device6_Stage14_Attention -> Layer0_Device6_Stage14_Accumulate	[pos="e,31767,17026 31478,17161 31478,17115 31478,17026 31478,17026 31478,17026 31757,17026 31757,17026"];
	Layer0_Device6_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="33611,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device6_Stage14_Accumulate -> Layer0_Device6_Stage15_Accumulate	[pos="e,33379,16909 31999,17008 31999,16971 31999,16909 31999,16909 31999,16909 33369,16909 33369,16909"];
	Layer0_Device6_Stage15_RecvKV -> Layer0_Device6_Stage15_Attention	[pos="e,33611,17062 33611,17134 33611,17134 33611,17072 33611,17072"];
	Layer0_Device6_Stage15_Attention -> Layer0_Device6_Stage15_Accumulate	[pos="e,33611,16936 33611,17008 33611,17008 33611,16946 33611,16946"];
	Layer0_Device6_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34017,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device6_Stage15_Accumulate -> Layer0_Device6_ConcatHeads	[pos="e,33817,16810 33817,16882 33817,16882 33817,16820 33817,16820"];
	Layer0_Device6_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34027,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device6_ConcatHeads -> Layer0_Device6_OutputProj	[pos="e,34027,16684 34027,16756 34027,16756 34027,16694 34027,16694"];
	Layer0_Device6_OutputProj -> Layer0_Device6_Residual1	[pos="e,34028,16558 34028,16630 34028,16630 34028,16568 34028,16568"];
	Layer0_Device6_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34049,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device6_Residual1 -> Layer0_Device6_LayerNorm2	[pos="e,34049,16432 34049,16504 34049,16504 34049,16442 34049,16442"];
	Layer0_Device6_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="34137,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device6_Residual1 -> Layer0_Device6_Residual2	[pos="e,34409,15750 34409,16504 34409,16504 34409,15760 34409,15760"];
	Layer0_Device6_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33756,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device6_LayerNorm2 -> Layer0_Device6_GateProj	[pos="e,33908,16306 33908,16378 33908,16378 33908,16316 33908,16316"];
	Layer0_Device6_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="34069,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device6_LayerNorm2 -> Layer0_Device6_UpProj	[pos="e,34122,16217 34122,16378 34122,16378 34122,16227 34122,16227"];
	Layer0_Device6_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33756,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device6_GateProj -> Layer0_Device6_Activation	[pos="e,33688,16128 33688,16252 33688,16252 33688,16138 33688,16138"];
	Layer0_Device6_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33852,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device6_UpProj -> Layer0_Device6_ElemMul	[pos="e,34122,16002 34122,16163 34122,16163 34122,16012 34122,16012"];
	Layer0_Device6_Activation -> Layer0_Device6_ElemMul	[pos="e,33756,16002 33756,16074 33756,16074 33756,16012 33756,16012"];
	Layer0_Device6_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33948,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device6_ElemMul -> Layer0_Device6_DownProj	[pos="e,33948,15876 33948,15948 33948,15948 33948,15886 33948,15886"];
	Layer0_Device6_DownProj -> Layer0_Device6_Residual2	[pos="e,33959,15750 33959,15822 33959,15822 33959,15760 33959,15760"];
	Layer0_Device6_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34137,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device6_Residual2 -> Layer0_Device6_Output	[pos="e,34137,15624 34137,15696 34137,15696 34137,15634 34137,15634"];
	Layer1_Device6_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34137,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device6_Output -> Layer1_Device6_Input	[pos="e,34137,15476 34137,15548 34137,15548 34137,15486 34137,15486"];
	Layer0_Device7_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38863,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device7_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38639,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device7_Input -> Layer0_Device7_LayerNorm1	[pos="e,38707,20402 38707,20480 38707,20480 38707,20412 38707,20412"];
	Layer0_Device7_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="39202,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device7_Input -> Layer0_Device7_Residual1	[pos="e,39145,16558 39145,20498 39145,20498 39145,16568 39145,16568"];
	Layer0_Device7_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38133,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device7_LayerNorm1 -> Layer0_Device7_QKVProj	[pos="e,38604,20276 38604,20348 38604,20348 38604,20286 38604,20286"];
	Layer0_Device7_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36912,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage0_RecvKV	[label="Local K,V",
		lp="37412,20179",
		pos="e,37658,20136 37658,20222 37658,20222 37658,20146 37658,20146"];
	Layer0_Device7_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37665,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage0_Attention	[label=Q_local,
		lp="37974,20083",
		pos="e,37864,19917 37864,20222 37864,20222 37864,19927 37864,19927"];
	Layer0_Device7_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35372,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage1_Attention	[label=Q_local,
		lp="35436,19986",
		pos="e,35338,19723 37483,20229 36657,20229 35338,20229 35338,20229 35338,20229 35338,19733 35338,19733"];
	Layer0_Device7_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35983,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage2_Attention	[label=Q_local,
		lp="38114,19890",
		pos="e,36153,19530 37959,20222 37959,20120 37959,19757 37959,19757 37959,19757 36153,19757 36153,19757 36153,19757 36153,19540 36153,\
19540"];
	Layer0_Device7_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37998,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage3_Attention	[label=Q_local,
		lp="38254,19793",
		pos="e,38171,19337 38171,20223 38171,20223 38171,19347 38171,19347"];
	Layer0_Device7_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35204,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage4_Attention	[label=Q_local,
		lp="35056,19697",
		pos="e,35056,19144 37483,20242 36581,20242 35056,20242 35056,20242 35056,20242 35056,19154 35056,19154"];
	Layer0_Device7_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38116,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage5_Attention	[label=Q_local,
		lp="38358,19600",
		pos="e,38288,18951 38288,20222 38288,20222 38288,18961 38288,18961"];
	Layer0_Device7_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38168,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage6_Attention	[label=Q_local,
		lp="38490,19504",
		pos="e,38374,18758 38374,20222 38374,20222 38374,18768 38374,18768"];
	Layer0_Device7_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35229,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage7_Attention	[label=Q_local,
		lp="34910,19407",
		pos="e,34997,18547 37483,20255 36540,20255 34902,20255 34902,20255 34902,20255 34902,18547 34902,18547 34902,18547 34987,18547 34987,\
18547"];
	Layer0_Device7_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37933,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage8_Attention	[label=Q_local,
		lp="38608,19311",
		pos="e,38165,18345 38433,20223 38433,19993 38433,18345 38433,18345 38433,18345 38175,18345 38175,18345"];
	Layer0_Device7_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="34994,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage9_Attention	[label=Q_local,
		lp="34792,19214",
		pos="e,34832,18179 37483,20262 36522,20262 34832,20262 34832,20262 34832,20262 34832,18189 34832,18189"];
	Layer0_Device7_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35225,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage10_Attention	[label=Q_local,
		lp="34652,19118",
		pos="e,34993,17959 37483,20269 36495,20269 34726,20269 34726,20269 34726,20269 34726,17959 34726,17959 34726,17959 34983,17959 34983,\
17959"];
	Layer0_Device7_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36439,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage11_Attention	[label=Q_local,
		lp="38729,19021",
		pos="e,36516,17793 38500,20222 38500,19961 38500,17866 38500,17866 38500,17866 36516,17866 36516,17866 36516,17866 36516,17803 36516,\
17803"];
	Layer0_Device7_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36769,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage12_Attention	[label=Q_local,
		lp="38872,18925",
		pos="e,36699,17600 38534,20222 38534,19960 38534,17864 38534,17864 38534,17864 36699,17864 36699,17864 36699,17864 36699,17610 36699,\
17610"];
	Layer0_Device7_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38784,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage13_Attention	[label=Q_local,
		lp="39012,18828",
		pos="e,38957,17408 38783,20235 38884,20235 38957,20235 38957,20235 38957,20235 38957,17418 38957,17418"];
	Layer0_Device7_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36366,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage14_Attention	[label=Q_local,
		lp="34506,18732",
		pos="e,36134,17187 38467,20222 38467,19972 38467,18040 38467,18040 38467,18040 35997,18040 35997,18040 35997,18040 35997,17187 35997,\
17187 35997,17187 36124,17187 36124,17187"];
	Layer0_Device7_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38499,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_QKVProj -> Layer0_Device7_Stage15_Attention	[label=Q_local,
		lp="39130,18635",
		pos="e,38731,17043 38783,20249 38943,20249 39070,20249 39070,20249 39070,20249 39070,17043 39070,17043 39070,17043 38741,17043 38741,\
17043"];
	Layer0_Device7_Stage0_RecvKV -> Layer0_Device7_Stage0_Attention	[pos="e,37633,19916 37633,20079 37633,20079 37633,19926 37633,19926"];
	Layer0_Device7_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36456,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage0_RecvKV -> Layer0_Device7_Stage1_RecvKV	[label="Ring transfer",
		lp="36732,19986",
		pos="e,36684,19943 36684,20029 36684,20029 36684,19953 36684,19953"];
	Layer0_Device7_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="35893,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage0_Attention -> Layer0_Device7_Stage0_Accumulate	[pos="e,35893,19723 37665,19863 37665,19825 37665,19760 37665,19760 37665,19760 35893,19760 35893,19760 35893,19760 35893,19733 35893,\
19733"];
	Layer0_Device7_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35462,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage0_Accumulate -> Layer0_Device7_Stage1_Accumulate	[pos="e,35678,19530 35678,19670 35678,19670 35678,19540 35678,19540"];
	Layer0_Device7_Stage1_RecvKV -> Layer0_Device7_Stage1_Attention	[pos="e,35570,19724 35570,19889 35570,19889 35570,19734 35570,19734"];
	Layer0_Device7_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37102,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage1_RecvKV -> Layer0_Device7_Stage2_RecvKV	[label="Ring transfer",
		lp="36827,19793",
		pos="e,36779,19750 36779,19836 36779,19836 36779,19760 36779,19760"];
	Layer0_Device7_Stage1_Attention -> Layer0_Device7_Stage1_Accumulate	[pos="e,35417,19530 35417,19670 35417,19670 35417,19540 35417,19540"];
	Layer0_Device7_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35580,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage1_Accumulate -> Layer0_Device7_Stage2_Accumulate	[pos="e,35521,19337 35521,19477 35521,19477 35521,19347 35521,19347"];
	Layer0_Device7_Stage2_RecvKV -> Layer0_Device7_Stage2_Attention	[pos="e,36198,19531 36198,19696 36198,19696 36198,19541 36198,19541"];
	Layer0_Device7_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37192,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage2_RecvKV -> Layer0_Device7_Stage3_RecvKV	[label="Ring transfer",
		lp="37195,19600",
		pos="e,37147,19557 37147,19643 37147,19643 37147,19567 37147,19567"];
	Layer0_Device7_Stage2_Attention -> Layer0_Device7_Stage2_Accumulate	[pos="e,35782,19337 35782,19477 35782,19477 35782,19347 35782,19347"];
	Layer0_Device7_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35725,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage2_Accumulate -> Layer0_Device7_Stage3_Accumulate	[pos="e,35652,19144 35652,19284 35652,19284 35652,19154 35652,19154"];
	Layer0_Device7_Stage3_RecvKV -> Layer0_Device7_Stage3_Attention	[pos="e,37939,19338 37939,19503 37939,19503 37939,19348 37939,19348"];
	Layer0_Device7_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36789,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage3_RecvKV -> Layer0_Device7_Stage4_RecvKV	[label="Ring transfer",
		lp="36973,19407",
		pos="e,36990,19364 36990,19450 36990,19450 36990,19374 36990,19374"];
	Layer0_Device7_Stage3_Attention -> Layer0_Device7_Stage3_Accumulate	[pos="e,35913,19145 37810,19284 37810,19254 37810,19209 37810,19209 37810,19209 35913,19209 35913,19209 35913,19209 35913,19155 35913,\
19155"];
	Layer0_Device7_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35698,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage3_Accumulate -> Layer0_Device7_Stage4_Accumulate	[pos="e,35712,18951 35712,19091 35712,19091 35712,18961 35712,18961"];
	Layer0_Device7_Stage4_RecvKV -> Layer0_Device7_Stage4_Attention	[pos="e,35392,19144 35913,19270 35632,19270 35392,19270 35392,19270 35392,19270 35392,19154 35392,19154"];
	Layer0_Device7_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36934,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage4_RecvKV -> Layer0_Device7_Stage5_RecvKV	[label="Ring transfer",
		lp="36909,19214",
		pos="e,36862,19171 36862,19257 36862,19257 36862,19181 36862,19181"];
	Layer0_Device7_Stage4_Attention -> Layer0_Device7_Stage4_Accumulate	[pos="e,35480,18951 35436,19117 35462,19117 35480,19117 35480,19117 35480,19117 35480,18961 35480,18961"];
	Layer0_Device7_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37647,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage4_Accumulate -> Layer0_Device7_Stage5_Accumulate	[pos="e,37621,18758 35724,18898 35724,18875 35724,18847 35724,18847 35724,18847 37621,18847 37621,18847 37621,18847 37621,18768 37621,\
18768"];
	Layer0_Device7_Stage5_RecvKV -> Layer0_Device7_Stage5_Attention	[pos="e,37884,18933 37840,19117 37840,19112 37840,18933 37840,18933 37840,18933 37874,18933 37874,18933"];
	Layer0_Device7_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36907,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage5_RecvKV -> Layer0_Device7_Stage6_RecvKV	[label="Ring transfer",
		lp="36968,19021",
		pos="e,36920,18978 36920,19064 36920,19064 36920,18988 36920,18988"];
	Layer0_Device7_Stage5_Attention -> Layer0_Device7_Stage5_Accumulate	[pos="e,37866,18758 37884,18915 37873,18915 37866,18915 37866,18915 37866,18915 37866,18768 37866,18768"];
	Layer0_Device7_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37647,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage5_Accumulate -> Layer0_Device7_Stage6_Accumulate	[pos="e,37647,18565 37647,18705 37647,18705 37647,18575 37647,18575"];
	Layer0_Device7_Stage6_RecvKV -> Layer0_Device7_Stage6_Attention	[pos="e,38142,18758 37495,18884 37813,18884 38142,18884 38142,18884 38142,18884 38142,18768 38142,18768"];
	Layer0_Device7_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36438,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage6_RecvKV -> Layer0_Device7_Stage7_RecvKV	[label="Ring transfer",
		lp="36655,18828",
		pos="e,36672,18785 36672,18871 36672,18871 36672,18795 36672,18795"];
	Layer0_Device7_Stage6_Attention -> Layer0_Device7_Stage6_Accumulate	[pos="e,37879,18529 38050,18705 38050,18650 38050,18529 38050,18529 38050,18529 37889,18529 37889,18529"];
	Layer0_Device7_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37412,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage6_Accumulate -> Layer0_Device7_Stage7_Accumulate	[pos="e,37530,18372 37530,18512 37530,18512 37530,18382 37530,18382"];
	Layer0_Device7_Stage7_RecvKV -> Layer0_Device7_Stage7_Attention	[pos="e,35448,18565 35562,18691 35492,18691 35448,18691 35448,18691 35448,18691 35448,18575 35448,18575"];
	Layer0_Device7_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36438,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage7_RecvKV -> Layer0_Device7_Stage8_RecvKV	[label="Ring transfer",
		lp="36486,18635",
		pos="e,36438,18592 36438,18678 36438,18678 36438,18602 36438,18602"];
	Layer0_Device7_Stage7_Attention -> Layer0_Device7_Stage7_Accumulate	[pos="e,37269,18372 35372,18512 35372,18475 35372,18413 35372,18413 35372,18413 37269,18413 37269,18413 37269,18413 37269,18382 37269,\
18382"];
	Layer0_Device7_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37412,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage7_Accumulate -> Layer0_Device7_Stage8_Accumulate	[pos="e,37412,18179 37412,18319 37412,18319 37412,18189 37412,18189"];
	Layer0_Device7_Stage8_RecvKV -> Layer0_Device7_Stage8_Attention	[pos="e,37790,18372 37026,18498 37390,18498 37790,18498 37790,18498 37790,18498 37790,18382 37790,18382"];
	Layer0_Device7_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36203,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage8_RecvKV -> Layer0_Device7_Stage9_RecvKV	[label="Ring transfer",
		lp="36439,18442",
		pos="e,36320,18399 36320,18485 36320,18485 36320,18409 36320,18409"];
	Layer0_Device7_Stage8_Attention -> Layer0_Device7_Stage8_Accumulate	[pos="e,37644,18152 37788,18319 37788,18266 37788,18152 37788,18152 37788,18152 37654,18152 37654,18152"];
	Layer0_Device7_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35746,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage8_Accumulate -> Layer0_Device7_Stage9_Accumulate	[pos="e,35823,17986 37412,18126 37412,18094 37412,18043 37412,18043 37412,18043 35823,18043 35823,18043 35823,18043 35823,17996 35823,\
17996"];
	Layer0_Device7_Stage9_RecvKV -> Layer0_Device7_Stage9_Attention	[pos="e,35112,18179 35327,18305 35200,18305 35112,18305 35112,18305 35112,18305 35112,18189 35112,18189"];
	Layer0_Device7_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36203,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage9_RecvKV -> Layer0_Device7_Stage10_RecvKV	[label="Ring transfer",
		lp="36251,18249",
		pos="e,36203,18206 36203,18292 36203,18292 36203,18216 36203,18216"];
	Layer0_Device7_Stage9_Attention -> Layer0_Device7_Stage9_Accumulate	[pos="e,35669,17986 35110,18126 35110,18089 35110,18026 35110,18026 35110,18026 35669,18026 35669,18026 35669,18026 35669,17996 35669,\
17996"];
	Layer0_Device7_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35746,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage9_Accumulate -> Layer0_Device7_Stage10_Accumulate	[pos="e,35746,17793 35746,17933 35746,17933 35746,17803 35746,17803"];
	Layer0_Device7_Stage10_RecvKV -> Layer0_Device7_Stage10_Attention	[pos="e,35370,17986 35370,18099 35370,18099 35370,17996 35370,17996"];
	Layer0_Device7_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36955,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage10_RecvKV -> Layer0_Device7_Stage11_RecvKV	[label="Ring transfer",
		lp="36471,18056",
		pos="e,36579,18013 36579,18099 36579,18099 36579,18023 36579,18023"];
	Layer0_Device7_Stage10_Attention -> Layer0_Device7_Stage10_Accumulate	[pos="e,35514,17766 35288,17933 35288,17880 35288,17766 35288,17766 35288,17766 35504,17766 35504,17766"];
	Layer0_Device7_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36248,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage10_Accumulate -> Layer0_Device7_Stage11_Accumulate	[pos="e,36026,17600 35978,17766 36006,17766 36026,17766 36026,17766 36026,17766 36026,17610 36026,17610"];
	Layer0_Device7_Stage11_RecvKV -> Layer0_Device7_Stage11_Attention	[pos="e,36362,17793 36362,17906 36362,17906 36362,17803 36362,17803"];
	Layer0_Device7_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37648,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage11_RecvKV -> Layer0_Device7_Stage12_RecvKV	[label="Ring transfer",
		lp="37482,17863",
		pos="e,37302,17820 37302,17906 37302,17906 37302,17830 37302,17830"];
	Layer0_Device7_Stage11_Attention -> Layer0_Device7_Stage11_Accumulate	[pos="e,36344,17600 36344,17740 36344,17740 36344,17610 36344,17610"];
	Layer0_Device7_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36366,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage11_Accumulate -> Layer0_Device7_Stage12_Accumulate	[pos="e,36307,17407 36307,17547 36307,17547 36307,17417 36307,17417"];
	Layer0_Device7_Stage12_RecvKV -> Layer0_Device7_Stage12_Attention	[pos="e,36864,17600 36864,17713 36864,17713 36864,17610 36864,17610"];
	Layer0_Device7_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37978,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage12_RecvKV -> Layer0_Device7_Stage13_RecvKV	[label="Ring transfer",
		lp="37861,17670",
		pos="e,37813,17627 37813,17713 37813,17713 37813,17637 37813,17637"];
	Layer0_Device7_Stage12_Attention -> Layer0_Device7_Stage12_Accumulate	[pos="e,36568,17407 36568,17547 36568,17547 36568,17417 36568,17417"];
	Layer0_Device7_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36887,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage12_Accumulate -> Layer0_Device7_Stage13_Accumulate	[pos="e,36655,17187 36598,17380 36615,17380 36626,17380 36626,17380 36626,17380 36626,17187 36626,17187 36626,17187 36645,17187 36645,\
17187"];
	Layer0_Device7_Stage13_RecvKV -> Layer0_Device7_Stage13_Attention	[pos="e,38725,17408 38725,17573 38725,17573 38725,17418 38725,17418"];
	Layer0_Device7_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37575,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage13_RecvKV -> Layer0_Device7_Stage14_RecvKV	[label="Ring transfer",
		lp="37759,17477",
		pos="e,37776,17434 37776,17520 37776,17520 37776,17444 37776,17444"];
	Layer0_Device7_Stage13_Attention -> Layer0_Device7_Stage13_Accumulate	[pos="e,36887,17214 38784,17354 38784,17324 38784,17281 38784,17281 38784,17281 36887,17281 36887,17281 36887,17281 36887,17224 36887,\
17224"];
	Layer0_Device7_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36887,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage13_Accumulate -> Layer0_Device7_Stage14_Accumulate	[pos="e,36887,17062 36887,17161 36887,17161 36887,17072 36887,17072"];
	Layer0_Device7_Stage14_RecvKV -> Layer0_Device7_Stage14_Attention	[pos="e,36366,17214 36699,17340 36510,17340 36366,17340 36366,17340 36366,17340 36366,17224 36366,17224"];
	Layer0_Device7_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="38096,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device7_Stage14_RecvKV -> Layer0_Device7_Stage15_RecvKV	[label="Ring transfer",
		lp="37883,17284",
		pos="e,37836,17241 37836,17327 37836,17327 37836,17251 37836,17251"];
	Layer0_Device7_Stage14_Attention -> Layer0_Device7_Stage14_Accumulate	[pos="e,36655,17026 36366,17161 36366,17115 36366,17026 36366,17026 36366,17026 36645,17026 36645,17026"];
	Layer0_Device7_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="38499,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device7_Stage14_Accumulate -> Layer0_Device7_Stage15_Accumulate	[pos="e,38267,16909 36887,17008 36887,16971 36887,16909 36887,16909 36887,16909 38257,16909 38257,16909"];
	Layer0_Device7_Stage15_RecvKV -> Layer0_Device7_Stage15_Attention	[pos="e,38499,17062 38499,17134 38499,17134 38499,17072 38499,17072"];
	Layer0_Device7_Stage15_Attention -> Layer0_Device7_Stage15_Accumulate	[pos="e,38499,16936 38499,17008 38499,17008 38499,16946 38499,16946"];
	Layer0_Device7_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38503,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device7_Stage15_Accumulate -> Layer0_Device7_ConcatHeads	[pos="e,38503,16810 38503,16882 38503,16882 38503,16820 38503,16820"];
	Layer0_Device7_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38912,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device7_ConcatHeads -> Layer0_Device7_OutputProj	[pos="e,38714,16684 38714,16756 38714,16756 38714,16694 38714,16694"];
	Layer0_Device7_OutputProj -> Layer0_Device7_Residual1	[pos="e,38970,16558 38970,16630 38970,16630 38970,16568 38970,16568"];
	Layer0_Device7_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38902,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device7_Residual1 -> Layer0_Device7_LayerNorm2	[pos="e,38965,16432 38965,16504 38965,16504 38965,16442 38965,16442"];
	Layer0_Device7_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="39202,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device7_Residual1 -> Layer0_Device7_Residual2	[pos="e,39368,15750 39368,16504 39368,16504 39368,15760 39368,15760"];
	Layer0_Device7_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38609,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device7_LayerNorm2 -> Layer0_Device7_GateProj	[pos="e,38762,16306 38762,16378 38762,16378 38762,16316 38762,16316"];
	Layer0_Device7_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38922,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device7_LayerNorm2 -> Layer0_Device7_UpProj	[pos="e,38974,16217 38974,16378 38974,16378 38974,16227 38974,16227"];
	Layer0_Device7_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38609,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device7_GateProj -> Layer0_Device7_Activation	[pos="e,38540,16128 38540,16252 38540,16252 38540,16138 38540,16138"];
	Layer0_Device7_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38705,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device7_UpProj -> Layer0_Device7_ElemMul	[pos="e,38975,16002 38975,16163 38975,16163 38975,16012 38975,16012"];
	Layer0_Device7_Activation -> Layer0_Device7_ElemMul	[pos="e,38609,16002 38609,16074 38609,16074 38609,16012 38609,16012"];
	Layer0_Device7_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38801,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device7_ElemMul -> Layer0_Device7_DownProj	[pos="e,38801,15876 38801,15948 38801,15948 38801,15886 38801,15886"];
	Layer0_Device7_DownProj -> Layer0_Device7_Residual2	[pos="e,38918,15750 38918,15822 38918,15822 38918,15760 38918,15760"];
	Layer0_Device7_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39202,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device7_Residual2 -> Layer0_Device7_Output	[pos="e,39202,15624 39202,15696 39202,15696 39202,15634 39202,15634"];
	Layer1_Device7_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39202,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device7_Output -> Layer1_Device7_Input	[pos="e,39202,15476 39202,15548 39202,15548 39202,15486 39202,15486"];
	Layer0_Device8_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43635,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device8_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43374,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device8_Input -> Layer0_Device8_LayerNorm1	[pos="e,43460,20402 43460,20481 43460,20481 43460,20412 43460,20412"];
	Layer0_Device8_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="44069,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device8_Input -> Layer0_Device8_Residual1	[pos="e,44248,16558 43937,20512 44091,20512 44248,20512 44248,20512 44248,20512 44248,16568 44248,16568"];
	Layer0_Device8_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43063,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device8_LayerNorm1 -> Layer0_Device8_QKVProj	[pos="e,43374,20276 43374,20348 43374,20348 43374,20286 43374,20286"];
	Layer0_Device8_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41817,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage0_RecvKV	[label="Local K,V",
		lp="42318,20179",
		pos="e,42575,20136 42575,20222 42575,20222 42575,20146 42575,20146"];
	Layer0_Device8_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42570,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage0_Attention	[label=Q_local,
		lp="42879,20083",
		pos="e,42769,19917 42769,20222 42769,20222 42769,19927 42769,19927"];
	Layer0_Device8_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40277,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage1_Attention	[label=Q_local,
		lp="40341,19986",
		pos="e,40243,19723 42413,20229 41581,20229 40243,20229 40243,20229 40243,20229 40243,19733 40243,19733"];
	Layer0_Device8_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40888,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage2_Attention	[label=Q_local,
		lp="43019,19890",
		pos="e,41058,19530 42864,20223 42864,20121 42864,19762 42864,19762 42864,19762 41058,19762 41058,19762 41058,19762 41058,19540 41058,\
19540"];
	Layer0_Device8_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42903,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage3_Attention	[label=Q_local,
		lp="43159,19793",
		pos="e,43076,19337 43076,20223 43076,20223 43076,19347 43076,19347"];
	Layer0_Device8_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40109,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage4_Attention	[label=Q_local,
		lp="39961,19697",
		pos="e,39962,19144 42413,20242 41504,20242 39962,20242 39962,20242 39962,20242 39962,19154 39962,19154"];
	Layer0_Device8_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43026,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage5_Attention	[label=Q_local,
		lp="43265,19600",
		pos="e,43196,18951 43196,20222 43196,20222 43196,18961 43196,18961"];
	Layer0_Device8_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43048,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage6_Attention	[label=Q_local,
		lp="43395,19504",
		pos="e,43268,18758 43268,20222 43268,20222 43268,18768 43268,18768"];
	Layer0_Device8_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40109,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage7_Attention	[label=Q_local,
		lp="39815,19407",
		pos="e,39877,18547 42413,20255 41465,20255 39813,20255 39813,20255 39813,20255 39813,18547 39813,18547 39813,18547 39867,18547 39867,\
18547"];
	Layer0_Device8_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42854,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage8_Attention	[label=Q_local,
		lp="43518,19311",
		pos="e,43086,18345 43320,20223 43320,19993 43320,18345 43320,18345 43320,18345 43096,18345 43096,18345"];
	Layer0_Device8_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="39915,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage9_Attention	[label=Q_local,
		lp="39697,19214",
		pos="e,39748,18179 42413,20262 41448,20262 39748,20262 39748,20262 39748,20262 39748,18189 39748,18189"];
	Layer0_Device8_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="39741,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage10_Attention	[label=Q_local,
		lp="39557,19118",
		pos="e,39596,17986 42413,20269 41410,20269 39596,20269 39596,20269 39596,20269 39596,17996 39596,17996"];
	Layer0_Device8_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41355,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage11_Attention	[label=Q_local,
		lp="43641,19021",
		pos="e,41432,17793 43402,20222 43402,19961 43402,17872 43402,17872 43402,17872 41432,17872 41432,17872 41432,17872 41432,17803 41432,\
17803"];
	Layer0_Device8_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41679,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage12_Attention	[label=Q_local,
		lp="43782,18925",
		pos="e,41615,17600 43443,20222 43443,19961 43443,17868 43443,17868 43443,17868 41615,17868 41615,17868 41615,17868 41615,17610 41615,\
17610"];
	Layer0_Device8_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43694,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage13_Attention	[label=Q_local,
		lp="43922,18828",
		pos="e,43867,17408 43713,20235 43803,20235 43867,20235 43867,20235 43867,20235 43867,17418 43867,17418"];
	Layer0_Device8_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41276,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage14_Attention	[label=Q_local,
		lp="39411,18732",
		pos="e,41044,17187 43361,20222 43361,19963 43361,17900 43361,17900 43361,17900 40894,17900 40894,17900 40894,17900 40894,17187 40894,\
17187 40894,17187 41034,17187 41034,17187"];
	Layer0_Device8_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43409,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_QKVProj -> Layer0_Device8_Stage15_Attention	[label=Q_local,
		lp="44040,18635",
		pos="e,43641,17043 43713,20249 43838,20249 43931,20249 43931,20249 43931,20249 43931,17043 43931,17043 43931,17043 43651,17043 43651,\
17043"];
	Layer0_Device8_Stage0_RecvKV -> Layer0_Device8_Stage0_Attention	[pos="e,42538,19916 42538,20079 42538,20079 42538,19926 42538,19926"];
	Layer0_Device8_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41361,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage0_RecvKV -> Layer0_Device8_Stage1_RecvKV	[label="Ring transfer",
		lp="41637,19986",
		pos="e,41589,19943 41589,20029 41589,20029 41589,19953 41589,19953"];
	Layer0_Device8_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="40798,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage0_Attention -> Layer0_Device8_Stage0_Accumulate	[pos="e,40798,19723 42570,19863 42570,19826 42570,19765 42570,19765 42570,19765 40798,19765 40798,19765 40798,19765 40798,19733 40798,\
19733"];
	Layer0_Device8_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40367,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage0_Accumulate -> Layer0_Device8_Stage1_Accumulate	[pos="e,40582,19530 40582,19670 40582,19670 40582,19540 40582,19540"];
	Layer0_Device8_Stage1_RecvKV -> Layer0_Device8_Stage1_Attention	[pos="e,40475,19724 40475,19889 40475,19889 40475,19734 40475,19734"];
	Layer0_Device8_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="42007,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage1_RecvKV -> Layer0_Device8_Stage2_RecvKV	[label="Ring transfer",
		lp="41678,19793",
		pos="e,41684,19750 41684,19836 41684,19836 41684,19760 41684,19760"];
	Layer0_Device8_Stage1_Attention -> Layer0_Device8_Stage1_Accumulate	[pos="e,40322,19530 40322,19670 40322,19670 40322,19540 40322,19540"];
	Layer0_Device8_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40485,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage1_Accumulate -> Layer0_Device8_Stage2_Accumulate	[pos="e,40426,19337 40426,19477 40426,19477 40426,19347 40426,19347"];
	Layer0_Device8_Stage2_RecvKV -> Layer0_Device8_Stage2_Attention	[pos="e,41103,19531 41103,19696 41103,19696 41103,19541 41103,19541"];
	Layer0_Device8_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="42097,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage2_RecvKV -> Layer0_Device8_Stage3_RecvKV	[label="Ring transfer",
		lp="42100,19600",
		pos="e,42052,19557 42052,19643 42052,19643 42052,19567 42052,19567"];
	Layer0_Device8_Stage2_Attention -> Layer0_Device8_Stage2_Accumulate	[pos="e,40686,19337 40686,19477 40686,19477 40686,19347 40686,19347"];
	Layer0_Device8_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40630,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage2_Accumulate -> Layer0_Device8_Stage3_Accumulate	[pos="e,40558,19144 40558,19284 40558,19284 40558,19154 40558,19154"];
	Layer0_Device8_Stage3_RecvKV -> Layer0_Device8_Stage3_Attention	[pos="e,42844,19338 42844,19503 42844,19503 42844,19348 42844,19348"];
	Layer0_Device8_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41694,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage3_RecvKV -> Layer0_Device8_Stage4_RecvKV	[label="Ring transfer",
		lp="42011,19407",
		pos="e,41896,19364 41896,19450 41896,19450 41896,19374 41896,19374"];
	Layer0_Device8_Stage3_Attention -> Layer0_Device8_Stage3_Accumulate	[pos="e,40818,19144 42715,19284 42715,19255 42715,19214 42715,19214 42715,19214 40818,19214 40818,19214 40818,19214 40818,19154 40818,\
19154"];
	Layer0_Device8_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40608,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage3_Accumulate -> Layer0_Device8_Stage4_Accumulate	[pos="e,40619,18951 40619,19091 40619,19091 40619,18961 40619,18961"];
	Layer0_Device8_Stage4_RecvKV -> Layer0_Device8_Stage4_Attention	[pos="e,40297,19144 40818,19270 40537,19270 40297,19270 40297,19270 40297,19270 40297,19154 40297,19154"];
	Layer0_Device8_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41839,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage4_RecvKV -> Layer0_Device8_Stage5_RecvKV	[label="Ring transfer",
		lp="41814,19214",
		pos="e,41766,19171 41766,19257 41766,19257 41766,19181 41766,19181"];
	Layer0_Device8_Stage4_Attention -> Layer0_Device8_Stage4_Accumulate	[pos="e,40388,18951 40341,19117 40369,19117 40388,19117 40388,19117 40388,19117 40388,18961 40388,18961"];
	Layer0_Device8_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42527,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage4_Accumulate -> Layer0_Device8_Stage5_Accumulate	[pos="e,42516,18758 40619,18898 40619,18863 40619,18808 40619,18808 40619,18808 42516,18808 42516,18808 42516,18808 42516,18768 42516,\
18768"];
	Layer0_Device8_Stage5_RecvKV -> Layer0_Device8_Stage5_Attention	[pos="e,42794,18924 42748,19117 42748,19112 42748,18924 42748,18924 42748,18924 42784,18924 42784,18924"];
	Layer0_Device8_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41817,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage5_RecvKV -> Layer0_Device8_Stage6_RecvKV	[label="Ring transfer",
		lp="41876,19021",
		pos="e,41828,18978 41828,19064 41828,19064 41828,18988 41828,18988"];
	Layer0_Device8_Stage5_Attention -> Layer0_Device8_Stage5_Accumulate	[pos="e,42759,18731 42806,18898 42806,18845 42806,18731 42806,18731 42806,18731 42769,18731 42769,18731"];
	Layer0_Device8_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42527,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage5_Accumulate -> Layer0_Device8_Stage6_Accumulate	[pos="e,42527,18565 42527,18705 42527,18705 42527,18575 42527,18575"];
	Layer0_Device8_Stage6_RecvKV -> Layer0_Device8_Stage6_Attention	[pos="e,43037,18758 42405,18884 42717,18884 43037,18884 43037,18884 43037,18884 43037,18768 43037,18768"];
	Layer0_Device8_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41318,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage6_RecvKV -> Layer0_Device8_Stage7_RecvKV	[label="Ring transfer",
		lp="41550,18828",
		pos="e,41568,18785 41568,18871 41568,18871 41568,18795 41568,18795"];
	Layer0_Device8_Stage6_Attention -> Layer0_Device8_Stage6_Accumulate	[pos="e,42759,18538 42951,18705 42951,18652 42951,18538 42951,18538 42951,18538 42769,18538 42769,18538"];
	Layer0_Device8_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42333,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage6_Accumulate -> Layer0_Device8_Stage7_Accumulate	[pos="e,42430,18372 42430,18512 42430,18512 42430,18382 42430,18382"];
	Layer0_Device8_Stage7_RecvKV -> Layer0_Device8_Stage7_Attention	[pos="e,40109,18565 40442,18691 40253,18691 40109,18691 40109,18691 40109,18691 40109,18575 40109,18575"];
	Layer0_Device8_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41318,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage7_RecvKV -> Layer0_Device8_Stage8_RecvKV	[label="Ring transfer",
		lp="41366,18635",
		pos="e,41318,18592 41318,18678 41318,18678 41318,18602 41318,18602"];
	Layer0_Device8_Stage7_Attention -> Layer0_Device8_Stage7_Accumulate	[pos="e,42170,18372 40272,18512 40272,18482 40272,18439 40272,18439 40272,18439 42170,18439 42170,18439 42170,18439 42170,18382 42170,\
18382"];
	Layer0_Device8_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42333,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage7_Accumulate -> Layer0_Device8_Stage8_Accumulate	[pos="e,42333,18179 42333,18319 42333,18319 42333,18189 42333,18189"];
	Layer0_Device8_Stage8_RecvKV -> Layer0_Device8_Stage8_Attention	[pos="e,42690,18372 41906,18498 42278,18498 42690,18498 42690,18498 42690,18498 42690,18382 42690,18382"];
	Layer0_Device8_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41124,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage8_RecvKV -> Layer0_Device8_Stage9_RecvKV	[label="Ring transfer",
		lp="41269,18442",
		pos="e,41221,18399 41221,18485 41221,18485 41221,18409 41221,18409"];
	Layer0_Device8_Stage8_Attention -> Layer0_Device8_Stage8_Accumulate	[pos="e,42565,18152 42854,18319 42854,18266 42854,18152 42854,18152 42854,18152 42575,18152 42575,18152"];
	Layer0_Device8_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40262,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage8_Accumulate -> Layer0_Device8_Stage9_Accumulate	[pos="e,40349,17986 42246,18126 42246,18107 42246,18085 42246,18085 42246,18085 40349,18085 40349,18085 40349,18085 40349,17996 40349,\
17996"];
	Layer0_Device8_Stage9_RecvKV -> Layer0_Device8_Stage9_Attention	[pos="e,40012,18179 40248,18305 40110,18305 40012,18305 40012,18305 40012,18305 40012,18189 40012,18189"];
	Layer0_Device8_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41124,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage9_RecvKV -> Layer0_Device8_Stage10_RecvKV	[label="Ring transfer",
		lp="41172,18249",
		pos="e,41124,18206 41124,18292 41124,18292 41124,18216 41124,18216"];
	Layer0_Device8_Stage9_Attention -> Layer0_Device8_Stage9_Accumulate	[pos="e,40088,17986 40088,18126 40088,18126 40088,17996 40088,17996"];
	Layer0_Device8_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40262,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage9_Accumulate -> Layer0_Device8_Stage10_Accumulate	[pos="e,40262,17793 40262,17933 40262,17933 40262,17803 40262,17803"];
	Layer0_Device8_Stage10_RecvKV -> Layer0_Device8_Stage10_Attention	[pos="e,39828,17986 40248,18112 40016,18112 39828,18112 39828,18112 39828,18112 39828,17996 39828,17996"];
	Layer0_Device8_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41471,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage10_RecvKV -> Layer0_Device8_Stage11_RecvKV	[label="Ring transfer",
		lp="41377,18056",
		pos="e,41298,18013 41298,18099 41298,18099 41298,18023 41298,18023"];
	Layer0_Device8_Stage10_Attention -> Layer0_Device8_Stage10_Accumulate	[pos="e,40030,17766 39907,17933 39907,17880 39907,17766 39907,17766 39907,17766 40020,17766 40020,17766"];
	Layer0_Device8_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41158,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage10_Accumulate -> Layer0_Device8_Stage11_Accumulate	[pos="e,40926,17573 40468,17740 40468,17687 40468,17573 40468,17573 40468,17573 40916,17573 40916,17573"];
	Layer0_Device8_Stage11_RecvKV -> Layer0_Device8_Stage11_Attention	[pos="e,41278,17793 41278,17906 41278,17906 41278,17803 41278,17803"];
	Layer0_Device8_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42564,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage11_RecvKV -> Layer0_Device8_Stage12_RecvKV	[label="Ring transfer",
		lp="42054,17863",
		pos="e,42018,17819 42018,17907 42018,17907 42018,17829 42018,17829"];
	Layer0_Device8_Stage11_Attention -> Layer0_Device8_Stage11_Accumulate	[pos="e,41256,17600 41256,17740 41256,17740 41256,17610 41256,17610"];
	Layer0_Device8_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41276,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage11_Accumulate -> Layer0_Device8_Stage12_Accumulate	[pos="e,41217,17407 41217,17547 41217,17547 41217,17417 41217,17417"];
	Layer0_Device8_Stage12_RecvKV -> Layer0_Device8_Stage12_Attention	[pos="e,41777,17600 41777,17713 41777,17713 41777,17610 41777,17610"];
	Layer0_Device8_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42888,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage12_RecvKV -> Layer0_Device8_Stage13_RecvKV	[label="Ring transfer",
		lp="42863,17670",
		pos="e,42726,17627 42726,17713 42726,17713 42726,17637 42726,17637"];
	Layer0_Device8_Stage12_Attention -> Layer0_Device8_Stage12_Accumulate	[pos="e,41478,17407 41478,17547 41478,17547 41478,17417 41478,17417"];
	Layer0_Device8_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41797,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage12_Accumulate -> Layer0_Device8_Stage13_Accumulate	[pos="e,41565,17187 41508,17380 41525,17380 41536,17380 41536,17380 41536,17380 41536,17187 41536,17187 41536,17187 41555,17187 41555,\
17187"];
	Layer0_Device8_Stage13_RecvKV -> Layer0_Device8_Stage13_Attention	[pos="e,43635,17408 43635,17573 43635,17573 43635,17418 43635,17418"];
	Layer0_Device8_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42485,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage13_RecvKV -> Layer0_Device8_Stage14_RecvKV	[label="Ring transfer",
		lp="42802,17477",
		pos="e,42686,17434 42686,17520 42686,17520 42686,17444 42686,17444"];
	Layer0_Device8_Stage13_Attention -> Layer0_Device8_Stage13_Accumulate	[pos="e,41797,17214 43694,17354 43694,17326 43694,17286 43694,17286 43694,17286 41797,17286 41797,17286 41797,17286 41797,17224 41797,\
17224"];
	Layer0_Device8_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41797,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage13_Accumulate -> Layer0_Device8_Stage14_Accumulate	[pos="e,41797,17062 41797,17161 41797,17161 41797,17072 41797,17072"];
	Layer0_Device8_Stage14_RecvKV -> Layer0_Device8_Stage14_Attention	[pos="e,41276,17214 41609,17340 41420,17340 41276,17340 41276,17340 41276,17340 41276,17224 41276,17224"];
	Layer0_Device8_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="43006,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device8_Stage14_RecvKV -> Layer0_Device8_Stage15_RecvKV	[label="Ring transfer",
		lp="42793,17284",
		pos="e,42746,17241 42746,17327 42746,17327 42746,17251 42746,17251"];
	Layer0_Device8_Stage14_Attention -> Layer0_Device8_Stage14_Accumulate	[pos="e,41565,17026 41276,17161 41276,17115 41276,17026 41276,17026 41276,17026 41555,17026 41555,17026"];
	Layer0_Device8_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="43409,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device8_Stage14_Accumulate -> Layer0_Device8_Stage15_Accumulate	[pos="e,43177,16909 41797,17008 41797,16971 41797,16909 41797,16909 41797,16909 43167,16909 43167,16909"];
	Layer0_Device8_Stage15_RecvKV -> Layer0_Device8_Stage15_Attention	[pos="e,43409,17062 43409,17134 43409,17134 43409,17072 43409,17072"];
	Layer0_Device8_Stage15_Attention -> Layer0_Device8_Stage15_Accumulate	[pos="e,43409,16936 43409,17008 43409,17008 43409,16946 43409,16946"];
	Layer0_Device8_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43815,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device8_Stage15_Accumulate -> Layer0_Device8_ConcatHeads	[pos="e,43615,16810 43615,16882 43615,16882 43615,16820 43615,16820"];
	Layer0_Device8_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43826,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device8_ConcatHeads -> Layer0_Device8_OutputProj	[pos="e,43826,16684 43826,16756 43826,16756 43826,16694 43826,16694"];
	Layer0_Device8_OutputProj -> Layer0_Device8_Residual1	[pos="e,43861,16558 43861,16630 43861,16630 43861,16568 43861,16568"];
	Layer0_Device8_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43976,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device8_Residual1 -> Layer0_Device8_LayerNorm2	[pos="e,43976,16432 43976,16504 43976,16504 43976,16442 43976,16442"];
	Layer0_Device8_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="44053,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device8_Residual1 -> Layer0_Device8_Residual2	[pos="e,44330,15750 44330,16504 44330,16504 44330,15760 44330,15760"];
	Layer0_Device8_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43683,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device8_LayerNorm2 -> Layer0_Device8_GateProj	[pos="e,43836,16306 43836,16378 43836,16378 43836,16316 43836,16316"];
	Layer0_Device8_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43996,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device8_LayerNorm2 -> Layer0_Device8_UpProj	[pos="e,44048,16217 44048,16378 44048,16378 44048,16227 44048,16227"];
	Layer0_Device8_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43683,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device8_GateProj -> Layer0_Device8_Activation	[pos="e,43614,16128 43614,16252 43614,16252 43614,16138 43614,16138"];
	Layer0_Device8_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43779,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device8_UpProj -> Layer0_Device8_ElemMul	[pos="e,44049,16002 44049,16163 44049,16163 44049,16012 44049,16012"];
	Layer0_Device8_Activation -> Layer0_Device8_ElemMul	[pos="e,43683,16002 43683,16074 43683,16074 43683,16012 43683,16012"];
	Layer0_Device8_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43924,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device8_ElemMul -> Layer0_Device8_DownProj	[pos="e,43924,15876 43924,15948 43924,15948 43924,15886 43924,15886"];
	Layer0_Device8_DownProj -> Layer0_Device8_Residual2	[pos="e,43924,15750 43924,15822 43924,15822 43924,15760 43924,15760"];
	Layer0_Device8_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="44053,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device8_Residual2 -> Layer0_Device8_Output	[pos="e,44053,15624 44053,15696 44053,15696 44053,15634 44053,15634"];
	Layer1_Device8_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="44053,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device8_Output -> Layer1_Device8_Input	[pos="e,44053,15476 44053,15548 44053,15548 44053,15486 44053,15486"];
	Layer0_Device9_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48929,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device9_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48444,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device9_Input -> Layer0_Device9_LayerNorm1	[pos="e,48642,20402 48642,20500 48642,20500 48642,20412 48642,20412"];
	Layer0_Device9_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="48619,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device9_Input -> Layer0_Device9_Residual1	[pos="e,48965,16558 48965,20475 48965,20475 48965,16568 48965,16568"];
	Layer0_Device9_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48025,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device9_LayerNorm1 -> Layer0_Device9_QKVProj	[pos="e,48444,20276 48444,20348 48444,20348 48444,20286 48444,20286"];
	Layer0_Device9_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46779,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage0_RecvKV	[label="Local K,V",
		lp="47280,20179",
		pos="e,47537,20136 47537,20222 47537,20222 47537,20146 47537,20146"];
	Layer0_Device9_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47532,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage0_Attention	[label=Q_local,
		lp="47841,20083",
		pos="e,47731,19917 47731,20222 47731,20222 47731,19927 47731,19927"];
	Layer0_Device9_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45239,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage1_Attention	[label=Q_local,
		lp="45303,19986",
		pos="e,45205,19723 47375,20229 46543,20229 45205,20229 45205,20229 45205,20229 45205,19733 45205,19733"];
	Layer0_Device9_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45850,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage2_Attention	[label=Q_local,
		lp="47981,19890",
		pos="e,46020,19530 47826,20223 47826,20122 47826,19768 47826,19768 47826,19768 46020,19768 46020,19768 46020,19768 46020,19540 46020,\
19540"];
	Layer0_Device9_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47865,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage3_Attention	[label=Q_local,
		lp="48121,19793",
		pos="e,48038,19337 48038,20223 48038,20223 48038,19347 48038,19347"];
	Layer0_Device9_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45071,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage4_Attention	[label=Q_local,
		lp="44923,19697",
		pos="e,44924,19144 47375,20242 46466,20242 44924,20242 44924,20242 44924,20242 44924,19154 44924,19154"];
	Layer0_Device9_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47985,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage5_Attention	[label=Q_local,
		lp="48225,19600",
		pos="e,48156,18951 48156,20222 48156,20222 48156,18961 48156,18961"];
	Layer0_Device9_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48002,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage6_Attention	[label=Q_local,
		lp="48357,19504",
		pos="e,48225,18758 48225,20222 48225,20222 48225,18768 48225,18768"];
	Layer0_Device9_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45063,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage7_Attention	[label=Q_local,
		lp="44777,19407",
		pos="e,44836,18565 47375,20255 46443,20255 44836,20255 44836,20255 44836,20255 44836,18575 44836,18575"];
	Layer0_Device9_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47949,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage8_Attention	[label=Q_local,
		lp="48477,19311",
		pos="e,48181,18345 48261,20223 48261,19993 48261,18345 48261,18345 48261,18345 48191,18345 48191,18345"];
	Layer0_Device9_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45010,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage9_Attention	[label=Q_local,
		lp="44659,19214",
		pos="e,44805,18179 47375,20262 46435,20262 44805,20262 44805,20262 44805,20262 44805,18189 44805,18189"];
	Layer0_Device9_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44716,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage10_Attention	[label=Q_local,
		lp="44519,19118",
		pos="e,44632,17986 47375,20269 46390,20269 44632,20269 44632,20269 44632,20269 44632,17996 44632,17996"];
	Layer0_Device9_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46241,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage11_Attention	[label=Q_local,
		lp="48598,19021",
		pos="e,46318,17793 48315,20223 48315,19963 48315,17887 48315,17887 48315,17887 46318,17887 46318,17887 46318,17887 46318,17803 46318,\
17803"];
	Layer0_Device9_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46638,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage12_Attention	[label=Q_local,
		lp="48741,18925",
		pos="e,46501,17600 48342,20222 48342,19962 48342,17874 48342,17874 48342,17874 46501,17874 46501,17874 46501,17874 46501,17610 46501,\
17610"];
	Layer0_Device9_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48653,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage13_Attention	[label=Q_local,
		lp="48881,18828",
		pos="e,48826,17408 48675,20235 48764,20235 48826,20235 48826,20235 48826,20235 48826,17418 48826,17418"];
	Layer0_Device9_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46235,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage14_Attention	[label=Q_local,
		lp="44373,18732",
		pos="e,46003,17187 48288,20222 48288,19963 48288,17902 48288,17902 48288,17902 45819,17902 45819,17902 45819,17902 45819,17187 45819,\
17187 45819,17187 45993,17187 45993,17187"];
	Layer0_Device9_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48368,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_QKVProj -> Layer0_Device9_Stage15_Attention	[label=Q_local,
		lp="48999,18635",
		pos="e,48600,17043 48675,20249 48816,20249 48925,20249 48925,20249 48925,20249 48925,17043 48925,17043 48925,17043 48610,17043 48610,\
17043"];
	Layer0_Device9_Stage0_RecvKV -> Layer0_Device9_Stage0_Attention	[pos="e,47500,19916 47500,20079 47500,20079 47500,19926 47500,19926"];
	Layer0_Device9_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46323,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage0_RecvKV -> Layer0_Device9_Stage1_RecvKV	[label="Ring transfer",
		lp="46680,19986",
		pos="e,46551,19943 46551,20029 46551,20029 46551,19953 46551,19953"];
	Layer0_Device9_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="45760,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage0_Attention -> Layer0_Device9_Stage0_Accumulate	[pos="e,45760,19724 47532,19863 47532,19828 47532,19770 47532,19770 47532,19770 45760,19770 45760,19770 45760,19770 45760,19734 45760,\
19734"];
	Layer0_Device9_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45329,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage0_Accumulate -> Layer0_Device9_Stage1_Accumulate	[pos="e,45544,19530 45544,19670 45544,19670 45544,19540 45544,19540"];
	Layer0_Device9_Stage1_RecvKV -> Layer0_Device9_Stage1_Attention	[pos="e,45437,19724 45437,19889 45437,19889 45437,19734 45437,19734"];
	Layer0_Device9_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46969,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage1_RecvKV -> Layer0_Device9_Stage2_RecvKV	[label="Ring transfer",
		lp="46640,19793",
		pos="e,46646,19750 46646,19836 46646,19836 46646,19760 46646,19760"];
	Layer0_Device9_Stage1_Attention -> Layer0_Device9_Stage1_Accumulate	[pos="e,45284,19530 45284,19670 45284,19670 45284,19540 45284,19540"];
	Layer0_Device9_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45447,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage1_Accumulate -> Layer0_Device9_Stage2_Accumulate	[pos="e,45388,19337 45388,19477 45388,19477 45388,19347 45388,19347"];
	Layer0_Device9_Stage2_RecvKV -> Layer0_Device9_Stage2_Attention	[pos="e,46065,19531 46065,19696 46065,19696 46065,19541 46065,19541"];
	Layer0_Device9_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="47059,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage2_RecvKV -> Layer0_Device9_Stage3_RecvKV	[label="Ring transfer",
		lp="47062,19600",
		pos="e,47014,19557 47014,19643 47014,19643 47014,19567 47014,19567"];
	Layer0_Device9_Stage2_Attention -> Layer0_Device9_Stage2_Accumulate	[pos="e,45648,19337 45648,19477 45648,19477 45648,19347 45648,19347"];
	Layer0_Device9_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45592,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage2_Accumulate -> Layer0_Device9_Stage3_Accumulate	[pos="e,45520,19144 45520,19284 45520,19284 45520,19154 45520,19154"];
	Layer0_Device9_Stage3_RecvKV -> Layer0_Device9_Stage3_Attention	[pos="e,47806,19338 47806,19503 47806,19503 47806,19348 47806,19348"];
	Layer0_Device9_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46656,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage3_RecvKV -> Layer0_Device9_Stage4_RecvKV	[label="Ring transfer",
		lp="46906,19407",
		pos="e,46858,19364 46858,19450 46858,19450 46858,19374 46858,19374"];
	Layer0_Device9_Stage3_Attention -> Layer0_Device9_Stage3_Accumulate	[pos="e,45780,19144 47677,19284 47677,19256 47677,19218 47677,19218 47677,19218 45780,19218 45780,19218 45780,19218 45780,19154 45780,\
19154"];
	Layer0_Device9_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45567,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage3_Accumulate -> Layer0_Device9_Stage4_Accumulate	[pos="e,45580,18951 45580,19091 45580,19091 45580,18961 45580,18961"];
	Layer0_Device9_Stage4_RecvKV -> Layer0_Device9_Stage4_Attention	[pos="e,45259,19144 45780,19270 45499,19270 45259,19270 45259,19270 45259,19270 45259,19154 45259,19154"];
	Layer0_Device9_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46801,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage4_RecvKV -> Layer0_Device9_Stage5_RecvKV	[label="Ring transfer",
		lp="46776,19214",
		pos="e,46728,19171 46728,19257 46728,19257 46728,19181 46728,19181"];
	Layer0_Device9_Stage4_Attention -> Layer0_Device9_Stage4_Accumulate	[pos="e,45348,18951 45303,19117 45330,19117 45348,19117 45348,19117 45348,19117 45348,18961 45348,18961"];
	Layer0_Device9_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47481,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage4_Accumulate -> Layer0_Device9_Stage5_Accumulate	[pos="e,47473,18758 45575,18898 45575,18861 45575,18799 45575,18799 45575,18799 47473,18799 47473,18799 47473,18799 47473,18768 47473,\
18768"];
	Layer0_Device9_Stage5_RecvKV -> Layer0_Device9_Stage5_Attention	[pos="e,47753,18933 47701,19117 47701,19112 47701,18933 47701,18933 47701,18933 47743,18933 47743,18933"];
	Layer0_Device9_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46776,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage5_RecvKV -> Layer0_Device9_Stage6_RecvKV	[label="Ring transfer",
		lp="46836,19021",
		pos="e,46788,18978 46788,19064 46788,19064 46788,18988 46788,18988"];
	Layer0_Device9_Stage5_Attention -> Layer0_Device9_Stage5_Accumulate	[pos="e,47707,18758 47753,18915 47726,18915 47707,18915 47707,18915 47707,18915 47707,18768 47707,18768"];
	Layer0_Device9_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47481,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage5_Accumulate -> Layer0_Device9_Stage6_Accumulate	[pos="e,47481,18565 47481,18705 47481,18705 47481,18575 47481,18575"];
	Layer0_Device9_Stage6_RecvKV -> Layer0_Device9_Stage6_Attention	[pos="e,47994,18758 47363,18884 47675,18884 47994,18884 47994,18884 47994,18884 47994,18768 47994,18768"];
	Layer0_Device9_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46272,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage6_RecvKV -> Layer0_Device9_Stage7_RecvKV	[label="Ring transfer",
		lp="46689,18828",
		pos="e,46524,18785 46524,18871 46524,18871 46524,18795 46524,18795"];
	Layer0_Device9_Stage6_Attention -> Layer0_Device9_Stage6_Accumulate	[pos="e,47713,18529 47976,18705 47976,18650 47976,18529 47976,18529 47976,18529 47723,18529 47723,18529"];
	Layer0_Device9_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47428,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage6_Accumulate -> Layer0_Device9_Stage7_Accumulate	[pos="e,47454,18372 47454,18512 47454,18512 47454,18382 47454,18382"];
	Layer0_Device9_Stage7_RecvKV -> Layer0_Device9_Stage7_Attention	[pos="e,45067,18565 45396,18691 45209,18691 45067,18691 45067,18691 45067,18691 45067,18575 45067,18575"];
	Layer0_Device9_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46272,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage7_RecvKV -> Layer0_Device9_Stage8_RecvKV	[label="Ring transfer",
		lp="46320,18635",
		pos="e,46272,18592 46272,18678 46272,18678 46272,18602 46272,18602"];
	Layer0_Device9_Stage7_Attention -> Layer0_Device9_Stage7_Accumulate	[pos="e,47223,18372 45268,18512 45268,18497 45268,18480 45268,18480 45268,18480 47223,18480 47223,18480 47223,18480 47223,18382 47223,\
18382"];
	Layer0_Device9_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47428,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage7_Accumulate -> Layer0_Device9_Stage8_Accumulate	[pos="e,47428,18179 47428,18319 47428,18319 47428,18189 47428,18189"];
	Layer0_Device9_Stage8_RecvKV -> Layer0_Device9_Stage8_Attention	[pos="e,47737,18372 46859,18498 47267,18498 47737,18498 47737,18498 47737,18498 47737,18382 47737,18382"];
	Layer0_Device9_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46219,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage8_RecvKV -> Layer0_Device9_Stage9_RecvKV	[label="Ring transfer",
		lp="46294,18442",
		pos="e,46246,18399 46246,18485 46246,18485 46246,18409 46246,18409"];
	Layer0_Device9_Stage8_Attention -> Layer0_Device9_Stage8_Accumulate	[pos="e,47660,18152 47949,18319 47949,18266 47949,18152 47949,18152 47949,18152 47670,18152 47670,18152"];
	Layer0_Device9_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45237,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage8_Accumulate -> Layer0_Device9_Stage9_Accumulate	[pos="e,45384,17986 47281,18126 47281,18108 47281,18088 47281,18088 47281,18088 45384,18088 45384,18088 45384,18088 45384,17996 45384,\
17996"];
	Layer0_Device9_Stage9_RecvKV -> Layer0_Device9_Stage9_Attention	[pos="e,45036,18179 45343,18305 45168,18305 45036,18305 45036,18305 45036,18305 45036,18189 45036,18189"];
	Layer0_Device9_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="46219,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage9_RecvKV -> Layer0_Device9_Stage10_RecvKV	[label="Ring transfer",
		lp="46267,18249",
		pos="e,46219,18206 46219,18292 46219,18292 46219,18216 46219,18216"];
	Layer0_Device9_Stage9_Attention -> Layer0_Device9_Stage9_Accumulate	[pos="e,45124,17986 45124,18126 45124,18126 45124,17996 45124,17996"];
	Layer0_Device9_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45237,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage9_Accumulate -> Layer0_Device9_Stage10_Accumulate	[pos="e,45237,17793 45237,17933 45237,17933 45237,17803 45237,17803"];
	Layer0_Device9_Stage10_RecvKV -> Layer0_Device9_Stage10_Attention	[pos="e,44863,17986 45343,18112 45081,18112 44863,18112 44863,18112 44863,18112 44863,17996 44863,17996"];
	Layer0_Device9_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="46446,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage10_RecvKV -> Layer0_Device9_Stage11_RecvKV	[label="Ring transfer",
		lp="46436,18056",
		pos="e,46332,18013 46332,18099 46332,18099 46332,18023 46332,18023"];
	Layer0_Device9_Stage10_Attention -> Layer0_Device9_Stage10_Accumulate	[pos="e,45005,17766 44924,17933 44924,17880 44924,17766 44924,17766 44924,17766 44995,17766 44995,17766"];
	Layer0_Device9_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46117,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage10_Accumulate -> Layer0_Device9_Stage11_Accumulate	[pos="e,45948,17600 45469,17766 45676,17766 45948,17766 45948,17766 45948,17766 45948,17610 45948,17610"];
	Layer0_Device9_Stage11_RecvKV -> Layer0_Device9_Stage11_Attention	[pos="e,46164,17793 46164,17906 46164,17906 46164,17803 46164,17803"];
	Layer0_Device9_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47450,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage11_RecvKV -> Layer0_Device9_Stage12_RecvKV	[label="Ring transfer",
		lp="46940,17863",
		pos="e,46948,17820 46948,17906 46948,17906 46948,17830 46948,17830"];
	Layer0_Device9_Stage11_Attention -> Layer0_Device9_Stage11_Accumulate	[pos="e,46179,17600 46179,17740 46179,17740 46179,17610 46179,17610"];
	Layer0_Device9_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46235,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage11_Accumulate -> Layer0_Device9_Stage12_Accumulate	[pos="e,46176,17407 46176,17547 46176,17547 46176,17417 46176,17417"];
	Layer0_Device9_Stage12_RecvKV -> Layer0_Device9_Stage12_Attention	[pos="e,46700,17600 46700,17713 46700,17713 46700,17610 46700,17610"];
	Layer0_Device9_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47847,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage12_RecvKV -> Layer0_Device9_Stage13_RecvKV	[label="Ring transfer",
		lp="47676,17670",
		pos="e,47648,17627 47648,17713 47648,17713 47648,17637 47648,17637"];
	Layer0_Device9_Stage12_Attention -> Layer0_Device9_Stage12_Accumulate	[pos="e,46436,17407 46436,17547 46436,17547 46436,17417 46436,17417"];
	Layer0_Device9_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46756,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage12_Accumulate -> Layer0_Device9_Stage13_Accumulate	[pos="e,46524,17187 46467,17380 46484,17380 46495,17380 46495,17380 46495,17380 46495,17187 46495,17187 46495,17187 46514,17187 46514,\
17187"];
	Layer0_Device9_Stage13_RecvKV -> Layer0_Device9_Stage13_Attention	[pos="e,48594,17408 48594,17573 48594,17573 48594,17418 48594,17418"];
	Layer0_Device9_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47444,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage13_RecvKV -> Layer0_Device9_Stage14_RecvKV	[label="Ring transfer",
		lp="47628,17477",
		pos="e,47646,17434 47646,17520 47646,17520 47646,17444 47646,17444"];
	Layer0_Device9_Stage13_Attention -> Layer0_Device9_Stage13_Accumulate	[pos="e,46756,17214 48653,17354 48653,17327 48653,17291 48653,17291 48653,17291 46756,17291 46756,17291 46756,17291 46756,17224 46756,\
17224"];
	Layer0_Device9_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46756,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage13_Accumulate -> Layer0_Device9_Stage14_Accumulate	[pos="e,46756,17062 46756,17161 46756,17161 46756,17072 46756,17072"];
	Layer0_Device9_Stage14_RecvKV -> Layer0_Device9_Stage14_Attention	[pos="e,46235,17214 46568,17340 46379,17340 46235,17340 46235,17340 46235,17340 46235,17224 46235,17224"];
	Layer0_Device9_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47965,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device9_Stage14_RecvKV -> Layer0_Device9_Stage15_RecvKV	[label="Ring transfer",
		lp="47752,17284",
		pos="e,47704,17241 47704,17327 47704,17327 47704,17251 47704,17251"];
	Layer0_Device9_Stage14_Attention -> Layer0_Device9_Stage14_Accumulate	[pos="e,46524,17026 46235,17161 46235,17115 46235,17026 46235,17026 46235,17026 46514,17026 46514,17026"];
	Layer0_Device9_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="48368,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device9_Stage14_Accumulate -> Layer0_Device9_Stage15_Accumulate	[pos="e,48136,16909 46756,17008 46756,16971 46756,16909 46756,16909 46756,16909 48126,16909 48126,16909"];
	Layer0_Device9_Stage15_RecvKV -> Layer0_Device9_Stage15_Attention	[pos="e,48368,17062 48368,17134 48368,17134 48368,17072 48368,17072"];
	Layer0_Device9_Stage15_Attention -> Layer0_Device9_Stage15_Accumulate	[pos="e,48368,16936 48368,17008 48368,17008 48368,16946 48368,16946"];
	Layer0_Device9_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48404,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device9_Stage15_Accumulate -> Layer0_Device9_ConcatHeads	[pos="e,48389,16810 48389,16882 48389,16882 48389,16820 48389,16820"];
	Layer0_Device9_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48538,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device9_ConcatHeads -> Layer0_Device9_OutputProj	[pos="e,48477,16684 48477,16756 48477,16756 48477,16694 48477,16694"];
	Layer0_Device9_OutputProj -> Layer0_Device9_Residual1	[pos="e,48538,16558 48538,16630 48538,16630 48538,16568 48538,16568"];
	Layer0_Device9_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48409,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device9_Residual1 -> Layer0_Device9_LayerNorm2	[pos="e,48427,16432 48427,16504 48427,16504 48427,16442 48427,16442"];
	Layer0_Device9_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="48641,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device9_Residual1 -> Layer0_Device9_Residual2	[pos="e,48830,15750 48830,16504 48830,16504 48830,15760 48830,15760"];
	Layer0_Device9_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48116,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device9_LayerNorm2 -> Layer0_Device9_GateProj	[pos="e,48268,16306 48268,16378 48268,16378 48268,16316 48268,16316"];
	Layer0_Device9_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48429,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device9_LayerNorm2 -> Layer0_Device9_UpProj	[pos="e,48482,16217 48482,16378 48482,16378 48482,16227 48482,16227"];
	Layer0_Device9_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48116,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device9_GateProj -> Layer0_Device9_Activation	[pos="e,48048,16128 48048,16252 48048,16252 48048,16138 48048,16138"];
	Layer0_Device9_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48212,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device9_UpProj -> Layer0_Device9_ElemMul	[pos="e,48482,16002 48482,16163 48482,16163 48482,16012 48482,16012"];
	Layer0_Device9_Activation -> Layer0_Device9_ElemMul	[pos="e,48116,16002 48116,16074 48116,16074 48116,16012 48116,16012"];
	Layer0_Device9_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48355,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device9_ElemMul -> Layer0_Device9_DownProj	[pos="e,48355,15876 48355,15948 48355,15948 48355,15886 48355,15886"];
	Layer0_Device9_DownProj -> Layer0_Device9_Residual2	[pos="e,48414,15750 48414,15822 48414,15822 48414,15760 48414,15760"];
	Layer0_Device9_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48641,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device9_Residual2 -> Layer0_Device9_Output	[pos="e,48641,15624 48641,15696 48641,15696 48641,15634 48641,15634"];
	Layer1_Device9_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48641,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device9_Output -> Layer1_Device9_Input	[pos="e,48641,15476 48641,15548 48641,15548 48641,15486 48641,15486"];
	Layer0_Device10_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53949,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device10_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53411,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device10_Input -> Layer0_Device10_LayerNorm1	[pos="e,53624,20375 53656,20503 53656,20473 53656,20375 53656,20375 53656,20375 53634,20375 53634,20375"];
	Layer0_Device10_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="53891,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device10_Input -> Layer0_Device10_Residual1	[pos="e,54094,16558 54094,20479 54094,20479 54094,16568 54094,16568"];
	Layer0_Device10_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52894,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device10_LayerNorm1 -> Layer0_Device10_QKVProj	[pos="e,53371,20276 53371,20348 53371,20348 53371,20286 53371,20286"];
	Layer0_Device10_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51673,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage0_RecvKV	[label="Local K,V",
		lp="52174,20179",
		pos="e,52419,20136 52419,20222 52419,20222 52419,20146 52419,20146"];
	Layer0_Device10_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52463,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage0_Attention	[label=Q_local,
		lp="52735,20083",
		pos="e,52644,19917 52644,20222 52644,20222 52644,19927 52644,19927"];
	Layer0_Device10_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50133,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage1_Attention	[label=Q_local,
		lp="50197,19986",
		pos="e,50118,19723 52244,20229 51424,20229 50118,20229 50118,20229 50118,20229 50118,19733 50118,19733"];
	Layer0_Device10_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50744,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage2_Attention	[label=Q_local,
		lp="52875,19890",
		pos="e,50914,19530 52739,20222 52739,20127 52739,19810 52739,19810 52739,19810 50914,19810 50914,19810 50914,19810 50914,19540 50914,\
19540"];
	Layer0_Device10_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52759,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage3_Attention	[label=Q_local,
		lp="53015,19793",
		pos="e,52932,19337 52932,20223 52932,20223 52932,19347 52932,19347"];
	Layer0_Device10_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49965,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage4_Attention	[label=Q_local,
		lp="49817,19697",
		pos="e,49818,19144 52244,20242 51342,20242 49818,20242 49818,20242 49818,20242 49818,19154 49818,19154"];
	Layer0_Device10_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52883,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage5_Attention	[label=Q_local,
		lp="53119,19600",
		pos="e,53052,18951 53052,20222 53052,20222 53052,18961 53052,18961"];
	Layer0_Device10_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52967,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage6_Attention	[label=Q_local,
		lp="53251,19504",
		pos="e,53156,18758 53156,20222 53156,20222 53156,18768 53156,18768"];
	Layer0_Device10_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50028,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage7_Attention	[label=Q_local,
		lp="49671,19407",
		pos="e,49796,18547 52244,20255 51308,20255 49690,20255 49690,20255 49690,20255 49690,18547 49690,18547 49690,18547 49786,18547 49786,\
18547"];
	Layer0_Device10_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52774,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage8_Attention	[label=Q_local,
		lp="53375,19311",
		pos="e,53006,18345 53226,20223 53226,19993 53226,18345 53226,18345 53226,18345 53016,18345 53016,18345"];
	Layer0_Device10_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49835,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage9_Attention	[label=Q_local,
		lp="49553,19214",
		pos="e,49647,18179 52244,20262 51297,20262 49647,20262 49647,20262 49647,20262 49647,18189 49647,18189"];
	Layer0_Device10_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49820,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage10_Attention	[label=Q_local,
		lp="49413,19118",
		pos="e,49596,17986 52244,20269 51284,20269 49596,20269 49596,20269 49596,20269 49596,17996 49596,17996"];
	Layer0_Device10_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51206,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage11_Attention	[label=Q_local,
		lp="53504,19021",
		pos="e,51283,17793 53280,20222 53280,19960 53280,17862 53280,17862 53280,17862 51283,17862 51283,17862 51283,17862 51283,17803 51283,\
17803"];
	Layer0_Device10_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51536,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage12_Attention	[label=Q_local,
		lp="53639,18925",
		pos="e,51466,17600 53307,20222 53307,19960 53307,17859 53307,17859 53307,17859 51466,17859 51466,17859 51466,17859 51466,17610 51466,\
17610"];
	Layer0_Device10_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="53551,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage13_Attention	[label=Q_local,
		lp="53779,18828",
		pos="e,53724,17408 53544,20235 53649,20235 53724,20235 53724,20235 53724,20235 53724,17418 53724,17418"];
	Layer0_Device10_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51133,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage14_Attention	[label=Q_local,
		lp="49267,18732",
		pos="e,50901,17187 53253,20222 53253,19961 53253,17891 53253,17891 53253,17891 50713,17891 50713,17891 50713,17891 50713,17187 50713,\
17187 50713,17187 50891,17187 50891,17187"];
	Layer0_Device10_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52902,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_QKVProj -> Layer0_Device10_Stage15_Attention	[label=Q_local,
		lp="53897,18635",
		pos="e,53134,17043 53544,20249 53756,20249 53938,20249 53938,20249 53938,20249 53938,17043 53938,17043 53938,17043 53144,17043 53144,\
17043"];
	Layer0_Device10_Stage0_RecvKV -> Layer0_Device10_Stage0_Attention	[pos="e,52412,19917 52412,20082 52412,20082 52412,19927 52412,19927"];
	Layer0_Device10_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51254,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage0_RecvKV -> Layer0_Device10_Stage1_RecvKV	[label="Ring transfer",
		lp="51583,19986",
		pos="e,51464,19943 51464,20029 51464,20029 51464,19953 51464,19953"];
	Layer0_Device10_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="50654,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage0_Attention -> Layer0_Device10_Stage0_Accumulate	[pos="e,50654,19723 52463,19863 52463,19840 52463,19812 52463,19812 52463,19812 50654,19812 50654,19812 50654,19812 50654,19733 50654,\
19733"];
	Layer0_Device10_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50223,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage0_Accumulate -> Layer0_Device10_Stage1_Accumulate	[pos="e,50438,19530 50438,19670 50438,19670 50438,19540 50438,19540"];
	Layer0_Device10_Stage1_RecvKV -> Layer0_Device10_Stage1_Attention	[pos="e,50349,19724 50349,19889 50349,19889 50349,19734 50349,19734"];
	Layer0_Device10_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51863,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage1_RecvKV -> Layer0_Device10_Stage2_RecvKV	[label="Ring transfer",
		lp="51571,19793",
		pos="e,51558,19750 51558,19836 51558,19836 51558,19760 51558,19760"];
	Layer0_Device10_Stage1_Attention -> Layer0_Device10_Stage1_Accumulate	[pos="e,50178,19530 50178,19670 50178,19670 50178,19540 50178,19540"];
	Layer0_Device10_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50341,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage1_Accumulate -> Layer0_Device10_Stage2_Accumulate	[pos="e,50282,19337 50282,19477 50282,19477 50282,19347 50282,19347"];
	Layer0_Device10_Stage2_RecvKV -> Layer0_Device10_Stage2_Attention	[pos="e,50959,19531 50959,19696 50959,19696 50959,19541 50959,19541"];
	Layer0_Device10_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51953,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage2_RecvKV -> Layer0_Device10_Stage3_RecvKV	[label="Ring transfer",
		lp="51956,19600",
		pos="e,51908,19557 51908,19643 51908,19643 51908,19567 51908,19567"];
	Layer0_Device10_Stage2_Attention -> Layer0_Device10_Stage2_Accumulate	[pos="e,50542,19337 50542,19477 50542,19477 50542,19347 50542,19347"];
	Layer0_Device10_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50486,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage2_Accumulate -> Layer0_Device10_Stage3_Accumulate	[pos="e,50414,19144 50414,19284 50414,19284 50414,19154 50414,19154"];
	Layer0_Device10_Stage3_RecvKV -> Layer0_Device10_Stage3_Attention	[pos="e,52700,19338 52700,19503 52700,19503 52700,19348 52700,19348"];
	Layer0_Device10_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51550,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage3_RecvKV -> Layer0_Device10_Stage4_RecvKV	[label="Ring transfer",
		lp="51734,19407",
		pos="e,51752,19364 51752,19450 51752,19450 51752,19374 51752,19374"];
	Layer0_Device10_Stage3_Attention -> Layer0_Device10_Stage3_Accumulate	[pos="e,50674,19144 52571,19284 52571,19258 52571,19223 52571,19223 52571,19223 50674,19223 50674,19223 50674,19223 50674,19154 50674,\
19154"];
	Layer0_Device10_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50465,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage3_Accumulate -> Layer0_Device10_Stage4_Accumulate	[pos="e,50476,18951 50476,19091 50476,19091 50476,18961 50476,18961"];
	Layer0_Device10_Stage4_RecvKV -> Layer0_Device10_Stage4_Attention	[pos="e,50153,19144 50674,19270 50393,19270 50153,19270 50153,19270 50153,19270 50153,19154 50153,19154"];
	Layer0_Device10_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51695,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage4_RecvKV -> Layer0_Device10_Stage5_RecvKV	[label="Ring transfer",
		lp="51670,19214",
		pos="e,51622,19171 51622,19257 51622,19257 51622,19181 51622,19181"];
	Layer0_Device10_Stage4_Attention -> Layer0_Device10_Stage4_Accumulate	[pos="e,50244,18951 50197,19117 50225,19117 50244,19117 50244,19117 50244,19117 50244,18961 50244,18961"];
	Layer0_Device10_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52446,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage4_Accumulate -> Layer0_Device10_Stage5_Accumulate	[pos="e,52404,18758 50507,18898 50507,18881 50507,18861 50507,18861 50507,18861 52404,18861 52404,18861 52404,18861 52404,18768 52404,\
18768"];
	Layer0_Device10_Stage5_RecvKV -> Layer0_Device10_Stage5_Attention	[pos="e,52651,18924 52604,19117 52604,19112 52604,18924 52604,18924 52604,18924 52641,18924 52641,18924"];
	Layer0_Device10_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51674,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage5_RecvKV -> Layer0_Device10_Stage6_RecvKV	[label="Ring transfer",
		lp="51732,19021",
		pos="e,51684,18978 51684,19064 51684,19064 51684,18988 51684,18988"];
	Layer0_Device10_Stage5_Attention -> Layer0_Device10_Stage5_Accumulate	[pos="e,52664,18758 52664,18898 52664,18898 52664,18768 52664,18768"];
	Layer0_Device10_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52446,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage5_Accumulate -> Layer0_Device10_Stage6_Accumulate	[pos="e,52446,18565 52446,18705 52446,18705 52446,18575 52446,18575"];
	Layer0_Device10_Stage6_RecvKV -> Layer0_Device10_Stage6_Attention	[pos="e,52925,18758 52261,18884 52586,18884 52925,18884 52925,18884 52925,18884 52925,18768 52925,18768"];
	Layer0_Device10_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51237,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage6_RecvKV -> Layer0_Device10_Stage7_RecvKV	[label="Ring transfer",
		lp="51535,18828",
		pos="e,51456,18785 51456,18871 51456,18871 51456,18795 51456,18795"];
	Layer0_Device10_Stage6_Attention -> Layer0_Device10_Stage6_Accumulate	[pos="e,52678,18529 52870,18705 52870,18650 52870,18529 52870,18529 52870,18529 52688,18529 52688,18529"];
	Layer0_Device10_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52253,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage6_Accumulate -> Layer0_Device10_Stage7_Accumulate	[pos="e,52350,18372 52350,18512 52350,18512 52350,18382 52350,18382"];
	Layer0_Device10_Stage7_RecvKV -> Layer0_Device10_Stage7_Attention	[pos="e,50246,18565 50361,18691 50291,18691 50246,18691 50246,18691 50246,18691 50246,18575 50246,18575"];
	Layer0_Device10_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51237,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage7_RecvKV -> Layer0_Device10_Stage8_RecvKV	[label="Ring transfer",
		lp="51285,18635",
		pos="e,51237,18592 51237,18678 51237,18678 51237,18602 51237,18602"];
	Layer0_Device10_Stage7_Attention -> Layer0_Device10_Stage7_Accumulate	[pos="e,52089,18372 50192,18512 50192,18485 50192,18449 50192,18449 50192,18449 52089,18449 52089,18449 52089,18449 52089,18382 52089,\
18382"];
	Layer0_Device10_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52253,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage7_Accumulate -> Layer0_Device10_Stage8_Accumulate	[pos="e,52253,18179 52253,18319 52253,18319 52253,18189 52253,18189"];
	Layer0_Device10_Stage8_RecvKV -> Layer0_Device10_Stage8_Attention	[pos="e,52610,18372 51825,18498 52198,18498 52610,18498 52610,18498 52610,18498 52610,18382 52610,18382"];
	Layer0_Device10_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51044,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage8_RecvKV -> Layer0_Device10_Stage9_RecvKV	[label="Ring transfer",
		lp="51222,18442",
		pos="e,51140,18399 51140,18485 51140,18485 51140,18409 51140,18409"];
	Layer0_Device10_Stage8_Attention -> Layer0_Device10_Stage8_Accumulate	[pos="e,52485,18152 52774,18319 52774,18266 52774,18152 52774,18152 52774,18152 52495,18152 52495,18152"];
	Layer0_Device10_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50341,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage8_Accumulate -> Layer0_Device10_Stage9_Accumulate	[pos="e,50348,17986 52246,18126 52246,18106 52246,18082 52246,18082 52246,18082 50348,18082 50348,18082 50348,18082 50348,17996 50348,\
17996"];
	Layer0_Device10_Stage9_RecvKV -> Layer0_Device10_Stage9_Attention	[pos="e,49932,18179 50168,18305 50030,18305 49932,18305 49932,18305 49932,18305 49932,18189 49932,18189"];
	Layer0_Device10_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51044,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage9_RecvKV -> Layer0_Device10_Stage10_RecvKV	[label="Ring transfer",
		lp="51092,18249",
		pos="e,51044,18206 51044,18292 51044,18292 51044,18216 51044,18216"];
	Layer0_Device10_Stage9_Attention -> Layer0_Device10_Stage9_Accumulate	[pos="e,50117,17986 50067,18152 50096,18152 50117,18152 50117,18152 50117,18152 50117,17996 50117,17996"];
	Layer0_Device10_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50341,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage9_Accumulate -> Layer0_Device10_Stage10_Accumulate	[pos="e,50341,17793 50341,17933 50341,17933 50341,17803 50341,17803"];
	Layer0_Device10_Stage10_RecvKV -> Layer0_Device10_Stage10_Attention	[pos="e,49828,17986 50168,18112 49975,18112 49828,18112 49828,18112 49828,18112 49828,17996 49828,17996"];
	Layer0_Device10_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51550,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage10_RecvKV -> Layer0_Device10_Stage11_RecvKV	[label="Ring transfer",
		lp="51254,18056",
		pos="e,51297,18013 51297,18099 51297,18099 51297,18023 51297,18023"];
	Layer0_Device10_Stage10_Attention -> Layer0_Device10_Stage10_Accumulate	[pos="e,50109,17766 49922,17933 49922,17880 49922,17766 49922,17766 49922,17766 50099,17766 50099,17766"];
	Layer0_Device10_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51015,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage10_Accumulate -> Layer0_Device10_Stage11_Accumulate	[pos="e,50879,17600 50573,17766 50718,17766 50879,17766 50879,17766 50879,17766 50879,17610 50879,17610"];
	Layer0_Device10_Stage11_RecvKV -> Layer0_Device10_Stage11_Attention	[pos="e,51129,17793 51129,17906 51129,17906 51129,17803 51129,17803"];
	Layer0_Device10_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52415,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage11_RecvKV -> Layer0_Device10_Stage12_RecvKV	[label="Ring transfer",
		lp="52249,17863",
		pos="e,51982,17820 51982,17906 51982,17906 51982,17830 51982,17830"];
	Layer0_Device10_Stage11_Attention -> Layer0_Device10_Stage11_Accumulate	[pos="e,51110,17600 51110,17740 51110,17740 51110,17610 51110,17610"];
	Layer0_Device10_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51133,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage11_Accumulate -> Layer0_Device10_Stage12_Accumulate	[pos="e,51074,17407 51074,17547 51074,17547 51074,17417 51074,17417"];
	Layer0_Device10_Stage12_RecvKV -> Layer0_Device10_Stage12_Attention	[pos="e,51631,17600 51631,17713 51631,17713 51631,17610 51631,17610"];
	Layer0_Device10_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52745,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage12_RecvKV -> Layer0_Device10_Stage13_RecvKV	[label="Ring transfer",
		lp="52770,17670",
		pos="e,52580,17627 52580,17713 52580,17713 52580,17637 52580,17637"];
	Layer0_Device10_Stage12_Attention -> Layer0_Device10_Stage12_Accumulate	[pos="e,51334,17407 51334,17547 51334,17547 51334,17417 51334,17417"];
	Layer0_Device10_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51654,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage12_Accumulate -> Layer0_Device10_Stage13_Accumulate	[pos="e,51422,17187 51365,17380 51382,17380 51393,17380 51393,17380 51393,17380 51393,17187 51393,17187 51393,17187 51412,17187 51412,\
17187"];
	Layer0_Device10_Stage13_RecvKV -> Layer0_Device10_Stage13_Attention	[pos="e,53492,17408 53492,17573 53492,17573 53492,17418 53492,17418"];
	Layer0_Device10_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52342,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage13_RecvKV -> Layer0_Device10_Stage14_RecvKV	[label="Ring transfer",
		lp="52591,17477",
		pos="e,52544,17434 52544,17520 52544,17520 52544,17444 52544,17444"];
	Layer0_Device10_Stage13_Attention -> Layer0_Device10_Stage13_Accumulate	[pos="e,51654,17214 53551,17354 53551,17329 53551,17296 53551,17296 53551,17296 51654,17296 51654,17296 51654,17296 51654,17224 51654,\
17224"];
	Layer0_Device10_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51654,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage13_Accumulate -> Layer0_Device10_Stage14_Accumulate	[pos="e,51654,17062 51654,17161 51654,17161 51654,17072 51654,17072"];
	Layer0_Device10_Stage14_RecvKV -> Layer0_Device10_Stage14_Attention	[pos="e,51133,17214 51466,17340 51277,17340 51133,17340 51133,17340 51133,17340 51133,17224 51133,17224"];
	Layer0_Device10_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52863,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device10_Stage14_RecvKV -> Layer0_Device10_Stage15_RecvKV	[label="Ring transfer",
		lp="52650,17284",
		pos="e,52602,17241 52602,17327 52602,17327 52602,17251 52602,17251"];
	Layer0_Device10_Stage14_Attention -> Layer0_Device10_Stage14_Accumulate	[pos="e,51422,17026 51133,17161 51133,17115 51133,17026 51133,17026 51133,17026 51412,17026 51412,17026"];
	Layer0_Device10_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52902,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device10_Stage14_Accumulate -> Layer0_Device10_Stage15_Accumulate	[pos="e,52670,16909 51654,17008 51654,16971 51654,16909 51654,16909 51654,16909 52660,16909 52660,16909"];
	Layer0_Device10_Stage15_RecvKV -> Layer0_Device10_Stage15_Attention	[pos="e,52902,17062 52902,17134 52902,17134 52902,17072 52902,17072"];
	Layer0_Device10_Stage15_Attention -> Layer0_Device10_Stage15_Accumulate	[pos="e,52902,16936 52902,17008 52902,17008 52902,16946 52902,16946"];
	Layer0_Device10_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53211,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device10_Stage15_Accumulate -> Layer0_Device10_ConcatHeads	[pos="e,53060,16810 53060,16882 53060,16882 53060,16820 53060,16820"];
	Layer0_Device10_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53276,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device10_ConcatHeads -> Layer0_Device10_OutputProj	[pos="e,53250,16684 53250,16756 53250,16756 53250,16694 53250,16694"];
	Layer0_Device10_OutputProj -> Layer0_Device10_Residual1	[pos="e,53504,16531 53478,16630 53478,16593 53478,16531 53478,16531 53478,16531 53494,16531 53494,16531"];
	Layer0_Device10_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53680,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device10_Residual1 -> Layer0_Device10_LayerNorm2	[pos="e,53699,16432 53699,16504 53699,16504 53699,16442 53699,16442"];
	Layer0_Device10_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="53781,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device10_Residual1 -> Layer0_Device10_Residual2	[pos="e,54046,15750 54046,16504 54046,16504 54046,15760 54046,15760"];
	Layer0_Device10_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53387,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device10_LayerNorm2 -> Layer0_Device10_GateProj	[pos="e,53540,16306 53540,16378 53540,16378 53540,16316 53540,16316"];
	Layer0_Device10_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53700,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device10_LayerNorm2 -> Layer0_Device10_UpProj	[pos="e,53752,16217 53752,16378 53752,16378 53752,16227 53752,16227"];
	Layer0_Device10_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53387,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device10_GateProj -> Layer0_Device10_Activation	[pos="e,53318,16128 53318,16252 53318,16252 53318,16138 53318,16138"];
	Layer0_Device10_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53483,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device10_UpProj -> Layer0_Device10_ElemMul	[pos="e,53753,16002 53753,16163 53753,16163 53753,16012 53753,16012"];
	Layer0_Device10_Activation -> Layer0_Device10_ElemMul	[pos="e,53387,16002 53387,16074 53387,16074 53387,16012 53387,16012"];
	Layer0_Device10_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53626,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device10_ElemMul -> Layer0_Device10_DownProj	[pos="e,53626,15876 53626,15948 53626,15948 53626,15886 53626,15886"];
	Layer0_Device10_DownProj -> Layer0_Device10_Residual2	[pos="e,53626,15750 53626,15822 53626,15822 53626,15760 53626,15760"];
	Layer0_Device10_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53781,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device10_Residual2 -> Layer0_Device10_Output	[pos="e,53781,15624 53781,15696 53781,15696 53781,15634 53781,15634"];
	Layer1_Device10_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53781,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device10_Output -> Layer1_Device10_Input	[pos="e,53781,15476 53781,15548 53781,15548 53781,15486 53781,15486"];
	Layer0_Device11_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58308,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device11_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58138,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device11_Input -> Layer0_Device11_LayerNorm1	[pos="e,58179,20402 58179,20478 58179,20478 58179,20412 58179,20412"];
	Layer0_Device11_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="58872,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device11_Input -> Layer0_Device11_Residual1	[pos="e,58976,16558 58609,20512 58787,20512 58976,20512 58976,20512 58976,20512 58976,16568 58976,16568"];
	Layer0_Device11_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57779,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device11_LayerNorm1 -> Layer0_Device11_QKVProj	[pos="e,58138,20276 58138,20348 58138,20348 58138,20286 58138,20286"];
	Layer0_Device11_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56568,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage0_RecvKV	[label="Local K,V",
		lp="57068,20179",
		pos="e,57309,20136 57309,20222 57309,20222 57309,20146 57309,20146"];
	Layer0_Device11_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57358,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage0_Attention	[label=Q_local,
		lp="57630,20083",
		pos="e,57539,19917 57539,20222 57539,20222 57539,19927 57539,19927"];
	Layer0_Device11_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55028,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage1_Attention	[label=Q_local,
		lp="55092,19986",
		pos="e,55013,19723 57129,20229 56312,20229 55013,20229 55013,20229 55013,20229 55013,19733 55013,19733"];
	Layer0_Device11_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55639,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage2_Attention	[label=Q_local,
		lp="57770,19890",
		pos="e,55809,19530 57634,20223 57634,20128 57634,19815 57634,19815 57634,19815 55809,19815 55809,19815 55809,19815 55809,19540 55809,\
19540"];
	Layer0_Device11_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57654,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage3_Attention	[label=Q_local,
		lp="57910,19793",
		pos="e,57827,19337 57827,20223 57827,20223 57827,19347 57827,19347"];
	Layer0_Device11_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54860,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage4_Attention	[label=Q_local,
		lp="54712,19697",
		pos="e,54712,19144 57129,20242 56229,20242 54712,20242 54712,20242 54712,20242 54712,19154 54712,19154"];
	Layer0_Device11_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57780,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage5_Attention	[label=Q_local,
		lp="58014,19600",
		pos="e,57948,18951 57948,20222 57948,20222 57948,18961 57948,18961"];
	Layer0_Device11_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57805,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage6_Attention	[label=Q_local,
		lp="58146,19504",
		pos="e,58024,18758 58024,20222 58024,20222 58024,18768 58024,18768"];
	Layer0_Device11_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54866,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage7_Attention	[label=Q_local,
		lp="54566,19407",
		pos="e,54634,18547 57129,20255 56190,20255 54566,20255 54566,20255 54566,20255 54566,18547 54566,18547 54566,18547 54624,18547 54624,\
18547"];
	Layer0_Device11_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57611,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage8_Attention	[label=Q_local,
		lp="58272,19311",
		pos="e,57843,18345 58091,20223 58091,19993 58091,18345 58091,18345 58091,18345 57853,18345 57853,18345"];
	Layer0_Device11_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54672,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage9_Attention	[label=Q_local,
		lp="54448,19214",
		pos="e,54503,18179 57129,20262 56174,20262 54503,20262 54503,20262 54503,20262 54503,18189 54503,18189"];
	Layer0_Device11_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54793,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage10_Attention	[label=Q_local,
		lp="54308,19118",
		pos="e,54561,17959 57129,20269 56150,20269 54411,20269 54411,20269 54411,20269 54411,17959 54411,17959 54411,17959 54551,17959 54551,\
17959"];
	Layer0_Device11_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56179,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage11_Attention	[label=Q_local,
		lp="58395,19021",
		pos="e,56256,17793 58199,20222 58199,19958 58199,17828 58199,17828 58199,17828 56256,17828 56256,17828 56256,17828 56256,17803 56256,\
17803"];
	Layer0_Device11_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56433,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage12_Attention	[label=Q_local,
		lp="58536,18925",
		pos="e,56439,17601 58253,20222 58253,19958 58253,17825 58253,17825 58253,17825 56439,17825 56439,17825 56439,17825 56439,17611 56439,\
17611"];
	Layer0_Device11_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="58448,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage13_Attention	[label=Q_local,
		lp="58676,18828",
		pos="e,58585,17408 58429,20240 58521,20240 58585,20240 58585,20240 58585,20240 58585,17418 58585,17418"];
	Layer0_Device11_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56030,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage14_Attention	[label=Q_local,
		lp="54162,18732",
		pos="e,55798,17187 58145,20223 58145,19963 58145,17889 58145,17889 58145,17889 55668,17889 55668,17889 55668,17889 55668,17187 55668,\
17187 55668,17187 55788,17187 55788,17187"];
	Layer0_Device11_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="58163,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_QKVProj -> Layer0_Device11_Stage15_Attention	[label=Q_local,
		lp="58794,18635",
		pos="e,58395,17043 58429,20257 58574,20257 58687,20257 58687,20257 58687,20257 58687,17043 58687,17043 58687,17043 58405,17043 58405,\
17043"];
	Layer0_Device11_Stage0_RecvKV -> Layer0_Device11_Stage0_Attention	[pos="e,57307,19917 57307,20082 57307,20082 57307,19927 57307,19927"];
	Layer0_Device11_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56149,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage0_RecvKV -> Layer0_Device11_Stage1_RecvKV	[label="Ring transfer",
		lp="56469,19986",
		pos="e,56358,19943 56358,20029 56358,20029 56358,19953 56358,19953"];
	Layer0_Device11_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="55549,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage0_Attention -> Layer0_Device11_Stage0_Accumulate	[pos="e,55549,19723 57358,19863 57358,19843 57358,19818 57358,19818 57358,19818 55549,19818 55549,19818 55549,19818 55549,19733 55549,\
19733"];
	Layer0_Device11_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55118,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage0_Accumulate -> Layer0_Device11_Stage1_Accumulate	[pos="e,55334,19530 55334,19670 55334,19670 55334,19540 55334,19540"];
	Layer0_Device11_Stage1_RecvKV -> Layer0_Device11_Stage1_Attention	[pos="e,55244,19724 55244,19889 55244,19889 55244,19734 55244,19734"];
	Layer0_Device11_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56758,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage1_RecvKV -> Layer0_Device11_Stage2_RecvKV	[label="Ring transfer",
		lp="56466,19793",
		pos="e,56454,19750 56454,19836 56454,19836 56454,19760 56454,19760"];
	Layer0_Device11_Stage1_Attention -> Layer0_Device11_Stage1_Accumulate	[pos="e,55073,19530 55073,19670 55073,19670 55073,19540 55073,19540"];
	Layer0_Device11_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55236,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage1_Accumulate -> Layer0_Device11_Stage2_Accumulate	[pos="e,55177,19337 55177,19477 55177,19477 55177,19347 55177,19347"];
	Layer0_Device11_Stage2_RecvKV -> Layer0_Device11_Stage2_Attention	[pos="e,55854,19531 55854,19696 55854,19696 55854,19541 55854,19541"];
	Layer0_Device11_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56848,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage2_RecvKV -> Layer0_Device11_Stage3_RecvKV	[label="Ring transfer",
		lp="56851,19600",
		pos="e,56803,19557 56803,19643 56803,19643 56803,19567 56803,19567"];
	Layer0_Device11_Stage2_Attention -> Layer0_Device11_Stage2_Accumulate	[pos="e,55438,19337 55438,19477 55438,19477 55438,19347 55438,19347"];
	Layer0_Device11_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55381,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage2_Accumulate -> Layer0_Device11_Stage3_Accumulate	[pos="e,55308,19144 55308,19284 55308,19284 55308,19154 55308,19154"];
	Layer0_Device11_Stage3_RecvKV -> Layer0_Device11_Stage3_Attention	[pos="e,57595,19338 57595,19503 57595,19503 57595,19348 57595,19348"];
	Layer0_Device11_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56445,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage3_RecvKV -> Layer0_Device11_Stage4_RecvKV	[label="Ring transfer",
		lp="56695,19407",
		pos="e,56646,19364 56646,19450 56646,19450 56646,19374 56646,19374"];
	Layer0_Device11_Stage3_Attention -> Layer0_Device11_Stage3_Accumulate	[pos="e,55569,19144 57466,19284 57466,19260 57466,19228 57466,19228 57466,19228 55569,19228 55569,19228 55569,19228 55569,19154 55569,\
19154"];
	Layer0_Device11_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55362,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage3_Accumulate -> Layer0_Device11_Stage4_Accumulate	[pos="e,55372,18951 55372,19091 55372,19091 55372,18961 55372,18961"];
	Layer0_Device11_Stage4_RecvKV -> Layer0_Device11_Stage4_Attention	[pos="e,55048,19144 55569,19270 55288,19270 55048,19270 55048,19270 55048,19270 55048,19154 55048,19154"];
	Layer0_Device11_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56590,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage4_RecvKV -> Layer0_Device11_Stage5_RecvKV	[label="Ring transfer",
		lp="56566,19214",
		pos="e,56518,19171 56518,19257 56518,19257 56518,19181 56518,19181"];
	Layer0_Device11_Stage4_Attention -> Layer0_Device11_Stage4_Accumulate	[pos="e,55140,18951 55092,19117 55120,19117 55140,19117 55140,19117 55140,19117 55140,18961 55140,18961"];
	Layer0_Device11_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57284,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage4_Accumulate -> Layer0_Device11_Stage5_Accumulate	[pos="e,57272,18758 55374,18898 55374,18869 55374,18828 55374,18828 55374,18828 57272,18828 57272,18828 57272,18828 57272,18768 57272,\
18768"];
	Layer0_Device11_Stage5_RecvKV -> Layer0_Device11_Stage5_Attention	[pos="e,57548,18924 57500,19117 57500,19112 57500,18924 57500,18924 57500,18924 57538,18924 57538,18924"];
	Layer0_Device11_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56571,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage5_RecvKV -> Layer0_Device11_Stage6_RecvKV	[label="Ring transfer",
		lp="56629,19021",
		pos="e,56580,18978 56580,19064 56580,19064 56580,18988 56580,18988"];
	Layer0_Device11_Stage5_Attention -> Layer0_Device11_Stage5_Accumulate	[pos="e,57516,18731 57561,18898 57561,18845 57561,18731 57561,18731 57561,18731 57526,18731 57526,18731"];
	Layer0_Device11_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57284,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage5_Accumulate -> Layer0_Device11_Stage6_Accumulate	[pos="e,57284,18565 57284,18705 57284,18705 57284,18575 57284,18575"];
	Layer0_Device11_Stage6_RecvKV -> Layer0_Device11_Stage6_Attention	[pos="e,57792,18758 57158,18884 57471,18884 57792,18884 57792,18884 57792,18884 57792,18768 57792,18768"];
	Layer0_Device11_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56075,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage6_RecvKV -> Layer0_Device11_Stage7_RecvKV	[label="Ring transfer",
		lp="56371,18828",
		pos="e,56323,18785 56323,18871 56323,18871 56323,18795 56323,18795"];
	Layer0_Device11_Stage6_Attention -> Layer0_Device11_Stage6_Accumulate	[pos="e,57516,18538 57708,18705 57708,18652 57708,18538 57708,18538 57708,18538 57526,18538 57526,18538"];
	Layer0_Device11_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57090,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage6_Accumulate -> Layer0_Device11_Stage7_Accumulate	[pos="e,57187,18372 57187,18512 57187,18512 57187,18382 57187,18382"];
	Layer0_Device11_Stage7_RecvKV -> Layer0_Device11_Stage7_Attention	[pos="e,54863,18565 55199,18691 55009,18691 54863,18691 54863,18691 54863,18691 54863,18575 54863,18575"];
	Layer0_Device11_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56075,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage7_RecvKV -> Layer0_Device11_Stage8_RecvKV	[label="Ring transfer",
		lp="56123,18635",
		pos="e,56075,18592 56075,18678 56075,18678 56075,18602 56075,18602"];
	Layer0_Device11_Stage7_Attention -> Layer0_Device11_Stage7_Accumulate	[pos="e,56927,18372 55029,18512 55029,18484 55029,18444 55029,18444 55029,18444 56927,18444 56927,18444 56927,18444 56927,18382 56927,\
18382"];
	Layer0_Device11_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57090,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage7_Accumulate -> Layer0_Device11_Stage8_Accumulate	[pos="e,57090,18179 57090,18319 57090,18319 57090,18189 57090,18189"];
	Layer0_Device11_Stage8_RecvKV -> Layer0_Device11_Stage8_Attention	[pos="e,57448,18372 56663,18498 57035,18498 57448,18498 57448,18498 57448,18498 57448,18382 57448,18382"];
	Layer0_Device11_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="55881,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage8_RecvKV -> Layer0_Device11_Stage9_RecvKV	[label="Ring transfer",
		lp="56026,18442",
		pos="e,55978,18399 55978,18485 55978,18485 55978,18409 55978,18409"];
	Layer0_Device11_Stage8_Attention -> Layer0_Device11_Stage8_Accumulate	[pos="e,57322,18152 57411,18319 57411,18266 57411,18152 57411,18152 57411,18152 57332,18152 57332,18152"];
	Layer0_Device11_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55314,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage8_Accumulate -> Layer0_Device11_Stage9_Accumulate	[pos="e,55391,17986 57090,18126 57090,18104 57090,18077 57090,18077 57090,18077 55391,18077 55391,18077 55391,18077 55391,17996 55391,\
17996"];
	Layer0_Device11_Stage9_RecvKV -> Layer0_Device11_Stage9_Attention	[pos="e,54769,18179 55005,18305 54867,18305 54769,18305 54769,18305 54769,18305 54769,18189 54769,18189"];
	Layer0_Device11_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="55881,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage9_RecvKV -> Layer0_Device11_Stage10_RecvKV	[label="Ring transfer",
		lp="55929,18249",
		pos="e,55881,18206 55881,18292 55881,18292 55881,18216 55881,18216"];
	Layer0_Device11_Stage9_Attention -> Layer0_Device11_Stage9_Accumulate	[pos="e,55237,17986 54732,18126 54732,18086 54732,18015 54732,18015 54732,18015 55237,18015 55237,18015 55237,18015 55237,17996 55237,\
17996"];
	Layer0_Device11_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55314,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage9_Accumulate -> Layer0_Device11_Stage10_Accumulate	[pos="e,55314,17793 55314,17933 55314,17933 55314,17803 55314,17803"];
	Layer0_Device11_Stage10_RecvKV -> Layer0_Device11_Stage10_Attention	[pos="e,54993,17987 54993,18152 54993,18152 54993,17997 54993,17997"];
	Layer0_Device11_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56523,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage10_RecvKV -> Layer0_Device11_Stage11_RecvKV	[label="Ring transfer",
		lp="56198,18056",
		pos="e,56202,18013 56202,18099 56202,18099 56202,18023 56202,18023"];
	Layer0_Device11_Stage10_Attention -> Layer0_Device11_Stage10_Accumulate	[pos="e,55082,17766 54914,17933 54914,17880 54914,17766 54914,17766 54914,17766 55072,17766 55072,17766"];
	Layer0_Device11_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55912,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage10_Accumulate -> Layer0_Device11_Stage11_Accumulate	[pos="e,55814,17600 55546,17766 55676,17766 55814,17766 55814,17766 55814,17766 55814,17610 55814,17610"];
	Layer0_Device11_Stage11_RecvKV -> Layer0_Device11_Stage11_Attention	[pos="e,56102,17793 56102,17906 56102,17906 56102,17803 56102,17803"];
	Layer0_Device11_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57388,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage11_RecvKV -> Layer0_Device11_Stage12_RecvKV	[label="Ring transfer",
		lp="57050,17863",
		pos="e,56956,17820 56956,17906 56956,17906 56956,17830 56956,17830"];
	Layer0_Device11_Stage11_Attention -> Layer0_Device11_Stage11_Accumulate	[pos="e,56046,17600 56046,17740 56046,17740 56046,17610 56046,17610"];
	Layer0_Device11_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56030,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage11_Accumulate -> Layer0_Device11_Stage12_Accumulate	[pos="e,55971,17407 55971,17547 55971,17547 55971,17417 55971,17417"];
	Layer0_Device11_Stage12_RecvKV -> Layer0_Device11_Stage12_Attention	[pos="e,56566,17600 56566,17713 56566,17713 56566,17610 56566,17610"];
	Layer0_Device11_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57642,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage12_RecvKV -> Layer0_Device11_Stage13_RecvKV	[label="Ring transfer",
		lp="57563,17670",
		pos="e,57515,17627 57515,17713 57515,17713 57515,17637 57515,17637"];
	Layer0_Device11_Stage12_Attention -> Layer0_Device11_Stage12_Accumulate	[pos="e,56232,17407 56232,17547 56232,17547 56232,17417 56232,17417"];
	Layer0_Device11_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56551,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage12_Accumulate -> Layer0_Device11_Stage13_Accumulate	[pos="e,56319,17187 56262,17380 56279,17380 56290,17380 56290,17380 56290,17380 56290,17187 56290,17187 56290,17187 56309,17187 56309,\
17187"];
	Layer0_Device11_Stage13_RecvKV -> Layer0_Device11_Stage13_Attention	[pos="e,58389,17408 58389,17573 58389,17573 58389,17418 58389,17418"];
	Layer0_Device11_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57239,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage13_RecvKV -> Layer0_Device11_Stage14_RecvKV	[label="Ring transfer",
		lp="57488,17477",
		pos="e,57440,17434 57440,17520 57440,17520 57440,17444 57440,17444"];
	Layer0_Device11_Stage13_Attention -> Layer0_Device11_Stage13_Accumulate	[pos="e,56551,17214 58448,17354 58448,17331 58448,17302 58448,17302 58448,17302 56551,17302 56551,17302 56551,17302 56551,17224 56551,\
17224"];
	Layer0_Device11_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56551,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage13_Accumulate -> Layer0_Device11_Stage14_Accumulate	[pos="e,56551,17062 56551,17161 56551,17161 56551,17072 56551,17072"];
	Layer0_Device11_Stage14_RecvKV -> Layer0_Device11_Stage14_Attention	[pos="e,56030,17214 56363,17340 56174,17340 56030,17340 56030,17340 56030,17340 56030,17224 56030,17224"];
	Layer0_Device11_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57760,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device11_Stage14_RecvKV -> Layer0_Device11_Stage15_RecvKV	[label="Ring transfer",
		lp="57547,17284",
		pos="e,57500,17241 57500,17327 57500,17327 57500,17251 57500,17251"];
	Layer0_Device11_Stage14_Attention -> Layer0_Device11_Stage14_Accumulate	[pos="e,56319,17026 56030,17161 56030,17115 56030,17026 56030,17026 56030,17026 56309,17026 56309,17026"];
	Layer0_Device11_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="58163,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device11_Stage14_Accumulate -> Layer0_Device11_Stage15_Accumulate	[pos="e,57931,16909 56551,17008 56551,16971 56551,16909 56551,16909 56551,16909 57921,16909 57921,16909"];
	Layer0_Device11_Stage15_RecvKV -> Layer0_Device11_Stage15_Attention	[pos="e,58163,17062 58163,17134 58163,17134 58163,17072 58163,17072"];
	Layer0_Device11_Stage15_Attention -> Layer0_Device11_Stage15_Accumulate	[pos="e,58163,16936 58163,17008 58163,17008 58163,16946 58163,16946"];
	Layer0_Device11_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58369,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device11_Stage15_Accumulate -> Layer0_Device11_ConcatHeads	[pos="e,58269,16810 58269,16882 58269,16882 58269,16820 58269,16820"];
	Layer0_Device11_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58481,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device11_ConcatHeads -> Layer0_Device11_OutputProj	[pos="e,58431,16684 58431,16756 58431,16756 58431,16694 58431,16694"];
	Layer0_Device11_OutputProj -> Layer0_Device11_Residual1	[pos="e,58590,16558 58590,16630 58590,16630 58590,16568 58590,16568"];
	Layer0_Device11_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58704,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device11_Residual1 -> Layer0_Device11_LayerNorm2	[pos="e,58704,16432 58704,16504 58704,16504 58704,16442 58704,16442"];
	Layer0_Device11_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="58696,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device11_Residual1 -> Layer0_Device11_Residual2	[pos="e,59016,15750 59016,16504 59016,16504 59016,15760 59016,15760"];
	Layer0_Device11_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58411,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device11_LayerNorm2 -> Layer0_Device11_GateProj	[pos="e,58564,16306 58564,16378 58564,16378 58564,16316 58564,16316"];
	Layer0_Device11_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58724,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device11_LayerNorm2 -> Layer0_Device11_UpProj	[pos="e,58776,16217 58776,16378 58776,16378 58776,16227 58776,16227"];
	Layer0_Device11_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58411,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device11_GateProj -> Layer0_Device11_Activation	[pos="e,58342,16128 58342,16252 58342,16252 58342,16138 58342,16138"];
	Layer0_Device11_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58507,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device11_UpProj -> Layer0_Device11_ElemMul	[pos="e,58777,16002 58777,16163 58777,16163 58777,16012 58777,16012"];
	Layer0_Device11_Activation -> Layer0_Device11_ElemMul	[pos="e,58411,16002 58411,16074 58411,16074 58411,16012 58411,16012"];
	Layer0_Device11_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58650,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device11_ElemMul -> Layer0_Device11_DownProj	[pos="e,58650,15876 58650,15948 58650,15948 58650,15886 58650,15886"];
	Layer0_Device11_DownProj -> Layer0_Device11_Residual2	[pos="e,58650,15750 58650,15822 58650,15822 58650,15760 58650,15760"];
	Layer0_Device11_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58696,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device11_Residual2 -> Layer0_Device11_Output	[pos="e,58696,15624 58696,15696 58696,15696 58696,15634 58696,15634"];
	Layer1_Device11_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58696,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device11_Output -> Layer1_Device11_Input	[pos="e,58696,15476 58696,15548 58696,15548 58696,15486 58696,15486"];
	Layer0_Device12_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63458,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device12_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63129,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device12_Input -> Layer0_Device12_LayerNorm1	[pos="e,63249,20402 63249,20485 63249,20485 63249,20412 63249,20412"];
	Layer0_Device12_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="63526,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device12_Input -> Layer0_Device12_Residual1	[pos="e,63723,16558 63723,20494 63723,20494 63723,16568 63723,16568"];
	Layer0_Device12_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62717,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device12_LayerNorm1 -> Layer0_Device12_QKVProj	[pos="e,63129,20276 63129,20348 63129,20348 63129,20286 63129,20286"];
	Layer0_Device12_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61471,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage0_RecvKV	[label="Local K,V",
		lp="61972,20179",
		pos="e,62229,20136 62229,20222 62229,20222 62229,20146 62229,20146"];
	Layer0_Device12_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62261,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage0_Attention	[label=Q_local,
		lp="62533,20083",
		pos="e,62442,19917 62442,20222 62442,20222 62442,19927 62442,19927"];
	Layer0_Device12_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59931,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage1_Attention	[label=Q_local,
		lp="59995,19986",
		pos="e,59916,19723 62067,20231 61240,20231 59916,20231 59916,20231 59916,20231 59916,19733 59916,19733"];
	Layer0_Device12_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60542,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage2_Attention	[label=Q_local,
		lp="62673,19890",
		pos="e,60712,19530 62537,20223 62537,20129 62537,19820 62537,19820 62537,19820 60712,19820 60712,19820 60712,19820 60712,19540 60712,\
19540"];
	Layer0_Device12_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62557,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage3_Attention	[label=Q_local,
		lp="62813,19793",
		pos="e,62730,19337 62730,20223 62730,20223 62730,19347 62730,19347"];
	Layer0_Device12_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59763,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage4_Attention	[label=Q_local,
		lp="59615,19697",
		pos="e,59616,19144 62067,20249 61158,20249 59616,20249 59616,20249 59616,20249 59616,19154 59616,19154"];
	Layer0_Device12_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62681,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage5_Attention	[label=Q_local,
		lp="62917,19600",
		pos="e,62871,18951 62871,20222 62871,20222 62871,18961 62871,18961"];
	Layer0_Device12_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62713,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage6_Attention	[label=Q_local,
		lp="63049,19504",
		pos="e,62928,18758 62928,20222 62928,20222 62928,18768 62928,18768"];
	Layer0_Device12_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59774,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage7_Attention	[label=Q_local,
		lp="59469,19407",
		pos="e,59998,18565 62830,20222 62830,20064 62830,19252 62830,19252 62830,19252 59998,19252 59998,19252 59998,19252 59998,18575 59998,\
18575"];
	Layer0_Device12_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62448,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage8_Attention	[label=Q_local,
		lp="63173,19311",
		pos="e,62680,18345 63001,20223 63001,19993 63001,18345 63001,18345 63001,18345 62690,18345 62690,18345"];
	Layer0_Device12_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59509,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage9_Attention	[label=Q_local,
		lp="59351,19214",
		pos="e,59404,18179 62067,20266 61103,20266 59404,20266 59404,20266 59404,20266 59404,18189 59404,18189"];
	Layer0_Device12_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59885,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage10_Attention	[label=Q_local,
		lp="59211,19118",
		pos="e,60010,17986 63058,20222 63058,19975 63058,18096 63058,18096 63058,18096 60010,18096 60010,18096 60010,18096 60010,17996 60010,\
17996"];
	Layer0_Device12_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61099,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage11_Attention	[label=Q_local,
		lp="63294,19021",
		pos="e,61176,17793 63114,20222 63114,19959 63114,17836 63114,17836 63114,17836 61176,17836 61176,17836 61176,17836 61176,17803 61176,\
17803"];
	Layer0_Device12_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61334,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage12_Attention	[label=Q_local,
		lp="63437,18925",
		pos="e,61359,17601 63171,20222 63171,19959 63171,17834 63171,17834 63171,17834 61359,17834 61359,17834 61359,17834 61359,17611 61359,\
17611"];
	Layer0_Device12_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="63349,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage13_Attention	[label=Q_local,
		lp="63577,18828",
		pos="e,63522,17408 63367,20235 63458,20235 63522,20235 63522,20235 63522,20235 63522,17418 63522,17418"];
	Layer0_Device12_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60931,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage14_Attention	[label=Q_local,
		lp="59065,18732",
		pos="e,60699,17187 63297,20222 63297,19949 63297,17670 63297,17670 63297,17670 60578,17670 60578,17670 60578,17670 60578,17187 60578,\
17187 60578,17187 60689,17187 60689,17187"];
	Layer0_Device12_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="63064,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_QKVProj -> Layer0_Device12_Stage15_Attention	[label=Q_local,
		lp="63695,18635",
		pos="e,63296,17043 63367,20249 63515,20249 63631,20249 63631,20249 63631,20249 63631,17043 63631,17043 63631,17043 63306,17043 63306,\
17043"];
	Layer0_Device12_Stage0_RecvKV -> Layer0_Device12_Stage0_Attention	[pos="e,62210,19917 62210,20082 62210,20082 62210,19927 62210,19927"];
	Layer0_Device12_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61052,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage0_RecvKV -> Layer0_Device12_Stage1_RecvKV	[label="Ring transfer",
		lp="61309,19986",
		pos="e,61262,19943 61262,20029 61262,20029 61262,19953 61262,19953"];
	Layer0_Device12_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="60452,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage0_Attention -> Layer0_Device12_Stage0_Accumulate	[pos="e,60452,19723 62261,19863 62261,19844 62261,19823 62261,19823 62261,19823 60452,19823 60452,19823 60452,19823 60452,19733 60452,\
19733"];
	Layer0_Device12_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60021,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage0_Accumulate -> Layer0_Device12_Stage1_Accumulate	[pos="e,60236,19530 60236,19670 60236,19670 60236,19540 60236,19540"];
	Layer0_Device12_Stage1_RecvKV -> Layer0_Device12_Stage1_Attention	[pos="e,60147,19724 60147,19889 60147,19889 60147,19734 60147,19734"];
	Layer0_Device12_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61661,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage1_RecvKV -> Layer0_Device12_Stage2_RecvKV	[label="Ring transfer",
		lp="61369,19793",
		pos="e,61356,19750 61356,19836 61356,19836 61356,19760 61356,19760"];
	Layer0_Device12_Stage1_Attention -> Layer0_Device12_Stage1_Accumulate	[pos="e,59976,19530 59976,19670 59976,19670 59976,19540 59976,19540"];
	Layer0_Device12_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60139,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage1_Accumulate -> Layer0_Device12_Stage2_Accumulate	[pos="e,60080,19337 60080,19477 60080,19477 60080,19347 60080,19347"];
	Layer0_Device12_Stage2_RecvKV -> Layer0_Device12_Stage2_Attention	[pos="e,60757,19531 60757,19696 60757,19696 60757,19541 60757,19541"];
	Layer0_Device12_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61751,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage2_RecvKV -> Layer0_Device12_Stage3_RecvKV	[label="Ring transfer",
		lp="61754,19600",
		pos="e,61706,19557 61706,19643 61706,19643 61706,19567 61706,19567"];
	Layer0_Device12_Stage2_Attention -> Layer0_Device12_Stage2_Accumulate	[pos="e,60340,19337 60340,19477 60340,19477 60340,19347 60340,19347"];
	Layer0_Device12_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60284,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage2_Accumulate -> Layer0_Device12_Stage3_Accumulate	[pos="e,60212,19144 60212,19284 60212,19284 60212,19154 60212,19154"];
	Layer0_Device12_Stage3_RecvKV -> Layer0_Device12_Stage3_Attention	[pos="e,62498,19338 62498,19503 62498,19503 62498,19348 62498,19348"];
	Layer0_Device12_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61348,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage3_RecvKV -> Layer0_Device12_Stage4_RecvKV	[label="Ring transfer",
		lp="61532,19407",
		pos="e,61550,19364 61550,19450 61550,19450 61550,19374 61550,19374"];
	Layer0_Device12_Stage3_Attention -> Layer0_Device12_Stage3_Accumulate	[pos="e,60472,19144 62369,19284 62369,19261 62369,19233 62369,19233 62369,19233 60472,19233 60472,19233 60472,19233 60472,19154 60472,\
19154"];
	Layer0_Device12_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60263,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage3_Accumulate -> Layer0_Device12_Stage4_Accumulate	[pos="e,60274,18951 60274,19091 60274,19091 60274,18961 60274,18961"];
	Layer0_Device12_Stage4_RecvKV -> Layer0_Device12_Stage4_Attention	[pos="e,59951,19144 60472,19270 60191,19270 59951,19270 59951,19270 59951,19270 59951,19154 59951,19154"];
	Layer0_Device12_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61493,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage4_RecvKV -> Layer0_Device12_Stage5_RecvKV	[label="Ring transfer",
		lp="61468,19214",
		pos="e,61420,19171 61420,19257 61420,19257 61420,19181 61420,19181"];
	Layer0_Device12_Stage4_Attention -> Layer0_Device12_Stage4_Accumulate	[pos="e,60042,18951 59995,19117 60023,19117 60042,19117 60042,19117 60042,19117 60042,18961 60042,18961"];
	Layer0_Device12_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="62192,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage4_Accumulate -> Layer0_Device12_Stage5_Accumulate	[pos="e,62176,18758 60279,18898 60279,18874 60279,18842 60279,18842 60279,18842 62176,18842 62176,18842 62176,18842 62176,18768 62176,\
18768"];
	Layer0_Device12_Stage5_RecvKV -> Layer0_Device12_Stage5_Attention	[pos="e,62449,18933 62402,19117 62402,19112 62402,18933 62402,18933 62402,18933 62439,18933 62439,18933"];
	Layer0_Device12_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61472,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage5_RecvKV -> Layer0_Device12_Stage6_RecvKV	[label="Ring transfer",
		lp="61528,19021",
		pos="e,61482,18978 61482,19064 61482,19064 61482,18988 61482,18988"];
	Layer0_Device12_Stage5_Attention -> Layer0_Device12_Stage5_Accumulate	[pos="e,62418,18758 62449,18915 62430,18915 62418,18915 62418,18915 62418,18915 62418,18768 62418,18768"];
	Layer0_Device12_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="62192,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage5_Accumulate -> Layer0_Device12_Stage6_Accumulate	[pos="e,62192,18565 62192,18705 62192,18705 62192,18575 62192,18575"];
	Layer0_Device12_Stage6_RecvKV -> Layer0_Device12_Stage6_Attention	[pos="e,62697,18758 62059,18884 62374,18884 62697,18884 62697,18884 62697,18884 62697,18768 62697,18768"];
	Layer0_Device12_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60983,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage6_RecvKV -> Layer0_Device12_Stage7_RecvKV	[label="Ring transfer",
		lp="61212,18828",
		pos="e,61228,18785 61228,18871 61228,18871 61228,18795 61228,18795"];
	Layer0_Device12_Stage6_Attention -> Layer0_Device12_Stage6_Accumulate	[pos="e,62424,18538 62580,18705 62580,18652 62580,18538 62580,18538 62580,18538 62434,18538 62434,18538"];
	Layer0_Device12_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61927,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage6_Accumulate -> Layer0_Device12_Stage7_Accumulate	[pos="e,62060,18372 62060,18512 62060,18512 62060,18382 62060,18382"];
	Layer0_Device12_Stage7_RecvKV -> Layer0_Device12_Stage7_Attention	[pos="e,60002,18565 60107,18691 60042,18691 60002,18691 60002,18691 60002,18691 60002,18575 60002,18575"];
	Layer0_Device12_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60983,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage7_RecvKV -> Layer0_Device12_Stage8_RecvKV	[label="Ring transfer",
		lp="61031,18635",
		pos="e,60983,18592 60983,18678 60983,18678 60983,18602 60983,18602"];
	Layer0_Device12_Stage7_Attention -> Layer0_Device12_Stage7_Accumulate	[pos="e,61799,18372 59902,18512 59902,18472 59902,18403 59902,18403 59902,18403 61799,18403 61799,18403 61799,18403 61799,18382 61799,\
18382"];
	Layer0_Device12_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61927,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage7_Accumulate -> Layer0_Device12_Stage8_Accumulate	[pos="e,61927,18179 61927,18319 61927,18319 61927,18189 61927,18189"];
	Layer0_Device12_Stage8_RecvKV -> Layer0_Device12_Stage8_Attention	[pos="e,62320,18372 61570,18498 61929,18498 62320,18498 62320,18498 62320,18498 62320,18382 62320,18382"];
	Layer0_Device12_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60718,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage8_RecvKV -> Layer0_Device12_Stage9_RecvKV	[label="Ring transfer",
		lp="60833,18442",
		pos="e,60850,18399 60850,18485 60850,18485 60850,18409 60850,18409"];
	Layer0_Device12_Stage8_Attention -> Layer0_Device12_Stage8_Accumulate	[pos="e,62159,18152 62376,18319 62376,18266 62376,18152 62376,18152 62376,18152 62169,18152 62169,18152"];
	Layer0_Device12_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60406,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage8_Accumulate -> Layer0_Device12_Stage9_Accumulate	[pos="e,60483,17986 61927,18126 61927,18091 61927,18035 61927,18035 61927,18035 60483,18035 60483,18035 60483,18035 60483,17996 60483,\
17996"];
	Layer0_Device12_Stage9_RecvKV -> Layer0_Device12_Stage9_Attention	[pos="e,59642,18179 59842,18305 59723,18305 59642,18305 59642,18305 59642,18305 59642,18189 59642,18189"];
	Layer0_Device12_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="60718,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage9_RecvKV -> Layer0_Device12_Stage10_RecvKV	[label="Ring transfer",
		lp="60766,18249",
		pos="e,60718,18206 60718,18292 60718,18292 60718,18216 60718,18216"];
	Layer0_Device12_Stage9_Attention -> Layer0_Device12_Stage9_Accumulate	[pos="e,60329,17987 59697,18126 59697,18082 59697,17999 59697,17999 59697,17999 60329,17999 60329,17999 60329,17999 60329,17997 60329,\
17997"];
	Layer0_Device12_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60406,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage9_Accumulate -> Layer0_Device12_Stage10_Accumulate	[pos="e,60406,17793 60406,17933 60406,17933 60406,17803 60406,17803"];
	Layer0_Device12_Stage10_RecvKV -> Layer0_Device12_Stage10_Attention	[pos="e,59904,17986 59904,18099 59904,18099 59904,17996 59904,17996"];
	Layer0_Device12_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61615,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage10_RecvKV -> Layer0_Device12_Stage11_RecvKV	[label="Ring transfer",
		lp="61035,18056",
		pos="e,61166,18013 61166,18099 61166,18099 61166,18023 61166,18023"];
	Layer0_Device12_Stage10_Attention -> Layer0_Device12_Stage10_Accumulate	[pos="e,60174,17766 59920,17933 59920,17880 59920,17766 59920,17766 59920,17766 60164,17766 60164,17766"];
	Layer0_Device12_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60813,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage10_Accumulate -> Layer0_Device12_Stage11_Accumulate	[pos="e,60610,17600 60610,17740 60610,17740 60610,17610 60610,17610"];
	Layer0_Device12_Stage11_RecvKV -> Layer0_Device12_Stage11_Attention	[pos="e,61022,17793 61022,17906 61022,17906 61022,17803 61022,17803"];
	Layer0_Device12_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62308,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage11_RecvKV -> Layer0_Device12_Stage12_RecvKV	[label="Ring transfer",
		lp="62096,17863",
		pos="e,61962,17820 61962,17906 61962,17906 61962,17830 61962,17830"];
	Layer0_Device12_Stage11_Attention -> Layer0_Device12_Stage11_Accumulate	[pos="e,60956,17600 60956,17740 60956,17740 60956,17610 60956,17610"];
	Layer0_Device12_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60931,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage11_Accumulate -> Layer0_Device12_Stage12_Accumulate	[pos="e,60872,17407 60872,17547 60872,17547 60872,17417 60872,17417"];
	Layer0_Device12_Stage12_RecvKV -> Layer0_Device12_Stage12_Attention	[pos="e,61477,17600 61477,17713 61477,17713 61477,17610 61477,17610"];
	Layer0_Device12_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62543,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage12_RecvKV -> Layer0_Device12_Stage13_RecvKV	[label="Ring transfer",
		lp="62474,17670",
		pos="e,62426,17627 62426,17713 62426,17713 62426,17637 62426,17637"];
	Layer0_Device12_Stage12_Attention -> Layer0_Device12_Stage12_Accumulate	[pos="e,61132,17407 61132,17547 61132,17547 61132,17417 61132,17417"];
	Layer0_Device12_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61452,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage12_Accumulate -> Layer0_Device12_Stage13_Accumulate	[pos="e,61220,17187 61163,17380 61180,17380 61191,17380 61191,17380 61191,17380 61191,17187 61191,17187 61191,17187 61210,17187 61210,\
17187"];
	Layer0_Device12_Stage13_RecvKV -> Layer0_Device12_Stage13_Attention	[pos="e,63290,17408 63290,17573 63290,17573 63290,17418 63290,17418"];
	Layer0_Device12_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62140,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage13_RecvKV -> Layer0_Device12_Stage14_RecvKV	[label="Ring transfer",
		lp="62389,17477",
		pos="e,62342,17434 62342,17520 62342,17520 62342,17444 62342,17444"];
	Layer0_Device12_Stage13_Attention -> Layer0_Device12_Stage13_Accumulate	[pos="e,61452,17214 63349,17354 63349,17333 63349,17307 63349,17307 63349,17307 61452,17307 61452,17307 61452,17307 61452,17224 61452,\
17224"];
	Layer0_Device12_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61452,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage13_Accumulate -> Layer0_Device12_Stage14_Accumulate	[pos="e,61452,17062 61452,17161 61452,17161 61452,17072 61452,17072"];
	Layer0_Device12_Stage14_RecvKV -> Layer0_Device12_Stage14_Attention	[pos="e,60931,17214 61264,17340 61075,17340 60931,17340 60931,17340 60931,17340 60931,17224 60931,17224"];
	Layer0_Device12_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62661,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device12_Stage14_RecvKV -> Layer0_Device12_Stage15_RecvKV	[label="Ring transfer",
		lp="62448,17284",
		pos="e,62400,17241 62400,17327 62400,17327 62400,17251 62400,17251"];
	Layer0_Device12_Stage14_Attention -> Layer0_Device12_Stage14_Accumulate	[pos="e,61220,17026 60931,17161 60931,17115 60931,17026 60931,17026 60931,17026 61210,17026 61210,17026"];
	Layer0_Device12_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="63064,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device12_Stage14_Accumulate -> Layer0_Device12_Stage15_Accumulate	[pos="e,62832,16909 61452,17008 61452,16971 61452,16909 61452,16909 61452,16909 62822,16909 62822,16909"];
	Layer0_Device12_Stage15_RecvKV -> Layer0_Device12_Stage15_Attention	[pos="e,63064,17062 63064,17134 63064,17134 63064,17072 63064,17072"];
	Layer0_Device12_Stage15_Attention -> Layer0_Device12_Stage15_Accumulate	[pos="e,63064,16936 63064,17008 63064,17008 63064,16946 63064,16946"];
	Layer0_Device12_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63455,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device12_Stage15_Accumulate -> Layer0_Device12_ConcatHeads	[pos="e,63262,16810 63262,16882 63262,16882 63262,16820 63262,16820"];
	Layer0_Device12_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63473,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device12_ConcatHeads -> Layer0_Device12_OutputProj	[pos="e,63470,16684 63470,16756 63470,16756 63470,16694 63470,16694"];
	Layer0_Device12_OutputProj -> Layer0_Device12_Residual1	[pos="e,63473,16558 63473,16630 63473,16630 63473,16568 63473,16568"];
	Layer0_Device12_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63226,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device12_Residual1 -> Layer0_Device12_LayerNorm2	[pos="e,63289,16432 63289,16504 63289,16504 63289,16442 63289,16442"];
	Layer0_Device12_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="63524,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device12_Residual1 -> Layer0_Device12_Residual2	[pos="e,63691,15750 63691,16504 63691,16504 63691,15760 63691,15760"];
	Layer0_Device12_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="62933,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device12_LayerNorm2 -> Layer0_Device12_GateProj	[pos="e,63086,16306 63086,16378 63086,16378 63086,16316 63086,16316"];
	Layer0_Device12_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63246,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device12_LayerNorm2 -> Layer0_Device12_UpProj	[pos="e,63298,16217 63298,16378 63298,16378 63298,16227 63298,16227"];
	Layer0_Device12_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="62933,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device12_GateProj -> Layer0_Device12_Activation	[pos="e,62864,16128 62864,16252 62864,16252 62864,16138 62864,16138"];
	Layer0_Device12_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63029,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device12_UpProj -> Layer0_Device12_ElemMul	[pos="e,63299,16002 63299,16163 63299,16163 63299,16012 63299,16012"];
	Layer0_Device12_Activation -> Layer0_Device12_ElemMul	[pos="e,62933,16002 62933,16074 62933,16074 62933,16012 62933,16012"];
	Layer0_Device12_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63125,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device12_ElemMul -> Layer0_Device12_DownProj	[pos="e,63125,15876 63125,15948 63125,15948 63125,15886 63125,15886"];
	Layer0_Device12_DownProj -> Layer0_Device12_Residual2	[pos="e,63241,15750 63241,15822 63241,15822 63241,15760 63241,15760"];
	Layer0_Device12_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63524,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device12_Residual2 -> Layer0_Device12_Output	[pos="e,63524,15624 63524,15696 63524,15696 63524,15634 63524,15634"];
	Layer1_Device12_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63524,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device12_Output -> Layer1_Device12_Input	[pos="e,63524,15476 63524,15548 63524,15548 63524,15486 63524,15486"];
	Layer0_Device13_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68210,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device13_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="67952,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device13_Input -> Layer0_Device13_LayerNorm1	[pos="e,68037,20402 68037,20481 68037,20481 68037,20412 68037,20412"];
	Layer0_Device13_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="68611,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device13_Input -> Layer0_Device13_Residual1	[pos="e,68508,16558 68508,20506 68508,20506 68508,16568 68508,16568"];
	Layer0_Device13_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67641,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device13_LayerNorm1 -> Layer0_Device13_QKVProj	[pos="e,67952,20276 67952,20348 67952,20348 67952,20286 67952,20286"];
	Layer0_Device13_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66395,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage0_RecvKV	[label="Local K,V",
		lp="66896,20179",
		pos="e,67153,20136 67153,20222 67153,20222 67153,20146 67153,20146"];
	Layer0_Device13_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67148,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage0_Attention	[label=Q_local,
		lp="67457,20083",
		pos="e,67347,19917 67347,20222 67347,20222 67347,19927 67347,19927"];
	Layer0_Device13_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64855,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage1_Attention	[label=Q_local,
		lp="64919,19986",
		pos="e,64821,19723 66991,20229 66159,20229 64821,20229 64821,20229 64821,20229 64821,19733 64821,19733"];
	Layer0_Device13_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65466,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage2_Attention	[label=Q_local,
		lp="67597,19890",
		pos="e,65636,19530 67442,20223 67442,20122 67442,19773 67442,19773 67442,19773 65636,19773 65636,19773 65636,19773 65636,19540 65636,\
19540"];
	Layer0_Device13_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67481,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage3_Attention	[label=Q_local,
		lp="67737,19793",
		pos="e,67654,19337 67654,20223 67654,20223 67654,19347 67654,19347"];
	Layer0_Device13_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64687,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage4_Attention	[label=Q_local,
		lp="64539,19697",
		pos="e,64540,19144 66991,20242 66082,20242 64540,20242 64540,20242 64540,20242 64540,19154 64540,19154"];
	Layer0_Device13_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67601,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage5_Attention	[label=Q_local,
		lp="67841,19600",
		pos="e,67772,18951 67772,20222 67772,20222 67772,18961 67772,18961"];
	Layer0_Device13_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67514,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage6_Attention	[label=Q_local,
		lp="67973,19504",
		pos="e,67746,18731 67869,20222 67869,20020 67869,18731 67869,18731 67869,18731 67756,18731 67756,18731"];
	Layer0_Device13_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64575,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage7_Attention	[label=Q_local,
		lp="64393,19407",
		pos="e,64400,18565 66991,20255 66045,20255 64400,20255 64400,20255 64400,20255 64400,18575 64400,18575"];
	Layer0_Device13_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67404,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage8_Attention	[label=Q_local,
		lp="68093,19311",
		pos="e,67636,18345 67906,20223 67906,19993 67906,18345 67906,18345 67906,18345 67646,18345 67646,18345"];
	Layer0_Device13_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64465,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage9_Attention	[label=Q_local,
		lp="64275,19214",
		pos="e,64288,18179 66991,20262 66016,20262 64288,20262 64288,20262 64288,20262 64288,18189 64288,18189"];
	Layer0_Device13_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64615,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage10_Attention	[label=Q_local,
		lp="64135,19118",
		pos="e,64383,17959 66991,20269 65984,20269 64160,20269 64160,20269 64160,20269 64160,17959 64160,17959 64160,17959 64373,17959 64373,\
17959"];
	Layer0_Device13_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65924,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage11_Attention	[label=Q_local,
		lp="68217,19021",
		pos="e,66001,17793 67979,20223 67979,19963 67979,17883 67979,17883 67979,17883 66001,17883 66001,17883 66001,17883 66001,17803 66001,\
17803"];
	Layer0_Device13_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="66254,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage12_Attention	[label=Q_local,
		lp="68357,18925",
		pos="e,66184,17600 68016,20222 68016,19962 68016,17876 68016,17876 68016,17876 66184,17876 66184,17876 66184,17876 66184,17610 66184,\
17610"];
	Layer0_Device13_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="68269,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage13_Attention	[label=Q_local,
		lp="68497,18828",
		pos="e,68442,17408 68291,20235 68380,20235 68442,20235 68442,20235 68442,20235 68442,17418 68442,17418"];
	Layer0_Device13_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65851,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage14_Attention	[label=Q_local,
		lp="63989,18732",
		pos="e,65619,17187 67943,20222 67943,19963 67943,17904 67943,17904 67943,17904 65487,17904 65487,17904 65487,17904 65487,17187 65487,\
17187 65487,17187 65609,17187 65609,17187"];
	Layer0_Device13_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67984,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_QKVProj -> Layer0_Device13_Stage15_Attention	[label=Q_local,
		lp="68615,18635",
		pos="e,68216,17043 68291,20249 68413,20249 68504,20249 68504,20249 68504,20249 68504,17043 68504,17043 68504,17043 68226,17043 68226,\
17043"];
	Layer0_Device13_Stage0_RecvKV -> Layer0_Device13_Stage0_Attention	[pos="e,67116,19916 67116,20079 67116,20079 67116,19926 67116,19926"];
	Layer0_Device13_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65939,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage0_RecvKV -> Layer0_Device13_Stage1_RecvKV	[label="Ring transfer",
		lp="66215,19986",
		pos="e,66167,19943 66167,20029 66167,20029 66167,19953 66167,19953"];
	Layer0_Device13_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="65376,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage0_Attention -> Layer0_Device13_Stage0_Accumulate	[pos="e,65376,19723 67148,19863 67148,19829 67148,19775 67148,19775 67148,19775 65376,19775 65376,19775 65376,19775 65376,19733 65376,\
19733"];
	Layer0_Device13_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64945,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage0_Accumulate -> Layer0_Device13_Stage1_Accumulate	[pos="e,65160,19530 65160,19670 65160,19670 65160,19540 65160,19540"];
	Layer0_Device13_Stage1_RecvKV -> Layer0_Device13_Stage1_Attention	[pos="e,65053,19724 65053,19889 65053,19889 65053,19734 65053,19734"];
	Layer0_Device13_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66585,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage1_RecvKV -> Layer0_Device13_Stage2_RecvKV	[label="Ring transfer",
		lp="66256,19793",
		pos="e,66262,19750 66262,19836 66262,19836 66262,19760 66262,19760"];
	Layer0_Device13_Stage1_Attention -> Layer0_Device13_Stage1_Accumulate	[pos="e,64900,19530 64900,19670 64900,19670 64900,19540 64900,19540"];
	Layer0_Device13_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65063,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage1_Accumulate -> Layer0_Device13_Stage2_Accumulate	[pos="e,65004,19337 65004,19477 65004,19477 65004,19347 65004,19347"];
	Layer0_Device13_Stage2_RecvKV -> Layer0_Device13_Stage2_Attention	[pos="e,65681,19531 65681,19696 65681,19696 65681,19541 65681,19541"];
	Layer0_Device13_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66675,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage2_RecvKV -> Layer0_Device13_Stage3_RecvKV	[label="Ring transfer",
		lp="66678,19600",
		pos="e,66630,19557 66630,19643 66630,19643 66630,19567 66630,19567"];
	Layer0_Device13_Stage2_Attention -> Layer0_Device13_Stage2_Accumulate	[pos="e,65264,19337 65264,19477 65264,19477 65264,19347 65264,19347"];
	Layer0_Device13_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65208,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage2_Accumulate -> Layer0_Device13_Stage3_Accumulate	[pos="e,65136,19144 65136,19284 65136,19284 65136,19154 65136,19154"];
	Layer0_Device13_Stage3_RecvKV -> Layer0_Device13_Stage3_Attention	[pos="e,67422,19338 67422,19503 67422,19503 67422,19348 67422,19348"];
	Layer0_Device13_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66272,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage3_RecvKV -> Layer0_Device13_Stage4_RecvKV	[label="Ring transfer",
		lp="66456,19407",
		pos="e,66474,19364 66474,19450 66474,19450 66474,19374 66474,19374"];
	Layer0_Device13_Stage3_Attention -> Layer0_Device13_Stage3_Accumulate	[pos="e,65396,19144 67293,19284 67293,19263 67293,19238 67293,19238 67293,19238 65396,19238 65396,19238 65396,19238 65396,19154 65396,\
19154"];
	Layer0_Device13_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65183,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage3_Accumulate -> Layer0_Device13_Stage4_Accumulate	[pos="e,65196,18951 65196,19091 65196,19091 65196,18961 65196,18961"];
	Layer0_Device13_Stage4_RecvKV -> Layer0_Device13_Stage4_Attention	[pos="e,64875,19144 65396,19270 65115,19270 64875,19270 64875,19270 64875,19270 64875,19154 64875,19154"];
	Layer0_Device13_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66417,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage4_RecvKV -> Layer0_Device13_Stage5_RecvKV	[label="Ring transfer",
		lp="66393,19214",
		pos="e,66344,19171 66344,19257 66344,19257 66344,19181 66344,19181"];
	Layer0_Device13_Stage4_Attention -> Layer0_Device13_Stage4_Accumulate	[pos="e,64964,18951 64919,19117 64946,19117 64964,19117 64964,19117 64964,19117 64964,18961 64964,18961"];
	Layer0_Device13_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66993,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage4_Accumulate -> Layer0_Device13_Stage5_Accumulate	[pos="e,66916,18758 65183,18898 65183,18860 65183,18794 65183,18794 65183,18794 66916,18794 66916,18794 66916,18794 66916,18768 66916,\
18768"];
	Layer0_Device13_Stage5_RecvKV -> Layer0_Device13_Stage5_Attention	[pos="e,67369,18924 67324,19117 67324,19112 67324,18924 67324,18924 67324,18924 67359,18924 67359,18924"];
	Layer0_Device13_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66392,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage5_RecvKV -> Layer0_Device13_Stage6_RecvKV	[label="Ring transfer",
		lp="66452,19021",
		pos="e,66404,18978 66404,19064 66404,19064 66404,18988 66404,18988"];
	Layer0_Device13_Stage5_Attention -> Layer0_Device13_Stage5_Accumulate	[pos="e,67070,18758 67558,18898 67558,18858 67558,18789 67558,18789 67558,18789 67070,18789 67070,18789 67070,18789 67070,18768 67070,\
18768"];
	Layer0_Device13_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66993,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage5_Accumulate -> Layer0_Device13_Stage6_Accumulate	[pos="e,66993,18565 66993,18705 66993,18705 66993,18575 66993,18575"];
	Layer0_Device13_Stage6_RecvKV -> Layer0_Device13_Stage6_Attention	[pos="e,67297,18759 67297,18924 67297,18924 67297,18769 67297,18769"];
	Layer0_Device13_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65784,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage6_RecvKV -> Layer0_Device13_Stage7_RecvKV	[label="Ring transfer",
		lp="66136,18828",
		pos="e,66088,18785 66088,18871 66088,18871 66088,18795 66088,18795"];
	Layer0_Device13_Stage6_Attention -> Layer0_Device13_Stage6_Accumulate	[pos="e,67225,18529 67459,18705 67459,18650 67459,18529 67459,18529 67459,18529 67235,18529 67235,18529"];
	Layer0_Device13_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66883,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage6_Accumulate -> Layer0_Device13_Stage7_Accumulate	[pos="e,66938,18372 66938,18512 66938,18512 66938,18382 66938,18382"];
	Layer0_Device13_Stage7_RecvKV -> Layer0_Device13_Stage7_Attention	[pos="e,64631,18565 64908,18691 64748,18691 64631,18691 64631,18691 64631,18691 64631,18575 64631,18575"];
	Layer0_Device13_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65784,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage7_RecvKV -> Layer0_Device13_Stage8_RecvKV	[label="Ring transfer",
		lp="65832,18635",
		pos="e,65784,18592 65784,18678 65784,18678 65784,18602 65784,18602"];
	Layer0_Device13_Stage7_Attention -> Layer0_Device13_Stage7_Accumulate	[pos="e,66678,18372 64780,18512 64780,18495 64780,18475 64780,18475 64780,18475 66678,18475 66678,18475 66678,18475 66678,18382 66678,\
18382"];
	Layer0_Device13_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66883,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage7_Accumulate -> Layer0_Device13_Stage8_Accumulate	[pos="e,66883,18179 66883,18319 66883,18319 66883,18189 66883,18189"];
	Layer0_Device13_Stage8_RecvKV -> Layer0_Device13_Stage8_Attention	[pos="e,67198,18372 66371,18498 66760,18498 67198,18498 67198,18498 67198,18498 67198,18382 67198,18382"];
	Layer0_Device13_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65674,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage8_RecvKV -> Layer0_Device13_Stage9_RecvKV	[label="Ring transfer",
		lp="65795,18442",
		pos="e,65729,18399 65729,18485 65729,18485 65729,18409 65729,18409"];
	Layer0_Device13_Stage8_Attention -> Layer0_Device13_Stage8_Accumulate	[pos="e,67115,18152 67219,18319 67219,18266 67219,18152 67219,18152 67219,18152 67125,18152 67125,18152"];
	Layer0_Device13_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65136,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage8_Accumulate -> Layer0_Device13_Stage9_Accumulate	[pos="e,65213,17986 66883,18126 66883,18101 66883,18068 66883,18068 66883,18068 65213,18068 65213,18068 65213,18068 65213,17996 65213,\
17996"];
	Layer0_Device13_Stage9_RecvKV -> Layer0_Device13_Stage9_Attention	[pos="e,64520,18179 64798,18305 64638,18305 64520,18305 64520,18305 64520,18305 64520,18189 64520,18189"];
	Layer0_Device13_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65674,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage9_RecvKV -> Layer0_Device13_Stage10_RecvKV	[label="Ring transfer",
		lp="65722,18249",
		pos="e,65674,18206 65674,18292 65674,18292 65674,18216 65674,18216"];
	Layer0_Device13_Stage9_Attention -> Layer0_Device13_Stage9_Accumulate	[pos="e,65059,17987 64540,18126 64540,18082 64540,17999 64540,17999 64540,17999 65059,17999 65059,17999 65059,17999 65059,17997 65059,\
17997"];
	Layer0_Device13_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65136,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage9_Accumulate -> Layer0_Device13_Stage10_Accumulate	[pos="e,65136,17793 65136,17933 65136,17933 65136,17803 65136,17803"];
	Layer0_Device13_Stage10_RecvKV -> Layer0_Device13_Stage10_Attention	[pos="e,64800,17987 64800,18152 64800,18152 64800,17997 64800,17997"];
	Layer0_Device13_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66345,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage10_RecvKV -> Layer0_Device13_Stage11_RecvKV	[label="Ring transfer",
		lp="65991,18056",
		pos="e,66010,18013 66010,18099 66010,18099 66010,18023 66010,18023"];
	Layer0_Device13_Stage10_Attention -> Layer0_Device13_Stage10_Accumulate	[pos="e,64904,17766 64734,17933 64734,17880 64734,17766 64734,17766 64734,17766 64894,17766 64894,17766"];
	Layer0_Device13_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65733,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage10_Accumulate -> Layer0_Device13_Stage11_Accumulate	[pos="e,65597,17600 65368,17766 65482,17766 65597,17766 65597,17766 65597,17766 65597,17610 65597,17610"];
	Layer0_Device13_Stage11_RecvKV -> Layer0_Device13_Stage11_Attention	[pos="e,65847,17793 65847,17906 65847,17906 65847,17803 65847,17803"];
	Layer0_Device13_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67133,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage11_RecvKV -> Layer0_Device13_Stage12_RecvKV	[label="Ring transfer",
		lp="66547,17863",
		pos="e,66739,17820 66739,17906 66739,17906 66739,17830 66739,17830"];
	Layer0_Device13_Stage11_Attention -> Layer0_Device13_Stage11_Accumulate	[pos="e,65828,17600 65828,17740 65828,17740 65828,17610 65828,17610"];
	Layer0_Device13_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65851,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage11_Accumulate -> Layer0_Device13_Stage12_Accumulate	[pos="e,65792,17407 65792,17547 65792,17547 65792,17417 65792,17417"];
	Layer0_Device13_Stage12_RecvKV -> Layer0_Device13_Stage12_Attention	[pos="e,66349,17600 66349,17713 66349,17713 66349,17610 66349,17610"];
	Layer0_Device13_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67463,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage12_RecvKV -> Layer0_Device13_Stage13_RecvKV	[label="Ring transfer",
		lp="67346,17670",
		pos="e,67298,17627 67298,17713 67298,17713 67298,17637 67298,17637"];
	Layer0_Device13_Stage12_Attention -> Layer0_Device13_Stage12_Accumulate	[pos="e,66052,17407 66052,17547 66052,17547 66052,17417 66052,17417"];
	Layer0_Device13_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66372,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage12_Accumulate -> Layer0_Device13_Stage13_Accumulate	[pos="e,66140,17187 66083,17380 66100,17380 66111,17380 66111,17380 66111,17380 66111,17187 66111,17187 66111,17187 66130,17187 66130,\
17187"];
	Layer0_Device13_Stage13_RecvKV -> Layer0_Device13_Stage13_Attention	[pos="e,68210,17408 68210,17573 68210,17573 68210,17418 68210,17418"];
	Layer0_Device13_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67060,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage13_RecvKV -> Layer0_Device13_Stage14_RecvKV	[label="Ring transfer",
		lp="67244,17477",
		pos="e,67262,17434 67262,17520 67262,17520 67262,17444 67262,17444"];
	Layer0_Device13_Stage13_Attention -> Layer0_Device13_Stage13_Accumulate	[pos="e,66372,17214 68269,17354 68269,17335 68269,17312 68269,17312 68269,17312 66372,17312 66372,17312 66372,17312 66372,17224 66372,\
17224"];
	Layer0_Device13_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66372,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage13_Accumulate -> Layer0_Device13_Stage14_Accumulate	[pos="e,66372,17062 66372,17161 66372,17161 66372,17072 66372,17072"];
	Layer0_Device13_Stage14_RecvKV -> Layer0_Device13_Stage14_Attention	[pos="e,65851,17214 66184,17340 65995,17340 65851,17340 65851,17340 65851,17340 65851,17224 65851,17224"];
	Layer0_Device13_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67581,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device13_Stage14_RecvKV -> Layer0_Device13_Stage15_RecvKV	[label="Ring transfer",
		lp="67368,17284",
		pos="e,67320,17241 67320,17327 67320,17327 67320,17251 67320,17251"];
	Layer0_Device13_Stage14_Attention -> Layer0_Device13_Stage14_Accumulate	[pos="e,66140,17026 65851,17161 65851,17115 65851,17026 65851,17026 65851,17026 66130,17026 66130,17026"];
	Layer0_Device13_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="67984,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device13_Stage14_Accumulate -> Layer0_Device13_Stage15_Accumulate	[pos="e,67752,16909 66372,17008 66372,16971 66372,16909 66372,16909 66372,16909 67742,16909 67742,16909"];
	Layer0_Device13_Stage15_RecvKV -> Layer0_Device13_Stage15_Attention	[pos="e,67984,17062 67984,17134 67984,17134 67984,17072 67984,17072"];
	Layer0_Device13_Stage15_Attention -> Layer0_Device13_Stage15_Accumulate	[pos="e,67984,16936 67984,17008 67984,17008 67984,16946 67984,16946"];
	Layer0_Device13_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="67988,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device13_Stage15_Accumulate -> Layer0_Device13_ConcatHeads	[pos="e,67988,16810 67988,16882 67988,16882 67988,16820 67988,16820"];
	Layer0_Device13_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68201,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device13_ConcatHeads -> Layer0_Device13_OutputProj	[pos="e,68101,16684 68101,16756 68101,16756 68101,16694 68101,16694"];
	Layer0_Device13_OutputProj -> Layer0_Device13_Residual1	[pos="e,68319,16558 68319,16630 68319,16630 68319,16568 68319,16568"];
	Layer0_Device13_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68418,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device13_Residual1 -> Layer0_Device13_LayerNorm2	[pos="e,68428,16432 68428,16504 68428,16504 68428,16442 68428,16442"];
	Layer0_Device13_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="68520,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device13_Residual1 -> Layer0_Device13_Residual2	[pos="e,68785,15750 68785,16504 68785,16504 68785,15760 68785,15760"];
	Layer0_Device13_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68126,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device13_LayerNorm2 -> Layer0_Device13_GateProj	[pos="e,68278,16306 68278,16378 68278,16378 68278,16316 68278,16316"];
	Layer0_Device13_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68439,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device13_LayerNorm2 -> Layer0_Device13_UpProj	[pos="e,68491,16217 68491,16378 68491,16378 68491,16227 68491,16227"];
	Layer0_Device13_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68126,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device13_GateProj -> Layer0_Device13_Activation	[pos="e,68058,16128 68058,16252 68058,16252 68058,16138 68058,16138"];
	Layer0_Device13_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68221,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device13_UpProj -> Layer0_Device13_ElemMul	[pos="e,68491,16002 68491,16163 68491,16163 68491,16012 68491,16012"];
	Layer0_Device13_Activation -> Layer0_Device13_ElemMul	[pos="e,68126,16002 68126,16074 68126,16074 68126,16012 68126,16012"];
	Layer0_Device13_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68364,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device13_ElemMul -> Layer0_Device13_DownProj	[pos="e,68364,15876 68364,15948 68364,15948 68364,15886 68364,15886"];
	Layer0_Device13_DownProj -> Layer0_Device13_Residual2	[pos="e,68364,15750 68364,15822 68364,15822 68364,15760 68364,15760"];
	Layer0_Device13_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68520,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device13_Residual2 -> Layer0_Device13_Output	[pos="e,68520,15624 68520,15696 68520,15696 68520,15634 68520,15634"];
	Layer1_Device13_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68520,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device13_Output -> Layer1_Device13_Input	[pos="e,68520,15476 68520,15548 68520,15548 68520,15486 68520,15486"];
	Layer0_Device14_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73288,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device14_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="72973,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device14_Input -> Layer0_Device14_LayerNorm1	[pos="e,73086,20402 73086,20484 73086,20484 73086,20412 73086,20412"];
	Layer0_Device14_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="73572,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device14_Input -> Layer0_Device14_Residual1	[pos="e,73553,16558 73553,20494 73553,20494 73553,16568 73553,16568"];
	Layer0_Device14_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72546,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device14_LayerNorm1 -> Layer0_Device14_QKVProj	[pos="e,72973,20276 72973,20348 72973,20348 72973,20286 72973,20286"];
	Layer0_Device14_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71300,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage0_RecvKV	[label="Local K,V",
		lp="71800,20179",
		pos="e,72058,20136 72058,20222 72058,20222 72058,20146 72058,20146"];
	Layer0_Device14_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72090,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage0_Attention	[label=Q_local,
		lp="72362,20083",
		pos="e,72271,19917 72271,20222 72271,20222 72271,19927 72271,19927"];
	Layer0_Device14_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69760,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage1_Attention	[label=Q_local,
		lp="69824,19986",
		pos="e,69745,19723 71896,20229 71069,20229 69745,20229 69745,20229 69745,20229 69745,19733 69745,19733"];
	Layer0_Device14_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70371,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage2_Attention	[label=Q_local,
		lp="72502,19890",
		pos="e,70541,19530 72366,20222 72366,20129 72366,19826 72366,19826 72366,19826 70541,19826 70541,19826 70541,19826 70541,19540 70541,\
19540"];
	Layer0_Device14_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72386,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage3_Attention	[label=Q_local,
		lp="72642,19793",
		pos="e,72559,19337 72559,20223 72559,20223 72559,19347 72559,19347"];
	Layer0_Device14_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69592,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage4_Attention	[label=Q_local,
		lp="69444,19697",
		pos="e,69444,19144 71896,20242 70987,20242 69444,20242 69444,20242 69444,20242 69444,19154 69444,19154"];
	Layer0_Device14_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72508,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage5_Attention	[label=Q_local,
		lp="72746,19600",
		pos="e,72678,18951 72678,20222 72678,20222 72678,18961 72678,18961"];
	Layer0_Device14_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72593,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage6_Attention	[label=Q_local,
		lp="72878,19504",
		pos="e,72782,18758 72782,20222 72782,20222 72782,18768 72782,18768"];
	Layer0_Device14_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69654,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage7_Attention	[label=Q_local,
		lp="69298,19407",
		pos="e,69422,18547 71896,20255 70953,20255 69316,20255 69316,20255 69316,20255 69316,18547 69316,18547 69316,18547 69412,18547 69412,\
18547"];
	Layer0_Device14_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72396,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage8_Attention	[label=Q_local,
		lp="73000,19311",
		pos="e,72628,18345 72852,20223 72852,19993 72852,18345 72852,18345 72852,18345 72638,18345 72638,18345"];
	Layer0_Device14_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69457,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage9_Attention	[label=Q_local,
		lp="69180,19214",
		pos="e,69270,18179 71896,20262 70941,20262 69270,20262 69270,20262 69270,20262 69270,18189 69270,18189"];
	Layer0_Device14_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69617,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage10_Attention	[label=Q_local,
		lp="69040,19118",
		pos="e,69385,17959 71896,20269 70914,20269 69164,20269 69164,20269 69164,20269 69164,17959 69164,17959 69164,17959 69375,17959 69375,\
17959"];
	Layer0_Device14_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70831,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage11_Attention	[label=Q_local,
		lp="73123,19021",
		pos="e,70908,17793 72906,20223 72906,19962 72906,17881 72906,17881 72906,17881 70908,17881 70908,17881 70908,17881 70908,17803 70908,\
17803"];
	Layer0_Device14_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71161,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage12_Attention	[label=Q_local,
		lp="73264,18925",
		pos="e,71091,17600 72933,20222 72933,19961 72933,17870 72933,17870 72933,17870 71091,17870 71091,17870 71091,17870 71091,17610 71091,\
17610"];
	Layer0_Device14_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73176,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage13_Attention	[label=Q_local,
		lp="73404,18828",
		pos="e,73349,17408 73196,20235 73286,20235 73349,20235 73349,20235 73349,20235 73349,17418 73349,17418"];
	Layer0_Device14_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70758,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage14_Attention	[label=Q_local,
		lp="68894,18732",
		pos="e,70526,17187 72879,20222 72879,19972 72879,18063 72879,18063 72879,18063 70389,18063 70389,18063 70389,18063 70389,17187 70389,\
17187 70389,17187 70516,17187 70516,17187"];
	Layer0_Device14_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73093,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_QKVProj -> Layer0_Device14_Stage15_Attention	[label=Q_local,
		lp="73522,18635",
		pos="e,73325,17043 73196,20249 73346,20249 73462,20249 73462,20249 73462,20249 73462,17043 73462,17043 73462,17043 73335,17043 73335,\
17043"];
	Layer0_Device14_Stage0_RecvKV -> Layer0_Device14_Stage0_Attention	[pos="e,72039,19917 72039,20082 72039,20082 72039,19927 72039,19927"];
	Layer0_Device14_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70881,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage0_RecvKV -> Layer0_Device14_Stage1_RecvKV	[label="Ring transfer",
		lp="71139,19986",
		pos="e,71090,19943 71090,20029 71090,20029 71090,19953 71090,19953"];
	Layer0_Device14_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="70281,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage0_Attention -> Layer0_Device14_Stage0_Accumulate	[pos="e,70281,19723 72090,19863 72090,19846 72090,19828 72090,19828 72090,19828 70281,19828 70281,19828 70281,19828 70281,19733 70281,\
19733"];
	Layer0_Device14_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69850,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage0_Accumulate -> Layer0_Device14_Stage1_Accumulate	[pos="e,70066,19530 70066,19670 70066,19670 70066,19540 70066,19540"];
	Layer0_Device14_Stage1_RecvKV -> Layer0_Device14_Stage1_Attention	[pos="e,69976,19724 69976,19889 69976,19889 69976,19734 69976,19734"];
	Layer0_Device14_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71490,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage1_RecvKV -> Layer0_Device14_Stage2_RecvKV	[label="Ring transfer",
		lp="71095,19793",
		pos="e,71186,19750 71186,19836 71186,19836 71186,19760 71186,19760"];
	Layer0_Device14_Stage1_Attention -> Layer0_Device14_Stage1_Accumulate	[pos="e,69805,19530 69805,19670 69805,19670 69805,19540 69805,19540"];
	Layer0_Device14_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69968,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage1_Accumulate -> Layer0_Device14_Stage2_Accumulate	[pos="e,69909,19337 69909,19477 69909,19477 69909,19347 69909,19347"];
	Layer0_Device14_Stage2_RecvKV -> Layer0_Device14_Stage2_Attention	[pos="e,70586,19531 70586,19696 70586,19696 70586,19541 70586,19541"];
	Layer0_Device14_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71580,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage2_RecvKV -> Layer0_Device14_Stage3_RecvKV	[label="Ring transfer",
		lp="71583,19600",
		pos="e,71535,19557 71535,19643 71535,19643 71535,19567 71535,19567"];
	Layer0_Device14_Stage2_Attention -> Layer0_Device14_Stage2_Accumulate	[pos="e,70170,19337 70170,19477 70170,19477 70170,19347 70170,19347"];
	Layer0_Device14_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70113,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage2_Accumulate -> Layer0_Device14_Stage3_Accumulate	[pos="e,70040,19144 70040,19284 70040,19284 70040,19154 70040,19154"];
	Layer0_Device14_Stage3_RecvKV -> Layer0_Device14_Stage3_Attention	[pos="e,72327,19338 72327,19503 72327,19503 72327,19348 72327,19348"];
	Layer0_Device14_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71177,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage3_RecvKV -> Layer0_Device14_Stage4_RecvKV	[label="Ring transfer",
		lp="71426,19407",
		pos="e,71378,19364 71378,19450 71378,19450 71378,19374 71378,19374"];
	Layer0_Device14_Stage3_Attention -> Layer0_Device14_Stage3_Accumulate	[pos="e,70301,19144 72198,19284 72198,19265 72198,19243 72198,19243 72198,19243 70301,19243 70301,19243 70301,19243 70301,19154 70301,\
19154"];
	Layer0_Device14_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70090,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage3_Accumulate -> Layer0_Device14_Stage4_Accumulate	[pos="e,70102,18951 70102,19091 70102,19091 70102,18961 70102,18961"];
	Layer0_Device14_Stage4_RecvKV -> Layer0_Device14_Stage4_Attention	[pos="e,69780,19144 70301,19270 70020,19270 69780,19270 69780,19270 69780,19270 69780,19154 69780,19154"];
	Layer0_Device14_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71322,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage4_RecvKV -> Layer0_Device14_Stage5_RecvKV	[label="Ring transfer",
		lp="71297,19214",
		pos="e,71250,19171 71250,19257 71250,19257 71250,19181 71250,19181"];
	Layer0_Device14_Stage4_Attention -> Layer0_Device14_Stage4_Accumulate	[pos="e,69870,18951 69824,19117 69851,19117 69870,19117 69870,19117 69870,19117 69870,18961 69870,18961"];
	Layer0_Device14_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72072,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage4_Accumulate -> Layer0_Device14_Stage5_Accumulate	[pos="e,72030,18758 70132,18898 70132,18883 70132,18866 70132,18866 70132,18866 72030,18866 72030,18866 72030,18866 72030,18768 72030,\
18768"];
	Layer0_Device14_Stage5_RecvKV -> Layer0_Device14_Stage5_Attention	[pos="e,72276,18924 72230,19117 72230,19112 72230,18924 72230,18924 72230,18924 72266,18924 72266,18924"];
	Layer0_Device14_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71299,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage5_RecvKV -> Layer0_Device14_Stage6_RecvKV	[label="Ring transfer",
		lp="71358,19021",
		pos="e,71310,18978 71310,19064 71310,19064 71310,18988 71310,18988"];
	Layer0_Device14_Stage5_Attention -> Layer0_Device14_Stage5_Accumulate	[pos="e,72290,18758 72290,18898 72290,18898 72290,18768 72290,18768"];
	Layer0_Device14_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72072,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage5_Accumulate -> Layer0_Device14_Stage6_Accumulate	[pos="e,72072,18565 72072,18705 72072,18705 72072,18575 72072,18575"];
	Layer0_Device14_Stage6_RecvKV -> Layer0_Device14_Stage6_Attention	[pos="e,72550,18758 71886,18884 72211,18884 72550,18884 72550,18884 72550,18884 72550,18768 72550,18768"];
	Layer0_Device14_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70863,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage6_RecvKV -> Layer0_Device14_Stage7_RecvKV	[label="Ring transfer",
		lp="71063,18828",
		pos="e,71081,18785 71081,18871 71081,18871 71081,18795 71081,18795"];
	Layer0_Device14_Stage6_Attention -> Layer0_Device14_Stage6_Accumulate	[pos="e,72304,18529 72494,18705 72494,18650 72494,18529 72494,18529 72494,18529 72314,18529 72314,18529"];
	Layer0_Device14_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71875,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage6_Accumulate -> Layer0_Device14_Stage7_Accumulate	[pos="e,71974,18372 71974,18512 71974,18512 71974,18382 71974,18382"];
	Layer0_Device14_Stage7_RecvKV -> Layer0_Device14_Stage7_Attention	[pos="e,69872,18565 69987,18691 69917,18691 69872,18691 69872,18691 69872,18691 69872,18575 69872,18575"];
	Layer0_Device14_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70863,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage7_RecvKV -> Layer0_Device14_Stage8_RecvKV	[label="Ring transfer",
		lp="70911,18635",
		pos="e,70863,18592 70863,18678 70863,18678 70863,18602 70863,18602"];
	Layer0_Device14_Stage7_Attention -> Layer0_Device14_Stage7_Accumulate	[pos="e,71713,18372 69816,18512 69816,18480 69816,18429 69816,18429 69816,18429 71713,18429 71713,18429 71713,18429 71713,18382 71713,\
18382"];
	Layer0_Device14_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71875,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage7_Accumulate -> Layer0_Device14_Stage8_Accumulate	[pos="e,71875,18179 71875,18319 71875,18319 71875,18189 71875,18189"];
	Layer0_Device14_Stage8_RecvKV -> Layer0_Device14_Stage8_Attention	[pos="e,72234,18372 71450,18498 71823,18498 72234,18498 72234,18498 72234,18498 72234,18382 72234,18382"];
	Layer0_Device14_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70666,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage8_RecvKV -> Layer0_Device14_Stage9_RecvKV	[label="Ring transfer",
		lp="70830,18442",
		pos="e,70764,18399 70764,18485 70764,18485 70764,18409 70764,18409"];
	Layer0_Device14_Stage8_Attention -> Layer0_Device14_Stage8_Accumulate	[pos="e,72107,18152 72216,18319 72216,18266 72216,18152 72216,18152 72216,18152 72117,18152 72117,18152"];
	Layer0_Device14_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70138,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage8_Accumulate -> Layer0_Device14_Stage9_Accumulate	[pos="e,70215,17986 71875,18126 71875,18100 71875,18065 71875,18065 71875,18065 70215,18065 70215,18065 70215,18065 70215,17996 70215,\
17996"];
	Layer0_Device14_Stage9_RecvKV -> Layer0_Device14_Stage9_Attention	[pos="e,69556,18179 69790,18305 69653,18305 69556,18305 69556,18305 69556,18305 69556,18189 69556,18189"];
	Layer0_Device14_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70666,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage9_RecvKV -> Layer0_Device14_Stage10_RecvKV	[label="Ring transfer",
		lp="70714,18249",
		pos="e,70666,18206 70666,18292 70666,18292 70666,18216 70666,18216"];
	Layer0_Device14_Stage9_Attention -> Layer0_Device14_Stage9_Accumulate	[pos="e,70061,17987 69537,18126 69537,18082 69537,17999 69537,17999 69537,17999 70061,17999 70061,17999 70061,17999 70061,17997 70061,\
17997"];
	Layer0_Device14_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70138,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage9_Accumulate -> Layer0_Device14_Stage10_Accumulate	[pos="e,70138,17793 70138,17933 70138,17933 70138,17803 70138,17803"];
	Layer0_Device14_Stage10_RecvKV -> Layer0_Device14_Stage10_Attention	[pos="e,69797,17987 69797,18152 69797,18152 69797,17997 69797,17997"];
	Layer0_Device14_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71347,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage10_RecvKV -> Layer0_Device14_Stage11_RecvKV	[label="Ring transfer",
		lp="70983,18056",
		pos="e,71006,18013 71006,18099 71006,18099 71006,18023 71006,18023"];
	Layer0_Device14_Stage10_Attention -> Layer0_Device14_Stage10_Accumulate	[pos="e,69906,17766 69792,17933 69792,17880 69792,17766 69792,17766 69792,17766 69896,17766 69896,17766"];
	Layer0_Device14_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70640,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage10_Accumulate -> Layer0_Device14_Stage11_Accumulate	[pos="e,70418,17600 70370,17766 70398,17766 70418,17766 70418,17766 70418,17766 70418,17610 70418,17610"];
	Layer0_Device14_Stage11_RecvKV -> Layer0_Device14_Stage11_Attention	[pos="e,70754,17793 70754,17906 70754,17906 70754,17803 70754,17803"];
	Layer0_Device14_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72040,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage11_RecvKV -> Layer0_Device14_Stage12_RecvKV	[label="Ring transfer",
		lp="71874,17863",
		pos="e,71694,17820 71694,17906 71694,17906 71694,17830 71694,17830"];
	Layer0_Device14_Stage11_Attention -> Layer0_Device14_Stage11_Accumulate	[pos="e,70736,17600 70736,17740 70736,17740 70736,17610 70736,17610"];
	Layer0_Device14_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70758,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage11_Accumulate -> Layer0_Device14_Stage12_Accumulate	[pos="e,70699,17407 70699,17547 70699,17547 70699,17417 70699,17417"];
	Layer0_Device14_Stage12_RecvKV -> Layer0_Device14_Stage12_Attention	[pos="e,71256,17600 71256,17713 71256,17713 71256,17610 71256,17610"];
	Layer0_Device14_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72370,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage12_RecvKV -> Layer0_Device14_Stage13_RecvKV	[label="Ring transfer",
		lp="72253,17670",
		pos="e,72205,17627 72205,17713 72205,17713 72205,17637 72205,17637"];
	Layer0_Device14_Stage12_Attention -> Layer0_Device14_Stage12_Accumulate	[pos="e,70960,17407 70960,17547 70960,17547 70960,17417 70960,17417"];
	Layer0_Device14_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71279,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage12_Accumulate -> Layer0_Device14_Stage13_Accumulate	[pos="e,71047,17187 70990,17380 71007,17380 71018,17380 71018,17380 71018,17380 71018,17187 71018,17187 71018,17187 71037,17187 71037,\
17187"];
	Layer0_Device14_Stage13_RecvKV -> Layer0_Device14_Stage13_Attention	[pos="e,73117,17408 73117,17573 73117,17573 73117,17418 73117,17418"];
	Layer0_Device14_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71967,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage13_RecvKV -> Layer0_Device14_Stage14_RecvKV	[label="Ring transfer",
		lp="72217,17477",
		pos="e,72168,17434 72168,17520 72168,17520 72168,17444 72168,17444"];
	Layer0_Device14_Stage13_Attention -> Layer0_Device14_Stage13_Accumulate	[pos="e,71279,17214 73176,17354 73176,17337 73176,17317 73176,17317 73176,17317 71279,17317 71279,17317 71279,17317 71279,17224 71279,\
17224"];
	Layer0_Device14_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71279,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage13_Accumulate -> Layer0_Device14_Stage14_Accumulate	[pos="e,71279,17062 71279,17161 71279,17161 71279,17072 71279,17072"];
	Layer0_Device14_Stage14_RecvKV -> Layer0_Device14_Stage14_Attention	[pos="e,70758,17214 71091,17340 70902,17340 70758,17340 70758,17340 70758,17340 70758,17224 70758,17224"];
	Layer0_Device14_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72488,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device14_Stage14_RecvKV -> Layer0_Device14_Stage15_RecvKV	[label="Ring transfer",
		lp="72275,17284",
		pos="e,72228,17241 72228,17327 72228,17327 72228,17251 72228,17251"];
	Layer0_Device14_Stage14_Attention -> Layer0_Device14_Stage14_Accumulate	[pos="e,71047,17026 70872,17161 70872,17115 70872,17026 70872,17026 70872,17026 71037,17026 71037,17026"];
	Layer0_Device14_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="73093,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device14_Stage14_Accumulate -> Layer0_Device14_Stage15_Accumulate	[pos="e,72861,16909 71279,17008 71279,16971 71279,16909 71279,16909 71279,16909 72851,16909 72851,16909"];
	Layer0_Device14_Stage15_RecvKV -> Layer0_Device14_Stage15_Attention	[pos="e,73093,17062 73093,17152 73093,17152 73093,17072 73093,17072"];
	Layer0_Device14_Stage15_Attention -> Layer0_Device14_Stage15_Accumulate	[pos="e,73093,16936 73093,17008 73093,17008 73093,16946 73093,16946"];
	Layer0_Device14_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73096,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device14_Stage15_Accumulate -> Layer0_Device14_ConcatHeads	[pos="e,73096,16810 73096,16882 73096,16882 73096,16820 73096,16820"];
	Layer0_Device14_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73304,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device14_ConcatHeads -> Layer0_Device14_OutputProj	[pos="e,73206,16684 73206,16756 73206,16756 73206,16694 73206,16694"];
	Layer0_Device14_OutputProj -> Layer0_Device14_Residual1	[pos="e,73351,16558 73351,16630 73351,16630 73351,16568 73351,16568"];
	Layer0_Device14_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73388,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device14_Residual1 -> Layer0_Device14_LayerNorm2	[pos="e,73393,16432 73393,16504 73393,16504 73393,16442 73393,16442"];
	Layer0_Device14_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="73596,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device14_Residual1 -> Layer0_Device14_Residual2	[pos="e,73796,15750 73796,16504 73796,16504 73796,15760 73796,15760"];
	Layer0_Device14_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73096,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device14_LayerNorm2 -> Layer0_Device14_GateProj	[pos="e,73248,16306 73248,16378 73248,16378 73248,16316 73248,16316"];
	Layer0_Device14_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73409,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device14_LayerNorm2 -> Layer0_Device14_UpProj	[pos="e,73461,16217 73461,16378 73461,16378 73461,16227 73461,16227"];
	Layer0_Device14_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73096,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device14_GateProj -> Layer0_Device14_Activation	[pos="e,73028,16128 73028,16252 73028,16252 73028,16138 73028,16138"];
	Layer0_Device14_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73191,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device14_UpProj -> Layer0_Device14_ElemMul	[pos="e,73461,16002 73461,16163 73461,16163 73461,16012 73461,16012"];
	Layer0_Device14_Activation -> Layer0_Device14_ElemMul	[pos="e,73096,16002 73096,16074 73096,16074 73096,16012 73096,16012"];
	Layer0_Device14_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73287,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device14_ElemMul -> Layer0_Device14_DownProj	[pos="e,73287,15876 73287,15948 73287,15948 73287,15886 73287,15886"];
	Layer0_Device14_DownProj -> Layer0_Device14_Residual2	[pos="e,73358,15750 73358,15822 73358,15822 73358,15760 73358,15760"];
	Layer0_Device14_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73596,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device14_Residual2 -> Layer0_Device14_Output	[pos="e,73596,15624 73596,15696 73596,15696 73596,15634 73596,15634"];
	Layer1_Device14_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73596,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device14_Output -> Layer1_Device14_Input	[pos="e,73596,15476 73596,15548 73596,15548 73596,15486 73596,15486"];
	Layer0_Device15_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="78187,20512",
		shape=ellipse,
		width=8.3674];
	Layer0_Device15_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77885,20375",
		shape=rectangle,
		width=5.9167];
	Layer0_Device15_Input -> Layer0_Device15_LayerNorm1	[pos="e,77992,20402 77992,20483 77992,20483 77992,20412 77992,20412"];
	Layer0_Device15_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="78217,16531",
		shape=rectangle,
		width=10.736];
	Layer0_Device15_Input -> Layer0_Device15_Residual1	[pos="e,78458,16558 78458,20495 78458,20495 78458,16568 78458,16568"];
	Layer0_Device15_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77446,20249",
		shape=rectangle,
		width=18.042];
	Layer0_Device15_LayerNorm1 -> Layer0_Device15_QKVProj	[pos="e,77884,20276 77884,20348 77884,20348 77884,20286 77884,20286"];
	Layer0_Device15_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76200,20083",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage0_RecvKV	[label="Local K,V",
		lp="76700,20179",
		pos="e,76958,20136 76958,20222 76958,20222 76958,20146 76958,20146"];
	Layer0_Device15_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76990,19890",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage0_Attention	[label=Q_local,
		lp="77262,20083",
		pos="e,77171,19917 77171,20222 77171,20222 77171,19927 77171,19927"];
	Layer0_Device15_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74660,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage1_Attention	[label=Q_local,
		lp="74724,19986",
		pos="e,74645,19723 76796,20229 75969,20229 74645,20229 74645,20229 74645,20229 74645,19733 74645,19733"];
	Layer0_Device15_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75271,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage2_Attention	[label=Q_local,
		lp="77402,19890",
		pos="e,75441,19530 77266,20222 77266,20130 77266,19831 77266,19831 77266,19831 75441,19831 75441,19831 75441,19831 75441,19540 75441,\
19540"];
	Layer0_Device15_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77286,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage3_Attention	[label=Q_local,
		lp="77542,19793",
		pos="e,77459,19337 77459,20223 77459,20223 77459,19347 77459,19347"];
	Layer0_Device15_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74492,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage4_Attention	[label=Q_local,
		lp="74344,19697",
		pos="e,74344,19144 76796,20242 75887,20242 74344,20242 74344,20242 74344,20242 74344,19154 74344,19154"];
	Layer0_Device15_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77410,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage5_Attention	[label=Q_local,
		lp="77639,19600",
		pos="e,77580,18951 77580,20222 77580,20222 77580,18961 77580,18961"];
	Layer0_Device15_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77434,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage6_Attention	[label=Q_local,
		lp="77778,19504",
		pos="e,77654,18758 77654,20222 77654,20222 77654,18768 77654,18768"];
	Layer0_Device15_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74495,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage7_Attention	[label=Q_local,
		lp="74198,19407",
		pos="e,74263,18547 76796,20255 75849,20255 74198,20255 74198,20255 74198,20255 74198,18547 74198,18547 74198,18547 74253,18547 74253,\
18547"];
	Layer0_Device15_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77243,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage8_Attention	[label=Q_local,
		lp="77902,19311",
		pos="e,77475,18345 77713,20223 77713,19993 77713,18345 77713,18345 77713,18345 77485,18345 77485,18345"];
	Layer0_Device15_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74304,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage9_Attention	[label=Q_local,
		lp="74080,19214",
		pos="e,74135,18179 76796,20262 75832,20262 74135,20262 74135,20262 74135,20262 74135,18189 74135,18189"];
	Layer0_Device15_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74433,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage10_Attention	[label=Q_local,
		lp="73940,19118",
		pos="e,74201,17959 76796,20269 75805,20269 74028,20269 74028,20269 74028,20269 74028,17959 74028,17959 74028,17959 74191,17959 74191,\
17959"];
	Layer0_Device15_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75774,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage11_Attention	[label=Q_local,
		lp="78025,19021",
		pos="e,75851,17793 77808,20223 77808,19960 77808,17845 77808,17845 77808,17845 75851,17845 75851,17845 75851,17845 75851,17803 75851,\
17803"];
	Layer0_Device15_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76063,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage12_Attention	[label=Q_local,
		lp="78166,18925",
		pos="e,76034,17600 77855,20223 77855,19960 77855,17842 77855,17842 77855,17842 76034,17842 76034,17842 76034,17842 76034,17610 76034,\
17610"];
	Layer0_Device15_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="78078,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage13_Attention	[label=Q_local,
		lp="78306,18828",
		pos="e,78251,17408 78096,20240 78187,20240 78251,20240 78251,20240 78251,20240 78251,17418 78251,17418"];
	Layer0_Device15_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75660,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage14_Attention	[label=Q_local,
		lp="73794,18732",
		pos="e,75428,17187 77760,20222 77760,19962 77760,17898 77760,17898 77760,17898 75280,17898 75280,17898 75280,17898 75280,17187 75280,\
17187 75280,17187 75418,17187 75418,17187"];
	Layer0_Device15_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77542,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_QKVProj -> Layer0_Device15_Stage15_Attention	[label=Q_local,
		lp="78424,18635",
		pos="e,77774,17035 78096,20257 78249,20257 78368,20257 78368,20257 78368,20257 78368,17035 78368,17035 78368,17035 77784,17035 77784,\
17035"];
	Layer0_Device15_Stage0_RecvKV -> Layer0_Device15_Stage0_Attention	[pos="e,76939,19917 76939,20082 76939,20082 76939,19927 76939,19927"];
	Layer0_Device15_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75781,19890",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage0_RecvKV -> Layer0_Device15_Stage1_RecvKV	[label="Ring transfer",
		lp="76039,19986",
		pos="e,75990,19943 75990,20029 75990,20029 75990,19953 75990,19953"];
	Layer0_Device15_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="75181,19697",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage0_Attention -> Layer0_Device15_Stage0_Accumulate	[pos="e,75181,19723 76990,19863 76990,19848 76990,19833 76990,19833 76990,19833 75181,19833 75181,19833 75181,19833 75181,19733 75181,\
19733"];
	Layer0_Device15_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74750,19504",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage0_Accumulate -> Layer0_Device15_Stage1_Accumulate	[pos="e,74966,19530 74966,19670 74966,19670 74966,19540 74966,19540"];
	Layer0_Device15_Stage1_RecvKV -> Layer0_Device15_Stage1_Attention	[pos="e,74876,19724 74876,19889 74876,19889 74876,19734 74876,19734"];
	Layer0_Device15_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76390,19697",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage1_RecvKV -> Layer0_Device15_Stage2_RecvKV	[label="Ring transfer",
		lp="76098,19793",
		pos="e,76086,19750 76086,19836 76086,19836 76086,19760 76086,19760"];
	Layer0_Device15_Stage1_Attention -> Layer0_Device15_Stage1_Accumulate	[pos="e,74705,19530 74705,19670 74705,19670 74705,19540 74705,19540"];
	Layer0_Device15_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74868,19311",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage1_Accumulate -> Layer0_Device15_Stage2_Accumulate	[pos="e,74809,19337 74809,19477 74809,19477 74809,19347 74809,19347"];
	Layer0_Device15_Stage2_RecvKV -> Layer0_Device15_Stage2_Attention	[pos="e,75486,19531 75486,19696 75486,19696 75486,19541 75486,19541"];
	Layer0_Device15_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76480,19504",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage2_RecvKV -> Layer0_Device15_Stage3_RecvKV	[label="Ring transfer",
		lp="76483,19600",
		pos="e,76435,19557 76435,19643 76435,19643 76435,19567 76435,19567"];
	Layer0_Device15_Stage2_Attention -> Layer0_Device15_Stage2_Accumulate	[pos="e,75070,19337 75070,19477 75070,19477 75070,19347 75070,19347"];
	Layer0_Device15_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75013,19118",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage2_Accumulate -> Layer0_Device15_Stage3_Accumulate	[pos="e,74940,19144 74940,19284 74940,19284 74940,19154 74940,19154"];
	Layer0_Device15_Stage3_RecvKV -> Layer0_Device15_Stage3_Attention	[pos="e,77227,19338 77227,19503 77227,19503 77227,19348 77227,19348"];
	Layer0_Device15_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76077,19311",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage3_RecvKV -> Layer0_Device15_Stage4_RecvKV	[label="Ring transfer",
		lp="76261,19407",
		pos="e,76278,19364 76278,19450 76278,19450 76278,19374 76278,19374"];
	Layer0_Device15_Stage3_Attention -> Layer0_Device15_Stage3_Accumulate	[pos="e,75201,19144 77098,19284 77098,19267 77098,19247 77098,19247 77098,19247 75201,19247 75201,19247 75201,19247 75201,19154 75201,\
19154"];
	Layer0_Device15_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74992,18925",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage3_Accumulate -> Layer0_Device15_Stage4_Accumulate	[pos="e,75002,18951 75002,19091 75002,19091 75002,18961 75002,18961"];
	Layer0_Device15_Stage4_RecvKV -> Layer0_Device15_Stage4_Attention	[pos="e,74680,19144 75201,19270 74920,19270 74680,19270 74680,19270 74680,19270 74680,19154 74680,19154"];
	Layer0_Device15_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76222,19118",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage4_RecvKV -> Layer0_Device15_Stage5_RecvKV	[label="Ring transfer",
		lp="76197,19214",
		pos="e,76150,19171 76150,19257 76150,19257 76150,19181 76150,19181"];
	Layer0_Device15_Stage4_Attention -> Layer0_Device15_Stage4_Accumulate	[pos="e,74771,18951 74724,19117 74752,19117 74771,19117 74771,19117 74771,19117 74771,18961 74771,18961"];
	Layer0_Device15_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76913,18732",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage4_Accumulate -> Layer0_Device15_Stage5_Accumulate	[pos="e,76901,18759 75004,18898 75004,18868 75004,18823 75004,18823 75004,18823 76901,18823 76901,18823 76901,18823 76901,18769 76901,\
18769"];
	Layer0_Device15_Stage5_RecvKV -> Layer0_Device15_Stage5_Attention	[pos="e,77178,18924 77131,19117 77131,19112 77131,18924 77131,18924 77131,18924 77168,18924 77168,18924"];
	Layer0_Device15_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="76201,18925",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage5_RecvKV -> Layer0_Device15_Stage6_RecvKV	[label="Ring transfer",
		lp="76251,19021",
		pos="e,76212,18978 76212,19064 76212,19064 76212,18988 76212,18988"];
	Layer0_Device15_Stage5_Attention -> Layer0_Device15_Stage5_Accumulate	[pos="e,77145,18731 77190,18898 77190,18845 77190,18731 77190,18731 77190,18731 77155,18731 77155,18731"];
	Layer0_Device15_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76913,18539",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage5_Accumulate -> Layer0_Device15_Stage6_Accumulate	[pos="e,76913,18565 76913,18705 76913,18705 76913,18575 76913,18575"];
	Layer0_Device15_Stage6_RecvKV -> Layer0_Device15_Stage6_Attention	[pos="e,77422,18758 76789,18884 77101,18884 77422,18884 77422,18884 77422,18884 77422,18768 77422,18768"];
	Layer0_Device15_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75704,18732",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage6_RecvKV -> Layer0_Device15_Stage7_RecvKV	[label="Ring transfer",
		lp="76001,18828",
		pos="e,75952,18785 75952,18871 75952,18871 75952,18795 75952,18795"];
	Layer0_Device15_Stage6_Attention -> Layer0_Device15_Stage6_Accumulate	[pos="e,77145,18538 77338,18705 77338,18652 77338,18538 77338,18538 77338,18538 77155,18538 77155,18538"];
	Layer0_Device15_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76722,18346",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage6_Accumulate -> Layer0_Device15_Stage7_Accumulate	[pos="e,76818,18372 76818,18512 76818,18512 76818,18382 76818,18382"];
	Layer0_Device15_Stage7_RecvKV -> Layer0_Device15_Stage7_Attention	[pos="e,74494,18565 74828,18691 74638,18691 74494,18691 74494,18691 74494,18691 74494,18575 74494,18575"];
	Layer0_Device15_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75704,18539",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage7_RecvKV -> Layer0_Device15_Stage8_RecvKV	[label="Ring transfer",
		lp="75752,18635",
		pos="e,75704,18592 75704,18678 75704,18678 75704,18602 75704,18602"];
	Layer0_Device15_Stage7_Attention -> Layer0_Device15_Stage7_Accumulate	[pos="e,76557,18372 74660,18512 74660,18487 74660,18454 74660,18454 74660,18454 76557,18454 76557,18454 76557,18454 76557,18382 76557,\
18382"];
	Layer0_Device15_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76722,18153",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage7_Accumulate -> Layer0_Device15_Stage8_Accumulate	[pos="e,76722,18179 76722,18319 76722,18319 76722,18189 76722,18189"];
	Layer0_Device15_Stage8_RecvKV -> Layer0_Device15_Stage8_Attention	[pos="e,77078,18372 76292,18498 76665,18498 77078,18498 77078,18498 77078,18498 77078,18382 77078,18382"];
	Layer0_Device15_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75513,18346",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage8_RecvKV -> Layer0_Device15_Stage9_RecvKV	[label="Ring transfer",
		lp="75717,18442",
		pos="e,75608,18399 75608,18485 75608,18485 75608,18409 75608,18409"];
	Layer0_Device15_Stage8_Attention -> Layer0_Device15_Stage8_Accumulate	[pos="e,76954,18152 77047,18319 77047,18266 77047,18152 77047,18152 77047,18152 76964,18152 76964,18152"];
	Layer0_Device15_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74954,17960",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage8_Accumulate -> Layer0_Device15_Stage9_Accumulate	[pos="e,75031,17986 76722,18126 76722,18103 76722,18074 76722,18074 76722,18074 75031,18074 75031,18074 75031,18074 75031,17996 75031,\
17996"];
	Layer0_Device15_Stage9_RecvKV -> Layer0_Device15_Stage9_Attention	[pos="e,74400,18179 74637,18305 74498,18305 74400,18305 74400,18305 74400,18305 74400,18189 74400,18189"];
	Layer0_Device15_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75513,18153",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage9_RecvKV -> Layer0_Device15_Stage10_RecvKV	[label="Ring transfer",
		lp="75561,18249",
		pos="e,75513,18206 75513,18292 75513,18292 75513,18216 75513,18216"];
	Layer0_Device15_Stage9_Attention -> Layer0_Device15_Stage9_Accumulate	[pos="e,74877,17986 74368,18126 74368,18087 74368,18018 74368,18018 74368,18018 74877,18018 74877,18018 74877,18018 74877,17996 74877,\
17996"];
	Layer0_Device15_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74954,17767",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage9_Accumulate -> Layer0_Device15_Stage10_Accumulate	[pos="e,74954,17793 74954,17933 74954,17933 74954,17803 74954,17803"];
	Layer0_Device15_Stage10_RecvKV -> Layer0_Device15_Stage10_Attention	[pos="e,74629,17987 74629,18152 74629,18152 74629,17997 74629,17997"];
	Layer0_Device15_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76163,17960",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage10_RecvKV -> Layer0_Device15_Stage11_RecvKV	[label="Ring transfer",
		lp="75830,18056",
		pos="e,75838,18013 75838,18099 75838,18099 75838,18023 75838,18023"];
	Layer0_Device15_Stage10_Attention -> Layer0_Device15_Stage10_Accumulate	[pos="e,74722,17766 74656,17933 74656,17880 74656,17766 74656,17766 74656,17766 74712,17766 74712,17766"];
	Layer0_Device15_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75542,17574",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage10_Accumulate -> Layer0_Device15_Stage11_Accumulate	[pos="e,75426,17600 75186,17766 75305,17766 75426,17766 75426,17766 75426,17766 75426,17610 75426,17610"];
	Layer0_Device15_Stage11_RecvKV -> Layer0_Device15_Stage11_Attention	[pos="e,75697,17793 75697,17906 75697,17906 75697,17803 75697,17803"];
	Layer0_Device15_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76983,17767",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage11_RecvKV -> Layer0_Device15_Stage12_RecvKV	[label="Ring transfer",
		lp="76668,17863",
		pos="e,76573,17820 76573,17906 76573,17906 76573,17830 76573,17830"];
	Layer0_Device15_Stage11_Attention -> Layer0_Device15_Stage11_Accumulate	[pos="e,75658,17600 75658,17740 75658,17740 75658,17610 75658,17610"];
	Layer0_Device15_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75660,17381",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage11_Accumulate -> Layer0_Device15_Stage12_Accumulate	[pos="e,75601,17407 75601,17547 75601,17547 75601,17417 75601,17417"];
	Layer0_Device15_Stage12_RecvKV -> Layer0_Device15_Stage12_Attention	[pos="e,76179,17600 76179,17713 76179,17713 76179,17610 76179,17610"];
	Layer0_Device15_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77272,17574",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage12_RecvKV -> Layer0_Device15_Stage13_RecvKV	[label="Ring transfer",
		lp="77175,17670",
		pos="e,77128,17627 77128,17713 77128,17713 77128,17637 77128,17637"];
	Layer0_Device15_Stage12_Attention -> Layer0_Device15_Stage12_Accumulate	[pos="e,75862,17407 75862,17547 75862,17547 75862,17417 75862,17417"];
	Layer0_Device15_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76181,17188",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage12_Accumulate -> Layer0_Device15_Stage13_Accumulate	[pos="e,75949,17187 75892,17380 75909,17380 75920,17380 75920,17380 75920,17380 75920,17187 75920,17187 75920,17187 75939,17187 75939,\
17187"];
	Layer0_Device15_Stage13_RecvKV -> Layer0_Device15_Stage13_Attention	[pos="e,78019,17408 78019,17573 78019,17573 78019,17418 78019,17418"];
	Layer0_Device15_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76869,17381",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage13_RecvKV -> Layer0_Device15_Stage14_RecvKV	[label="Ring transfer",
		lp="77118,17477",
		pos="e,77070,17434 77070,17520 77070,17520 77070,17444 77070,17444"];
	Layer0_Device15_Stage13_Attention -> Layer0_Device15_Stage13_Accumulate	[pos="e,76181,17214 78078,17354 78078,17339 78078,17322 78078,17322 78078,17322 76181,17322 76181,17322 76181,17322 76181,17224 76181,\
17224"];
	Layer0_Device15_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76181,17035",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage13_Accumulate -> Layer0_Device15_Stage14_Accumulate	[pos="e,76181,17062 76181,17161 76181,17161 76181,17072 76181,17072"];
	Layer0_Device15_Stage14_RecvKV -> Layer0_Device15_Stage14_Attention	[pos="e,75660,17214 75993,17340 75804,17340 75660,17340 75660,17340 75660,17340 75660,17224 75660,17224"];
	Layer0_Device15_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77390,17188",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer0_Device15_Stage14_RecvKV -> Layer0_Device15_Stage15_RecvKV	[label="Ring transfer",
		lp="77096,17284",
		pos="e,77130,17241 77130,17327 77130,17327 77130,17251 77130,17251"];
	Layer0_Device15_Stage14_Attention -> Layer0_Device15_Stage14_Accumulate	[pos="e,75949,17026 75780,17161 75780,17115 75780,17026 75780,17026 75780,17026 75939,17026 75939,17026"];
	Layer0_Device15_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="77542,16909",
		shape=rectangle,
		width=6.4306];
	Layer0_Device15_Stage14_Accumulate -> Layer0_Device15_Stage15_Accumulate	[pos="e,77310,16909 76181,17008 76181,16971 76181,16909 76181,16909 76181,16909 77300,16909 77300,16909"];
	Layer0_Device15_Stage15_RecvKV -> Layer0_Device15_Stage15_Attention	[pos="e,77542,17062 77542,17134 77542,17134 77542,17072 77542,17072"];
	Layer0_Device15_Stage15_Attention -> Layer0_Device15_Stage15_Accumulate	[pos="e,77542,16936 77542,17008 77542,17008 77542,16946 77542,16946"];
	Layer0_Device15_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77796,16783",
		shape=rectangle,
		width=6.2639];
	Layer0_Device15_Stage15_Accumulate -> Layer0_Device15_ConcatHeads	[pos="e,77672,16810 77672,16882 77672,16882 77672,16820 77672,16820"];
	Layer0_Device15_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="78214,16657",
		shape=rectangle,
		width=5.9167];
	Layer0_Device15_ConcatHeads -> Layer0_Device15_OutputProj	[pos="e,78011,16684 78011,16756 78011,16756 78011,16694 78011,16694"];
	Layer0_Device15_OutputProj -> Layer0_Device15_Residual1	[pos="e,78214,16558 78214,16630 78214,16630 78214,16568 78214,16568"];
	Layer0_Device15_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77923,16405",
		shape=rectangle,
		width=5.9167];
	Layer0_Device15_Residual1 -> Layer0_Device15_LayerNorm2	[pos="e,77983,16432 77983,16504 77983,16504 77983,16442 77983,16442"];
	Layer0_Device15_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="77925,15723",
		shape=rectangle,
		width=10.736];
	Layer0_Device15_Residual1 -> Layer0_Device15_Residual2	[pos="e,78240,15750 78240,16504 78240,16504 78240,15760 78240,15760"];
	Layer0_Device15_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77631,16279",
		shape=rectangle,
		width=6.25];
	Layer0_Device15_LayerNorm2 -> Layer0_Device15_GateProj	[pos="e,77783,16306 77783,16378 77783,16378 77783,16316 77783,16316"];
	Layer0_Device15_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77944,16190",
		shape=rectangle,
		width=6.25];
	Layer0_Device15_LayerNorm2 -> Layer0_Device15_UpProj	[pos="e,77996,16217 77996,16378 77996,16378 77996,16227 77996,16227"];
	Layer0_Device15_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77631,16101",
		shape=rectangle,
		width=6.25];
	Layer0_Device15_GateProj -> Layer0_Device15_Activation	[pos="e,77562,16128 77562,16252 77562,16252 77562,16138 77562,16138"];
	Layer0_Device15_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77726,15975",
		shape=rectangle,
		width=11.403];
	Layer0_Device15_UpProj -> Layer0_Device15_ElemMul	[pos="e,77996,16002 77996,16163 77996,16163 77996,16012 77996,16012"];
	Layer0_Device15_Activation -> Layer0_Device15_ElemMul	[pos="e,77631,16002 77631,16074 77631,16074 77631,16012 77631,16012"];
	Layer0_Device15_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77915,15849",
		shape=rectangle,
		width=6.0833];
	Layer0_Device15_ElemMul -> Layer0_Device15_DownProj	[pos="e,77915,15876 77915,15948 77915,15948 77915,15886 77915,15886"];
	Layer0_Device15_DownProj -> Layer0_Device15_Residual2	[pos="e,77915,15750 77915,15822 77915,15822 77915,15760 77915,15760"];
	Layer0_Device15_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 0 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77925,15586",
		shape=ellipse,
		width=8.3674];
	Layer0_Device15_Residual2 -> Layer0_Device15_Output	[pos="e,77925,15624 77925,15696 77925,15696 77925,15634 77925,15634"];
	Layer1_Device15_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77925,15438",
		shape=ellipse,
		width=8.3674];
	Layer0_Device15_Output -> Layer1_Device15_Input	[pos="e,77925,15476 77925,15548 77925,15548 77925,15486 77925,15486"];
	Layer1_Device0_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4259,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device0_Input -> Layer1_Device0_LayerNorm1	[pos="e,4450.9,15328 4450.9,15424 4450.9,15424 4450.9,15338 4450.9,15338"];
	Layer1_Device0_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="4721,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device0_Input -> Layer1_Device0_Residual1	[pos="e,4834.1,11484 4834.1,15403 4834.1,15403 4834.1,11494 4834.1,11494"];
	Layer1_Device0_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3678,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device0_LayerNorm1 -> Layer1_Device0_QKVProj	[pos="e,4186.8,15202 4186.8,15274 4186.8,15274 4186.8,15212 4186.8,15212"];
	Layer1_Device0_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2432,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage0_RecvKV	[label="Local K,V",
		lp="2932.5,15105",
		pos="e,3190.1,15062 3190.1,15148 3190.1,15148 3190.1,15072 3190.1,15072"];
	Layer1_Device0_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3185,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage0_Attention	[label=Q_local,
		lp="3494,15009",
		pos="e,3384.1,14843 3384.1,15148 3384.1,15148 3384.1,14853 3384.1,14853"];
	Layer1_Device0_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="892,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage1_Attention	[label=Q_local,
		lp="956,14912",
		pos="e,1041.4,14650 3028.4,15157 2247.7,15157 1041.4,15157 1041.4,15157 1041.4,15157 1041.4,14660 1041.4,14660"];
	Layer1_Device0_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1503,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage2_Attention	[label=Q_local,
		lp="3634,14816",
		pos="e,1673.4,14456 3479.1,15148 3479.1,15045 3479.1,14678 3479.1,14678 3479.1,14678 1673.4,14678 1673.4,14678 1673.4,14678 1673.4,14466 \
1673.4,14466"];
	Layer1_Device0_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3518,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage3_Attention	[label=Q_local,
		lp="3774,14719",
		pos="e,3690.6,14264 3690.6,15149 3690.6,15149 3690.6,14274 3690.6,14274"];
	Layer1_Device0_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="724,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage4_Attention	[label=Q_local,
		lp="576,14623",
		pos="e,612,14070 3028.2,15166 2128.3,15166 612,15166 612,15166 612,15166 612,14080 612,14080"];
	Layer1_Device0_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3647,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage5_Attention	[label=Q_local,
		lp="3878,14526",
		pos="e,3814,13877 3814,15148 3814,15148 3814,13887 3814,13887"];
	Layer1_Device0_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3732,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage6_Attention	[label=Q_local,
		lp="4010,14430",
		pos="e,3921,13684 3921,15149 3921,15149 3921,13694 3921,13694"];
	Layer1_Device0_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="793,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage7_Attention	[label=Q_local,
		lp="430,14333",
		pos="e,561.17,13464 3028.4,15175 2091.7,15175 472.83,15175 472.83,15175 472.83,15175 472.83,13464 472.83,13464 472.83,13464 551.17,13464 \
551.17,13464"];
	Layer1_Device0_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3528,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage8_Attention	[label=Q_local,
		lp="4139,14237",
		pos="e,3759.5,13271 3997.3,15149 3997.3,14919 3997.3,13271 3997.3,13271 3997.3,13271 3769.5,13271 3769.5,13271"];
	Layer1_Device0_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="589,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage9_Attention	[label=Q_local,
		lp="312,14140",
		pos="e,453.17,13105 3028.3,15184 2086.5,15184 453.17,15184 453.17,15184 453.17,15184 453.17,13115 453.17,13115"];
	Layer1_Device0_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="797,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage10_Attention	[label=Q_local,
		lp="172,14044",
		pos="e,565.23,12885 3028.4,15192 2057.2,15192 339.5,15192 339.5,15192 339.5,15192 339.5,12885 339.5,12885 339.5,12885 555.23,12885 555.23,\
12885"];
	Layer1_Device0_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1970,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage11_Attention	[label=Q_local,
		lp="4262,13947",
		pos="e,2047.2,12719 4031.1,15149 4031.1,14889 4031.1,12810 4031.1,12810 4031.1,12810 2047.2,12810 2047.2,12810 2047.2,12810 2047.2,12729 \
2047.2,12729"];
	Layer1_Device0_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2300,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage12_Attention	[label=Q_local,
		lp="4403,13851",
		pos="e,2230.4,12526 4064.8,15149 4064.8,14887 4064.8,12797 4064.8,12797 4064.8,12797 2230.4,12797 2230.4,12797 2230.4,12797 2230.4,12536 \
2230.4,12536"];
	Layer1_Device0_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4315,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage13_Attention	[label=Q_local,
		lp="4543,13754",
		pos="e,4450.3,12334 4327.5,15160 4401.1,15160 4450.3,15160 4450.3,15160 4450.3,15160 4450.3,12344 4450.3,12344"];
	Layer1_Device0_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1897,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage14_Attention	[label=Q_local,
		lp="26,13658",
		pos="e,1974.2,12140 4327.7,15172 4455.8,15172 4551.6,15172 4551.6,15172 4551.6,15172 4551.6,12233 4551.6,12233 4551.6,12233 1974.2,12233 \
1974.2,12233 1974.2,12233 1974.2,12150 1974.2,12150"];
	Layer1_Device0_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4030,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_QKVProj -> Layer1_Device0_Stage15_Attention	[label=Q_local,
		lp="4661,13561",
		pos="e,4261.7,11970 4327.7,15178 4458.4,15178 4556.5,15178 4556.5,15178 4556.5,15178 4556.5,11970 4556.5,11970 4556.5,11970 4271.7,11970 \
4271.7,11970"];
	Layer1_Device0_Stage0_RecvKV -> Layer1_Device0_Stage0_Attention	[pos="e,3152.6,14842 3152.6,15005 3152.6,15005 3152.6,14852 3152.6,14852"];
	Layer1_Device0_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="1976,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage0_RecvKV -> Layer1_Device0_Stage1_RecvKV	[label="Ring transfer",
		lp="2333,14912",
		pos="e,2204,14869 2204,14955 2204,14955 2204,14879 2204,14879"];
	Layer1_Device0_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="1413,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage0_Attention -> Layer1_Device0_Stage0_Accumulate	[pos="e,1413,14649 3185,14789 3185,14749 3185,14680 3185,14680 3185,14680 1413,14680 1413,14680 1413,14680 1413,14659 1413,14659"];
	Layer1_Device0_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="982,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage0_Accumulate -> Layer1_Device0_Stage1_Accumulate	[pos="e,1197.5,14456 1197.5,14596 1197.5,14596 1197.5,14466 1197.5,14466"];
	Layer1_Device0_Stage1_RecvKV -> Layer1_Device0_Stage1_Attention	[pos="e,1089.9,14650 1089.9,14816 1089.9,14816 1089.9,14660 1089.9,14660"];
	Layer1_Device0_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2622,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage1_RecvKV -> Layer1_Device0_Stage2_RecvKV	[label="Ring transfer",
		lp="2293,14719",
		pos="e,2299,14676 2299,14762 2299,14762 2299,14686 2299,14686"];
	Layer1_Device0_Stage1_Attention -> Layer1_Device0_Stage1_Accumulate	[pos="e,937,14456 937,14596 937,14596 937,14466 937,14466"];
	Layer1_Device0_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1100,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage1_Accumulate -> Layer1_Device0_Stage2_Accumulate	[pos="e,1041,14263 1041,14403 1041,14403 1041,14273 1041,14273"];
	Layer1_Device0_Stage2_RecvKV -> Layer1_Device0_Stage2_Attention	[pos="e,1718.4,14457 1718.4,14623 1718.4,14623 1718.4,14467 1718.4,14467"];
	Layer1_Device0_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2712,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage2_RecvKV -> Layer1_Device0_Stage3_RecvKV	[label="Ring transfer",
		lp="2715,14526",
		pos="e,2667,14483 2667,14569 2667,14569 2667,14493 2667,14493"];
	Layer1_Device0_Stage2_Attention -> Layer1_Device0_Stage2_Accumulate	[pos="e,1301.5,14263 1301.5,14403 1301.5,14403 1301.5,14273 1301.5,14273"];
	Layer1_Device0_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1245,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage2_Accumulate -> Layer1_Device0_Stage3_Accumulate	[pos="e,1172.5,14070 1172.5,14210 1172.5,14210 1172.5,14080 1172.5,14080"];
	Layer1_Device0_Stage3_RecvKV -> Layer1_Device0_Stage3_Attention	[pos="e,3459.1,14264 3459.1,14430 3459.1,14430 3459.1,14274 3459.1,14274"];
	Layer1_Device0_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2309,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage3_RecvKV -> Layer1_Device0_Stage4_RecvKV	[label="Ring transfer",
		lp="2493,14333",
		pos="e,2510.5,14290 2510.5,14376 2510.5,14376 2510.5,14300 2510.5,14300"];
	Layer1_Device0_Stage3_Attention -> Layer1_Device0_Stage3_Accumulate	[pos="e,1432.9,14070 3330.1,14210 3330.1,14170 3330.1,14101 3330.1,14101 3330.1,14101 1432.9,14101 1432.9,14101 1432.9,14101 1432.9,14080 \
1432.9,14080"];
	Layer1_Device0_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1229,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage3_Accumulate -> Layer1_Device0_Stage4_Accumulate	[pos="e,1237,13877 1237,14017 1237,14017 1237,13887 1237,13887"];
	Layer1_Device0_Stage4_RecvKV -> Layer1_Device0_Stage4_Attention	[pos="e,912,14070 1432.7,14196 1151.8,14196 912,14196 912,14196 912,14196 912,14080 912,14080"];
	Layer1_Device0_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2454,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage4_RecvKV -> Layer1_Device0_Stage5_RecvKV	[label="Ring transfer",
		lp="2429,14140",
		pos="e,2381.5,14097 2381.5,14183 2381.5,14183 2381.5,14107 2381.5,14107"];
	Layer1_Device0_Stage4_Attention -> Layer1_Device0_Stage4_Accumulate	[pos="e,1005.5,13877 955.82,14043 985.35,14043 1005.5,14043 1005.5,14043 1005.5,14043 1005.5,13887 1005.5,13887"];
	Layer1_Device0_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3211,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage4_Accumulate -> Layer1_Device0_Stage5_Accumulate	[pos="e,3168.6,13684 1271.4,13824 1271.4,13799 1271.4,13767 1271.4,13767 1271.4,13767 3168.6,13767 3168.6,13767 3168.6,13767 3168.6,13694 \
3168.6,13694"];
	Layer1_Device0_Stage5_RecvKV -> Layer1_Device0_Stage5_Attention	[pos="e,3415.3,13850 3365.6,14044 3365.6,14038 3365.6,13850 3365.6,13850 3365.6,13850 3405.3,13850 3405.3,13850"];
	Layer1_Device0_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2438,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage5_RecvKV -> Layer1_Device0_Stage6_RecvKV	[label="Ring transfer",
		lp="2494,13947",
		pos="e,2446,13904 2446,13990 2446,13990 2446,13914 2446,13914"];
	Layer1_Device0_Stage5_Attention -> Layer1_Device0_Stage5_Accumulate	[pos="e,3429,13684 3429,13824 3429,13824 3429,13694 3429,13694"];
	Layer1_Device0_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3211,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage5_Accumulate -> Layer1_Device0_Stage6_Accumulate	[pos="e,3211,13491 3211,13631 3211,13631 3211,13501 3211,13501"];
	Layer1_Device0_Stage6_RecvKV -> Layer1_Device0_Stage6_Attention	[pos="e,3689.5,13684 3025.3,13810 3350.4,13810 3689.5,13810 3689.5,13810 3689.5,13810 3689.5,13694 3689.5,13694"];
	Layer1_Device0_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2002,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage6_RecvKV -> Layer1_Device0_Stage7_RecvKV	[label="Ring transfer",
		lp="2299,13754",
		pos="e,2220,13711 2220,13797 2220,13797 2220,13721 2220,13721"];
	Layer1_Device0_Stage6_Attention -> Layer1_Device0_Stage6_Accumulate	[pos="e,3442.6,13455 3630,13631 3630,13576 3630,13455 3630,13455 3630,13455 3452.6,13455 3452.6,13455"];
	Layer1_Device0_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3007,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage6_Accumulate -> Layer1_Device0_Stage7_Accumulate	[pos="e,3109,13298 3109,13438 3109,13438 3109,13308 3109,13308"];
	Layer1_Device0_Stage7_RecvKV -> Layer1_Device0_Stage7_Attention	[pos="e,1011,13491 1267.8,13657 1121.1,13657 1011,13657 1011,13657 1011,13657 1011,13501 1011,13501"];
	Layer1_Device0_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2002,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage7_RecvKV -> Layer1_Device0_Stage8_RecvKV	[label="Ring transfer",
		lp="2050,13561",
		pos="e,2002,13518 2002,13604 2002,13604 2002,13528 2002,13528"];
	Layer1_Device0_Stage7_Attention -> Layer1_Device0_Stage7_Accumulate	[pos="e,2848.6,13299 951.45,13438 951.45,13408 951.45,13365 951.45,13365 951.45,13365 2848.6,13365 2848.6,13365 2848.6,13365 2848.6,13309 \
2848.6,13309"];
	Layer1_Device0_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3007,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage7_Accumulate -> Layer1_Device0_Stage8_Accumulate	[pos="e,3007,13105 3007,13245 3007,13245 3007,13115 3007,13115"];
	Layer1_Device0_Stage8_RecvKV -> Layer1_Device0_Stage8_Attention	[pos="e,3369.5,13298 2589.4,13424 2960.2,13424 3369.5,13424 3369.5,13424 3369.5,13424 3369.5,13308 3369.5,13308"];
	Layer1_Device0_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="1798,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage8_RecvKV -> Layer1_Device0_Stage9_RecvKV	[label="Ring transfer",
		lp="1882,13368",
		pos="e,1900,13325 1900,13411 1900,13411 1900,13335 1900,13335"];
	Layer1_Device0_Stage8_Attention -> Layer1_Device0_Stage8_Accumulate	[pos="e,3238.6,13078 3371.6,13245 3371.6,13192 3371.6,13078 3371.6,13078 3371.6,13078 3248.6,13078 3248.6,13078"];
	Layer1_Device0_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1318,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage8_Accumulate -> Layer1_Device0_Stage9_Accumulate	[pos="e,1395.2,12912 3007,13052 3007,13025 3007,12987 3007,12987 3007,12987 1395.2,12987 1395.2,12987 1395.2,12987 1395.2,12922 1395.2,\
12922"];
	Layer1_Device0_Stage9_RecvKV -> Layer1_Device0_Stage9_Attention	[pos="e,691,13105 1063.9,13271 858.86,13271 691,13271 691,13271 691,13271 691,13115 691,13115"];
	Layer1_Device0_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="1798,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage9_RecvKV -> Layer1_Device0_Stage10_RecvKV	[label="Ring transfer",
		lp="1846,13175",
		pos="e,1798,13132 1798,13218 1798,13218 1798,13142 1798,13142"];
	Layer1_Device0_Stage9_Attention -> Layer1_Device0_Stage9_Accumulate	[pos="e,1240.8,12913 693,13052 693,13008 693,12925 693,12925 693,12925 1240.8,12925 1240.8,12925 1240.8,12925 1240.8,12923 1240.8,12923"];
	Layer1_Device0_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1318,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage9_Accumulate -> Layer1_Device0_Stage10_Accumulate	[pos="e,1318,12719 1318,12859 1318,12859 1318,12729 1318,12729"];
	Layer1_Device0_Stage10_RecvKV -> Layer1_Device0_Stage10_Attention	[pos="e,953.45,12913 953.45,13079 953.45,13079 953.45,12923 953.45,12923"];
	Layer1_Device0_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2527,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage10_RecvKV -> Layer1_Device0_Stage11_RecvKV	[label="Ring transfer",
		lp="2115,12982",
		pos="e,2162.5,12939 2162.5,13025 2162.5,13025 2162.5,12949 2162.5,12949"];
	Layer1_Device0_Stage10_Attention -> Layer1_Device0_Stage10_Accumulate	[pos="e,1086.4,12692 900,12859 900,12806 900,12692 900,12692 900,12692 1076.4,12692 1076.4,12692"];
	Layer1_Device0_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1779,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage10_Accumulate -> Layer1_Device0_Stage11_Accumulate	[pos="e,1578.4,12526 1549.6,12692 1567.3,12692 1578.4,12692 1578.4,12692 1578.4,12692 1578.4,12536 1578.4,12536"];
	Layer1_Device0_Stage11_RecvKV -> Layer1_Device0_Stage11_Attention	[pos="e,1892.8,12719 1892.8,12832 1892.8,12832 1892.8,12729 1892.8,12729"];
	Layer1_Device0_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3179,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage11_RecvKV -> Layer1_Device0_Stage12_RecvKV	[label="Ring transfer",
		lp="2967,12789",
		pos="e,2853,12746 2853,12832 2853,12832 2853,12756 2853,12756"];
	Layer1_Device0_Stage11_Attention -> Layer1_Device0_Stage11_Accumulate	[pos="e,1874.5,12526 1874.5,12666 1874.5,12666 1874.5,12536 1874.5,12536"];
	Layer1_Device0_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1897,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage11_Accumulate -> Layer1_Device0_Stage12_Accumulate	[pos="e,1838,12333 1838,12473 1838,12473 1838,12343 1838,12343"];
	Layer1_Device0_Stage12_RecvKV -> Layer1_Device0_Stage12_Attention	[pos="e,2395.4,12526 2395.4,12639 2395.4,12639 2395.4,12536 2395.4,12536"];
	Layer1_Device0_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3509,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage12_RecvKV -> Layer1_Device0_Stage13_RecvKV	[label="Ring transfer",
		lp="3487,12596",
		pos="e,3344,12553 3344,12639 3344,12639 3344,12563 3344,12563"];
	Layer1_Device0_Stage12_Attention -> Layer1_Device0_Stage12_Accumulate	[pos="e,2098.5,12333 2098.5,12473 2098.5,12473 2098.5,12343 2098.5,12343"];
	Layer1_Device0_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2418,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage12_Accumulate -> Layer1_Device0_Stage13_Accumulate	[pos="e,2186.3,12113 2128.6,12306 2146.3,12306 2157.4,12306 2157.4,12306 2157.4,12306 2157.4,12113 2157.4,12113 2157.4,12113 2176.3,12113 \
2176.3,12113"];
	Layer1_Device0_Stage13_RecvKV -> Layer1_Device0_Stage13_Attention	[pos="e,4256.1,12334 4256.1,12500 4256.1,12500 4256.1,12344 4256.1,12344"];
	Layer1_Device0_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3106,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage13_RecvKV -> Layer1_Device0_Stage14_RecvKV	[label="Ring transfer",
		lp="3290,12403",
		pos="e,3307.5,12360 3307.5,12446 3307.5,12446 3307.5,12370 3307.5,12370"];
	Layer1_Device0_Stage13_Attention -> Layer1_Device0_Stage13_Accumulate	[pos="e,2418,12141 4315,12280 4315,12240 4315,12170 4315,12170 4315,12170 2418,12170 2418,12170 2418,12170 2418,12151 2418,12151"];
	Layer1_Device0_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2418,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage13_Accumulate -> Layer1_Device0_Stage14_Accumulate	[pos="e,2418,11988 2418,12087 2418,12087 2418,11998 2418,11998"];
	Layer1_Device0_Stage14_RecvKV -> Layer1_Device0_Stage14_Attention	[pos="e,1819.8,12140 2229.9,12266 2002.4,12266 1819.8,12266 1819.8,12266 1819.8,12266 1819.8,12150 1819.8,12150"];
	Layer1_Device0_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3627,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device0_Stage14_RecvKV -> Layer1_Device0_Stage15_RecvKV	[label="Ring transfer",
		lp="3414,12210",
		pos="e,3366.5,12167 3366.5,12253 3366.5,12253 3366.5,12177 3366.5,12177"];
	Layer1_Device0_Stage14_Attention -> Layer1_Device0_Stage14_Accumulate	[pos="e,2186.4,11961 1897,12087 1897,12043 1897,11961 1897,11961 1897,11961 2176.4,11961 2176.4,11961"];
	Layer1_Device0_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="4030,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device0_Stage14_Accumulate -> Layer1_Device0_Stage15_Accumulate	[pos="e,3798.2,11835 2418,11934 2418,11897 2418,11835 2418,11835 2418,11835 3788.2,11835 3788.2,11835"];
	Layer1_Device0_Stage15_RecvKV -> Layer1_Device0_Stage15_Attention	[pos="e,4030,11988 4030,12060 4030,12060 4030,11998 4030,11998"];
	Layer1_Device0_Stage15_Attention -> Layer1_Device0_Stage15_Accumulate	[pos="e,4030,11862 4030,11934 4030,11934 4030,11872 4030,11872"];
	Layer1_Device0_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4336,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device0_Stage15_Accumulate -> Layer1_Device0_ConcatHeads	[pos="e,4186,11736 4186,11808 4186,11808 4186,11746 4186,11746"];
	Layer1_Device0_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4423,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device0_ConcatHeads -> Layer1_Device0_OutputProj	[pos="e,4385.8,11610 4385.8,11682 4385.8,11682 4385.8,11620 4385.8,11620"];
	Layer1_Device0_OutputProj -> Layer1_Device0_Residual1	[pos="e,4485.2,11484 4485.2,11556 4485.2,11556 4485.2,11494 4485.2,11494"];
	Layer1_Device0_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4498,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device0_Residual1 -> Layer1_Device0_LayerNorm2	[pos="e,4522.8,11358 4522.8,11430 4522.8,11430 4522.8,11368 4522.8,11368"];
	Layer1_Device0_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="4792,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device0_Residual1 -> Layer1_Device0_Residual2	[pos="e,4925.8,10676 4925.8,11430 4925.8,11430 4925.8,10686 4925.8,10686"];
	Layer1_Device0_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4206,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device0_LayerNorm2 -> Layer1_Device0_GateProj	[pos="e,4358,11232 4358,11304 4358,11304 4358,11242 4358,11242"];
	Layer1_Device0_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4519,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device0_LayerNorm2 -> Layer1_Device0_UpProj	[pos="e,4571,11143 4571,11304 4571,11304 4571,11153 4571,11153"];
	Layer1_Device0_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4206,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device0_GateProj -> Layer1_Device0_Activation	[pos="e,4137.5,11054 4137.5,11179 4137.5,11179 4137.5,11064 4137.5,11064"];
	Layer1_Device0_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4301,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device0_UpProj -> Layer1_Device0_ElemMul	[pos="e,4571.2,10928 4571.2,11089 4571.2,11089 4571.2,10938 4571.2,10938"];
	Layer1_Device0_Activation -> Layer1_Device0_ElemMul	[pos="e,4206,10928 4206,11000 4206,11000 4206,10938 4206,10938"];
	Layer1_Device0_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4444,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device0_ElemMul -> Layer1_Device0_DownProj	[pos="e,4444,10802 4444,10874 4444,10874 4444,10812 4444,10812"];
	Layer1_Device0_DownProj -> Layer1_Device0_Residual2	[pos="e,4534.2,10676 4534.2,10748 4534.2,10748 4534.2,10686 4534.2,10686"];
	Layer1_Device0_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4792,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device0_Residual2 -> Layer1_Device0_Output	[pos="e,4792,10550 4792,10622 4792,10622 4792,10560 4792,10560"];
	Layer2_Device0_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4792,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device0_Output -> Layer2_Device0_Input	[pos="e,4792,10402 4792,10475 4792,10475 4792,10412 4792,10412"];
	Layer1_Device1_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="8934,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device1_Input -> Layer1_Device1_LayerNorm1	[pos="e,9147,15301 9517.2,15429 9517.2,15398 9517.2,15301 9517.2,15301 9517.2,15301 9157,15301 9157,15301"];
	Layer1_Device1_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="9555,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device1_Input -> Layer1_Device1_Residual1	[pos="e,9850.5,11484 9850.5,15401 9850.5,15401 9850.5,11494 9850.5,11494"];
	Layer1_Device1_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8782,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device1_LayerNorm1 -> Layer1_Device1_QKVProj	[pos="e,8934,15202 8934,15274 8934,15274 8934,15212 8934,15212"];
	Layer1_Device1_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7536,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage0_RecvKV	[label="Local K,V",
		lp="8036.5,15105",
		pos="e,8294.1,15062 8294.1,15148 8294.1,15148 8294.1,15072 8294.1,15072"];
	Layer1_Device1_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8326,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage0_Attention	[label=Q_local,
		lp="8598,15009",
		pos="e,8506.6,14843 8506.6,15148 8506.6,15148 8506.6,14853 8506.6,14853"];
	Layer1_Device1_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5996,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage1_Attention	[label=Q_local,
		lp="6060,14912",
		pos="e,6112.9,14649 8132.5,15154 7342.4,15154 6112.9,15154 6112.9,15154 6112.9,15154 6112.9,14659 6112.9,14659"];
	Layer1_Device1_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6607,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage2_Attention	[label=Q_local,
		lp="8738,14816",
		pos="e,6777.4,14457 8601.6,15149 8601.6,15049 8601.6,14704 8601.6,14704 8601.6,14704 6777.4,14704 6777.4,14704 6777.4,14704 6777.4,14467 \
6777.4,14467"];
	Layer1_Device1_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8622,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage3_Attention	[label=Q_local,
		lp="8878,14719",
		pos="e,8794.6,14264 8794.6,15149 8794.6,15149 8794.6,14274 8794.6,14274"];
	Layer1_Device1_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5828,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage4_Attention	[label=Q_local,
		lp="5680,14623",
		pos="e,5680.5,14070 8132.3,15166 7222.9,15166 5680.5,15166 5680.5,15166 5680.5,15166 5680.5,14080 5680.5,14080"];
	Layer1_Device1_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8745,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage5_Attention	[label=Q_local,
		lp="8982,14526",
		pos="e,8915,13877 8915,15148 8915,15148 8915,13887 8915,13887"];
	Layer1_Device1_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8773,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage6_Attention	[label=Q_local,
		lp="9114,14430",
		pos="e,8990.5,13684 8990.5,15149 8990.5,15149 8990.5,13694 8990.5,13694"];
	Layer1_Device1_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5834,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage7_Attention	[label=Q_local,
		lp="5534,14333",
		pos="e,5602.3,13473 8132.3,15184 7193,15184 5566.8,15184 5566.8,15184 5566.8,15184 5566.8,13473 5566.8,13473 5566.8,13473 5592.3,13473 \
5592.3,13473"];
	Layer1_Device1_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8576,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage8_Attention	[label=Q_local,
		lp="9237,14237",
		pos="e,8807.7,13271 9024.3,15149 9024.3,14919 9024.3,13271 9024.3,13271 9024.3,13271 8817.7,13271 8817.7,13271"];
	Layer1_Device1_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5637,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage9_Attention	[label=Q_local,
		lp="5416,14140",
		pos="e,5537.2,13105 8132.2,15189 7185.1,15189 5537.2,15189 5537.2,15189 5537.2,15189 5537.2,13115 5537.2,13115"];
	Layer1_Device1_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5473,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage10_Attention	[label=Q_local,
		lp="5276,14044",
		pos="e,5332.6,12912 8132.4,15195 7133.1,15195 5332.6,15195 5332.6,15195 5332.6,15195 5332.6,12922 5332.6,12922"];
	Layer1_Device1_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6955,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage11_Attention	[label=Q_local,
		lp="9360,13947",
		pos="e,7032.2,12719 9063.8,15148 9063.8,14887 9063.8,12817 9063.8,12817 9063.8,12817 7032.2,12817 7032.2,12817 7032.2,12817 7032.2,12729 \
7032.2,12729"];
	Layer1_Device1_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7398,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage12_Attention	[label=Q_local,
		lp="9501,13851",
		pos="e,7501.1,12526 9257.6,15149 9257.6,14874 9257.6,12570 9257.6,12570 9257.6,12570 7501.1,12570 7501.1,12570 7501.1,12570 7501.1,12536 \
7501.1,12536"];
	Layer1_Device1_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9413,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage13_Attention	[label=Q_local,
		lp="9641,13754",
		pos="e,9585.6,12334 9431.6,15161 9522.4,15161 9585.6,15161 9585.6,15161 9585.6,15161 9585.6,12344 9585.6,12344"];
	Layer1_Device1_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6995,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage14_Attention	[label=Q_local,
		lp="5130,13658",
		pos="e,6763.3,12113 9044.1,15148 9044.1,14888 9044.1,12822 9044.1,12822 9044.1,12822 6604.9,12822 6604.9,12822 6604.9,12822 6604.9,12113 \
6604.9,12113 6604.9,12113 6753.3,12113 6753.3,12113"];
	Layer1_Device1_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9330,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_QKVProj -> Layer1_Device1_Stage15_Attention	[label=Q_local,
		lp="9759,13561",
		pos="e,9561.7,11970 9431.6,15175 9583.5,15175 9702.1,15175 9702.1,15175 9702.1,15175 9702.1,11970 9702.1,11970 9702.1,11970 9571.7,11970 \
9571.7,11970"];
	Layer1_Device1_Stage0_RecvKV -> Layer1_Device1_Stage0_Attention	[pos="e,8275.1,14843 8275.1,15009 8275.1,15009 8275.1,14853 8275.1,14853"];
	Layer1_Device1_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7117,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage0_RecvKV -> Layer1_Device1_Stage1_RecvKV	[label="Ring transfer",
		lp="7374,14912",
		pos="e,7326.5,14869 7326.5,14955 7326.5,14955 7326.5,14879 7326.5,14879"];
	Layer1_Device1_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="6517,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage0_Attention -> Layer1_Device1_Stage0_Accumulate	[pos="e,6517,14649 8326,14789 8326,14757 8326,14707 8326,14707 8326,14707 6517,14707 6517,14707 6517,14707 6517,14659 6517,14659"];
	Layer1_Device1_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6086,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage0_Accumulate -> Layer1_Device1_Stage1_Accumulate	[pos="e,6301.5,14456 6301.5,14596 6301.5,14596 6301.5,14466 6301.5,14466"];
	Layer1_Device1_Stage1_RecvKV -> Layer1_Device1_Stage1_Attention	[pos="e,6212.4,14650 6212.4,14816 6212.4,14816 6212.4,14660 6212.4,14660"];
	Layer1_Device1_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7726,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage1_RecvKV -> Layer1_Device1_Stage2_RecvKV	[label="Ring transfer",
		lp="7424,14719",
		pos="e,7421.5,14676 7421.5,14762 7421.5,14762 7421.5,14686 7421.5,14686"];
	Layer1_Device1_Stage1_Attention -> Layer1_Device1_Stage1_Accumulate	[pos="e,6041,14456 6041,14596 6041,14596 6041,14466 6041,14466"];
	Layer1_Device1_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6204,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage1_Accumulate -> Layer1_Device1_Stage2_Accumulate	[pos="e,6145,14263 6145,14403 6145,14403 6145,14273 6145,14273"];
	Layer1_Device1_Stage2_RecvKV -> Layer1_Device1_Stage2_Attention	[pos="e,6822.4,14457 6822.4,14623 6822.4,14623 6822.4,14467 6822.4,14467"];
	Layer1_Device1_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7816,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage2_RecvKV -> Layer1_Device1_Stage3_RecvKV	[label="Ring transfer",
		lp="7819,14526",
		pos="e,7771,14483 7771,14569 7771,14569 7771,14493 7771,14493"];
	Layer1_Device1_Stage2_Attention -> Layer1_Device1_Stage2_Accumulate	[pos="e,6405.5,14263 6405.5,14403 6405.5,14403 6405.5,14273 6405.5,14273"];
	Layer1_Device1_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6349,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage2_Accumulate -> Layer1_Device1_Stage3_Accumulate	[pos="e,6276.5,14070 6276.5,14210 6276.5,14210 6276.5,14080 6276.5,14080"];
	Layer1_Device1_Stage3_RecvKV -> Layer1_Device1_Stage3_Attention	[pos="e,8563.1,14264 8563.1,14430 8563.1,14430 8563.1,14274 8563.1,14274"];
	Layer1_Device1_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7413,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage3_RecvKV -> Layer1_Device1_Stage4_RecvKV	[label="Ring transfer",
		lp="7597,14333",
		pos="e,7614.5,14290 7614.5,14376 7614.5,14376 7614.5,14300 7614.5,14300"];
	Layer1_Device1_Stage3_Attention -> Layer1_Device1_Stage3_Accumulate	[pos="e,6536.9,14070 8434.1,14210 8434.1,14172 8434.1,14106 8434.1,14106 8434.1,14106 6536.9,14106 6536.9,14106 6536.9,14106 6536.9,14080 \
6536.9,14080"];
	Layer1_Device1_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6327,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage3_Accumulate -> Layer1_Device1_Stage4_Accumulate	[pos="e,6338,13877 6338,14017 6338,14017 6338,13887 6338,13887"];
	Layer1_Device1_Stage4_RecvKV -> Layer1_Device1_Stage4_Attention	[pos="e,6016,14070 6536.7,14196 6255.8,14196 6016,14196 6016,14196 6016,14196 6016,14080 6016,14080"];
	Layer1_Device1_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7558,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage4_RecvKV -> Layer1_Device1_Stage5_RecvKV	[label="Ring transfer",
		lp="7533,14140",
		pos="e,7485.5,14097 7485.5,14183 7485.5,14183 7485.5,14107 7485.5,14107"];
	Layer1_Device1_Stage4_Attention -> Layer1_Device1_Stage4_Accumulate	[pos="e,6106.5,13877 6059.6,14043 6087.6,14043 6106.5,14043 6106.5,14043 6106.5,14043 6106.5,13887 6106.5,13887"];
	Layer1_Device1_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8252,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage4_Accumulate -> Layer1_Device1_Stage5_Accumulate	[pos="e,8238.1,13685 6340.9,13824 6340.9,13789 6340.9,13731 6340.9,13731 6340.9,13731 8238.1,13731 8238.1,13731 8238.1,13731 8238.1,13695 \
8238.1,13695"];
	Layer1_Device1_Stage5_RecvKV -> Layer1_Device1_Stage5_Attention	[pos="e,8513.2,13850 8466.6,14044 8466.6,14038 8466.6,13850 8466.6,13850 8466.6,13850 8503.2,13850 8503.2,13850"];
	Layer1_Device1_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7536,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage5_RecvKV -> Layer1_Device1_Stage6_RecvKV	[label="Ring transfer",
		lp="7595,13947",
		pos="e,7547,13904 7547,13990 7547,13990 7547,13914 7547,13914"];
	Layer1_Device1_Stage5_Attention -> Layer1_Device1_Stage5_Accumulate	[pos="e,8483.5,13657 8527.5,13824 8527.5,13771 8527.5,13657 8527.5,13657 8527.5,13657 8493.5,13657 8493.5,13657"];
	Layer1_Device1_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8252,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage5_Accumulate -> Layer1_Device1_Stage6_Accumulate	[pos="e,8252,13491 8252,13631 8252,13631 8252,13501 8252,13501"];
	Layer1_Device1_Stage6_RecvKV -> Layer1_Device1_Stage6_Attention	[pos="e,8759,13684 8123.3,13810 8436.9,13810 8759,13810 8759,13810 8759,13810 8759,13694 8759,13694"];
	Layer1_Device1_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7043,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage6_RecvKV -> Layer1_Device1_Stage7_RecvKV	[label="Ring transfer",
		lp="7272,13754",
		pos="e,7289.5,13711 7289.5,13797 7289.5,13797 7289.5,13721 7289.5,13721"];
	Layer1_Device1_Stage6_Attention -> Layer1_Device1_Stage6_Accumulate	[pos="e,8483.6,13455 8674.5,13631 8674.5,13576 8674.5,13455 8674.5,13455 8674.5,13455 8493.6,13455 8493.6,13455"];
	Layer1_Device1_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8055,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage6_Accumulate -> Layer1_Device1_Stage7_Accumulate	[pos="e,8153.5,13298 8153.5,13438 8153.5,13438 8153.5,13308 8153.5,13308"];
	Layer1_Device1_Stage7_RecvKV -> Layer1_Device1_Stage7_Attention	[pos="e,5831,13491 6166.6,13617 5976.5,13617 5831,13617 5831,13617 5831,13617 5831,13501 5831,13501"];
	Layer1_Device1_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7043,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage7_RecvKV -> Layer1_Device1_Stage8_RecvKV	[label="Ring transfer",
		lp="7091,13561",
		pos="e,7043,13518 7043,13604 7043,13604 7043,13528 7043,13528"];
	Layer1_Device1_Stage7_Attention -> Layer1_Device1_Stage7_Accumulate	[pos="e,7893.1,13298 5995.9,13438 5995.9,13410 5995.9,13370 5995.9,13370 5995.9,13370 7893.1,13370 7893.1,13370 7893.1,13370 7893.1,13308 \
7893.1,13308"];
	Layer1_Device1_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8055,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage7_Accumulate -> Layer1_Device1_Stage8_Accumulate	[pos="e,8055,13105 8055,13245 8055,13245 8055,13115 8055,13115"];
	Layer1_Device1_Stage8_RecvKV -> Layer1_Device1_Stage8_Attention	[pos="e,8414,13298 7630.4,13424 8002.5,13424 8414,13424 8414,13424 8414,13424 8414,13308 8414,13308"];
	Layer1_Device1_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="6846,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage8_RecvKV -> Layer1_Device1_Stage9_RecvKV	[label="Ring transfer",
		lp="6958,13368",
		pos="e,6944.5,13325 6944.5,13411 6944.5,13411 6944.5,13335 6944.5,13335"];
	Layer1_Device1_Stage8_Attention -> Layer1_Device1_Stage8_Accumulate	[pos="e,8286.6,13078 8576,13245 8576,13192 8576,13078 8576,13078 8576,13078 8296.6,13078 8296.6,13078"];
	Layer1_Device1_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="5994,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage8_Accumulate -> Layer1_Device1_Stage9_Accumulate	[pos="e,6075.9,12913 7973.1,13052 7973.1,13033 7973.1,13012 7973.1,13012 7973.1,13012 6075.9,13012 6075.9,13012 6075.9,13012 6075.9,12923 \
6075.9,12923"];
	Layer1_Device1_Stage9_RecvKV -> Layer1_Device1_Stage9_Attention	[pos="e,5735.5,13105 5969.7,13231 5832.7,13231 5735.5,13231 5735.5,13231 5735.5,13231 5735.5,13115 5735.5,13115"];
	Layer1_Device1_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="6846,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage9_RecvKV -> Layer1_Device1_Stage10_RecvKV	[label="Ring transfer",
		lp="6894,13175",
		pos="e,6846,13132 6846,13218 6846,13218 6846,13142 6846,13142"];
	Layer1_Device1_Stage9_Attention -> Layer1_Device1_Stage9_Accumulate	[pos="e,5815.5,12912 5815.5,13052 5815.5,13052 5815.5,12922 5815.5,12922"];
	Layer1_Device1_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="5994,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage9_Accumulate -> Layer1_Device1_Stage10_Accumulate	[pos="e,5994,12719 5994,12859 5994,12859 5994,12729 5994,12729"];
	Layer1_Device1_Stage10_RecvKV -> Layer1_Device1_Stage10_Attention	[pos="e,5555,12912 5969.7,13038 5739.9,13038 5555,13038 5555,13038 5555,13038 5555,12922 5555,12922"];
	Layer1_Device1_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7203,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage10_RecvKV -> Layer1_Device1_Stage11_RecvKV	[label="Ring transfer",
		lp="7106,12982",
		pos="e,7024.5,12939 7024.5,13025 7024.5,13025 7024.5,12949 7024.5,12949"];
	Layer1_Device1_Stage10_Attention -> Layer1_Device1_Stage10_Accumulate	[pos="e,5762.4,12692 5624.5,12859 5624.5,12806 5624.5,12692 5624.5,12692 5624.5,12692 5752.4,12692 5752.4,12692"];
	Layer1_Device1_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6877,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage10_Accumulate -> Layer1_Device1_Stage11_Accumulate	[pos="e,6684.5,12526 6225.8,12692 6426.4,12692 6684.5,12692 6684.5,12692 6684.5,12692 6684.5,12536 6684.5,12536"];
	Layer1_Device1_Stage11_RecvKV -> Layer1_Device1_Stage11_Attention	[pos="e,6877.8,12719 6877.8,12832 6877.8,12832 6877.8,12729 6877.8,12729"];
	Layer1_Device1_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8164,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage11_RecvKV -> Layer1_Device1_Stage12_RecvKV	[label="Ring transfer",
		lp="7737,12789",
		pos="e,7683.5,12746 7683.5,12832 7683.5,12832 7683.5,12756 7683.5,12756"];
	Layer1_Device1_Stage11_Attention -> Layer1_Device1_Stage11_Accumulate	[pos="e,6916,12526 6916,12666 6916,12666 6916,12536 6916,12536"];
	Layer1_Device1_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6995,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage11_Accumulate -> Layer1_Device1_Stage12_Accumulate	[pos="e,6936,12333 6936,12473 6936,12473 6936,12343 6936,12343"];
	Layer1_Device1_Stage12_RecvKV -> Layer1_Device1_Stage12_Attention	[pos="e,7372.8,12526 7372.8,12639 7372.8,12639 7372.8,12536 7372.8,12536"];
	Layer1_Device1_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8607,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage12_RecvKV -> Layer1_Device1_Stage13_RecvKV	[label="Ring transfer",
		lp="8307,12596",
		pos="e,8385.5,12553 8385.5,12639 8385.5,12639 8385.5,12563 8385.5,12563"];
	Layer1_Device1_Stage12_Attention -> Layer1_Device1_Stage12_Accumulate	[pos="e,7196.5,12333 7196.5,12473 7196.5,12473 7196.5,12343 7196.5,12343"];
	Layer1_Device1_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7516,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage12_Accumulate -> Layer1_Device1_Stage13_Accumulate	[pos="e,7284.3,12113 7226.6,12306 7244.3,12306 7255.4,12306 7255.4,12306 7255.4,12306 7255.4,12113 7255.4,12113 7255.4,12113 7274.3,12113 \
7274.3,12113"];
	Layer1_Device1_Stage13_RecvKV -> Layer1_Device1_Stage13_Attention	[pos="e,9354.1,12334 9354.1,12500 9354.1,12500 9354.1,12344 9354.1,12344"];
	Layer1_Device1_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8204,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage13_RecvKV -> Layer1_Device1_Stage14_RecvKV	[label="Ring transfer",
		lp="8388,12403",
		pos="e,8405.5,12360 8405.5,12446 8405.5,12446 8405.5,12370 8405.5,12370"];
	Layer1_Device1_Stage13_Attention -> Layer1_Device1_Stage13_Accumulate	[pos="e,7516,12140 9413,12280 9413,12241 9413,12174 9413,12174 9413,12174 7516,12174 7516,12174 7516,12174 7516,12150 7516,12150"];
	Layer1_Device1_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7516,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage13_Accumulate -> Layer1_Device1_Stage14_Accumulate	[pos="e,7516,11988 7516,12087 7516,12087 7516,11998 7516,11998"];
	Layer1_Device1_Stage14_RecvKV -> Layer1_Device1_Stage14_Attention	[pos="e,6995,12140 7327.8,12266 7139.1,12266 6995,12266 6995,12266 6995,12266 6995,12150 6995,12150"];
	Layer1_Device1_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8725,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device1_Stage14_RecvKV -> Layer1_Device1_Stage15_RecvKV	[label="Ring transfer",
		lp="8298,12210",
		pos="e,8464.5,12167 8464.5,12253 8464.5,12253 8464.5,12177 8464.5,12177"];
	Layer1_Device1_Stage14_Attention -> Layer1_Device1_Stage14_Accumulate	[pos="e,7284.4,11952 6995,12087 6995,12041 6995,11952 6995,11952 6995,11952 7274.4,11952 7274.4,11952"];
	Layer1_Device1_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="9330,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device1_Stage14_Accumulate -> Layer1_Device1_Stage15_Accumulate	[pos="e,9098.4,11835 7516,11934 7516,11897 7516,11835 7516,11835 7516,11835 9088.4,11835 9088.4,11835"];
	Layer1_Device1_Stage15_RecvKV -> Layer1_Device1_Stage15_Attention	[pos="e,9330,11988 9330,12078 9330,12078 9330,11998 9330,11998"];
	Layer1_Device1_Stage15_Attention -> Layer1_Device1_Stage15_Accumulate	[pos="e,9330,11862 9330,11934 9330,11934 9330,11872 9330,11872"];
	Layer1_Device1_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9534,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device1_Stage15_Accumulate -> Layer1_Device1_ConcatHeads	[pos="e,9435,11736 9435,11808 9435,11808 9435,11746 9435,11746"];
	Layer1_Device1_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9545,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device1_ConcatHeads -> Layer1_Device1_OutputProj	[pos="e,9545,11610 9545,11682 9545,11682 9545,11620 9545,11620"];
	Layer1_Device1_OutputProj -> Layer1_Device1_Residual1	[pos="e,9545,11484 9545,11556 9545,11556 9545,11494 9545,11494"];
	Layer1_Device1_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9278,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device1_Residual1 -> Layer1_Device1_LayerNorm2	[pos="e,9329.8,11358 9329.8,11430 9329.8,11430 9329.8,11368 9329.8,11368"];
	Layer1_Device1_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="9510,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device1_Residual1 -> Layer1_Device1_Residual2	[pos="e,9709.8,10676 9709.8,11430 9709.8,11430 9709.8,10686 9709.8,10686"];
	Layer1_Device1_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="8985,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device1_LayerNorm2 -> Layer1_Device1_GateProj	[pos="e,9137.5,11232 9137.5,11304 9137.5,11304 9137.5,11242 9137.5,11242"];
	Layer1_Device1_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9298,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device1_LayerNorm2 -> Layer1_Device1_UpProj	[pos="e,9350.5,11143 9350.5,11304 9350.5,11304 9350.5,11153 9350.5,11153"];
	Layer1_Device1_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="8985,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device1_GateProj -> Layer1_Device1_Activation	[pos="e,8916.5,11054 8916.5,11179 8916.5,11179 8916.5,11064 8916.5,11064"];
	Layer1_Device1_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9081,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device1_UpProj -> Layer1_Device1_ElemMul	[pos="e,9350.8,10928 9350.8,11089 9350.8,11089 9350.8,10938 9350.8,10938"];
	Layer1_Device1_Activation -> Layer1_Device1_ElemMul	[pos="e,8985,10928 8985,11000 8985,11000 8985,10938 8985,10938"];
	Layer1_Device1_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9224,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device1_ElemMul -> Layer1_Device1_DownProj	[pos="e,9224,10802 9224,10874 9224,10874 9224,10812 9224,10812"];
	Layer1_Device1_DownProj -> Layer1_Device1_Residual2	[pos="e,9283.2,10676 9283.2,10748 9283.2,10748 9283.2,10686 9283.2,10686"];
	Layer1_Device1_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9510,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device1_Residual2 -> Layer1_Device1_Output	[pos="e,9510,10550 9510,10622 9510,10622 9510,10560 9510,10560"];
	Layer2_Device1_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9510,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device1_Output -> Layer2_Device1_Input	[pos="e,9510,10402 9510,10475 9510,10475 9510,10412 9510,10412"];
	Layer1_Device2_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14237,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device2_Input -> Layer1_Device2_LayerNorm1	[pos="e,14440,15328 14440,15429 14440,15429 14440,15338 14440,15338"];
	Layer1_Device2_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="14624,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device2_Input -> Layer1_Device2_Residual1	[pos="e,14820,11484 14820,15402 14820,15402 14820,11494 14820,11494"];
	Layer1_Device2_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13675,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device2_LayerNorm1 -> Layer1_Device2_QKVProj	[pos="e,14174,15202 14174,15274 14174,15274 14174,15212 14174,15212"];
	Layer1_Device2_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12429,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage0_RecvKV	[label="Local K,V",
		lp="12930,15105",
		pos="e,13187,15062 13187,15148 13187,15148 13187,15072 13187,15072"];
	Layer1_Device2_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13182,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage0_Attention	[label=Q_local,
		lp="13491,15009",
		pos="e,13381,14843 13381,15148 13381,15148 13381,14853 13381,14853"];
	Layer1_Device2_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10889,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage1_Attention	[label=Q_local,
		lp="10953,14912",
		pos="e,10855,14649 13025,15155 12193,15155 10855,15155 10855,15155 10855,15155 10855,14659 10855,14659"];
	Layer1_Device2_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11500,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage2_Attention	[label=Q_local,
		lp="13631,14816",
		pos="e,11670,14456 13476,15148 13476,15046 13476,14683 13476,14683 13476,14683 11670,14683 11670,14683 11670,14683 11670,14466 11670,\
14466"];
	Layer1_Device2_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13515,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage3_Attention	[label=Q_local,
		lp="13771,14719",
		pos="e,13688,14264 13688,15149 13688,15149 13688,14274 13688,14274"];
	Layer1_Device2_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10721,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage4_Attention	[label=Q_local,
		lp="10573,14623",
		pos="e,10650,14070 13025,15168 12136,15168 10650,15168 10650,15168 10650,15168 10650,14080 10650,14080"];
	Layer1_Device2_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13633,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage5_Attention	[label=Q_local,
		lp="13868,14526",
		pos="e,13806,13877 13806,15148 13806,15148 13806,13887 13806,13887"];
	Layer1_Device2_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13718,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage6_Attention	[label=Q_local,
		lp="14007,14430",
		pos="e,13907,13684 13907,15149 13907,15149 13907,13694 13907,13694"];
	Layer1_Device2_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10779,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage7_Attention	[label=Q_local,
		lp="10427,14333",
		pos="e,10547,13473 13025,15181 12079,15181 10435,15181 10435,15181 10435,15181 10435,13473 10435,13473 10435,13473 10537,13473 10537,\
13473"];
	Layer1_Device2_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13497,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage8_Attention	[label=Q_local,
		lp="14125,14237",
		pos="e,13729,13271 13980,15149 13980,14919 13980,13271 13980,13271 13980,13271 13739,13271 13739,13271"];
	Layer1_Device2_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10558,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage9_Attention	[label=Q_local,
		lp="10309,14140",
		pos="e,10381,13105 13025,15188 12065,15188 10381,15188 10381,15188 10381,15188 10381,13115 10381,13115"];
	Layer1_Device2_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10765,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage10_Attention	[label=Q_local,
		lp="10169,14044",
		pos="e,10533,12885 13025,15195 12047,15195 10310,15195 10310,15195 10310,15195 10310,12885 10310,12885 10310,12885 10523,12885 10523,\
12885"];
	Layer1_Device2_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11972,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage11_Attention	[label=Q_local,
		lp="14243,13947",
		pos="e,12049,12719 14040,15149 14040,14888 14040,12802 14040,12802 14040,12802 12049,12802 12049,12802 12049,12802 12049,12729 12049,\
12729"];
	Layer1_Device2_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="12286,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage12_Attention	[label=Q_local,
		lp="14389,13851",
		pos="e,12232,12526 14070,15149 14070,14888 14070,12800 14070,12800 14070,12800 12232,12800 12232,12800 12232,12800 12232,12536 12232,\
12536"];
	Layer1_Device2_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14301,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage13_Attention	[label=Q_local,
		lp="14529,13754",
		pos="e,14432,12334 14325,15161 14390,15161 14432,15161 14432,15161 14432,15161 14432,12344 14432,12344"];
	Layer1_Device2_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11883,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage14_Attention	[label=Q_local,
		lp="10023,13658",
		pos="e,11651,12113 14010,15148 14010,14899 14010,12990 14010,12990 14010,12990 11526,12990 11526,12990 11526,12990 11526,12113 11526,\
12113 11526,12113 11641,12113 11641,12113"];
	Layer1_Device2_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13686,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_QKVProj -> Layer1_Device2_Stage15_Attention	[label=Q_local,
		lp="14647,13561",
		pos="e,13918,11970 14325,15175 14469,15175 14581,15175 14581,15175 14581,15175 14581,11970 14581,11970 14581,11970 13928,11970 13928,\
11970"];
	Layer1_Device2_Stage0_RecvKV -> Layer1_Device2_Stage0_Attention	[pos="e,13150,14842 13150,15005 13150,15005 13150,14852 13150,14852"];
	Layer1_Device2_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="11973,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage0_RecvKV -> Layer1_Device2_Stage1_RecvKV	[label="Ring transfer",
		lp="12330,14912",
		pos="e,12201,14869 12201,14955 12201,14955 12201,14879 12201,14879"];
	Layer1_Device2_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="11410,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage0_Attention -> Layer1_Device2_Stage0_Accumulate	[pos="e,11410,14650 13182,14789 13182,14751 13182,14686 13182,14686 13182,14686 11410,14686 11410,14686 11410,14686 11410,14660 11410,\
14660"];
	Layer1_Device2_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="10979,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage0_Accumulate -> Layer1_Device2_Stage1_Accumulate	[pos="e,11194,14456 11194,14596 11194,14596 11194,14466 11194,14466"];
	Layer1_Device2_Stage1_RecvKV -> Layer1_Device2_Stage1_Attention	[pos="e,11087,14650 11087,14816 11087,14816 11087,14660 11087,14660"];
	Layer1_Device2_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12619,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage1_RecvKV -> Layer1_Device2_Stage2_RecvKV	[label="Ring transfer",
		lp="12290,14719",
		pos="e,12296,14676 12296,14762 12296,14762 12296,14686 12296,14686"];
	Layer1_Device2_Stage1_Attention -> Layer1_Device2_Stage1_Accumulate	[pos="e,10934,14456 10934,14596 10934,14596 10934,14466 10934,14466"];
	Layer1_Device2_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11097,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage1_Accumulate -> Layer1_Device2_Stage2_Accumulate	[pos="e,11038,14263 11038,14403 11038,14403 11038,14273 11038,14273"];
	Layer1_Device2_Stage2_RecvKV -> Layer1_Device2_Stage2_Attention	[pos="e,11715,14457 11715,14623 11715,14623 11715,14467 11715,14467"];
	Layer1_Device2_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12709,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage2_RecvKV -> Layer1_Device2_Stage3_RecvKV	[label="Ring transfer",
		lp="12712,14526",
		pos="e,12664,14483 12664,14569 12664,14569 12664,14493 12664,14493"];
	Layer1_Device2_Stage2_Attention -> Layer1_Device2_Stage2_Accumulate	[pos="e,11298,14263 11298,14403 11298,14403 11298,14273 11298,14273"];
	Layer1_Device2_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11242,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage2_Accumulate -> Layer1_Device2_Stage3_Accumulate	[pos="e,11170,14070 11170,14210 11170,14210 11170,14080 11170,14080"];
	Layer1_Device2_Stage3_RecvKV -> Layer1_Device2_Stage3_Attention	[pos="e,13456,14264 13456,14430 13456,14430 13456,14274 13456,14274"];
	Layer1_Device2_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12306,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage3_RecvKV -> Layer1_Device2_Stage4_RecvKV	[label="Ring transfer",
		lp="12623,14333",
		pos="e,12508,14290 12508,14376 12508,14376 12508,14300 12508,14300"];
	Layer1_Device2_Stage3_Attention -> Layer1_Device2_Stage3_Accumulate	[pos="e,11430,14070 13327,14210 13327,14173 13327,14112 13327,14112 13327,14112 11430,14112 11430,14112 11430,14112 11430,14080 11430,\
14080"];
	Layer1_Device2_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11215,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage3_Accumulate -> Layer1_Device2_Stage4_Accumulate	[pos="e,11228,13877 11228,14017 11228,14017 11228,13887 11228,13887"];
	Layer1_Device2_Stage4_RecvKV -> Layer1_Device2_Stage4_Attention	[pos="e,10909,14070 11430,14196 11149,14196 10909,14196 10909,14196 10909,14196 10909,14080 10909,14080"];
	Layer1_Device2_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12451,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage4_RecvKV -> Layer1_Device2_Stage5_RecvKV	[label="Ring transfer",
		lp="12473,14140",
		pos="e,12378,14097 12378,14183 12378,14183 12378,14107 12378,14107"];
	Layer1_Device2_Stage4_Attention -> Layer1_Device2_Stage4_Accumulate	[pos="e,10997,13877 10953,14043 10979,14043 10997,14043 10997,14043 10997,14043 10997,13887 10997,13887"];
	Layer1_Device2_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13197,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage4_Accumulate -> Layer1_Device2_Stage5_Accumulate	[pos="e,13155,13684 11257,13824 11257,13801 11257,13772 11257,13772 11257,13772 13155,13772 13155,13772 13155,13772 13155,13694 13155,\
13694"];
	Layer1_Device2_Stage5_RecvKV -> Layer1_Device2_Stage5_Attention	[pos="e,13401,13850 13357,14044 13357,14038 13357,13850 13357,13850 13357,13850 13391,13850 13391,13850"];
	Layer1_Device2_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12424,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage5_RecvKV -> Layer1_Device2_Stage6_RecvKV	[label="Ring transfer",
		lp="12486,13947",
		pos="e,12438,13904 12438,13990 12438,13990 12438,13914 12438,13914"];
	Layer1_Device2_Stage5_Attention -> Layer1_Device2_Stage5_Accumulate	[pos="e,13415,13684 13415,13824 13415,13824 13415,13694 13415,13694"];
	Layer1_Device2_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13197,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage5_Accumulate -> Layer1_Device2_Stage6_Accumulate	[pos="e,13197,13491 13197,13631 13197,13631 13197,13501 13197,13501"];
	Layer1_Device2_Stage6_RecvKV -> Layer1_Device2_Stage6_Attention	[pos="e,13676,13684 13011,13810 13336,13810 13676,13810 13676,13810 13676,13810 13676,13694 13676,13694"];
	Layer1_Device2_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11988,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage6_RecvKV -> Layer1_Device2_Stage7_RecvKV	[label="Ring transfer",
		lp="12330,13754",
		pos="e,12206,13711 12206,13797 12206,13797 12206,13721 12206,13721"];
	Layer1_Device2_Stage6_Attention -> Layer1_Device2_Stage6_Accumulate	[pos="e,13429,13455 13608,13631 13608,13576 13608,13455 13608,13455 13608,13455 13439,13455 13439,13455"];
	Layer1_Device2_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12976,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage6_Accumulate -> Layer1_Device2_Stage7_Accumulate	[pos="e,13086,13298 13086,13438 13086,13438 13086,13308 13086,13308"];
	Layer1_Device2_Stage7_RecvKV -> Layer1_Device2_Stage7_Attention	[pos="e,10997,13491 11112,13617 11042,13617 10997,13617 10997,13617 10997,13617 10997,13501 10997,13501"];
	Layer1_Device2_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11988,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage7_RecvKV -> Layer1_Device2_Stage8_RecvKV	[label="Ring transfer",
		lp="12036,13561",
		pos="e,11988,13518 11988,13604 11988,13604 11988,13528 11988,13528"];
	Layer1_Device2_Stage7_Attention -> Layer1_Device2_Stage7_Accumulate	[pos="e,12826,13299 10929,13438 10929,13406 10929,13355 10929,13355 10929,13355 12826,13355 12826,13355 12826,13355 12826,13309 12826,\
13309"];
	Layer1_Device2_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12976,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage7_Accumulate -> Layer1_Device2_Stage8_Accumulate	[pos="e,12976,13105 12976,13245 12976,13245 12976,13115 12976,13115"];
	Layer1_Device2_Stage8_RecvKV -> Layer1_Device2_Stage8_Attention	[pos="e,13347,13298 12575,13424 12943,13424 13347,13424 13347,13424 13347,13424 13347,13308 13347,13308"];
	Layer1_Device2_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="11767,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage8_RecvKV -> Layer1_Device2_Stage9_RecvKV	[label="Ring transfer",
		lp="11909,13368",
		pos="e,11878,13325 11878,13411 11878,13411 11878,13335 11878,13335"];
	Layer1_Device2_Stage8_Attention -> Layer1_Device2_Stage8_Accumulate	[pos="e,13208,13078 13340,13245 13340,13192 13340,13078 13340,13078 13340,13078 13218,13078 13218,13078"];
	Layer1_Device2_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11286,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage8_Accumulate -> Layer1_Device2_Stage9_Accumulate	[pos="e,11363,12912 12976,13052 12976,13027 12976,12993 12976,12993 12976,12993 11363,12993 11363,12993 11363,12993 11363,12922 11363,\
12922"];
	Layer1_Device2_Stage9_RecvKV -> Layer1_Device2_Stage9_Attention	[pos="e,10668,13105 10891,13231 10760,13231 10668,13231 10668,13231 10668,13231 10668,13115 10668,13115"];
	Layer1_Device2_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11767,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage9_RecvKV -> Layer1_Device2_Stage10_RecvKV	[label="Ring transfer",
		lp="11815,13175",
		pos="e,11767,13132 11767,13218 11767,13218 11767,13142 11767,13142"];
	Layer1_Device2_Stage9_Attention -> Layer1_Device2_Stage9_Accumulate	[pos="e,11209,12912 10662,13052 10662,13013 10662,12946 10662,12946 10662,12946 11209,12946 11209,12946 11209,12946 11209,12922 11209,\
12922"];
	Layer1_Device2_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11286,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage9_Accumulate -> Layer1_Device2_Stage10_Accumulate	[pos="e,11286,12719 11286,12859 11286,12859 11286,12729 11286,12729"];
	Layer1_Device2_Stage10_RecvKV -> Layer1_Device2_Stage10_Attention	[pos="e,10922,12913 10922,13079 10922,13079 10922,12923 10922,12923"];
	Layer1_Device2_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12495,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage10_RecvKV -> Layer1_Device2_Stage11_RecvKV	[label="Ring transfer",
		lp="12084,12982",
		pos="e,12131,12939 12131,13025 12131,13025 12131,12949 12131,12949"];
	Layer1_Device2_Stage10_Attention -> Layer1_Device2_Stage10_Accumulate	[pos="e,11054,12692 10858,12859 10858,12806 10858,12692 10858,12692 10858,12692 11044,12692 11044,12692"];
	Layer1_Device2_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11765,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage10_Accumulate -> Layer1_Device2_Stage11_Accumulate	[pos="e,11554,12526 11518,12692 11540,12692 11554,12692 11554,12692 11554,12692 11554,12536 11554,12536"];
	Layer1_Device2_Stage11_RecvKV -> Layer1_Device2_Stage11_Attention	[pos="e,11895,12719 11895,12832 11895,12832 11895,12729 11895,12729"];
	Layer1_Device2_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13181,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage11_RecvKV -> Layer1_Device2_Stage12_RecvKV	[label="Ring transfer",
		lp="12886,12789",
		pos="e,12838,12746 12838,12832 12838,12832 12838,12756 12838,12756"];
	Layer1_Device2_Stage11_Attention -> Layer1_Device2_Stage11_Accumulate	[pos="e,11868,12526 11868,12666 11868,12666 11868,12536 11868,12536"];
	Layer1_Device2_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11883,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage11_Accumulate -> Layer1_Device2_Stage12_Accumulate	[pos="e,11824,12333 11824,12473 11824,12473 11824,12343 11824,12343"];
	Layer1_Device2_Stage12_RecvKV -> Layer1_Device2_Stage12_Attention	[pos="e,12389,12526 12389,12639 12389,12639 12389,12536 12389,12536"];
	Layer1_Device2_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13495,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage12_RecvKV -> Layer1_Device2_Stage13_RecvKV	[label="Ring transfer",
		lp="13473,12596",
		pos="e,13338,12553 13338,12639 13338,12639 13338,12563 13338,12563"];
	Layer1_Device2_Stage12_Attention -> Layer1_Device2_Stage12_Accumulate	[pos="e,12084,12333 12084,12473 12084,12473 12084,12343 12084,12343"];
	Layer1_Device2_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12404,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage12_Accumulate -> Layer1_Device2_Stage13_Accumulate	[pos="e,12172,12113 12115,12306 12132,12306 12143,12306 12143,12306 12143,12306 12143,12113 12143,12113 12143,12113 12162,12113 12162,\
12113"];
	Layer1_Device2_Stage13_RecvKV -> Layer1_Device2_Stage13_Attention	[pos="e,14242,12334 14242,12500 14242,12500 14242,12344 14242,12344"];
	Layer1_Device2_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13092,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage13_RecvKV -> Layer1_Device2_Stage14_RecvKV	[label="Ring transfer",
		lp="13276,12403",
		pos="e,13294,12360 13294,12446 13294,12446 13294,12370 13294,12370"];
	Layer1_Device2_Stage13_Attention -> Layer1_Device2_Stage13_Accumulate	[pos="e,12404,12140 14301,12280 14301,12242 14301,12178 14301,12178 14301,12178 12404,12178 12404,12178 12404,12178 12404,12150 12404,\
12150"];
	Layer1_Device2_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12404,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage13_Accumulate -> Layer1_Device2_Stage14_Accumulate	[pos="e,12404,11988 12404,12087 12404,12087 12404,11998 12404,11998"];
	Layer1_Device2_Stage14_RecvKV -> Layer1_Device2_Stage14_Attention	[pos="e,11883,12140 12216,12266 12027,12266 11883,12266 11883,12266 11883,12266 11883,12150 11883,12150"];
	Layer1_Device2_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13613,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device2_Stage14_RecvKV -> Layer1_Device2_Stage15_RecvKV	[label="Ring transfer",
		lp="13319,12210",
		pos="e,13352,12167 13352,12253 13352,12253 13352,12177 13352,12177"];
	Layer1_Device2_Stage14_Attention -> Layer1_Device2_Stage14_Accumulate	[pos="e,12172,11952 11883,12087 11883,12041 11883,11952 11883,11952 11883,11952 12162,11952 12162,11952"];
	Layer1_Device2_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13686,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device2_Stage14_Accumulate -> Layer1_Device2_Stage15_Accumulate	[pos="e,13454,11835 12404,11934 12404,11897 12404,11835 12404,11835 12404,11835 13444,11835 13444,11835"];
	Layer1_Device2_Stage15_RecvKV -> Layer1_Device2_Stage15_Attention	[pos="e,13686,11988 13686,12060 13686,12060 13686,11998 13686,11998"];
	Layer1_Device2_Stage15_Attention -> Layer1_Device2_Stage15_Accumulate	[pos="e,13686,11862 13686,11934 13686,11934 13686,11872 13686,11872"];
	Layer1_Device2_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="13901,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device2_Stage15_Accumulate -> Layer1_Device2_ConcatHeads	[pos="e,13796,11736 13796,11808 13796,11808 13796,11746 13796,11746"];
	Layer1_Device2_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14416,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device2_ConcatHeads -> Layer1_Device2_OutputProj	[pos="e,14203,11583 14014,11682 14014,11645 14014,11583 14014,11583 14014,11583 14193,11583 14193,11583"];
	Layer1_Device2_OutputProj -> Layer1_Device2_Residual1	[pos="e,14433,11484 14433,11556 14433,11556 14433,11494 14433,11494"];
	Layer1_Device2_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14420,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device2_Residual1 -> Layer1_Device2_LayerNorm2	[pos="e,14435,11358 14435,11430 14435,11430 14435,11368 14435,11368"];
	Layer1_Device2_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="14616,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device2_Residual1 -> Layer1_Device2_Residual2	[pos="e,14834,10676 14834,11430 14834,11430 14834,10686 14834,10686"];
	Layer1_Device2_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14127,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device2_LayerNorm2 -> Layer1_Device2_GateProj	[pos="e,14280,11232 14280,11304 14280,11304 14280,11242 14280,11242"];
	Layer1_Device2_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14440,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device2_LayerNorm2 -> Layer1_Device2_UpProj	[pos="e,14492,11143 14492,11304 14492,11304 14492,11153 14492,11153"];
	Layer1_Device2_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14127,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device2_GateProj -> Layer1_Device2_Activation	[pos="e,14058,11054 14058,11179 14058,11179 14058,11064 14058,11064"];
	Layer1_Device2_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14223,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device2_UpProj -> Layer1_Device2_ElemMul	[pos="e,14493,10928 14493,11089 14493,11089 14493,10938 14493,10938"];
	Layer1_Device2_Activation -> Layer1_Device2_ElemMul	[pos="e,14127,10928 14127,11000 14127,11000 14127,10938 14127,10938"];
	Layer1_Device2_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14366,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device2_ElemMul -> Layer1_Device2_DownProj	[pos="e,14366,10802 14366,10874 14366,10874 14366,10812 14366,10812"];
	Layer1_Device2_DownProj -> Layer1_Device2_Residual2	[pos="e,14407,10676 14407,10748 14407,10748 14407,10686 14407,10686"];
	Layer1_Device2_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14616,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device2_Residual2 -> Layer1_Device2_Output	[pos="e,14616,10550 14616,10622 14616,10622 14616,10560 14616,10560"];
	Layer2_Device2_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14616,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device2_Output -> Layer2_Device2_Input	[pos="e,14616,10402 14616,10475 14616,10475 14616,10412 14616,10412"];
	Layer1_Device3_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19119,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device3_Input -> Layer1_Device3_LayerNorm1	[pos="e,19277,15328 19277,15416 19277,15416 19277,15338 19277,15338"];
	Layer1_Device3_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="19178,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device3_Input -> Layer1_Device3_Residual1	[pos="e,19517,11484 19517,15401 19517,15401 19517,11494 19517,11494"];
	Layer1_Device3_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18564,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device3_LayerNorm1 -> Layer1_Device3_QKVProj	[pos="e,19060,15202 19060,15274 19060,15274 19060,15212 19060,15212"];
	Layer1_Device3_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17318,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage0_RecvKV	[label="Local K,V",
		lp="17818,15105",
		pos="e,18076,15062 18076,15148 18076,15148 18076,15072 18076,15072"];
	Layer1_Device3_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18071,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage0_Attention	[label=Q_local,
		lp="18380,15009",
		pos="e,18270,14843 18270,15148 18270,15148 18270,14853 18270,14853"];
	Layer1_Device3_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15778,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage1_Attention	[label=Q_local,
		lp="15842,14912",
		pos="e,15748,14649 17914,15155 17083,15155 15748,15155 15748,15155 15748,15155 15748,14659 15748,14659"];
	Layer1_Device3_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16389,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage2_Attention	[label=Q_local,
		lp="18520,14816",
		pos="e,16559,14456 18365,15149 18365,15047 18365,14688 18365,14688 18365,14688 16559,14688 16559,14688 16559,14688 16559,14466 16559,\
14466"];
	Layer1_Device3_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18404,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage3_Attention	[label=Q_local,
		lp="18660,14719",
		pos="e,18577,14264 18577,15149 18577,15149 18577,14274 18577,14274"];
	Layer1_Device3_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15610,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage4_Attention	[label=Q_local,
		lp="15462,14623",
		pos="e,15462,14070 17914,15168 17005,15168 15462,15168 15462,15168 15462,15168 15462,14080 15462,14080"];
	Layer1_Device3_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18522,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage5_Attention	[label=Q_local,
		lp="18757,14526",
		pos="e,18694,13877 18694,15148 18694,15148 18694,13887 18694,13887"];
	Layer1_Device3_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18607,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage6_Attention	[label=Q_local,
		lp="18896,14430",
		pos="e,18796,13684 18796,15149 18796,15149 18796,13694 18796,13694"];
	Layer1_Device3_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15668,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage7_Attention	[label=Q_local,
		lp="15316,14333",
		pos="e,15436,13473 17914,15181 16971,15181 15334,15181 15334,15181 15334,15181 15334,13473 15334,13473 15334,13473 15426,13473 15426,\
13473"];
	Layer1_Device3_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18414,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage8_Attention	[label=Q_local,
		lp="19014,14237",
		pos="e,18646,13271 18896,15149 18896,14919 18896,13271 18896,13271 18896,13271 18656,13271 18656,13271"];
	Layer1_Device3_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15475,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage9_Attention	[label=Q_local,
		lp="15198,14140",
		pos="e,15288,13105 17914,15188 16959,15188 15288,15188 15288,15188 15288,15188 15288,13115 15288,13115"];
	Layer1_Device3_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15726,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage10_Attention	[label=Q_local,
		lp="15058,14044",
		pos="e,15494,12885 17914,15195 16942,15195 15222,15195 15222,15195 15222,15195 15222,12885 15222,12885 15222,12885 15484,12885 15484,\
12885"];
	Layer1_Device3_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16940,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage11_Attention	[label=Q_local,
		lp="19135,13947",
		pos="e,17017,12719 18954,15149 18954,14886 18954,12765 18954,12765 18954,12765 17017,12765 17017,12765 17017,12765 17017,12729 17017,\
12729"];
	Layer1_Device3_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="17175,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage12_Attention	[label=Q_local,
		lp="19278,13851",
		pos="e,17200,12526 19011,15149 19011,14885 19011,12763 19011,12763 19011,12763 17200,12763 17200,12763 17200,12763 17200,12536 17200,\
12536"];
	Layer1_Device3_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="19190,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage13_Attention	[label=Q_local,
		lp="19418,13754",
		pos="e,19323,12334 19214,15166 19279,15166 19323,15166 19323,15166 19323,15166 19323,12344 19323,12344"];
	Layer1_Device3_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16772,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage14_Attention	[label=Q_local,
		lp="14912,13658",
		pos="e,17004,12122 19214,15160 19274,15160 19313,15160 19313,15160 19313,15160 19313,12403 19313,12403 19313,12403 17042,12403 17042,\
12403 17042,12403 17042,12122 17042,12122 17042,12122 17014,12122 17014,12122"];
	Layer1_Device3_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18591,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_QKVProj -> Layer1_Device3_Stage15_Attention	[label=Q_local,
		lp="19536,13561",
		pos="e,18823,11970 19214,15178 19358,15178 19469,15178 19469,15178 19469,15178 19469,11970 19469,11970 19469,11970 18833,11970 18833,\
11970"];
	Layer1_Device3_Stage0_RecvKV -> Layer1_Device3_Stage0_Attention	[pos="e,18039,14842 18039,15005 18039,15005 18039,14852 18039,14852"];
	Layer1_Device3_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="16862,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage0_RecvKV -> Layer1_Device3_Stage1_RecvKV	[label="Ring transfer",
		lp="17138,14912",
		pos="e,17090,14869 17090,14955 17090,14955 17090,14879 17090,14879"];
	Layer1_Device3_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="16299,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage0_Attention -> Layer1_Device3_Stage0_Accumulate	[pos="e,16299,14649 18071,14789 18071,14752 18071,14691 18071,14691 18071,14691 16299,14691 16299,14691 16299,14691 16299,14659 16299,\
14659"];
	Layer1_Device3_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="15868,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage0_Accumulate -> Layer1_Device3_Stage1_Accumulate	[pos="e,16084,14456 16084,14596 16084,14596 16084,14466 16084,14466"];
	Layer1_Device3_Stage1_RecvKV -> Layer1_Device3_Stage1_Attention	[pos="e,15976,14650 15976,14816 15976,14816 15976,14660 15976,14660"];
	Layer1_Device3_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17508,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage1_RecvKV -> Layer1_Device3_Stage2_RecvKV	[label="Ring transfer",
		lp="17179,14719",
		pos="e,17185,14676 17185,14762 17185,14762 17185,14686 17185,14686"];
	Layer1_Device3_Stage1_Attention -> Layer1_Device3_Stage1_Accumulate	[pos="e,15823,14456 15823,14596 15823,14596 15823,14466 15823,14466"];
	Layer1_Device3_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="15986,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage1_Accumulate -> Layer1_Device3_Stage2_Accumulate	[pos="e,15927,14263 15927,14403 15927,14403 15927,14273 15927,14273"];
	Layer1_Device3_Stage2_RecvKV -> Layer1_Device3_Stage2_Attention	[pos="e,16604,14457 16604,14623 16604,14623 16604,14467 16604,14467"];
	Layer1_Device3_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17598,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage2_RecvKV -> Layer1_Device3_Stage3_RecvKV	[label="Ring transfer",
		lp="17601,14526",
		pos="e,17553,14483 17553,14569 17553,14569 17553,14493 17553,14493"];
	Layer1_Device3_Stage2_Attention -> Layer1_Device3_Stage2_Accumulate	[pos="e,16188,14263 16188,14403 16188,14403 16188,14273 16188,14273"];
	Layer1_Device3_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16131,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage2_Accumulate -> Layer1_Device3_Stage3_Accumulate	[pos="e,16058,14070 16058,14210 16058,14210 16058,14080 16058,14080"];
	Layer1_Device3_Stage3_RecvKV -> Layer1_Device3_Stage3_Attention	[pos="e,18345,14264 18345,14430 18345,14430 18345,14274 18345,14274"];
	Layer1_Device3_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17195,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage3_RecvKV -> Layer1_Device3_Stage4_RecvKV	[label="Ring transfer",
		lp="17445,14333",
		pos="e,17396,14290 17396,14376 17396,14376 17396,14300 17396,14300"];
	Layer1_Device3_Stage3_Attention -> Layer1_Device3_Stage3_Accumulate	[pos="e,16319,14071 18216,14210 18216,14175 18216,14117 18216,14117 18216,14117 16319,14117 16319,14117 16319,14117 16319,14081 16319,\
14081"];
	Layer1_Device3_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16104,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage3_Accumulate -> Layer1_Device3_Stage4_Accumulate	[pos="e,16118,13877 16118,14017 16118,14017 16118,13887 16118,13887"];
	Layer1_Device3_Stage4_RecvKV -> Layer1_Device3_Stage4_Attention	[pos="e,15798,14070 16319,14196 16038,14196 15798,14196 15798,14196 15798,14196 15798,14080 15798,14080"];
	Layer1_Device3_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17340,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage4_RecvKV -> Layer1_Device3_Stage5_RecvKV	[label="Ring transfer",
		lp="17315,14140",
		pos="e,17268,14097 17268,14183 17268,14183 17268,14107 17268,14107"];
	Layer1_Device3_Stage4_Attention -> Layer1_Device3_Stage4_Accumulate	[pos="e,15886,13877 15842,14043 15868,14043 15886,14043 15886,14043 15886,14043 15886,13887 15886,13887"];
	Layer1_Device3_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18086,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage4_Accumulate -> Layer1_Device3_Stage5_Accumulate	[pos="e,18044,13684 16146,13824 16146,13803 16146,13777 16146,13777 16146,13777 18044,13777 18044,13777 18044,13777 18044,13694 18044,\
13694"];
	Layer1_Device3_Stage5_RecvKV -> Layer1_Device3_Stage5_Attention	[pos="e,18290,13850 18246,14044 18246,14038 18246,13850 18246,13850 18246,13850 18280,13850 18280,13850"];
	Layer1_Device3_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17313,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage5_RecvKV -> Layer1_Device3_Stage6_RecvKV	[label="Ring transfer",
		lp="17374,13947",
		pos="e,17326,13904 17326,13990 17326,13990 17326,13914 17326,13914"];
	Layer1_Device3_Stage5_Attention -> Layer1_Device3_Stage5_Accumulate	[pos="e,18304,13684 18304,13824 18304,13824 18304,13694 18304,13694"];
	Layer1_Device3_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18086,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage5_Accumulate -> Layer1_Device3_Stage6_Accumulate	[pos="e,18086,13491 18086,13631 18086,13631 18086,13501 18086,13501"];
	Layer1_Device3_Stage6_RecvKV -> Layer1_Device3_Stage6_Attention	[pos="e,18564,13684 17900,13810 18225,13810 18564,13810 18564,13810 18564,13810 18564,13694 18564,13694"];
	Layer1_Device3_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16877,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage6_RecvKV -> Layer1_Device3_Stage7_RecvKV	[label="Ring transfer",
		lp="17219,13754",
		pos="e,17095,13711 17095,13797 17095,13797 17095,13721 17095,13721"];
	Layer1_Device3_Stage6_Attention -> Layer1_Device3_Stage6_Accumulate	[pos="e,18318,13455 18510,13631 18510,13576 18510,13455 18510,13455 18510,13455 18328,13455 18328,13455"];
	Layer1_Device3_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17893,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage6_Accumulate -> Layer1_Device3_Stage7_Accumulate	[pos="e,17990,13298 17990,13438 17990,13438 17990,13308 17990,13308"];
	Layer1_Device3_Stage7_RecvKV -> Layer1_Device3_Stage7_Attention	[pos="e,15886,13491 16001,13617 15931,13617 15886,13617 15886,13617 15886,13617 15886,13501 15886,13501"];
	Layer1_Device3_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16877,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage7_RecvKV -> Layer1_Device3_Stage8_RecvKV	[label="Ring transfer",
		lp="16925,13561",
		pos="e,16877,13518 16877,13604 16877,13604 16877,13528 16877,13528"];
	Layer1_Device3_Stage7_Attention -> Layer1_Device3_Stage7_Accumulate	[pos="e,17729,13298 15832,13438 15832,13413 15832,13381 15832,13381 15832,13381 17729,13381 17729,13381 17729,13381 17729,13308 17729,\
13308"];
	Layer1_Device3_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17893,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage7_Accumulate -> Layer1_Device3_Stage8_Accumulate	[pos="e,17893,13105 17893,13245 17893,13245 17893,13115 17893,13115"];
	Layer1_Device3_Stage8_RecvKV -> Layer1_Device3_Stage8_Attention	[pos="e,18250,13298 17464,13424 17837,13424 18250,13424 18250,13424 18250,13424 18250,13308 18250,13308"];
	Layer1_Device3_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16684,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage8_RecvKV -> Layer1_Device3_Stage9_RecvKV	[label="Ring transfer",
		lp="16844,13368",
		pos="e,16780,13325 16780,13411 16780,13411 16780,13335 16780,13335"];
	Layer1_Device3_Stage8_Attention -> Layer1_Device3_Stage8_Accumulate	[pos="e,18125,13078 18279,13245 18279,13192 18279,13078 18279,13078 18279,13078 18135,13078 18135,13078"];
	Layer1_Device3_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16247,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage8_Accumulate -> Layer1_Device3_Stage9_Accumulate	[pos="e,16324,12913 17893,13052 17893,13021 17893,12974 17893,12974 17893,12974 16324,12974 16324,12974 16324,12974 16324,12923 16324,\
12923"];
	Layer1_Device3_Stage9_RecvKV -> Layer1_Device3_Stage9_Attention	[pos="e,15572,13105 15808,13231 15670,13231 15572,13231 15572,13231 15572,13231 15572,13115 15572,13115"];
	Layer1_Device3_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16684,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage9_RecvKV -> Layer1_Device3_Stage10_RecvKV	[label="Ring transfer",
		lp="16732,13175",
		pos="e,16684,13132 16684,13218 16684,13218 16684,13142 16684,13142"];
	Layer1_Device3_Stage9_Attention -> Layer1_Device3_Stage9_Accumulate	[pos="e,16170,12912 15600,13052 15600,13016 15600,12955 15600,12955 15600,12955 16170,12955 16170,12955 16170,12955 16170,12922 16170,\
12922"];
	Layer1_Device3_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16247,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage9_Accumulate -> Layer1_Device3_Stage10_Accumulate	[pos="e,16247,12719 16247,12859 16247,12859 16247,12729 16247,12729"];
	Layer1_Device3_Stage10_RecvKV -> Layer1_Device3_Stage10_Attention	[pos="e,15861,12912 15861,13025 15861,13025 15861,12922 15861,12922"];
	Layer1_Device3_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17456,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage10_RecvKV -> Layer1_Device3_Stage11_RecvKV	[label="Ring transfer",
		lp="17001,12982",
		pos="e,17070,12939 17070,13025 17070,13025 17070,12949 17070,12949"];
	Layer1_Device3_Stage10_Attention -> Layer1_Device3_Stage10_Accumulate	[pos="e,16015,12692 15788,12859 15788,12806 15788,12692 15788,12692 15788,12692 16005,12692 16005,12692"];
	Layer1_Device3_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16654,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage10_Accumulate -> Layer1_Device3_Stage11_Accumulate	[pos="e,16450,12526 16450,12666 16450,12666 16450,12536 16450,12536"];
	Layer1_Device3_Stage11_RecvKV -> Layer1_Device3_Stage11_Attention	[pos="e,16863,12719 16863,12832 16863,12832 16863,12729 16863,12729"];
	Layer1_Device3_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18149,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage11_RecvKV -> Layer1_Device3_Stage12_RecvKV	[label="Ring transfer",
		lp="17851,12789",
		pos="e,17802,12746 17802,12832 17802,12832 17802,12756 17802,12756"];
	Layer1_Device3_Stage11_Attention -> Layer1_Device3_Stage11_Accumulate	[pos="e,16797,12526 16797,12666 16797,12666 16797,12536 16797,12536"];
	Layer1_Device3_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16772,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage11_Accumulate -> Layer1_Device3_Stage12_Accumulate	[pos="e,16713,12333 16713,12473 16713,12473 16713,12343 16713,12343"];
	Layer1_Device3_Stage12_RecvKV -> Layer1_Device3_Stage12_Attention	[pos="e,17318,12526 17318,12639 17318,12639 17318,12536 17318,12536"];
	Layer1_Device3_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18384,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage12_RecvKV -> Layer1_Device3_Stage13_RecvKV	[label="Ring transfer",
		lp="18362,12596",
		pos="e,18266,12553 18266,12639 18266,12639 18266,12563 18266,12563"];
	Layer1_Device3_Stage12_Attention -> Layer1_Device3_Stage12_Accumulate	[pos="e,16974,12333 16974,12473 16974,12473 16974,12343 16974,12343"];
	Layer1_Device3_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17293,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage12_Accumulate -> Layer1_Device3_Stage13_Accumulate	[pos="e,17061,12104 17004,12306 17016,12306 17023,12306 17023,12306 17023,12306 17023,12104 17023,12104 17023,12104 17051,12104 17051,\
12104"];
	Layer1_Device3_Stage13_RecvKV -> Layer1_Device3_Stage13_Attention	[pos="e,19131,12334 19131,12500 19131,12500 19131,12344 19131,12344"];
	Layer1_Device3_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17981,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage13_RecvKV -> Layer1_Device3_Stage14_RecvKV	[label="Ring transfer",
		lp="18165,12403",
		pos="e,18182,12360 18182,12446 18182,12446 18182,12370 18182,12370"];
	Layer1_Device3_Stage13_Attention -> Layer1_Device3_Stage13_Accumulate	[pos="e,17293,12140 19190,12280 19190,12243 19190,12182 19190,12182 19190,12182 17293,12182 17293,12182 17293,12182 17293,12150 17293,\
12150"];
	Layer1_Device3_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17293,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage13_Accumulate -> Layer1_Device3_Stage14_Accumulate	[pos="e,17293,11988 17293,12087 17293,12087 17293,11998 17293,11998"];
	Layer1_Device3_Stage14_RecvKV -> Layer1_Device3_Stage14_Attention	[pos="e,16772,12140 17105,12266 16916,12266 16772,12266 16772,12266 16772,12266 16772,12150 16772,12150"];
	Layer1_Device3_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18502,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device3_Stage14_RecvKV -> Layer1_Device3_Stage15_RecvKV	[label="Ring transfer",
		lp="18273,12210",
		pos="e,18242,12167 18242,12253 18242,12253 18242,12177 18242,12177"];
	Layer1_Device3_Stage14_Attention -> Layer1_Device3_Stage14_Accumulate	[pos="e,17061,11952 16772,12087 16772,12041 16772,11952 16772,11952 16772,11952 17051,11952 17051,11952"];
	Layer1_Device3_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18591,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device3_Stage14_Accumulate -> Layer1_Device3_Stage15_Accumulate	[pos="e,18359,11835 17293,11934 17293,11897 17293,11835 17293,11835 17293,11835 18349,11835 18349,11835"];
	Layer1_Device3_Stage15_RecvKV -> Layer1_Device3_Stage15_Attention	[pos="e,18591,11988 18591,12060 18591,12060 18591,11998 18591,11998"];
	Layer1_Device3_Stage15_Attention -> Layer1_Device3_Stage15_Accumulate	[pos="e,18591,11862 18591,11934 18591,11934 18591,11872 18591,11872"];
	Layer1_Device3_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="18649,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device3_Stage15_Accumulate -> Layer1_Device3_ConcatHeads	[pos="e,18623,11736 18623,11808 18623,11808 18623,11746 18623,11746"];
	Layer1_Device3_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="18827,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device3_ConcatHeads -> Layer1_Device3_OutputProj	[pos="e,18744,11610 18744,11682 18744,11682 18744,11620 18744,11620"];
	Layer1_Device3_OutputProj -> Layer1_Device3_Residual1	[pos="e,18916,11484 18916,11556 18916,11556 18916,11494 18916,11494"];
	Layer1_Device3_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19027,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device3_Residual1 -> Layer1_Device3_LayerNorm2	[pos="e,19027,11358 19027,11430 19027,11430 19027,11368 19027,11368"];
	Layer1_Device3_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="19275,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device3_Residual1 -> Layer1_Device3_Residual2	[pos="e,19418,10676 19418,11430 19418,11430 19418,10686 19418,10686"];
	Layer1_Device3_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="18734,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device3_LayerNorm2 -> Layer1_Device3_GateProj	[pos="e,18886,11232 18886,11304 18886,11304 18886,11242 18886,11242"];
	Layer1_Device3_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19047,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device3_LayerNorm2 -> Layer1_Device3_UpProj	[pos="e,19100,11143 19100,11304 19100,11304 19100,11153 19100,11153"];
	Layer1_Device3_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="18734,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device3_GateProj -> Layer1_Device3_Activation	[pos="e,18666,11054 18666,11179 18666,11179 18666,11064 18666,11064"];
	Layer1_Device3_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="18830,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device3_UpProj -> Layer1_Device3_ElemMul	[pos="e,19100,10928 19100,11089 19100,11089 19100,10938 19100,10938"];
	Layer1_Device3_Activation -> Layer1_Device3_ElemMul	[pos="e,18734,10928 18734,11000 18734,11000 18734,10938 18734,10938"];
	Layer1_Device3_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="18973,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device3_ElemMul -> Layer1_Device3_DownProj	[pos="e,18973,10802 18973,10874 18973,10874 18973,10812 18973,10812"];
	Layer1_Device3_DownProj -> Layer1_Device3_Residual2	[pos="e,19040,10676 19040,10748 19040,10748 19040,10686 19040,10686"];
	Layer1_Device3_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19275,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device3_Residual2 -> Layer1_Device3_Output	[pos="e,19275,10550 19275,10622 19275,10622 19275,10560 19275,10560"];
	Layer2_Device3_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19275,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device3_Output -> Layer2_Device3_Input	[pos="e,19275,10402 19275,10475 19275,10475 19275,10412 19275,10412"];
	Layer1_Device4_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24214,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device4_Input -> Layer1_Device4_LayerNorm1	[pos="e,24214,15328 24214,15400 24214,15400 24214,15338 24214,15338"];
	Layer1_Device4_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="24361,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device4_Input -> Layer1_Device4_Residual1	[pos="e,24473,11484 24473,15418 24473,15418 24473,11494 24473,11494"];
	Layer1_Device4_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23452,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device4_LayerNorm1 -> Layer1_Device4_QKVProj	[pos="e,24051,15202 24051,15274 24051,15274 24051,15212 24051,15212"];
	Layer1_Device4_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22206,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage0_RecvKV	[label="Local K,V",
		lp="22706,15105",
		pos="e,22964,15062 22964,15148 22964,15148 22964,15072 22964,15072"];
	Layer1_Device4_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="22996,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage0_Attention	[label=Q_local,
		lp="23268,15009",
		pos="e,23177,14843 23177,15148 23177,15148 23177,14853 23177,14853"];
	Layer1_Device4_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20666,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage1_Attention	[label=Q_local,
		lp="20730,14912",
		pos="e,20844,14649 22802,15154 22030,15154 20844,15154 20844,15154 20844,15154 20844,14659 20844,14659"];
	Layer1_Device4_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21277,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage2_Attention	[label=Q_local,
		lp="23408,14816",
		pos="e,21447,14456 23272,15148 23272,15049 23272,14709 23272,14709 23272,14709 21447,14709 21447,14709 21447,14709 21447,14466 21447,\
14466"];
	Layer1_Device4_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23292,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage3_Attention	[label=Q_local,
		lp="23548,14719",
		pos="e,23465,14264 23465,15149 23465,15149 23465,14274 23465,14274"];
	Layer1_Device4_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20498,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage4_Attention	[label=Q_local,
		lp="20350,14623",
		pos="e,20367,14070 22802,15172 21897,15172 20367,15172 20367,15172 20367,15172 20367,14080 20367,14080"];
	Layer1_Device4_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23410,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage5_Attention	[label=Q_local,
		lp="23652,14526",
		pos="e,23582,13877 23582,15148 23582,15148 23582,13887 23582,13887"];
	Layer1_Device4_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23437,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage6_Attention	[label=Q_local,
		lp="23784,14430",
		pos="e,23655,13684 23655,15149 23655,15149 23655,13694 23655,13694"];
	Layer1_Device4_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20498,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage7_Attention	[label=Q_local,
		lp="20204,14333",
		pos="e,20266,13473 22802,15184 21858,15184 20219,15184 20219,15184 20219,15184 20219,13473 20219,13473 20219,13473 20256,13473 20256,\
13473"];
	Layer1_Device4_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23183,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage8_Attention	[label=Q_local,
		lp="23902,14237",
		pos="e,23415,13271 23716,15149 23716,14919 23716,13271 23716,13271 23716,13271 23425,13271 23425,13271"];
	Layer1_Device4_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20244,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage9_Attention	[label=Q_local,
		lp="20086,14140",
		pos="e,20172,13105 22802,15189 21846,15189 20172,15189 20172,15189 20172,15189 20172,13115 20172,13115"];
	Layer1_Device4_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20449,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage10_Attention	[label=Q_local,
		lp="19946,14044",
		pos="e,20217,12885 22802,15195 21798,15195 19984,15195 19984,15195 19984,15195 19984,12885 19984,12885 19984,12885 20207,12885 20207,\
12885"];
	Layer1_Device4_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21779,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage11_Attention	[label=Q_local,
		lp="24025,13947",
		pos="e,21856,12719 23812,15148 23812,14886 23812,12790 23812,12790 23812,12790 21856,12790 21856,12790 21856,12790 21856,12729 21856,\
12729"];
	Layer1_Device4_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="22063,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage12_Attention	[label=Q_local,
		lp="24166,13851",
		pos="e,22039,12526 23860,15148 23860,14886 23860,12787 23860,12787 23860,12787 22039,12787 22039,12787 22039,12787 22039,12536 22039,\
12536"];
	Layer1_Device4_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="24078,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage13_Attention	[label=Q_local,
		lp="24306,13754",
		pos="e,24251,12334 24102,15161 24190,15161 24251,15161 24251,15161 24251,15161 24251,12344 24251,12344"];
	Layer1_Device4_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21660,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage14_Attention	[label=Q_local,
		lp="19800,13658",
		pos="e,21428,12113 23764,15148 23764,14889 23764,12830 23764,12830 23764,12830 21285,12830 21285,12830 21285,12830 21285,12113 21285,\
12113 21285,12113 21418,12113 21418,12113"];
	Layer1_Device4_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23793,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_QKVProj -> Layer1_Device4_Stage15_Attention	[label=Q_local,
		lp="24424,13561",
		pos="e,24025,11970 24102,15175 24250,15175 24364,15175 24364,15175 24364,15175 24364,11970 24364,11970 24364,11970 24035,11970 24035,\
11970"];
	Layer1_Device4_Stage0_RecvKV -> Layer1_Device4_Stage0_Attention	[pos="e,22945,14843 22945,15009 22945,15009 22945,14853 22945,14853"];
	Layer1_Device4_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="21787,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage0_RecvKV -> Layer1_Device4_Stage1_RecvKV	[label="Ring transfer",
		lp="22116,14912",
		pos="e,21996,14869 21996,14955 21996,14955 21996,14879 21996,14879"];
	Layer1_Device4_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="21187,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage0_Attention -> Layer1_Device4_Stage0_Accumulate	[pos="e,21187,14650 22996,14789 22996,14758 22996,14712 22996,14712 22996,14712 21187,14712 21187,14712 21187,14712 21187,14660 21187,\
14660"];
	Layer1_Device4_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20756,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage0_Accumulate -> Layer1_Device4_Stage1_Accumulate	[pos="e,20972,14456 20972,14596 20972,14596 20972,14466 20972,14466"];
	Layer1_Device4_Stage1_RecvKV -> Layer1_Device4_Stage1_Attention	[pos="e,20882,14650 20882,14816 20882,14816 20882,14660 20882,14660"];
	Layer1_Device4_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22396,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage1_RecvKV -> Layer1_Device4_Stage2_RecvKV	[label="Ring transfer",
		lp="22094,14719",
		pos="e,22092,14676 22092,14762 22092,14762 22092,14686 22092,14686"];
	Layer1_Device4_Stage1_Attention -> Layer1_Device4_Stage1_Accumulate	[pos="e,20711,14456 20711,14596 20711,14596 20711,14466 20711,14466"];
	Layer1_Device4_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20874,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage1_Accumulate -> Layer1_Device4_Stage2_Accumulate	[pos="e,20815,14263 20815,14403 20815,14403 20815,14273 20815,14273"];
	Layer1_Device4_Stage2_RecvKV -> Layer1_Device4_Stage2_Attention	[pos="e,21492,14457 21492,14623 21492,14623 21492,14467 21492,14467"];
	Layer1_Device4_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22486,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage2_RecvKV -> Layer1_Device4_Stage3_RecvKV	[label="Ring transfer",
		lp="22489,14526",
		pos="e,22441,14483 22441,14569 22441,14569 22441,14493 22441,14493"];
	Layer1_Device4_Stage2_Attention -> Layer1_Device4_Stage2_Accumulate	[pos="e,21076,14263 21076,14403 21076,14403 21076,14273 21076,14273"];
	Layer1_Device4_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21019,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage2_Accumulate -> Layer1_Device4_Stage3_Accumulate	[pos="e,20946,14070 20946,14210 20946,14210 20946,14080 20946,14080"];
	Layer1_Device4_Stage3_RecvKV -> Layer1_Device4_Stage3_Attention	[pos="e,23233,14264 23233,14430 23233,14430 23233,14274 23233,14274"];
	Layer1_Device4_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22083,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage3_RecvKV -> Layer1_Device4_Stage4_RecvKV	[label="Ring transfer",
		lp="22267,14333",
		pos="e,22284,14290 22284,14376 22284,14376 22284,14300 22284,14300"];
	Layer1_Device4_Stage3_Attention -> Layer1_Device4_Stage3_Accumulate	[pos="e,21207,14070 23104,14210 23104,14176 23104,14122 23104,14122 23104,14122 21207,14122 21207,14122 21207,14122 21207,14080 21207,\
14080"];
	Layer1_Device4_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20992,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage3_Accumulate -> Layer1_Device4_Stage4_Accumulate	[pos="e,21006,13877 21006,14017 21006,14017 21006,13887 21006,13887"];
	Layer1_Device4_Stage4_RecvKV -> Layer1_Device4_Stage4_Attention	[pos="e,20686,14070 21207,14196 20926,14196 20686,14196 20686,14196 20686,14196 20686,14080 20686,14080"];
	Layer1_Device4_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22228,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage4_RecvKV -> Layer1_Device4_Stage5_RecvKV	[label="Ring transfer",
		lp="22203,14140",
		pos="e,22156,14097 22156,14183 22156,14183 22156,14107 22156,14107"];
	Layer1_Device4_Stage4_Attention -> Layer1_Device4_Stage4_Accumulate	[pos="e,20774,13877 20730,14043 20756,14043 20774,14043 20774,14043 20774,14043 20774,13887 20774,13887"];
	Layer1_Device4_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22916,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage4_Accumulate -> Layer1_Device4_Stage5_Accumulate	[pos="e,22903,13684 21005,13824 21005,13786 21005,13720 21005,13720 21005,13720 22903,13720 22903,13720 22903,13720 22903,13694 22903,\
13694"];
	Layer1_Device4_Stage5_RecvKV -> Layer1_Device4_Stage5_Attention	[pos="e,23178,13859 23130,14044 23130,14038 23130,13859 23130,13859 23130,13859 23168,13859 23168,13859"];
	Layer1_Device4_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22201,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage5_RecvKV -> Layer1_Device4_Stage6_RecvKV	[label="Ring transfer",
		lp="22262,13947",
		pos="e,22214,13904 22214,13990 22214,13990 22214,13914 22214,13914"];
	Layer1_Device4_Stage5_Attention -> Layer1_Device4_Stage5_Accumulate	[pos="e,23139,13684 23178,13841 23154,13841 23139,13841 23139,13841 23139,13841 23139,13694 23139,13694"];
	Layer1_Device4_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22916,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage5_Accumulate -> Layer1_Device4_Stage6_Accumulate	[pos="e,22916,13491 22916,13631 22916,13631 22916,13501 22916,13501"];
	Layer1_Device4_Stage6_RecvKV -> Layer1_Device4_Stage6_Attention	[pos="e,23424,13684 22788,13810 23102,13810 23424,13810 23424,13810 23424,13810 23424,13694 23424,13694"];
	Layer1_Device4_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21707,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage6_RecvKV -> Layer1_Device4_Stage7_RecvKV	[label="Ring transfer",
		lp="22063,13754",
		pos="e,21954,13711 21954,13797 21954,13797 21954,13721 21954,13721"];
	Layer1_Device4_Stage6_Attention -> Layer1_Device4_Stage6_Accumulate	[pos="e,23148,13455 23310,13631 23310,13576 23310,13455 23310,13455 23310,13455 23158,13455 23158,13455"];
	Layer1_Device4_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22662,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage6_Accumulate -> Layer1_Device4_Stage7_Accumulate	[pos="e,22789,13298 22789,13438 22789,13438 22789,13308 22789,13308"];
	Layer1_Device4_Stage7_RecvKV -> Layer1_Device4_Stage7_Attention	[pos="e,20498,13491 20831,13617 20642,13617 20498,13617 20498,13617 20498,13617 20498,13501 20498,13501"];
	Layer1_Device4_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21707,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage7_RecvKV -> Layer1_Device4_Stage8_RecvKV	[label="Ring transfer",
		lp="21755,13561",
		pos="e,21707,13518 21707,13604 21707,13604 21707,13528 21707,13528"];
	Layer1_Device4_Stage7_Attention -> Layer1_Device4_Stage7_Accumulate	[pos="e,22529,13298 20631,13438 20631,13400 20631,13334 20631,13334 20631,13334 22529,13334 22529,13334 22529,13334 22529,13308 22529,\
13308"];
	Layer1_Device4_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22662,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage7_Accumulate -> Layer1_Device4_Stage8_Accumulate	[pos="e,22662,13105 22662,13245 22662,13245 22662,13115 22662,13115"];
	Layer1_Device4_Stage8_RecvKV -> Layer1_Device4_Stage8_Attention	[pos="e,23050,13298 22294,13424 22655,13424 23050,13424 23050,13424 23050,13424 23050,13308 23050,13308"];
	Layer1_Device4_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21453,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage8_RecvKV -> Layer1_Device4_Stage9_RecvKV	[label="Ring transfer",
		lp="21688,13368",
		pos="e,21580,13325 21580,13411 21580,13411 21580,13335 21580,13335"];
	Layer1_Device4_Stage8_Attention -> Layer1_Device4_Stage8_Accumulate	[pos="e,22894,13078 23025,13245 23025,13192 23025,13078 23025,13078 23025,13078 22904,13078 22904,13078"];
	Layer1_Device4_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20970,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage8_Accumulate -> Layer1_Device4_Stage9_Accumulate	[pos="e,21047,12912 22662,13052 22662,13027 22662,12995 22662,12995 22662,12995 21047,12995 21047,12995 21047,12995 21047,12922 21047,\
12922"];
	Layer1_Device4_Stage9_RecvKV -> Layer1_Device4_Stage9_Attention	[pos="e,20371,13105 20577,13231 20455,13231 20371,13231 20371,13231 20371,13231 20371,13115 20371,13115"];
	Layer1_Device4_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21453,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage9_RecvKV -> Layer1_Device4_Stage10_RecvKV	[label="Ring transfer",
		lp="21501,13175",
		pos="e,21453,13132 21453,13218 21453,13218 21453,13142 21453,13142"];
	Layer1_Device4_Stage9_Attention -> Layer1_Device4_Stage9_Accumulate	[pos="e,20893,12912 20346,13052 20346,13013 20346,12944 20346,12944 20346,12944 20893,12944 20893,12944 20893,12944 20893,12922 20893,\
12922"];
	Layer1_Device4_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20970,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage9_Accumulate -> Layer1_Device4_Stage10_Accumulate	[pos="e,20970,12719 20970,12859 20970,12859 20970,12729 20970,12729"];
	Layer1_Device4_Stage10_RecvKV -> Layer1_Device4_Stage10_Attention	[pos="e,20607,12913 20607,13079 20607,13079 20607,12923 20607,12923"];
	Layer1_Device4_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22179,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage10_RecvKV -> Layer1_Device4_Stage11_RecvKV	[label="Ring transfer",
		lp="21770,12982",
		pos="e,21816,12939 21816,13025 21816,13025 21816,12949 21816,12949"];
	Layer1_Device4_Stage10_Attention -> Layer1_Device4_Stage10_Accumulate	[pos="e,20738,12692 20595,12859 20595,12806 20595,12692 20595,12692 20595,12692 20728,12692 20728,12692"];
	Layer1_Device4_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21542,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage10_Accumulate -> Layer1_Device4_Stage11_Accumulate	[pos="e,21429,12526 21202,12692 21316,12692 21429,12692 21429,12692 21429,12692 21429,12536 21429,12536"];
	Layer1_Device4_Stage11_RecvKV -> Layer1_Device4_Stage11_Attention	[pos="e,21702,12719 21702,12832 21702,12832 21702,12729 21702,12729"];
	Layer1_Device4_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22988,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage11_RecvKV -> Layer1_Device4_Stage12_RecvKV	[label="Ring transfer",
		lp="22631,12789",
		pos="e,22584,12746 22584,12832 22584,12832 22584,12756 22584,12756"];
	Layer1_Device4_Stage11_Attention -> Layer1_Device4_Stage11_Accumulate	[pos="e,21660,12526 21660,12666 21660,12666 21660,12536 21660,12536"];
	Layer1_Device4_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21660,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage11_Accumulate -> Layer1_Device4_Stage12_Accumulate	[pos="e,21601,12333 21601,12473 21601,12473 21601,12343 21601,12343"];
	Layer1_Device4_Stage12_RecvKV -> Layer1_Device4_Stage12_Attention	[pos="e,22181,12526 22181,12639 22181,12639 22181,12536 22181,12536"];
	Layer1_Device4_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23272,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage12_RecvKV -> Layer1_Device4_Stage13_RecvKV	[label="Ring transfer",
		lp="23178,12596",
		pos="e,23130,12553 23130,12639 23130,12639 23130,12563 23130,12563"];
	Layer1_Device4_Stage12_Attention -> Layer1_Device4_Stage12_Accumulate	[pos="e,21862,12333 21862,12473 21862,12473 21862,12343 21862,12343"];
	Layer1_Device4_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22181,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage12_Accumulate -> Layer1_Device4_Stage13_Accumulate	[pos="e,21949,12113 21892,12306 21909,12306 21920,12306 21920,12306 21920,12306 21920,12113 21920,12113 21920,12113 21939,12113 21939,\
12113"];
	Layer1_Device4_Stage13_RecvKV -> Layer1_Device4_Stage13_Attention	[pos="e,24019,12334 24019,12500 24019,12500 24019,12344 24019,12344"];
	Layer1_Device4_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22869,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage13_RecvKV -> Layer1_Device4_Stage14_RecvKV	[label="Ring transfer",
		lp="23053,12403",
		pos="e,23070,12360 23070,12446 23070,12446 23070,12370 23070,12370"];
	Layer1_Device4_Stage13_Attention -> Layer1_Device4_Stage13_Accumulate	[pos="e,22181,12140 24078,12280 24078,12244 24078,12186 24078,12186 24078,12186 22181,12186 22181,12186 22181,12186 22181,12150 22181,\
12150"];
	Layer1_Device4_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22181,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage13_Accumulate -> Layer1_Device4_Stage14_Accumulate	[pos="e,22181,11988 22181,12087 22181,12087 22181,11998 22181,11998"];
	Layer1_Device4_Stage14_RecvKV -> Layer1_Device4_Stage14_Attention	[pos="e,21660,12140 21993,12266 21804,12266 21660,12266 21660,12266 21660,12266 21660,12150 21660,12150"];
	Layer1_Device4_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23390,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device4_Stage14_RecvKV -> Layer1_Device4_Stage15_RecvKV	[label="Ring transfer",
		lp="23177,12210",
		pos="e,23130,12167 23130,12253 23130,12253 23130,12177 23130,12177"];
	Layer1_Device4_Stage14_Attention -> Layer1_Device4_Stage14_Accumulate	[pos="e,21949,11952 21660,12087 21660,12041 21660,11952 21660,11952 21660,11952 21939,11952 21939,11952"];
	Layer1_Device4_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="23793,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device4_Stage14_Accumulate -> Layer1_Device4_Stage15_Accumulate	[pos="e,23561,11835 22181,11934 22181,11897 22181,11835 22181,11835 22181,11835 23551,11835 23551,11835"];
	Layer1_Device4_Stage15_RecvKV -> Layer1_Device4_Stage15_Attention	[pos="e,23793,11988 23793,12060 23793,12060 23793,11998 23793,11998"];
	Layer1_Device4_Stage15_Attention -> Layer1_Device4_Stage15_Accumulate	[pos="e,23793,11862 23793,11934 23793,11934 23793,11872 23793,11872"];
	Layer1_Device4_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="23999,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device4_Stage15_Accumulate -> Layer1_Device4_ConcatHeads	[pos="e,23899,11736 23899,11808 23899,11808 23899,11746 23899,11746"];
	Layer1_Device4_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24206,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device4_ConcatHeads -> Layer1_Device4_OutputProj	[pos="e,24109,11610 24109,11682 24109,11682 24109,11620 24109,11620"];
	Layer1_Device4_OutputProj -> Layer1_Device4_Residual1	[pos="e,24206,11484 24206,11556 24206,11556 24206,11494 24206,11494"];
	Layer1_Device4_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24245,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device4_Residual1 -> Layer1_Device4_LayerNorm2	[pos="e,24245,11358 24245,11430 24245,11430 24245,11368 24245,11368"];
	Layer1_Device4_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="24300,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device4_Residual1 -> Layer1_Device4_Residual2	[pos="e,24589,10676 24589,11430 24589,11430 24589,10686 24589,10686"];
	Layer1_Device4_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="23953,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device4_LayerNorm2 -> Layer1_Device4_GateProj	[pos="e,24105,11232 24105,11304 24105,11304 24105,11242 24105,11242"];
	Layer1_Device4_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24266,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device4_LayerNorm2 -> Layer1_Device4_UpProj	[pos="e,24318,11143 24318,11304 24318,11304 24318,11153 24318,11153"];
	Layer1_Device4_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="23953,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device4_GateProj -> Layer1_Device4_Activation	[pos="e,23884,11054 23884,11179 23884,11179 23884,11064 23884,11064"];
	Layer1_Device4_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24048,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device4_UpProj -> Layer1_Device4_ElemMul	[pos="e,24318,10928 24318,11089 24318,11089 24318,10938 24318,10938"];
	Layer1_Device4_Activation -> Layer1_Device4_ElemMul	[pos="e,23953,10928 23953,11000 23953,11000 23953,10938 23953,10938"];
	Layer1_Device4_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24191,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device4_ElemMul -> Layer1_Device4_DownProj	[pos="e,24191,10802 24191,10874 24191,10874 24191,10812 24191,10812"];
	Layer1_Device4_DownProj -> Layer1_Device4_Residual2	[pos="e,24191,10676 24191,10748 24191,10748 24191,10686 24191,10686"];
	Layer1_Device4_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24300,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device4_Residual2 -> Layer1_Device4_Output	[pos="e,24300,10550 24300,10622 24300,10622 24300,10560 24300,10560"];
	Layer2_Device4_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24300,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device4_Output -> Layer2_Device4_Input	[pos="e,24300,10402 24300,10475 24300,10475 24300,10412 24300,10412"];
	Layer1_Device5_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="28931,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device5_Input -> Layer1_Device5_LayerNorm1	[pos="e,29069,15328 29069,15413 29069,15413 29069,15338 29069,15338"];
	Layer1_Device5_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="29318,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device5_Input -> Layer1_Device5_Residual1	[pos="e,29448,11484 29448,15406 29448,15406 29448,11494 29448,11494"];
	Layer1_Device5_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28350,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device5_LayerNorm1 -> Layer1_Device5_QKVProj	[pos="e,28859,15202 28859,15274 28859,15274 28859,15212 28859,15212"];
	Layer1_Device5_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27104,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage0_RecvKV	[label="Local K,V",
		lp="27604,15105",
		pos="e,27862,15062 27862,15148 27862,15148 27862,15072 27862,15072"];
	Layer1_Device5_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="27894,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage0_Attention	[label=Q_local,
		lp="28166,15009",
		pos="e,28075,14843 28075,15148 28075,15148 28075,14853 28075,14853"];
	Layer1_Device5_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25564,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage1_Attention	[label=Q_local,
		lp="25628,14912",
		pos="e,25731,14649 27700,15155 26925,15155 25731,15155 25731,15155 25731,15155 25731,14659 25731,14659"];
	Layer1_Device5_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26175,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage2_Attention	[label=Q_local,
		lp="28306,14816",
		pos="e,26345,14456 28170,15148 28170,15050 28170,14715 28170,14715 28170,14715 26345,14715 26345,14715 26345,14715 26345,14466 26345,\
14466"];
	Layer1_Device5_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28190,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage3_Attention	[label=Q_local,
		lp="28446,14719",
		pos="e,28363,14264 28363,15149 28363,15149 28363,14274 28363,14274"];
	Layer1_Device5_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25396,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage4_Attention	[label=Q_local,
		lp="25248,14623",
		pos="e,25254,14070 27700,15168 26792,15168 25254,15168 25254,15168 25254,15168 25254,14080 25254,14080"];
	Layer1_Device5_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28311,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage5_Attention	[label=Q_local,
		lp="28550,14526",
		pos="e,28482,13877 28482,15148 28482,15148 28482,13887 28482,13887"];
	Layer1_Device5_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28337,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage6_Attention	[label=Q_local,
		lp="28682,14430",
		pos="e,28556,13684 28556,15149 28556,15149 28556,13694 28556,13694"];
	Layer1_Device5_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25398,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage7_Attention	[label=Q_local,
		lp="25102,14333",
		pos="e,25166,13473 27700,15181 26753,15181 25104,15181 25104,15181 25104,15181 25104,13473 25104,13473 25104,13473 25156,13473 25156,\
13473"];
	Layer1_Device5_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28147,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage8_Attention	[label=Q_local,
		lp="28803,14237",
		pos="e,28379,13271 28607,15149 28607,14919 28607,13271 28607,13271 28607,13271 28389,13271 28389,13271"];
	Layer1_Device5_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25208,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage9_Attention	[label=Q_local,
		lp="24984,14140",
		pos="e,25043,13105 27700,15188 26737,15188 25043,15188 25043,15188 25043,15188 25043,13115 25043,13115"];
	Layer1_Device5_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25304,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage10_Attention	[label=Q_local,
		lp="24844,14044",
		pos="e,25072,12885 27700,15195 26718,15195 24971,15195 24971,15195 24971,15195 24971,12885 24971,12885 24971,12885 25062,12885 25062,\
12885"];
	Layer1_Device5_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26634,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage11_Attention	[label=Q_local,
		lp="28926,13947",
		pos="e,26711,12719 28685,15149 28685,14889 28685,12812 28685,12812 28685,12812 26711,12812 26711,12812 26711,12812 26711,12729 26711,\
12729"];
	Layer1_Device5_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26964,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage12_Attention	[label=Q_local,
		lp="29067,13851",
		pos="e,26894,12526 28724,15149 28724,14888 28724,12805 28724,12805 28724,12805 26894,12805 26894,12805 26894,12805 26894,12536 26894,\
12536"];
	Layer1_Device5_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28979,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage13_Attention	[label=Q_local,
		lp="29207,13754",
		pos="e,29118,12334 29000,15163 29071,15163 29118,15163 29118,15163 29118,15163 29118,12344 29118,12344"];
	Layer1_Device5_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26561,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage14_Attention	[label=Q_local,
		lp="24698,13658",
		pos="e,26329,12113 28646,15148 28646,14888 28646,12825 28646,12825 28646,12825 26163,12825 26163,12825 26163,12825 26163,12113 26163,\
12113 26163,12113 26319,12113 26319,12113"];
	Layer1_Device5_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28995,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_QKVProj -> Layer1_Device5_Stage15_Attention	[label=Q_local,
		lp="29325,13561",
		pos="e,29219,11988 29000,15179 29125,15179 29219,15179 29219,15179 29219,15179 29219,11998 29219,11998"];
	Layer1_Device5_Stage0_RecvKV -> Layer1_Device5_Stage0_Attention	[pos="e,27843,14843 27843,15009 27843,15009 27843,14853 27843,14853"];
	Layer1_Device5_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="26685,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage0_RecvKV -> Layer1_Device5_Stage1_RecvKV	[label="Ring transfer",
		lp="26942,14912",
		pos="e,26894,14869 26894,14955 26894,14955 26894,14879 26894,14879"];
	Layer1_Device5_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="26085,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage0_Attention -> Layer1_Device5_Stage0_Accumulate	[pos="e,26085,14649 27894,14789 27894,14760 27894,14717 27894,14717 27894,14717 26085,14717 26085,14717 26085,14717 26085,14659 26085,\
14659"];
	Layer1_Device5_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25654,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage0_Accumulate -> Layer1_Device5_Stage1_Accumulate	[pos="e,25870,14456 25870,14596 25870,14596 25870,14466 25870,14466"];
	Layer1_Device5_Stage1_RecvKV -> Layer1_Device5_Stage1_Attention	[pos="e,25780,14650 25780,14816 25780,14816 25780,14660 25780,14660"];
	Layer1_Device5_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27294,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage1_RecvKV -> Layer1_Device5_Stage2_RecvKV	[label="Ring transfer",
		lp="27002,14719",
		pos="e,26990,14676 26990,14762 26990,14762 26990,14686 26990,14686"];
	Layer1_Device5_Stage1_Attention -> Layer1_Device5_Stage1_Accumulate	[pos="e,25609,14456 25609,14596 25609,14596 25609,14466 25609,14466"];
	Layer1_Device5_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25772,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage1_Accumulate -> Layer1_Device5_Stage2_Accumulate	[pos="e,25713,14263 25713,14403 25713,14403 25713,14273 25713,14273"];
	Layer1_Device5_Stage2_RecvKV -> Layer1_Device5_Stage2_Attention	[pos="e,26390,14457 26390,14623 26390,14623 26390,14467 26390,14467"];
	Layer1_Device5_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27384,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage2_RecvKV -> Layer1_Device5_Stage3_RecvKV	[label="Ring transfer",
		lp="27387,14526",
		pos="e,27339,14483 27339,14569 27339,14569 27339,14493 27339,14493"];
	Layer1_Device5_Stage2_Attention -> Layer1_Device5_Stage2_Accumulate	[pos="e,25974,14263 25974,14403 25974,14403 25974,14273 25974,14273"];
	Layer1_Device5_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25917,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage2_Accumulate -> Layer1_Device5_Stage3_Accumulate	[pos="e,25844,14070 25844,14210 25844,14210 25844,14080 25844,14080"];
	Layer1_Device5_Stage3_RecvKV -> Layer1_Device5_Stage3_Attention	[pos="e,28131,14264 28131,14430 28131,14430 28131,14274 28131,14274"];
	Layer1_Device5_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="26981,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage3_RecvKV -> Layer1_Device5_Stage4_RecvKV	[label="Ring transfer",
		lp="27165,14333",
		pos="e,27182,14290 27182,14376 27182,14376 27182,14300 27182,14300"];
	Layer1_Device5_Stage3_Attention -> Layer1_Device5_Stage3_Accumulate	[pos="e,26105,14071 28002,14210 28002,14178 28002,14127 28002,14127 28002,14127 26105,14127 26105,14127 26105,14127 26105,14081 26105,\
14081"];
	Layer1_Device5_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25893,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage3_Accumulate -> Layer1_Device5_Stage4_Accumulate	[pos="e,25905,13877 25905,14017 25905,14017 25905,13887 25905,13887"];
	Layer1_Device5_Stage4_RecvKV -> Layer1_Device5_Stage4_Attention	[pos="e,25584,14070 26105,14196 25824,14196 25584,14196 25584,14196 25584,14196 25584,14080 25584,14080"];
	Layer1_Device5_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27126,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage4_RecvKV -> Layer1_Device5_Stage5_RecvKV	[label="Ring transfer",
		lp="27101,14140",
		pos="e,27054,14097 27054,14183 27054,14183 27054,14107 27054,14107"];
	Layer1_Device5_Stage4_Attention -> Layer1_Device5_Stage4_Accumulate	[pos="e,25674,13877 25628,14043 25655,14043 25674,14043 25674,14043 25674,14043 25674,13887 25674,13887"];
	Layer1_Device5_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27816,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage4_Accumulate -> Layer1_Device5_Stage5_Accumulate	[pos="e,27803,13684 25906,13824 25906,13784 25906,13715 25906,13715 25906,13715 27803,13715 27803,13715 27803,13715 27803,13694 27803,\
13694"];
	Layer1_Device5_Stage5_RecvKV -> Layer1_Device5_Stage5_Attention	[pos="e,28079,13850 28034,14044 28034,14038 28034,13850 28034,13850 28034,13850 28069,13850 28069,13850"];
	Layer1_Device5_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27102,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage5_RecvKV -> Layer1_Device5_Stage6_RecvKV	[label="Ring transfer",
		lp="27163,13947",
		pos="e,27114,13904 27114,13990 27114,13990 27114,13914 27114,13914"];
	Layer1_Device5_Stage5_Attention -> Layer1_Device5_Stage5_Accumulate	[pos="e,28048,13657 28092,13824 28092,13771 28092,13657 28092,13657 28092,13657 28058,13657 28058,13657"];
	Layer1_Device5_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27816,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage5_Accumulate -> Layer1_Device5_Stage6_Accumulate	[pos="e,27816,13491 27816,13631 27816,13631 27816,13501 27816,13501"];
	Layer1_Device5_Stage6_RecvKV -> Layer1_Device5_Stage6_Attention	[pos="e,28324,13684 27689,13810 28002,13810 28324,13810 28324,13810 28324,13810 28324,13694 28324,13694"];
	Layer1_Device5_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26607,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage6_RecvKV -> Layer1_Device5_Stage7_RecvKV	[label="Ring transfer",
		lp="26843,13754",
		pos="e,26854,13711 26854,13797 26854,13797 26854,13721 26854,13721"];
	Layer1_Device5_Stage6_Attention -> Layer1_Device5_Stage6_Accumulate	[pos="e,28048,13455 28242,13631 28242,13576 28242,13455 28242,13455 28242,13455 28058,13455 28058,13455"];
	Layer1_Device5_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27626,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage6_Accumulate -> Layer1_Device5_Stage7_Accumulate	[pos="e,27721,13298 27721,13438 27721,13438 27721,13308 27721,13308"];
	Layer1_Device5_Stage7_RecvKV -> Layer1_Device5_Stage7_Attention	[pos="e,25397,13491 25731,13617 25542,13617 25397,13617 25397,13617 25397,13617 25397,13501 25397,13501"];
	Layer1_Device5_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26607,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage7_RecvKV -> Layer1_Device5_Stage8_RecvKV	[label="Ring transfer",
		lp="26655,13561",
		pos="e,26607,13518 26607,13604 26607,13604 26607,13528 26607,13528"];
	Layer1_Device5_Stage7_Attention -> Layer1_Device5_Stage7_Accumulate	[pos="e,27461,13298 25563,13438 25563,13415 25563,13386 25563,13386 25563,13386 27461,13386 27461,13386 27461,13386 27461,13308 27461,\
13308"];
	Layer1_Device5_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27626,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage7_Accumulate -> Layer1_Device5_Stage8_Accumulate	[pos="e,27626,13105 27626,13245 27626,13245 27626,13115 27626,13115"];
	Layer1_Device5_Stage8_RecvKV -> Layer1_Device5_Stage8_Attention	[pos="e,27982,13298 27194,13424 27568,13424 27982,13424 27982,13424 27982,13424 27982,13308 27982,13308"];
	Layer1_Device5_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26417,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage8_RecvKV -> Layer1_Device5_Stage9_RecvKV	[label="Ring transfer",
		lp="26560,13368",
		pos="e,26512,13325 26512,13411 26512,13411 26512,13335 26512,13335"];
	Layer1_Device5_Stage8_Attention -> Layer1_Device5_Stage8_Accumulate	[pos="e,27858,13078 27935,13245 27935,13192 27935,13078 27935,13078 27935,13078 27868,13078 27868,13078"];
	Layer1_Device5_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25825,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage8_Accumulate -> Layer1_Device5_Stage9_Accumulate	[pos="e,25902,12912 27626,13052 27626,13030 27626,13003 27626,13003 27626,13003 25902,13003 25902,13003 25902,13003 25902,12922 25902,\
12922"];
	Layer1_Device5_Stage9_RecvKV -> Layer1_Device5_Stage9_Attention	[pos="e,25303,13105 25541,13231 25402,13231 25303,13231 25303,13231 25303,13231 25303,13115 25303,13115"];
	Layer1_Device5_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26417,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage9_RecvKV -> Layer1_Device5_Stage10_RecvKV	[label="Ring transfer",
		lp="26465,13175",
		pos="e,26417,13132 26417,13218 26417,13218 26417,13142 26417,13142"];
	Layer1_Device5_Stage9_Attention -> Layer1_Device5_Stage9_Accumulate	[pos="e,25748,12913 25256,13052 25256,13008 25256,12925 25256,12925 25256,12925 25748,12925 25748,12925 25748,12925 25748,12923 25748,\
12923"];
	Layer1_Device5_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25825,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage9_Accumulate -> Layer1_Device5_Stage10_Accumulate	[pos="e,25825,12719 25825,12859 25825,12859 25825,12729 25825,12729"];
	Layer1_Device5_Stage10_RecvKV -> Layer1_Device5_Stage10_Attention	[pos="e,25516,12913 25516,13079 25516,13079 25516,12923 25516,12923"];
	Layer1_Device5_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27034,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage10_RecvKV -> Layer1_Device5_Stage11_RecvKV	[label="Ring transfer",
		lp="26734,12982",
		pos="e,26726,12939 26726,13025 26726,13025 26726,12949 26726,12949"];
	Layer1_Device5_Stage10_Attention -> Layer1_Device5_Stage10_Accumulate	[pos="e,25593,12692 25466,12859 25466,12806 25466,12692 25466,12692 25466,12692 25583,12692 25583,12692"];
	Layer1_Device5_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26443,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage10_Accumulate -> Layer1_Device5_Stage11_Accumulate	[pos="e,26307,12526 26057,12692 26180,12692 26307,12692 26307,12692 26307,12692 26307,12536 26307,12536"];
	Layer1_Device5_Stage11_RecvKV -> Layer1_Device5_Stage11_Attention	[pos="e,26557,12719 26557,12832 26557,12832 26557,12729 26557,12729"];
	Layer1_Device5_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27843,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage11_RecvKV -> Layer1_Device5_Stage12_RecvKV	[label="Ring transfer",
		lp="27487,12789",
		pos="e,27438,12746 27438,12832 27438,12832 27438,12756 27438,12756"];
	Layer1_Device5_Stage11_Attention -> Layer1_Device5_Stage11_Accumulate	[pos="e,26538,12526 26538,12666 26538,12666 26538,12536 26538,12536"];
	Layer1_Device5_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26561,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage11_Accumulate -> Layer1_Device5_Stage12_Accumulate	[pos="e,26502,12333 26502,12473 26502,12473 26502,12343 26502,12343"];
	Layer1_Device5_Stage12_RecvKV -> Layer1_Device5_Stage12_Attention	[pos="e,27059,12526 27059,12639 27059,12639 27059,12536 27059,12536"];
	Layer1_Device5_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28173,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage12_RecvKV -> Layer1_Device5_Stage13_RecvKV	[label="Ring transfer",
		lp="28056,12596",
		pos="e,28008,12553 28008,12639 28008,12639 28008,12563 28008,12563"];
	Layer1_Device5_Stage12_Attention -> Layer1_Device5_Stage12_Accumulate	[pos="e,26762,12333 26762,12473 26762,12473 26762,12343 26762,12343"];
	Layer1_Device5_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27082,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage12_Accumulate -> Layer1_Device5_Stage13_Accumulate	[pos="e,26850,12113 26793,12306 26810,12306 26821,12306 26821,12306 26821,12306 26821,12113 26821,12113 26821,12113 26840,12113 26840,\
12113"];
	Layer1_Device5_Stage13_RecvKV -> Layer1_Device5_Stage13_Attention	[pos="e,28920,12334 28920,12500 28920,12500 28920,12344 28920,12344"];
	Layer1_Device5_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27770,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage13_RecvKV -> Layer1_Device5_Stage14_RecvKV	[label="Ring transfer",
		lp="28087,12403",
		pos="e,27972,12360 27972,12446 27972,12446 27972,12370 27972,12370"];
	Layer1_Device5_Stage13_Attention -> Layer1_Device5_Stage13_Accumulate	[pos="e,27082,12140 28979,12280 28979,12245 28979,12190 28979,12190 28979,12190 27082,12190 27082,12190 27082,12190 27082,12150 27082,\
12150"];
	Layer1_Device5_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27082,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage13_Accumulate -> Layer1_Device5_Stage14_Accumulate	[pos="e,27082,11988 27082,12087 27082,12087 27082,11998 27082,11998"];
	Layer1_Device5_Stage14_RecvKV -> Layer1_Device5_Stage14_Attention	[pos="e,26561,12140 26894,12266 26705,12266 26561,12266 26561,12266 26561,12266 26561,12150 26561,12150"];
	Layer1_Device5_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28291,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device5_Stage14_RecvKV -> Layer1_Device5_Stage15_RecvKV	[label="Ring transfer",
		lp="28078,12210",
		pos="e,28030,12167 28030,12253 28030,12253 28030,12177 28030,12177"];
	Layer1_Device5_Stage14_Attention -> Layer1_Device5_Stage14_Accumulate	[pos="e,26850,11952 26561,12087 26561,12041 26561,11952 26561,11952 26561,11952 26840,11952 26840,11952"];
	Layer1_Device5_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28995,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device5_Stage14_Accumulate -> Layer1_Device5_Stage15_Accumulate	[pos="e,28763,11835 27082,11934 27082,11897 27082,11835 27082,11835 27082,11835 28753,11835 28753,11835"];
	Layer1_Device5_Stage15_RecvKV -> Layer1_Device5_Stage15_Attention	[pos="e,28987,11988 28987,12104 28987,12104 28987,11998 28987,11998"];
	Layer1_Device5_Stage15_Attention -> Layer1_Device5_Stage15_Accumulate	[pos="e,28995,11862 28995,11934 28995,11934 28995,11872 28995,11872"];
	Layer1_Device5_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29050,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device5_Stage15_Accumulate -> Layer1_Device5_ConcatHeads	[pos="e,29026,11736 29026,11808 29026,11808 29026,11746 29026,11746"];
	Layer1_Device5_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29087,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device5_ConcatHeads -> Layer1_Device5_OutputProj	[pos="e,29075,11610 29075,11682 29075,11682 29075,11620 29075,11620"];
	Layer1_Device5_OutputProj -> Layer1_Device5_Residual1	[pos="e,29116,11484 29116,11556 29116,11556 29116,11494 29116,11494"];
	Layer1_Device5_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29167,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device5_Residual1 -> Layer1_Device5_LayerNorm2	[pos="e,29167,11358 29167,11430 29167,11430 29167,11368 29167,11368"];
	Layer1_Device5_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="29273,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device5_Residual1 -> Layer1_Device5_Residual2	[pos="e,29536,10676 29536,11430 29536,11430 29536,10686 29536,10686"];
	Layer1_Device5_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="28875,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device5_LayerNorm2 -> Layer1_Device5_GateProj	[pos="e,29027,11232 29027,11304 29027,11304 29027,11242 29027,11242"];
	Layer1_Device5_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29188,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device5_LayerNorm2 -> Layer1_Device5_UpProj	[pos="e,29240,11143 29240,11304 29240,11304 29240,11153 29240,11153"];
	Layer1_Device5_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="28875,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device5_GateProj -> Layer1_Device5_Activation	[pos="e,28806,11054 28806,11179 28806,11179 28806,11064 28806,11064"];
	Layer1_Device5_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="28970,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device5_UpProj -> Layer1_Device5_ElemMul	[pos="e,29240,10928 29240,11089 29240,11089 29240,10938 29240,10938"];
	Layer1_Device5_Activation -> Layer1_Device5_ElemMul	[pos="e,28875,10928 28875,11000 28875,11000 28875,10938 28875,10938"];
	Layer1_Device5_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29066,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device5_ElemMul -> Layer1_Device5_DownProj	[pos="e,29066,10802 29066,10874 29066,10874 29066,10812 29066,10812"];
	Layer1_Device5_DownProj -> Layer1_Device5_Residual2	[pos="e,29086,10676 29086,10748 29086,10748 29086,10686 29086,10686"];
	Layer1_Device5_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29273,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device5_Residual2 -> Layer1_Device5_Output	[pos="e,29273,10550 29273,10622 29273,10622 29273,10560 29273,10560"];
	Layer2_Device5_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29273,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device5_Output -> Layer2_Device5_Input	[pos="e,29273,10402 29273,10475 29273,10475 29273,10412 29273,10412"];
	Layer1_Device6_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33710,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device6_Input -> Layer1_Device6_LayerNorm1	[pos="e,33879,15328 33879,15419 33879,15419 33879,15338 33879,15338"];
	Layer1_Device6_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="34039,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device6_Input -> Layer1_Device6_Residual1	[pos="e,34317,11484 34317,15408 34317,15408 34317,11494 34317,11494"];
	Layer1_Device6_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33246,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device6_LayerNorm1 -> Layer1_Device6_QKVProj	[pos="e,33696,15202 33696,15274 33696,15274 33696,15212 33696,15212"];
	Layer1_Device6_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32000,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage0_RecvKV	[label="Local K,V",
		lp="32500,15105",
		pos="e,32758,15062 32758,15148 32758,15148 32758,15072 32758,15072"];
	Layer1_Device6_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="32753,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage0_Attention	[label=Q_local,
		lp="33062,15009",
		pos="e,32952,14843 32952,15148 32952,15148 32952,14853 32952,14853"];
	Layer1_Device6_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30460,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage1_Attention	[label=Q_local,
		lp="30524,14912",
		pos="e,30610,14650 32596,15156 31816,15156 30610,15156 30610,15156 30610,15156 30610,14660 30610,14660"];
	Layer1_Device6_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31071,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage2_Attention	[label=Q_local,
		lp="33202,14816",
		pos="e,31241,14457 33047,15149 33047,15048 33047,14694 33047,14694 33047,14694 31241,14694 31241,14694 31241,14694 31241,14467 31241,\
14467"];
	Layer1_Device6_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33086,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage3_Attention	[label=Q_local,
		lp="33342,14719",
		pos="e,33259,14264 33259,15149 33259,15149 33259,14274 33259,14274"];
	Layer1_Device6_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30292,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage4_Attention	[label=Q_local,
		lp="30144,14623",
		pos="e,30180,14070 32596,15171 31697,15171 30180,15171 30180,15171 30180,15171 30180,14080 30180,14080"];
	Layer1_Device6_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33206,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage5_Attention	[label=Q_local,
		lp="33440,14526",
		pos="e,33378,13877 33378,15148 33378,15148 33378,13887 33378,13887"];
	Layer1_Device6_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33258,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage6_Attention	[label=Q_local,
		lp="33578,14430",
		pos="e,33464,13684 33464,15149 33464,15149 33464,13694 33464,13694"];
	Layer1_Device6_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30319,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage7_Attention	[label=Q_local,
		lp="29998,14333",
		pos="e,30087,13473 32596,15186 31653,15186 30018,15186 30018,15186 30018,15186 30018,13473 30018,13473 30018,13473 30077,13473 30077,\
13473"];
	Layer1_Device6_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33023,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage8_Attention	[label=Q_local,
		lp="33698,14237",
		pos="e,33255,13271 33551,15149 33551,14919 33551,13271 33551,13271 33551,13271 33265,13271 33265,13271"];
	Layer1_Device6_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30084,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage9_Attention	[label=Q_local,
		lp="29880,14140",
		pos="e,29974,13105 32596,15194 31642,15194 29974,15194 29974,15194 29974,15194 29974,13115 29974,13115"];
	Layer1_Device6_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30528,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage10_Attention	[label=Q_local,
		lp="29740,14044",
		pos="e,30631,12912 33613,15149 33613,14902 33613,13017 33613,13017 33613,13017 30631,13017 30631,13017 30631,13017 30631,12922 30631,\
12922"];
	Layer1_Device6_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31669,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage11_Attention	[label=Q_local,
		lp="33819,13947",
		pos="e,31746,12719 33674,15148 33674,14884 33674,12750 33674,12750 33674,12750 31746,12750 31746,12750 31746,12750 31746,12729 31746,\
12729"];
	Layer1_Device6_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31859,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage12_Attention	[label=Q_local,
		lp="33962,13851",
		pos="e,31929,12526 33736,15148 33736,14884 33736,12748 33736,12748 33736,12748 31929,12748 31929,12748 31929,12748 31929,12536 31929,\
12536"];
	Layer1_Device6_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33874,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage13_Attention	[label=Q_local,
		lp="34102,13754",
		pos="e,34047,12334 33896,15160 33985,15160 34047,15160 34047,15160 34047,15160 34047,12344 34047,12344"];
	Layer1_Device6_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31456,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage14_Attention	[label=Q_local,
		lp="29594,13658",
		pos="e,31533,12140 33896,15172 34034,15172 34140,15172 34140,15172 34140,15172 34140,12241 34140,12241 34140,12241 31533,12241 31533,\
12241 31533,12241 31533,12150 31533,12150"];
	Layer1_Device6_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33329,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_QKVProj -> Layer1_Device6_Stage15_Attention	[label=Q_local,
		lp="34220,13561",
		pos="e,33561,11970 33896,15178 34052,15178 34174,15178 34174,15178 34174,15178 34174,11970 34174,11970 34174,11970 33571,11970 33571,\
11970"];
	Layer1_Device6_Stage0_RecvKV -> Layer1_Device6_Stage0_Attention	[pos="e,32721,14842 32721,15005 32721,15005 32721,14852 32721,14852"];
	Layer1_Device6_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31544,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage0_RecvKV -> Layer1_Device6_Stage1_RecvKV	[label="Ring transfer",
		lp="31901,14912",
		pos="e,31772,14869 31772,14955 31772,14955 31772,14879 31772,14879"];
	Layer1_Device6_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="30981,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage0_Attention -> Layer1_Device6_Stage0_Accumulate	[pos="e,30981,14650 32753,14789 32753,14754 32753,14696 32753,14696 32753,14696 30981,14696 30981,14696 30981,14696 30981,14660 30981,\
14660"];
	Layer1_Device6_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30550,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage0_Accumulate -> Layer1_Device6_Stage1_Accumulate	[pos="e,30766,14456 30766,14596 30766,14596 30766,14466 30766,14466"];
	Layer1_Device6_Stage1_RecvKV -> Layer1_Device6_Stage1_Attention	[pos="e,30658,14650 30658,14816 30658,14816 30658,14660 30658,14660"];
	Layer1_Device6_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32190,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage1_RecvKV -> Layer1_Device6_Stage2_RecvKV	[label="Ring transfer",
		lp="31861,14719",
		pos="e,31867,14676 31867,14762 31867,14762 31867,14686 31867,14686"];
	Layer1_Device6_Stage1_Attention -> Layer1_Device6_Stage1_Accumulate	[pos="e,30505,14456 30505,14596 30505,14596 30505,14466 30505,14466"];
	Layer1_Device6_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30668,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage1_Accumulate -> Layer1_Device6_Stage2_Accumulate	[pos="e,30609,14263 30609,14403 30609,14403 30609,14273 30609,14273"];
	Layer1_Device6_Stage2_RecvKV -> Layer1_Device6_Stage2_Attention	[pos="e,31286,14457 31286,14623 31286,14623 31286,14467 31286,14467"];
	Layer1_Device6_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32280,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage2_RecvKV -> Layer1_Device6_Stage3_RecvKV	[label="Ring transfer",
		lp="32283,14526",
		pos="e,32235,14483 32235,14569 32235,14569 32235,14493 32235,14493"];
	Layer1_Device6_Stage2_Attention -> Layer1_Device6_Stage2_Accumulate	[pos="e,30870,14263 30870,14403 30870,14403 30870,14273 30870,14273"];
	Layer1_Device6_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30813,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage2_Accumulate -> Layer1_Device6_Stage3_Accumulate	[pos="e,30740,14070 30740,14210 30740,14210 30740,14080 30740,14080"];
	Layer1_Device6_Stage3_RecvKV -> Layer1_Device6_Stage3_Attention	[pos="e,33027,14264 33027,14430 33027,14430 33027,14274 33027,14274"];
	Layer1_Device6_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31877,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage3_RecvKV -> Layer1_Device6_Stage4_RecvKV	[label="Ring transfer",
		lp="32126,14333",
		pos="e,32078,14290 32078,14376 32078,14376 32078,14300 32078,14300"];
	Layer1_Device6_Stage3_Attention -> Layer1_Device6_Stage3_Accumulate	[pos="e,31001,14071 32898,14210 32898,14179 32898,14132 32898,14132 32898,14132 31001,14132 31001,14132 31001,14132 31001,14081 31001,\
14081"];
	Layer1_Device6_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30788,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage3_Accumulate -> Layer1_Device6_Stage4_Accumulate	[pos="e,30800,13877 30800,14017 30800,14017 30800,13887 30800,13887"];
	Layer1_Device6_Stage4_RecvKV -> Layer1_Device6_Stage4_Attention	[pos="e,30480,14070 31001,14196 30720,14196 30480,14196 30480,14196 30480,14196 30480,14080 30480,14080"];
	Layer1_Device6_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32022,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage4_RecvKV -> Layer1_Device6_Stage5_RecvKV	[label="Ring transfer",
		lp="31997,14140",
		pos="e,31950,14097 31950,14183 31950,14183 31950,14107 31950,14107"];
	Layer1_Device6_Stage4_Attention -> Layer1_Device6_Stage4_Accumulate	[pos="e,30569,13877 30524,14043 30551,14043 30569,14043 30569,14043 30569,14043 30569,13887 30569,13887"];
	Layer1_Device6_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32737,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage4_Accumulate -> Layer1_Device6_Stage5_Accumulate	[pos="e,32711,13684 30814,13824 30814,13796 30814,13756 30814,13756 30814,13756 32711,13756 32711,13756 32711,13756 32711,13694 32711,\
13694"];
	Layer1_Device6_Stage5_RecvKV -> Layer1_Device6_Stage5_Attention	[pos="e,32974,13859 32929,14044 32929,14038 32929,13859 32929,13859 32929,13859 32964,13859 32964,13859"];
	Layer1_Device6_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31997,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage5_RecvKV -> Layer1_Device6_Stage6_RecvKV	[label="Ring transfer",
		lp="32057,13947",
		pos="e,32010,13904 32010,13990 32010,13990 32010,13914 32010,13914"];
	Layer1_Device6_Stage5_Attention -> Layer1_Device6_Stage5_Accumulate	[pos="e,32955,13684 32974,13841 32962,13841 32955,13841 32955,13841 32955,13841 32955,13694 32955,13694"];
	Layer1_Device6_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32737,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage5_Accumulate -> Layer1_Device6_Stage6_Accumulate	[pos="e,32737,13491 32737,13631 32737,13631 32737,13501 32737,13501"];
	Layer1_Device6_Stage6_RecvKV -> Layer1_Device6_Stage6_Attention	[pos="e,33232,13684 32584,13810 32903,13810 33232,13810 33232,13810 33232,13810 33232,13694 33232,13694"];
	Layer1_Device6_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31528,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage6_RecvKV -> Layer1_Device6_Stage7_RecvKV	[label="Ring transfer",
		lp="31811,13754",
		pos="e,31762,13711 31762,13797 31762,13797 31762,13721 31762,13721"];
	Layer1_Device6_Stage6_Attention -> Layer1_Device6_Stage6_Accumulate	[pos="e,32969,13455 33140,13631 33140,13576 33140,13455 33140,13455 33140,13455 32979,13455 32979,13455"];
	Layer1_Device6_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32502,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage6_Accumulate -> Layer1_Device6_Stage7_Accumulate	[pos="e,32620,13298 32620,13438 32620,13438 32620,13308 32620,13308"];
	Layer1_Device6_Stage7_RecvKV -> Layer1_Device6_Stage7_Attention	[pos="e,30537,13491 30652,13617 30582,13617 30537,13617 30537,13617 30537,13617 30537,13501 30537,13501"];
	Layer1_Device6_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31528,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage7_RecvKV -> Layer1_Device6_Stage8_RecvKV	[label="Ring transfer",
		lp="31576,13561",
		pos="e,31528,13518 31528,13604 31528,13604 31528,13528 31528,13528"];
	Layer1_Device6_Stage7_Attention -> Layer1_Device6_Stage7_Accumulate	[pos="e,32359,13298 30462,13438 30462,13404 30462,13350 30462,13350 30462,13350 32359,13350 32359,13350 32359,13350 32359,13308 32359,\
13308"];
	Layer1_Device6_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32502,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage7_Accumulate -> Layer1_Device6_Stage8_Accumulate	[pos="e,32502,13105 32502,13245 32502,13245 32502,13115 32502,13115"];
	Layer1_Device6_Stage8_RecvKV -> Layer1_Device6_Stage8_Attention	[pos="e,32880,13298 32115,13424 32480,13424 32880,13424 32880,13424 32880,13424 32880,13308 32880,13308"];
	Layer1_Device6_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31293,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage8_RecvKV -> Layer1_Device6_Stage9_RecvKV	[label="Ring transfer",
		lp="31482,13368",
		pos="e,31410,13325 31410,13411 31410,13411 31410,13335 31410,13335"];
	Layer1_Device6_Stage8_Attention -> Layer1_Device6_Stage8_Accumulate	[pos="e,32734,13078 32985,13245 32985,13192 32985,13078 32985,13078 32985,13078 32744,13078 32744,13078"];
	Layer1_Device6_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31049,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage8_Accumulate -> Layer1_Device6_Stage9_Accumulate	[pos="e,31126,12912 32502,13052 32502,13018 32502,12963 32502,12963 32502,12963 31126,12963 31126,12963 31126,12963 31126,12922 31126,\
12922"];
	Layer1_Device6_Stage9_RecvKV -> Layer1_Device6_Stage9_Attention	[pos="e,30202,13105 30417,13231 30290,13231 30202,13231 30202,13231 30202,13231 30202,13115 30202,13115"];
	Layer1_Device6_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31293,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage9_RecvKV -> Layer1_Device6_Stage10_RecvKV	[label="Ring transfer",
		lp="31341,13175",
		pos="e,31293,13132 31293,13218 31293,13218 31293,13142 31293,13142"];
	Layer1_Device6_Stage9_Attention -> Layer1_Device6_Stage9_Accumulate	[pos="e,30972,12913 30306,13052 30306,13008 30306,12925 30306,12925 30306,12925 30972,12925 30972,12925 30972,12925 30972,12923 30972,\
12923"];
	Layer1_Device6_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31049,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage9_Accumulate -> Layer1_Device6_Stage10_Accumulate	[pos="e,31049,12719 31049,12859 31049,12859 31049,12729 31049,12729"];
	Layer1_Device6_Stage10_RecvKV -> Layer1_Device6_Stage10_Attention	[pos="e,30502,12912 30502,13025 30502,13025 30502,12922 30502,12922"];
	Layer1_Device6_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32258,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage10_RecvKV -> Layer1_Device6_Stage11_RecvKV	[label="Ring transfer",
		lp="31610,12982",
		pos="e,31776,12939 31776,13025 31776,13025 31776,12949 31776,12949"];
	Layer1_Device6_Stage10_Attention -> Layer1_Device6_Stage10_Accumulate	[pos="e,30817,12692 30722,12859 30722,12806 30722,12692 30722,12692 30722,12692 30807,12692 30807,12692"];
	Layer1_Device6_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31338,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage10_Accumulate -> Layer1_Device6_Stage11_Accumulate	[pos="e,31194,12526 31194,12666 31194,12666 31194,12536 31194,12536"];
	Layer1_Device6_Stage11_RecvKV -> Layer1_Device6_Stage11_Attention	[pos="e,31592,12719 31592,12832 31592,12832 31592,12729 31592,12729"];
	Layer1_Device6_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32878,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage11_RecvKV -> Layer1_Device6_Stage12_RecvKV	[label="Ring transfer",
		lp="32616,12789",
		pos="e,32568,12746 32568,12832 32568,12832 32568,12756 32568,12756"];
	Layer1_Device6_Stage11_Attention -> Layer1_Device6_Stage11_Accumulate	[pos="e,31504,12526 31504,12666 31504,12666 31504,12536 31504,12536"];
	Layer1_Device6_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31456,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage11_Accumulate -> Layer1_Device6_Stage12_Accumulate	[pos="e,31397,12333 31397,12473 31397,12473 31397,12343 31397,12343"];
	Layer1_Device6_Stage12_RecvKV -> Layer1_Device6_Stage12_Attention	[pos="e,32024,12527 32024,12693 32024,12693 32024,12537 32024,12537"];
	Layer1_Device6_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33068,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage12_RecvKV -> Layer1_Device6_Stage13_RecvKV	[label="Ring transfer",
		lp="33021,12596",
		pos="e,32973,12553 32973,12639 32973,12639 32973,12563 32973,12563"];
	Layer1_Device6_Stage12_Attention -> Layer1_Device6_Stage12_Accumulate	[pos="e,31658,12333 31658,12473 31658,12473 31658,12343 31658,12343"];
	Layer1_Device6_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31977,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage12_Accumulate -> Layer1_Device6_Stage13_Accumulate	[pos="e,31745,12113 31688,12306 31705,12306 31716,12306 31716,12306 31716,12306 31716,12113 31716,12113 31716,12113 31735,12113 31735,\
12113"];
	Layer1_Device6_Stage13_RecvKV -> Layer1_Device6_Stage13_Attention	[pos="e,33815,12334 33815,12500 33815,12500 33815,12344 33815,12344"];
	Layer1_Device6_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32665,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage13_RecvKV -> Layer1_Device6_Stage14_RecvKV	[label="Ring transfer",
		lp="32849,12403",
		pos="e,32866,12360 32866,12446 32866,12446 32866,12370 32866,12370"];
	Layer1_Device6_Stage13_Attention -> Layer1_Device6_Stage13_Accumulate	[pos="e,31977,12140 33874,12280 33874,12247 33874,12194 33874,12194 33874,12194 31977,12194 31977,12194 31977,12194 31977,12150 31977,\
12150"];
	Layer1_Device6_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31977,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage13_Accumulate -> Layer1_Device6_Stage14_Accumulate	[pos="e,31977,11988 31977,12087 31977,12087 31977,11998 31977,11998"];
	Layer1_Device6_Stage14_RecvKV -> Layer1_Device6_Stage14_Attention	[pos="e,31379,12140 31789,12266 31561,12266 31379,12266 31379,12266 31379,12266 31379,12150 31379,12150"];
	Layer1_Device6_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33186,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device6_Stage14_RecvKV -> Layer1_Device6_Stage15_RecvKV	[label="Ring transfer",
		lp="32973,12210",
		pos="e,32926,12167 32926,12253 32926,12253 32926,12177 32926,12177"];
	Layer1_Device6_Stage14_Attention -> Layer1_Device6_Stage14_Accumulate	[pos="e,31745,11961 31456,12087 31456,12043 31456,11961 31456,11961 31456,11961 31735,11961 31735,11961"];
	Layer1_Device6_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="33329,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device6_Stage14_Accumulate -> Layer1_Device6_Stage15_Accumulate	[pos="e,33097,11835 31977,11934 31977,11897 31977,11835 31977,11835 31977,11835 33087,11835 33087,11835"];
	Layer1_Device6_Stage15_RecvKV -> Layer1_Device6_Stage15_Attention	[pos="e,33329,11988 33329,12060 33329,12060 33329,11998 33329,11998"];
	Layer1_Device6_Stage15_Attention -> Layer1_Device6_Stage15_Accumulate	[pos="e,33329,11862 33329,11934 33329,11934 33329,11872 33329,11872"];
	Layer1_Device6_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33583,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device6_Stage15_Accumulate -> Layer1_Device6_ConcatHeads	[pos="e,33459,11736 33459,11808 33459,11808 33459,11746 33459,11746"];
	Layer1_Device6_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33995,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device6_ConcatHeads -> Layer1_Device6_OutputProj	[pos="e,33795,11610 33795,11682 33795,11682 33795,11620 33795,11620"];
	Layer1_Device6_OutputProj -> Layer1_Device6_Residual1	[pos="e,33995,11484 33995,11556 33995,11556 33995,11494 33995,11494"];
	Layer1_Device6_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33888,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device6_Residual1 -> Layer1_Device6_LayerNorm2	[pos="e,33888,11358 33888,11430 33888,11430 33888,11368 33888,11368"];
	Layer1_Device6_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="33988,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device6_Residual1 -> Layer1_Device6_Residual2	[pos="e,34254,10676 34254,11430 34254,11430 34254,10686 34254,10686"];
	Layer1_Device6_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33596,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device6_LayerNorm2 -> Layer1_Device6_GateProj	[pos="e,33748,11232 33748,11304 33748,11304 33748,11242 33748,11242"];
	Layer1_Device6_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33909,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device6_LayerNorm2 -> Layer1_Device6_UpProj	[pos="e,33961,11143 33961,11304 33961,11304 33961,11153 33961,11153"];
	Layer1_Device6_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33596,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device6_GateProj -> Layer1_Device6_Activation	[pos="e,33528,11054 33528,11179 33528,11179 33528,11064 33528,11064"];
	Layer1_Device6_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33691,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device6_UpProj -> Layer1_Device6_ElemMul	[pos="e,33961,10928 33961,11089 33961,11089 33961,10938 33961,10938"];
	Layer1_Device6_Activation -> Layer1_Device6_ElemMul	[pos="e,33596,10928 33596,11000 33596,11000 33596,10938 33596,10938"];
	Layer1_Device6_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33834,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device6_ElemMul -> Layer1_Device6_DownProj	[pos="e,33834,10802 33834,10874 33834,10874 33834,10812 33834,10812"];
	Layer1_Device6_DownProj -> Layer1_Device6_Residual2	[pos="e,33834,10676 33834,10748 33834,10748 33834,10686 33834,10686"];
	Layer1_Device6_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33988,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device6_Residual2 -> Layer1_Device6_Output	[pos="e,33988,10550 33988,10622 33988,10622 33988,10560 33988,10560"];
	Layer2_Device6_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33988,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device6_Output -> Layer2_Device6_Input	[pos="e,33988,10402 33988,10475 33988,10475 33988,10412 33988,10412"];
	Layer1_Device7_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38717,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device7_Input -> Layer1_Device7_LayerNorm1	[pos="e,38915,15328 38915,15427 38915,15427 38915,15338 38915,15338"];
	Layer1_Device7_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="39053,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device7_Input -> Layer1_Device7_Residual1	[pos="e,39292,11484 39292,15402 39292,15402 39292,11494 39292,11494"];
	Layer1_Device7_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38138,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device7_LayerNorm1 -> Layer1_Device7_QKVProj	[pos="e,38646,15202 38646,15274 38646,15274 38646,15212 38646,15212"];
	Layer1_Device7_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36892,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage0_RecvKV	[label="Local K,V",
		lp="37392,15105",
		pos="e,37650,15062 37650,15148 37650,15148 37650,15072 37650,15072"];
	Layer1_Device7_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37682,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage0_Attention	[label=Q_local,
		lp="37954,15009",
		pos="e,37863,14843 37863,15148 37863,15148 37863,14853 37863,14853"];
	Layer1_Device7_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35352,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage1_Attention	[label=Q_local,
		lp="35416,14912",
		pos="e,35534,14649 37488,15154 36717,15154 35534,15154 35534,15154 35534,15154 35534,14659 35534,14659"];
	Layer1_Device7_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35963,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage2_Attention	[label=Q_local,
		lp="38094,14816",
		pos="e,36133,14456 37958,15148 37958,15051 37958,14720 37958,14720 37958,14720 36133,14720 36133,14720 36133,14720 36133,14466 36133,\
14466"];
	Layer1_Device7_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37978,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage3_Attention	[label=Q_local,
		lp="38234,14719",
		pos="e,38151,14264 38151,15149 38151,15149 38151,14274 38151,14274"];
	Layer1_Device7_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35184,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage4_Attention	[label=Q_local,
		lp="35036,14623",
		pos="e,35057,14070 37488,15166 36584,15166 35057,15166 35057,15166 35057,15166 35057,14080 35057,14080"];
	Layer1_Device7_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38097,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage5_Attention	[label=Q_local,
		lp="38339,14526",
		pos="e,38269,13877 38269,15148 38269,15148 38269,13887 38269,13887"];
	Layer1_Device7_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38196,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage6_Attention	[label=Q_local,
		lp="38470,14430",
		pos="e,38378,13684 38378,15149 38378,15149 38378,13694 38378,13694"];
	Layer1_Device7_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35257,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage7_Attention	[label=Q_local,
		lp="34890,14333",
		pos="e,35025,13473 37488,15184 36550,15184 34929,15184 34929,15184 34929,15184 34929,13473 34929,13473 34929,13473 35015,13473 35015,\
13473"];
	Layer1_Device7_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38053,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage8_Attention	[label=Q_local,
		lp="38589,14237",
		pos="e,38285,13271 38470,15149 38470,14919 38470,13271 38470,13271 38470,13271 38295,13271 38295,13271"];
	Layer1_Device7_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35114,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage9_Attention	[label=Q_local,
		lp="34772,14140",
		pos="e,34906,13105 37488,15189 36545,15189 34906,15189 34906,15189 34906,15189 34906,13115 34906,13115"];
	Layer1_Device7_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35047,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage10_Attention	[label=Q_local,
		lp="34632,14044",
		pos="e,34849,12912 37488,15195 36530,15195 34849,15195 34849,15195 34849,15195 34849,12922 34849,12922"];
	Layer1_Device7_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36510,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage11_Attention	[label=Q_local,
		lp="38710,13947",
		pos="e,36587,12720 38554,15149 38554,14887 38554,12775 38554,12775 38554,12775 36587,12775 36587,12775 36587,12775 36587,12730 36587,\
12730"];
	Layer1_Device7_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36750,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage12_Attention	[label=Q_local,
		lp="38853,13851",
		pos="e,36770,12526 38596,15149 38596,14887 38596,12773 38596,12773 38596,12773 36770,12773 36770,12773 36770,12773 36770,12536 36770,\
12536"];
	Layer1_Device7_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38765,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage13_Attention	[label=Q_local,
		lp="38993,13754",
		pos="e,38904,12334 38788,15161 38858,15161 38904,15161 38904,15161 38904,15161 38904,12344 38904,12344"];
	Layer1_Device7_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36347,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage14_Attention	[label=Q_local,
		lp="34486,13658",
		pos="e,36115,12113 38512,15148 38512,14889 38512,12827 38512,12827 38512,12827 35927,12827 35927,12827 35927,12827 35927,12113 35927,\
12113 35927,12113 36105,12113 36105,12113"];
	Layer1_Device7_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38493,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_QKVProj -> Layer1_Device7_Stage15_Attention	[label=Q_local,
		lp="39111,13561",
		pos="e,38725,11970 38788,15175 38982,15175 39144,15175 39144,15175 39144,15175 39144,11970 39144,11970 39144,11970 38735,11970 38735,\
11970"];
	Layer1_Device7_Stage0_RecvKV -> Layer1_Device7_Stage0_Attention	[pos="e,37631,14843 37631,15009 37631,15009 37631,14853 37631,14853"];
	Layer1_Device7_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36473,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage0_RecvKV -> Layer1_Device7_Stage1_RecvKV	[label="Ring transfer",
		lp="36802,14912",
		pos="e,36682,14869 36682,14955 36682,14955 36682,14879 36682,14879"];
	Layer1_Device7_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="35873,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage0_Attention -> Layer1_Device7_Stage0_Accumulate	[pos="e,35873,14649 37682,14789 37682,14762 37682,14723 37682,14723 37682,14723 35873,14723 35873,14723 35873,14723 35873,14659 35873,\
14659"];
	Layer1_Device7_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35442,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage0_Accumulate -> Layer1_Device7_Stage1_Accumulate	[pos="e,35658,14456 35658,14596 35658,14596 35658,14466 35658,14466"];
	Layer1_Device7_Stage1_RecvKV -> Layer1_Device7_Stage1_Attention	[pos="e,35568,14650 35568,14816 35568,14816 35568,14660 35568,14660"];
	Layer1_Device7_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37082,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage1_RecvKV -> Layer1_Device7_Stage2_RecvKV	[label="Ring transfer",
		lp="36826,14719",
		pos="e,36778,14676 36778,14762 36778,14762 36778,14686 36778,14686"];
	Layer1_Device7_Stage1_Attention -> Layer1_Device7_Stage1_Accumulate	[pos="e,35397,14456 35397,14596 35397,14596 35397,14466 35397,14466"];
	Layer1_Device7_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35560,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage1_Accumulate -> Layer1_Device7_Stage2_Accumulate	[pos="e,35501,14263 35501,14403 35501,14403 35501,14273 35501,14273"];
	Layer1_Device7_Stage2_RecvKV -> Layer1_Device7_Stage2_Attention	[pos="e,36178,14457 36178,14623 36178,14623 36178,14467 36178,14467"];
	Layer1_Device7_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37172,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage2_RecvKV -> Layer1_Device7_Stage3_RecvKV	[label="Ring transfer",
		lp="37175,14526",
		pos="e,37127,14483 37127,14569 37127,14569 37127,14493 37127,14493"];
	Layer1_Device7_Stage2_Attention -> Layer1_Device7_Stage2_Accumulate	[pos="e,35762,14263 35762,14403 35762,14403 35762,14273 35762,14273"];
	Layer1_Device7_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35705,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage2_Accumulate -> Layer1_Device7_Stage3_Accumulate	[pos="e,35632,14070 35632,14210 35632,14210 35632,14080 35632,14080"];
	Layer1_Device7_Stage3_RecvKV -> Layer1_Device7_Stage3_Attention	[pos="e,37919,14264 37919,14430 37919,14430 37919,14274 37919,14274"];
	Layer1_Device7_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36769,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage3_RecvKV -> Layer1_Device7_Stage4_RecvKV	[label="Ring transfer",
		lp="36953,14333",
		pos="e,36970,14290 36970,14376 36970,14376 36970,14300 36970,14300"];
	Layer1_Device7_Stage3_Attention -> Layer1_Device7_Stage3_Accumulate	[pos="e,35893,14071 37790,14210 37790,14180 37790,14137 37790,14137 37790,14137 35893,14137 35893,14137 35893,14137 35893,14081 35893,\
14081"];
	Layer1_Device7_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35679,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage3_Accumulate -> Layer1_Device7_Stage4_Accumulate	[pos="e,35692,13877 35692,14017 35692,14017 35692,13887 35692,13887"];
	Layer1_Device7_Stage4_RecvKV -> Layer1_Device7_Stage4_Attention	[pos="e,35372,14070 35893,14196 35612,14196 35372,14196 35372,14196 35372,14196 35372,14080 35372,14080"];
	Layer1_Device7_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36914,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage4_RecvKV -> Layer1_Device7_Stage5_RecvKV	[label="Ring transfer",
		lp="36889,14140",
		pos="e,36842,14097 36842,14183 36842,14183 36842,14107 36842,14107"];
	Layer1_Device7_Stage4_Attention -> Layer1_Device7_Stage4_Accumulate	[pos="e,35460,13877 35416,14043 35443,14043 35460,14043 35460,14043 35460,14043 35460,13887 35460,13887"];
	Layer1_Device7_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37675,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage4_Accumulate -> Layer1_Device7_Stage5_Accumulate	[pos="e,37626,13684 35728,13824 35728,13805 35728,13782 35728,13782 35728,13782 37626,13782 37626,13782 37626,13782 37626,13694 37626,\
13694"];
	Layer1_Device7_Stage5_RecvKV -> Layer1_Device7_Stage5_Attention	[pos="e,37865,13850 37821,14044 37821,14038 37821,13850 37821,13850 37821,13850 37855,13850 37855,13850"];
	Layer1_Device7_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36888,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage5_RecvKV -> Layer1_Device7_Stage6_RecvKV	[label="Ring transfer",
		lp="36949,13947",
		pos="e,36901,13904 36901,13990 36901,13990 36901,13914 36901,13914"];
	Layer1_Device7_Stage5_Attention -> Layer1_Device7_Stage5_Accumulate	[pos="e,37886,13684 37886,13824 37886,13824 37886,13694 37886,13694"];
	Layer1_Device7_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37675,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage5_Accumulate -> Layer1_Device7_Stage6_Accumulate	[pos="e,37675,13491 37675,13631 37675,13631 37675,13501 37675,13501"];
	Layer1_Device7_Stage6_RecvKV -> Layer1_Device7_Stage6_Attention	[pos="e,38146,13684 37475,13810 37803,13810 38146,13810 38146,13810 38146,13810 38146,13694 38146,13694"];
	Layer1_Device7_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36466,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage6_RecvKV -> Layer1_Device7_Stage7_RecvKV	[label="Ring transfer",
		lp="36692,13754",
		pos="e,36677,13711 36677,13797 36677,13797 36677,13721 36677,13721"];
	Layer1_Device7_Stage6_Attention -> Layer1_Device7_Stage6_Accumulate	[pos="e,37907,13455 38124,13631 38124,13576 38124,13455 38124,13455 38124,13455 37917,13455 37917,13455"];
	Layer1_Device7_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37532,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage6_Accumulate -> Layer1_Device7_Stage7_Accumulate	[pos="e,37604,13298 37604,13438 37604,13438 37604,13308 37604,13308"];
	Layer1_Device7_Stage7_RecvKV -> Layer1_Device7_Stage7_Attention	[pos="e,35468,13491 35590,13617 35516,13617 35468,13617 35468,13617 35468,13617 35468,13501 35468,13501"];
	Layer1_Device7_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36466,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage7_RecvKV -> Layer1_Device7_Stage8_RecvKV	[label="Ring transfer",
		lp="36514,13561",
		pos="e,36466,13518 36466,13604 36466,13604 36466,13528 36466,13528"];
	Layer1_Device7_Stage7_Attention -> Layer1_Device7_Stage7_Accumulate	[pos="e,37343,13298 35446,13438 35446,13419 35446,13396 35446,13396 35446,13396 37343,13396 37343,13396 37343,13396 37343,13308 37343,\
13308"];
	Layer1_Device7_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37532,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage7_Accumulate -> Layer1_Device7_Stage8_Accumulate	[pos="e,37532,13105 37532,13245 37532,13245 37532,13115 37532,13115"];
	Layer1_Device7_Stage8_RecvKV -> Layer1_Device7_Stage8_Attention	[pos="e,37864,13298 37053,13424 37436,13424 37864,13424 37864,13424 37864,13424 37864,13308 37864,13308"];
	Layer1_Device7_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36323,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage8_RecvKV -> Layer1_Device7_Stage9_RecvKV	[label="Ring transfer",
		lp="36442,13368",
		pos="e,36394,13325 36394,13411 36394,13411 36394,13335 36394,13335"];
	Layer1_Device7_Stage8_Attention -> Layer1_Device7_Stage8_Accumulate	[pos="e,37764,13078 38053,13245 38053,13192 38053,13078 38053,13078 38053,13078 37774,13078 37774,13078"];
	Layer1_Device7_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35568,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage8_Accumulate -> Layer1_Device7_Stage9_Accumulate	[pos="e,35601,12912 37499,13052 37499,13032 37499,13009 37499,13009 37499,13009 35601,13009 35601,13009 35601,13009 35601,12922 35601,\
12922"];
	Layer1_Device7_Stage9_RecvKV -> Layer1_Device7_Stage9_Attention	[pos="e,35186,13105 35447,13231 35295,13231 35186,13231 35186,13231 35186,13231 35186,13115 35186,13115"];
	Layer1_Device7_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36323,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage9_RecvKV -> Layer1_Device7_Stage10_RecvKV	[label="Ring transfer",
		lp="36371,13175",
		pos="e,36323,13132 36323,13218 36323,13218 36323,13142 36323,13142"];
	Layer1_Device7_Stage9_Attention -> Layer1_Device7_Stage9_Accumulate	[pos="e,35341,12912 35341,13052 35341,13052 35341,12922 35341,12922"];
	Layer1_Device7_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35568,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage9_Accumulate -> Layer1_Device7_Stage10_Accumulate	[pos="e,35568,12719 35568,12859 35568,12859 35568,12729 35568,12729"];
	Layer1_Device7_Stage10_RecvKV -> Layer1_Device7_Stage10_Attention	[pos="e,35080,12912 35447,13038 35241,13038 35080,13038 35080,13038 35080,13038 35080,12922 35080,12922"];
	Layer1_Device7_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36777,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage10_RecvKV -> Layer1_Device7_Stage11_RecvKV	[label="Ring transfer",
		lp="36598,12982",
		pos="e,36550,12939 36550,13025 36550,13025 36550,12949 36550,12949"];
	Layer1_Device7_Stage10_Attention -> Layer1_Device7_Stage10_Accumulate	[pos="e,35336,12692 35232,12859 35232,12806 35232,12692 35232,12692 35232,12692 35326,12692 35326,12692"];
	Layer1_Device7_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36229,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage10_Accumulate -> Layer1_Device7_Stage11_Accumulate	[pos="e,35997,12499 35708,12666 35708,12613 35708,12499 35708,12499 35708,12499 35987,12499 35987,12499"];
	Layer1_Device7_Stage11_RecvKV -> Layer1_Device7_Stage11_Attention	[pos="e,36433,12719 36433,12832 36433,12832 36433,12729 36433,12729"];
	Layer1_Device7_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37719,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage11_RecvKV -> Layer1_Device7_Stage12_RecvKV	[label="Ring transfer",
		lp="36876,12789",
		pos="e,37248,12746 37248,12832 37248,12832 37248,12756 37248,12756"];
	Layer1_Device7_Stage11_Attention -> Layer1_Device7_Stage11_Accumulate	[pos="e,36370,12526 36370,12666 36370,12666 36370,12536 36370,12536"];
	Layer1_Device7_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36347,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage11_Accumulate -> Layer1_Device7_Stage12_Accumulate	[pos="e,36288,12333 36288,12473 36288,12473 36288,12343 36288,12343"];
	Layer1_Device7_Stage12_RecvKV -> Layer1_Device7_Stage12_Attention	[pos="e,36890,12526 36890,12639 36890,12639 36890,12536 36890,12536"];
	Layer1_Device7_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37959,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage12_RecvKV -> Layer1_Device7_Stage13_RecvKV	[label="Ring transfer",
		lp="37887,12596",
		pos="e,37839,12553 37839,12639 37839,12639 37839,12563 37839,12563"];
	Layer1_Device7_Stage12_Attention -> Layer1_Device7_Stage12_Accumulate	[pos="e,36548,12333 36548,12473 36548,12473 36548,12343 36548,12343"];
	Layer1_Device7_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36868,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage12_Accumulate -> Layer1_Device7_Stage13_Accumulate	[pos="e,36636,12113 36579,12306 36596,12306 36607,12306 36607,12306 36607,12306 36607,12113 36607,12113 36607,12113 36626,12113 36626,\
12113"];
	Layer1_Device7_Stage13_RecvKV -> Layer1_Device7_Stage13_Attention	[pos="e,38706,12334 38706,12500 38706,12500 38706,12344 38706,12344"];
	Layer1_Device7_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37556,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage13_RecvKV -> Layer1_Device7_Stage14_RecvKV	[label="Ring transfer",
		lp="37873,12403",
		pos="e,37758,12360 37758,12446 37758,12446 37758,12370 37758,12370"];
	Layer1_Device7_Stage13_Attention -> Layer1_Device7_Stage13_Accumulate	[pos="e,36868,12140 38765,12280 38765,12248 38765,12198 38765,12198 38765,12198 36868,12198 36868,12198 36868,12198 36868,12150 36868,\
12150"];
	Layer1_Device7_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36868,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage13_Accumulate -> Layer1_Device7_Stage14_Accumulate	[pos="e,36868,11988 36868,12087 36868,12087 36868,11998 36868,11998"];
	Layer1_Device7_Stage14_RecvKV -> Layer1_Device7_Stage14_Attention	[pos="e,36347,12140 36680,12266 36491,12266 36347,12266 36347,12266 36347,12266 36347,12150 36347,12150"];
	Layer1_Device7_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="38077,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device7_Stage14_RecvKV -> Layer1_Device7_Stage15_RecvKV	[label="Ring transfer",
		lp="37864,12210",
		pos="e,37816,12167 37816,12253 37816,12253 37816,12177 37816,12177"];
	Layer1_Device7_Stage14_Attention -> Layer1_Device7_Stage14_Accumulate	[pos="e,36636,11952 36347,12087 36347,12041 36347,11952 36347,11952 36347,11952 36626,11952 36626,11952"];
	Layer1_Device7_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="38493,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device7_Stage14_Accumulate -> Layer1_Device7_Stage15_Accumulate	[pos="e,38261,11835 36868,11934 36868,11897 36868,11835 36868,11835 36868,11835 38251,11835 38251,11835"];
	Layer1_Device7_Stage15_RecvKV -> Layer1_Device7_Stage15_Attention	[pos="e,38493,11988 38493,12060 38493,12060 38493,11998 38493,11998"];
	Layer1_Device7_Stage15_Attention -> Layer1_Device7_Stage15_Accumulate	[pos="e,38493,11862 38493,11934 38493,11934 38493,11872 38493,11872"];
	Layer1_Device7_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38651,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device7_Stage15_Accumulate -> Layer1_Device7_ConcatHeads	[pos="e,38575,11736 38575,11808 38575,11808 38575,11746 38575,11746"];
	Layer1_Device7_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38691,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device7_ConcatHeads -> Layer1_Device7_OutputProj	[pos="e,38677,11610 38677,11682 38677,11682 38677,11620 38677,11620"];
	Layer1_Device7_OutputProj -> Layer1_Device7_Residual1	[pos="e,38785,11484 38785,11556 38785,11556 38785,11494 38785,11494"];
	Layer1_Device7_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38902,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device7_Residual1 -> Layer1_Device7_LayerNorm2	[pos="e,38902,11358 38902,11430 38902,11430 38902,11368 38902,11368"];
	Layer1_Device7_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="38954,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device7_Residual1 -> Layer1_Device7_Residual2	[pos="e,39244,10676 39244,11430 39244,11430 39244,10686 39244,10686"];
	Layer1_Device7_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38609,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device7_LayerNorm2 -> Layer1_Device7_GateProj	[pos="e,38762,11232 38762,11304 38762,11304 38762,11242 38762,11242"];
	Layer1_Device7_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38922,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device7_LayerNorm2 -> Layer1_Device7_UpProj	[pos="e,38974,11143 38974,11304 38974,11304 38974,11153 38974,11153"];
	Layer1_Device7_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38609,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device7_GateProj -> Layer1_Device7_Activation	[pos="e,38540,11054 38540,11179 38540,11179 38540,11064 38540,11064"];
	Layer1_Device7_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38705,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device7_UpProj -> Layer1_Device7_ElemMul	[pos="e,38975,10928 38975,11089 38975,11089 38975,10938 38975,10938"];
	Layer1_Device7_Activation -> Layer1_Device7_ElemMul	[pos="e,38609,10928 38609,11000 38609,11000 38609,10938 38609,10938"];
	Layer1_Device7_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38848,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device7_ElemMul -> Layer1_Device7_DownProj	[pos="e,38848,10802 38848,10874 38848,10874 38848,10812 38848,10812"];
	Layer1_Device7_DownProj -> Layer1_Device7_Residual2	[pos="e,38848,10676 38848,10748 38848,10748 38848,10686 38848,10686"];
	Layer1_Device7_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38954,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device7_Residual2 -> Layer1_Device7_Output	[pos="e,38954,10550 38954,10622 38954,10622 38954,10560 38954,10560"];
	Layer2_Device7_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38954,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device7_Output -> Layer2_Device7_Input	[pos="e,38954,10402 38954,10475 38954,10475 38954,10412 38954,10412"];
	Layer1_Device8_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43444,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device8_Input -> Layer1_Device8_LayerNorm1	[pos="e,43657,15301 43759,15430 43759,15400 43759,15301 43759,15301 43759,15301 43667,15301 43667,15301"];
	Layer1_Device8_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="44059,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device8_Input -> Layer1_Device8_Residual1	[pos="e,44126,11484 44126,15402 44126,15402 44126,11494 44126,11494"];
	Layer1_Device8_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43027,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device8_LayerNorm1 -> Layer1_Device8_QKVProj	[pos="e,43444,15202 43444,15274 43444,15274 43444,15212 43444,15212"];
	Layer1_Device8_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41781,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage0_RecvKV	[label="Local K,V",
		lp="42282,15105",
		pos="e,42539,15062 42539,15148 42539,15148 42539,15072 42539,15072"];
	Layer1_Device8_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42571,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage0_Attention	[label=Q_local,
		lp="42843,15009",
		pos="e,42752,14843 42752,15148 42752,15148 42752,14853 42752,14853"];
	Layer1_Device8_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40241,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage1_Attention	[label=Q_local,
		lp="40305,14912",
		pos="e,40236,14649 42377,15155 41553,15155 40236,15155 40236,15155 40236,15155 40236,14659 40236,14659"];
	Layer1_Device8_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40852,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage2_Attention	[label=Q_local,
		lp="42983,14816",
		pos="e,41022,14456 42847,15148 42847,15051 42847,14725 42847,14725 42847,14725 41022,14725 41022,14725 41022,14725 41022,14466 41022,\
14466"];
	Layer1_Device8_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42867,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage3_Attention	[label=Q_local,
		lp="43123,14719",
		pos="e,43040,14264 43040,15149 43040,15149 43040,14274 43040,14274"];
	Layer1_Device8_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40073,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage4_Attention	[label=Q_local,
		lp="39925,14623",
		pos="e,39991,14070 42377,15168 41485,15168 39991,15168 39991,15168 39991,15168 39991,14080 39991,14080"];
	Layer1_Device8_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42985,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage5_Attention	[label=Q_local,
		lp="43227,14526",
		pos="e,43158,13877 43158,15148 43158,15148 43158,13887 43158,13887"];
	Layer1_Device8_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43037,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage6_Attention	[label=Q_local,
		lp="43359,14430",
		pos="e,43242,13684 43242,15149 43242,15149 43242,13694 43242,13694"];
	Layer1_Device8_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40098,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage7_Attention	[label=Q_local,
		lp="39779,14333",
		pos="e,39866,13473 42377,15181 41428,15181 39771,15181 39771,15181 39771,15181 39771,13473 39771,13473 39771,13473 39856,13473 39856,\
13473"];
	Layer1_Device8_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42801,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage8_Attention	[label=Q_local,
		lp="43477,14237",
		pos="e,43033,13271 43321,15149 43321,14919 43321,13271 43321,13271 43321,13271 43043,13271 43043,13271"];
	Layer1_Device8_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="39862,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage9_Attention	[label=Q_local,
		lp="39661,14140",
		pos="e,39701,13105 42377,15188 41409,15188 39701,15188 39701,15188 39701,15188 39701,13115 39701,13115"];
	Layer1_Device8_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40073,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage10_Attention	[label=Q_local,
		lp="39521,14044",
		pos="e,39841,12885 42377,15195 41389,15195 39620,15195 39620,15195 39620,15195 39620,12885 39620,12885 39620,12885 39831,12885 39831,\
12885"];
	Layer1_Device8_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41403,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage11_Attention	[label=Q_local,
		lp="43601,13947",
		pos="e,41480,12719 43426,15149 43426,14886 43426,12770 43426,12770 43426,12770 41480,12770 41480,12770 41480,12770 41480,12729 41480,\
12729"];
	Layer1_Device8_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41638,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage12_Attention	[label=Q_local,
		lp="43741,13851",
		pos="e,41663,12526 43479,15149 43479,14886 43479,12768 43479,12768 43479,12768 41663,12768 41663,12768 41663,12768 41663,12536 41663,\
12536"];
	Layer1_Device8_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43653,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage13_Attention	[label=Q_local,
		lp="43881,13754",
		pos="e,43826,12334 43677,15161 43765,15161 43826,15161 43826,15161 43826,15161 43826,12344 43826,12344"];
	Layer1_Device8_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41235,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage14_Attention	[label=Q_local,
		lp="39375,13658",
		pos="e,41003,12113 43374,15149 43374,14900 43374,12982 43374,12982 43374,12982 40854,12982 40854,12982 40854,12982 40854,12113 40854,\
12113 40854,12113 40993,12113 40993,12113"];
	Layer1_Device8_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43368,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_QKVProj -> Layer1_Device8_Stage15_Attention	[label=Q_local,
		lp="43999,13561",
		pos="e,43600,11970 43677,15175 43800,15175 43891,15175 43891,15175 43891,15175 43891,11970 43891,11970 43891,11970 43610,11970 43610,\
11970"];
	Layer1_Device8_Stage0_RecvKV -> Layer1_Device8_Stage0_Attention	[pos="e,42520,14843 42520,15009 42520,15009 42520,14853 42520,14853"];
	Layer1_Device8_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41362,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage0_RecvKV -> Layer1_Device8_Stage1_RecvKV	[label="Ring transfer",
		lp="41619,14912",
		pos="e,41572,14869 41572,14955 41572,14955 41572,14879 41572,14879"];
	Layer1_Device8_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="40762,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage0_Attention -> Layer1_Device8_Stage0_Accumulate	[pos="e,40762,14649 42571,14789 42571,14763 42571,14728 42571,14728 42571,14728 40762,14728 40762,14728 40762,14728 40762,14659 40762,\
14659"];
	Layer1_Device8_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40331,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage0_Accumulate -> Layer1_Device8_Stage1_Accumulate	[pos="e,40546,14456 40546,14596 40546,14596 40546,14466 40546,14466"];
	Layer1_Device8_Stage1_RecvKV -> Layer1_Device8_Stage1_Attention	[pos="e,40457,14650 40457,14816 40457,14816 40457,14660 40457,14660"];
	Layer1_Device8_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41971,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage1_RecvKV -> Layer1_Device8_Stage2_RecvKV	[label="Ring transfer",
		lp="41679,14719",
		pos="e,41666,14676 41666,14762 41666,14762 41666,14686 41666,14686"];
	Layer1_Device8_Stage1_Attention -> Layer1_Device8_Stage1_Accumulate	[pos="e,40286,14456 40286,14596 40286,14596 40286,14466 40286,14466"];
	Layer1_Device8_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40449,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage1_Accumulate -> Layer1_Device8_Stage2_Accumulate	[pos="e,40390,14263 40390,14403 40390,14403 40390,14273 40390,14273"];
	Layer1_Device8_Stage2_RecvKV -> Layer1_Device8_Stage2_Attention	[pos="e,41067,14457 41067,14623 41067,14623 41067,14467 41067,14467"];
	Layer1_Device8_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="42061,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage2_RecvKV -> Layer1_Device8_Stage3_RecvKV	[label="Ring transfer",
		lp="42064,14526",
		pos="e,42016,14483 42016,14569 42016,14569 42016,14493 42016,14493"];
	Layer1_Device8_Stage2_Attention -> Layer1_Device8_Stage2_Accumulate	[pos="e,40650,14263 40650,14403 40650,14403 40650,14273 40650,14273"];
	Layer1_Device8_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40594,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage2_Accumulate -> Layer1_Device8_Stage3_Accumulate	[pos="e,40522,14070 40522,14210 40522,14210 40522,14080 40522,14080"];
	Layer1_Device8_Stage3_RecvKV -> Layer1_Device8_Stage3_Attention	[pos="e,42808,14264 42808,14430 42808,14430 42808,14274 42808,14274"];
	Layer1_Device8_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41658,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage3_RecvKV -> Layer1_Device8_Stage4_RecvKV	[label="Ring transfer",
		lp="41975,14333",
		pos="e,41860,14290 41860,14376 41860,14376 41860,14300 41860,14300"];
	Layer1_Device8_Stage3_Attention -> Layer1_Device8_Stage3_Accumulate	[pos="e,40782,14070 42679,14210 42679,14182 42679,14142 42679,14142 42679,14142 40782,14142 40782,14142 40782,14142 40782,14080 40782,\
14080"];
	Layer1_Device8_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40567,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage3_Accumulate -> Layer1_Device8_Stage4_Accumulate	[pos="e,40580,13877 40580,14017 40580,14017 40580,13887 40580,13887"];
	Layer1_Device8_Stage4_RecvKV -> Layer1_Device8_Stage4_Attention	[pos="e,40261,14070 40782,14196 40501,14196 40261,14196 40261,14196 40261,14196 40261,14080 40261,14080"];
	Layer1_Device8_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41803,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage4_RecvKV -> Layer1_Device8_Stage5_RecvKV	[label="Ring transfer",
		lp="41778,14140",
		pos="e,41730,14097 41730,14183 41730,14183 41730,14107 41730,14107"];
	Layer1_Device8_Stage4_Attention -> Layer1_Device8_Stage4_Accumulate	[pos="e,40349,13877 40305,14043 40331,14043 40349,14043 40349,14043 40349,14043 40349,13887 40349,13887"];
	Layer1_Device8_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42516,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage4_Accumulate -> Layer1_Device8_Stage5_Accumulate	[pos="e,42490,13684 40593,13824 40593,13797 40593,13761 40593,13761 40593,13761 42490,13761 42490,13761 42490,13761 42490,13694 42490,\
13694"];
	Layer1_Device8_Stage5_RecvKV -> Layer1_Device8_Stage5_Attention	[pos="e,42753,13859 42709,14044 42709,14038 42709,13859 42709,13859 42709,13859 42743,13859 42743,13859"];
	Layer1_Device8_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41776,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage5_RecvKV -> Layer1_Device8_Stage6_RecvKV	[label="Ring transfer",
		lp="41837,13947",
		pos="e,41790,13904 41790,13990 41790,13990 41790,13914 41790,13914"];
	Layer1_Device8_Stage5_Attention -> Layer1_Device8_Stage5_Accumulate	[pos="e,42735,13684 42753,13841 42742,13841 42735,13841 42735,13841 42735,13841 42735,13694 42735,13694"];
	Layer1_Device8_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42516,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage5_Accumulate -> Layer1_Device8_Stage6_Accumulate	[pos="e,42516,13491 42516,13631 42516,13631 42516,13501 42516,13501"];
	Layer1_Device8_Stage6_RecvKV -> Layer1_Device8_Stage6_Attention	[pos="e,43011,13684 42363,13810 42682,13810 43011,13810 43011,13810 43011,13810 43011,13694 43011,13694"];
	Layer1_Device8_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41307,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage6_RecvKV -> Layer1_Device8_Stage7_RecvKV	[label="Ring transfer",
		lp="41589,13754",
		pos="e,41542,13711 41542,13797 41542,13797 41542,13721 41542,13721"];
	Layer1_Device8_Stage6_Attention -> Layer1_Device8_Stage6_Accumulate	[pos="e,42748,13455 42919,13631 42919,13576 42919,13455 42919,13455 42919,13455 42758,13455 42758,13455"];
	Layer1_Device8_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42280,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage6_Accumulate -> Layer1_Device8_Stage7_Accumulate	[pos="e,42398,13298 42398,13438 42398,13438 42398,13308 42398,13308"];
	Layer1_Device8_Stage7_RecvKV -> Layer1_Device8_Stage7_Attention	[pos="e,40317,13491 40431,13617 40361,13617 40317,13617 40317,13617 40317,13617 40317,13501 40317,13501"];
	Layer1_Device8_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41307,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage7_RecvKV -> Layer1_Device8_Stage8_RecvKV	[label="Ring transfer",
		lp="41355,13561",
		pos="e,41307,13518 41307,13604 41307,13604 41307,13528 41307,13528"];
	Layer1_Device8_Stage7_Attention -> Layer1_Device8_Stage7_Accumulate	[pos="e,42138,13299 40240,13438 40240,13403 40240,13345 40240,13345 40240,13345 42138,13345 42138,13345 42138,13345 42138,13309 42138,\
13309"];
	Layer1_Device8_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42280,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage7_Accumulate -> Layer1_Device8_Stage8_Accumulate	[pos="e,42280,13105 42280,13245 42280,13245 42280,13115 42280,13115"];
	Layer1_Device8_Stage8_RecvKV -> Layer1_Device8_Stage8_Attention	[pos="e,42658,13298 41894,13424 42259,13424 42658,13424 42658,13424 42658,13424 42658,13308 42658,13308"];
	Layer1_Device8_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41071,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage8_RecvKV -> Layer1_Device8_Stage9_RecvKV	[label="Ring transfer",
		lp="41307,13368",
		pos="e,41189,13325 41189,13411 41189,13411 41189,13335 41189,13335"];
	Layer1_Device8_Stage8_Attention -> Layer1_Device8_Stage8_Accumulate	[pos="e,42512,13078 42646,13245 42646,13192 42646,13078 42646,13078 42646,13078 42522,13078 42522,13078"];
	Layer1_Device8_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40594,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage8_Accumulate -> Layer1_Device8_Stage9_Accumulate	[pos="e,40671,12912 42280,13052 42280,13024 42280,12984 42280,12984 42280,12984 40671,12984 40671,12984 40671,12984 40671,12922 40671,\
12922"];
	Layer1_Device8_Stage9_RecvKV -> Layer1_Device8_Stage9_Attention	[pos="e,39980,13105 40195,13231 40068,13231 39980,13231 39980,13231 39980,13231 39980,13115 39980,13115"];
	Layer1_Device8_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41071,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage9_RecvKV -> Layer1_Device8_Stage10_RecvKV	[label="Ring transfer",
		lp="41119,13175",
		pos="e,41071,13132 41071,13218 41071,13218 41071,13142 41071,13142"];
	Layer1_Device8_Stage9_Attention -> Layer1_Device8_Stage9_Accumulate	[pos="e,40517,12913 39968,13052 39968,13014 39968,12949 39968,12949 39968,12949 40517,12949 40517,12949 40517,12949 40517,12923 40517,\
12923"];
	Layer1_Device8_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40594,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage9_Accumulate -> Layer1_Device8_Stage10_Accumulate	[pos="e,40594,12719 40594,12859 40594,12859 40594,12729 40594,12729"];
	Layer1_Device8_Stage10_RecvKV -> Layer1_Device8_Stage10_Attention	[pos="e,40228,12913 40228,13079 40228,13079 40228,12923 40228,12923"];
	Layer1_Device8_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41803,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage10_RecvKV -> Layer1_Device8_Stage11_RecvKV	[label="Ring transfer",
		lp="41388,12982",
		pos="e,41437,12939 41437,13025 41437,13025 41437,12949 41437,12949"];
	Layer1_Device8_Stage10_Attention -> Layer1_Device8_Stage10_Accumulate	[pos="e,40362,12692 40192,12859 40192,12806 40192,12692 40192,12692 40192,12692 40352,12692 40352,12692"];
	Layer1_Device8_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41117,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage10_Accumulate -> Layer1_Device8_Stage11_Accumulate	[pos="e,41028,12526 40826,12692 40929,12692 41028,12692 41028,12692 41028,12692 41028,12536 41028,12536"];
	Layer1_Device8_Stage11_RecvKV -> Layer1_Device8_Stage11_Attention	[pos="e,41326,12719 41326,12832 41326,12832 41326,12729 41326,12729"];
	Layer1_Device8_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42612,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage11_RecvKV -> Layer1_Device8_Stage12_RecvKV	[label="Ring transfer",
		lp="42256,12789",
		pos="e,42208,12746 42208,12832 42208,12832 42208,12756 42208,12756"];
	Layer1_Device8_Stage11_Attention -> Layer1_Device8_Stage11_Accumulate	[pos="e,41260,12526 41260,12666 41260,12666 41260,12536 41260,12536"];
	Layer1_Device8_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41235,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage11_Accumulate -> Layer1_Device8_Stage12_Accumulate	[pos="e,41176,12333 41176,12473 41176,12473 41176,12343 41176,12343"];
	Layer1_Device8_Stage12_RecvKV -> Layer1_Device8_Stage12_Attention	[pos="e,41781,12526 41781,12639 41781,12639 41781,12536 41781,12536"];
	Layer1_Device8_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42847,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage12_RecvKV -> Layer1_Device8_Stage13_RecvKV	[label="Ring transfer",
		lp="42730,12596",
		pos="e,42730,12553 42730,12639 42730,12639 42730,12563 42730,12563"];
	Layer1_Device8_Stage12_Attention -> Layer1_Device8_Stage12_Accumulate	[pos="e,41436,12333 41436,12473 41436,12473 41436,12343 41436,12343"];
	Layer1_Device8_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41756,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage12_Accumulate -> Layer1_Device8_Stage13_Accumulate	[pos="e,41524,12113 41467,12306 41484,12306 41495,12306 41495,12306 41495,12306 41495,12113 41495,12113 41495,12113 41514,12113 41514,\
12113"];
	Layer1_Device8_Stage13_RecvKV -> Layer1_Device8_Stage13_Attention	[pos="e,43594,12334 43594,12500 43594,12500 43594,12344 43594,12344"];
	Layer1_Device8_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42444,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage13_RecvKV -> Layer1_Device8_Stage14_RecvKV	[label="Ring transfer",
		lp="42628,12403",
		pos="e,42646,12360 42646,12446 42646,12446 42646,12370 42646,12370"];
	Layer1_Device8_Stage13_Attention -> Layer1_Device8_Stage13_Accumulate	[pos="e,41756,12141 43653,12280 43653,12249 43653,12202 43653,12202 43653,12202 41756,12202 41756,12202 41756,12202 41756,12151 41756,\
12151"];
	Layer1_Device8_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41756,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage13_Accumulate -> Layer1_Device8_Stage14_Accumulate	[pos="e,41756,11988 41756,12087 41756,12087 41756,11998 41756,11998"];
	Layer1_Device8_Stage14_RecvKV -> Layer1_Device8_Stage14_Attention	[pos="e,41235,12140 41568,12266 41379,12266 41235,12266 41235,12266 41235,12266 41235,12150 41235,12150"];
	Layer1_Device8_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42965,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device8_Stage14_RecvKV -> Layer1_Device8_Stage15_RecvKV	[label="Ring transfer",
		lp="42752,12210",
		pos="e,42704,12167 42704,12253 42704,12253 42704,12177 42704,12177"];
	Layer1_Device8_Stage14_Attention -> Layer1_Device8_Stage14_Accumulate	[pos="e,41524,11952 41235,12087 41235,12041 41235,11952 41235,11952 41235,11952 41514,11952 41514,11952"];
	Layer1_Device8_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="43368,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device8_Stage14_Accumulate -> Layer1_Device8_Stage15_Accumulate	[pos="e,43136,11835 41756,11934 41756,11897 41756,11835 41756,11835 41756,11835 43126,11835 43126,11835"];
	Layer1_Device8_Stage15_RecvKV -> Layer1_Device8_Stage15_Attention	[pos="e,43368,11988 43368,12060 43368,12060 43368,11998 43368,11998"];
	Layer1_Device8_Stage15_Attention -> Layer1_Device8_Stage15_Accumulate	[pos="e,43368,11862 43368,11934 43368,11934 43368,11872 43368,11872"];
	Layer1_Device8_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43573,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device8_Stage15_Accumulate -> Layer1_Device8_ConcatHeads	[pos="e,43474,11736 43474,11808 43474,11808 43474,11746 43474,11746"];
	Layer1_Device8_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43684,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device8_ConcatHeads -> Layer1_Device8_OutputProj	[pos="e,43635,11610 43635,11682 43635,11682 43635,11620 43635,11620"];
	Layer1_Device8_OutputProj -> Layer1_Device8_Residual1	[pos="e,43785,11484 43785,11556 43785,11556 43785,11494 43785,11494"];
	Layer1_Device8_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43859,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device8_Residual1 -> Layer1_Device8_LayerNorm2	[pos="e,43872,11358 43872,11430 43872,11430 43872,11368 43872,11368"];
	Layer1_Device8_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="44009,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device8_Residual1 -> Layer1_Device8_Residual2	[pos="e,44250,10676 44250,11430 44250,11430 44250,10686 44250,10686"];
	Layer1_Device8_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43567,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device8_LayerNorm2 -> Layer1_Device8_GateProj	[pos="e,43719,11232 43719,11304 43719,11304 43719,11242 43719,11242"];
	Layer1_Device8_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43880,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device8_LayerNorm2 -> Layer1_Device8_UpProj	[pos="e,43932,11143 43932,11304 43932,11304 43932,11153 43932,11153"];
	Layer1_Device8_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43567,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device8_GateProj -> Layer1_Device8_Activation	[pos="e,43498,11054 43498,11179 43498,11179 43498,11064 43498,11064"];
	Layer1_Device8_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43662,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device8_UpProj -> Layer1_Device8_ElemMul	[pos="e,43932,10928 43932,11089 43932,11089 43932,10938 43932,10938"];
	Layer1_Device8_Activation -> Layer1_Device8_ElemMul	[pos="e,43567,10928 43567,11000 43567,11000 43567,10938 43567,10938"];
	Layer1_Device8_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43758,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device8_ElemMul -> Layer1_Device8_DownProj	[pos="e,43758,10802 43758,10874 43758,10874 43758,10812 43758,10812"];
	Layer1_Device8_DownProj -> Layer1_Device8_Residual2	[pos="e,43800,10676 43800,10748 43800,10748 43800,10686 43800,10686"];
	Layer1_Device8_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="44009,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device8_Residual2 -> Layer1_Device8_Output	[pos="e,44009,10550 44009,10622 44009,10622 44009,10560 44009,10560"];
	Layer2_Device8_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="44009,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device8_Output -> Layer2_Device8_Input	[pos="e,44009,10402 44009,10475 44009,10475 44009,10412 44009,10412"];
	Layer1_Device9_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48198,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device9_Input -> Layer1_Device9_LayerNorm1	[pos="e,48375,15328 48375,15420 48375,15420 48375,15338 48375,15338"];
	Layer1_Device9_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="48802,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device9_Input -> Layer1_Device9_Residual1	[pos="e,48911,11484 48911,15421 48911,15421 48911,11494 48911,11494"];
	Layer1_Device9_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47919,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device9_LayerNorm1 -> Layer1_Device9_QKVProj	[pos="e,48198,15202 48198,15274 48198,15274 48198,15212 48198,15212"];
	Layer1_Device9_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46673,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage0_RecvKV	[label="Local K,V",
		lp="47174,15105",
		pos="e,47431,15062 47431,15148 47431,15148 47431,15072 47431,15072"];
	Layer1_Device9_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47463,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage0_Attention	[label=Q_local,
		lp="47735,15009",
		pos="e,47644,14843 47644,15148 47644,15148 47644,14853 47644,14853"];
	Layer1_Device9_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45133,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage1_Attention	[label=Q_local,
		lp="45197,14912",
		pos="e,45170,14649 47269,15155 46456,15155 45170,15155 45170,15155 45170,15155 45170,14659 45170,14659"];
	Layer1_Device9_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45744,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage2_Attention	[label=Q_local,
		lp="47875,14816",
		pos="e,45914,14456 47739,15148 47739,15052 47739,14731 47739,14731 47739,14731 45914,14731 45914,14731 45914,14731 45914,14466 45914,\
14466"];
	Layer1_Device9_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47759,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage3_Attention	[label=Q_local,
		lp="48015,14719",
		pos="e,47932,14264 47932,15149 47932,15149 47932,14274 47932,14274"];
	Layer1_Device9_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44965,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage4_Attention	[label=Q_local,
		lp="44817,14623",
		pos="e,44818,14070 47269,15168 46360,15168 44818,15168 44818,15168 44818,15168 44818,14080 44818,14080"];
	Layer1_Device9_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47877,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage5_Attention	[label=Q_local,
		lp="48112,14526",
		pos="e,48050,13877 48050,15148 48050,15148 48050,13887 48050,13887"];
	Layer1_Device9_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47905,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage6_Attention	[label=Q_local,
		lp="48251,14430",
		pos="e,48122,13684 48122,15149 48122,15149 48122,13694 48122,13694"];
	Layer1_Device9_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44966,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage7_Attention	[label=Q_local,
		lp="44671,14333",
		pos="e,44734,13473 47269,15181 46321,15181 44669,15181 44669,15181 44669,15181 44669,13473 44669,13473 44669,13473 44724,13473 44724,\
13473"];
	Layer1_Device9_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47711,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage8_Attention	[label=Q_local,
		lp="48369,14237",
		pos="e,47943,13271 48193,15149 48193,14919 48193,13271 48193,13271 48193,13271 47953,13271 47953,13271"];
	Layer1_Device9_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44772,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage9_Attention	[label=Q_local,
		lp="44553,14140",
		pos="e,44605,13105 47269,15188 46304,15188 44605,15188 44605,15188 44605,15188 44605,13115 44605,13115"];
	Layer1_Device9_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45003,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage10_Attention	[label=Q_local,
		lp="44413,14044",
		pos="e,44771,12885 47269,15195 46285,15195 44529,15195 44529,15195 44529,15195 44529,12885 44529,12885 44529,12885 44761,12885 44761,\
12885"];
	Layer1_Device9_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46291,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage11_Attention	[label=Q_local,
		lp="48490,13947",
		pos="e,46368,12719 48306,15148 48306,14885 48306,12780 48306,12780 48306,12780 46368,12780 46368,12780 46368,12780 46368,12729 46368,\
12729"];
	Layer1_Device9_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46530,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage12_Attention	[label=Q_local,
		lp="48633,13851",
		pos="e,46551,12526 48363,15149 48363,14887 48363,12778 48363,12778 48363,12778 46551,12778 46551,12778 46551,12778 46551,12536 46551,\
12536"];
	Layer1_Device9_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48545,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage13_Attention	[label=Q_local,
		lp="48773,13754",
		pos="e,48718,12334 48569,15161 48657,15161 48718,15161 48718,15161 48718,15161 48718,12344 48718,12344"];
	Layer1_Device9_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46127,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage14_Attention	[label=Q_local,
		lp="44267,13658",
		pos="e,45895,12113 48250,15149 48250,14899 48250,12976 48250,12976 48250,12976 45766,12976 45766,12976 45766,12976 45766,12113 45766,\
12113 45766,12113 45885,12113 45885,12113"];
	Layer1_Device9_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48617,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_QKVProj -> Layer1_Device9_Stage15_Attention	[label=Q_local,
		lp="48891,13561",
		pos="e,48813,11988 48569,15175 48707,15175 48813,15175 48813,15175 48813,15175 48813,11998 48813,11998"];
	Layer1_Device9_Stage0_RecvKV -> Layer1_Device9_Stage0_Attention	[pos="e,47412,14843 47412,15009 47412,15009 47412,14853 47412,14853"];
	Layer1_Device9_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46254,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage0_RecvKV -> Layer1_Device9_Stage1_RecvKV	[label="Ring transfer",
		lp="46583,14912",
		pos="e,46464,14869 46464,14955 46464,14955 46464,14879 46464,14879"];
	Layer1_Device9_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="45654,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage0_Attention -> Layer1_Device9_Stage0_Accumulate	[pos="e,45654,14649 47463,14789 47463,14765 47463,14733 47463,14733 47463,14733 45654,14733 45654,14733 45654,14733 45654,14659 45654,\
14659"];
	Layer1_Device9_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45223,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage0_Accumulate -> Layer1_Device9_Stage1_Accumulate	[pos="e,45438,14456 45438,14596 45438,14596 45438,14466 45438,14466"];
	Layer1_Device9_Stage1_RecvKV -> Layer1_Device9_Stage1_Attention	[pos="e,45349,14650 45349,14816 45349,14816 45349,14660 45349,14660"];
	Layer1_Device9_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46863,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage1_RecvKV -> Layer1_Device9_Stage2_RecvKV	[label="Ring transfer",
		lp="46571,14719",
		pos="e,46558,14676 46558,14762 46558,14762 46558,14686 46558,14686"];
	Layer1_Device9_Stage1_Attention -> Layer1_Device9_Stage1_Accumulate	[pos="e,45178,14456 45178,14596 45178,14596 45178,14466 45178,14466"];
	Layer1_Device9_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45341,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage1_Accumulate -> Layer1_Device9_Stage2_Accumulate	[pos="e,45282,14263 45282,14403 45282,14403 45282,14273 45282,14273"];
	Layer1_Device9_Stage2_RecvKV -> Layer1_Device9_Stage2_Attention	[pos="e,45959,14457 45959,14623 45959,14623 45959,14467 45959,14467"];
	Layer1_Device9_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46953,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage2_RecvKV -> Layer1_Device9_Stage3_RecvKV	[label="Ring transfer",
		lp="46956,14526",
		pos="e,46908,14483 46908,14569 46908,14569 46908,14493 46908,14493"];
	Layer1_Device9_Stage2_Attention -> Layer1_Device9_Stage2_Accumulate	[pos="e,45542,14263 45542,14403 45542,14403 45542,14273 45542,14273"];
	Layer1_Device9_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45486,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage2_Accumulate -> Layer1_Device9_Stage3_Accumulate	[pos="e,45414,14070 45414,14210 45414,14210 45414,14080 45414,14080"];
	Layer1_Device9_Stage3_RecvKV -> Layer1_Device9_Stage3_Attention	[pos="e,47700,14264 47700,14430 47700,14430 47700,14274 47700,14274"];
	Layer1_Device9_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46550,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage3_RecvKV -> Layer1_Device9_Stage4_RecvKV	[label="Ring transfer",
		lp="46734,14333",
		pos="e,46752,14290 46752,14376 46752,14376 46752,14300 46752,14300"];
	Layer1_Device9_Stage3_Attention -> Layer1_Device9_Stage3_Accumulate	[pos="e,45674,14070 47571,14210 47571,14183 47571,14147 47571,14147 47571,14147 45674,14147 45674,14147 45674,14147 45674,14080 45674,\
14080"];
	Layer1_Device9_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45459,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage3_Accumulate -> Layer1_Device9_Stage4_Accumulate	[pos="e,45472,13877 45472,14017 45472,14017 45472,13887 45472,13887"];
	Layer1_Device9_Stage4_RecvKV -> Layer1_Device9_Stage4_Attention	[pos="e,45153,14070 45674,14196 45393,14196 45153,14196 45153,14196 45153,14196 45153,14080 45153,14080"];
	Layer1_Device9_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46695,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage4_RecvKV -> Layer1_Device9_Stage5_RecvKV	[label="Ring transfer",
		lp="46658,14140",
		pos="e,46622,14097 46622,14183 46622,14183 46622,14107 46622,14107"];
	Layer1_Device9_Stage4_Attention -> Layer1_Device9_Stage4_Accumulate	[pos="e,45241,13877 45197,14043 45223,14043 45241,14043 45241,14043 45241,14043 45241,13887 45241,13887"];
	Layer1_Device9_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47384,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage4_Accumulate -> Layer1_Device9_Stage5_Accumulate	[pos="e,47370,13684 45473,13824 45473,13790 45473,13736 45473,13736 45473,13736 47370,13736 47370,13736 47370,13736 47370,13694 47370,\
13694"];
	Layer1_Device9_Stage5_RecvKV -> Layer1_Device9_Stage5_Attention	[pos="e,47645,13859 47597,14044 47597,14038 47597,13859 47597,13859 47597,13859 47635,13859 47635,13859"];
	Layer1_Device9_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46668,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage5_RecvKV -> Layer1_Device9_Stage6_RecvKV	[label="Ring transfer",
		lp="46729,13947",
		pos="e,46682,13904 46682,13990 46682,13990 46682,13914 46682,13914"];
	Layer1_Device9_Stage5_Attention -> Layer1_Device9_Stage5_Accumulate	[pos="e,47606,13684 47645,13841 47621,13841 47606,13841 47606,13841 47606,13841 47606,13694 47606,13694"];
	Layer1_Device9_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47384,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage5_Accumulate -> Layer1_Device9_Stage6_Accumulate	[pos="e,47384,13491 47384,13631 47384,13631 47384,13501 47384,13501"];
	Layer1_Device9_Stage6_RecvKV -> Layer1_Device9_Stage6_Attention	[pos="e,47891,13684 47255,13810 47569,13810 47891,13810 47891,13810 47891,13810 47891,13694 47891,13694"];
	Layer1_Device9_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46175,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage6_RecvKV -> Layer1_Device9_Stage7_RecvKV	[label="Ring transfer",
		lp="46469,13754",
		pos="e,46422,13711 46422,13797 46422,13797 46422,13721 46422,13721"];
	Layer1_Device9_Stage6_Attention -> Layer1_Device9_Stage6_Accumulate	[pos="e,47616,13455 47808,13631 47808,13576 47808,13455 47808,13455 47808,13455 47626,13455 47626,13455"];
	Layer1_Device9_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47190,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage6_Accumulate -> Layer1_Device9_Stage7_Accumulate	[pos="e,47287,13298 47287,13438 47287,13438 47287,13308 47287,13308"];
	Layer1_Device9_Stage7_RecvKV -> Layer1_Device9_Stage7_Attention	[pos="e,44966,13491 45299,13617 45110,13617 44966,13617 44966,13617 44966,13617 44966,13501 44966,13501"];
	Layer1_Device9_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46175,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage7_RecvKV -> Layer1_Device9_Stage8_RecvKV	[label="Ring transfer",
		lp="46223,13561",
		pos="e,46175,13518 46175,13604 46175,13604 46175,13528 46175,13528"];
	Layer1_Device9_Stage7_Attention -> Layer1_Device9_Stage7_Accumulate	[pos="e,47027,13298 45129,13438 45129,13411 45129,13375 45129,13375 45129,13375 47027,13375 47027,13375 47027,13375 47027,13308 47027,\
13308"];
	Layer1_Device9_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47190,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage7_Accumulate -> Layer1_Device9_Stage8_Accumulate	[pos="e,47190,13105 47190,13245 47190,13245 47190,13115 47190,13115"];
	Layer1_Device9_Stage8_RecvKV -> Layer1_Device9_Stage8_Attention	[pos="e,47548,13298 46762,13424 47135,13424 47548,13424 47548,13424 47548,13424 47548,13308 47548,13308"];
	Layer1_Device9_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="45981,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage8_RecvKV -> Layer1_Device9_Stage9_RecvKV	[label="Ring transfer",
		lp="46158,13368",
		pos="e,46078,13325 46078,13411 46078,13411 46078,13335 46078,13335"];
	Layer1_Device9_Stage8_Attention -> Layer1_Device9_Stage8_Accumulate	[pos="e,47422,13078 47566,13245 47566,13192 47566,13078 47566,13078 47566,13078 47432,13078 47432,13078"];
	Layer1_Device9_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45524,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage8_Accumulate -> Layer1_Device9_Stage9_Accumulate	[pos="e,45601,12913 47190,13052 47190,13022 47190,12979 47190,12979 47190,12979 45601,12979 45601,12979 45601,12979 45601,12923 45601,\
12923"];
	Layer1_Device9_Stage9_RecvKV -> Layer1_Device9_Stage9_Attention	[pos="e,44869,13105 45105,13231 44967,13231 44869,13231 44869,13231 44869,13231 44869,13115 44869,13115"];
	Layer1_Device9_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="45981,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage9_RecvKV -> Layer1_Device9_Stage10_RecvKV	[label="Ring transfer",
		lp="46029,13175",
		pos="e,45981,13132 45981,13218 45981,13218 45981,13142 45981,13142"];
	Layer1_Device9_Stage9_Attention -> Layer1_Device9_Stage9_Accumulate	[pos="e,45447,12912 44888,13052 44888,13015 44888,12952 44888,12952 44888,12952 45447,12952 45447,12952 45447,12952 45447,12922 45447,\
12922"];
	Layer1_Device9_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45524,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage9_Accumulate -> Layer1_Device9_Stage10_Accumulate	[pos="e,45524,12719 45524,12859 45524,12859 45524,12729 45524,12729"];
	Layer1_Device9_Stage10_RecvKV -> Layer1_Device9_Stage10_Attention	[pos="e,45148,12912 45148,13025 45148,13025 45148,12922 45148,12922"];
	Layer1_Device9_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="46733,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage10_RecvKV -> Layer1_Device9_Stage11_RecvKV	[label="Ring transfer",
		lp="46298,12982",
		pos="e,46357,12939 46357,13025 46357,13025 46357,12949 46357,12949"];
	Layer1_Device9_Stage10_Attention -> Layer1_Device9_Stage10_Accumulate	[pos="e,45292,12692 45102,12859 45102,12806 45102,12692 45102,12692 45102,12692 45282,12692 45282,12692"];
	Layer1_Device9_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46009,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage10_Accumulate -> Layer1_Device9_Stage11_Accumulate	[pos="e,45795,12526 45756,12692 45780,12692 45795,12692 45795,12692 45795,12692 45795,12536 45795,12536"];
	Layer1_Device9_Stage11_RecvKV -> Layer1_Device9_Stage11_Attention	[pos="e,46214,12719 46214,12832 46214,12832 46214,12729 46214,12729"];
	Layer1_Device9_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47500,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage11_RecvKV -> Layer1_Device9_Stage12_RecvKV	[label="Ring transfer",
		lp="46976,12789",
		pos="e,47116,12746 47116,12832 47116,12832 47116,12756 47116,12756"];
	Layer1_Device9_Stage11_Attention -> Layer1_Device9_Stage11_Accumulate	[pos="e,46150,12526 46150,12666 46150,12666 46150,12536 46150,12536"];
	Layer1_Device9_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46127,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage11_Accumulate -> Layer1_Device9_Stage12_Accumulate	[pos="e,46068,12333 46068,12473 46068,12473 46068,12343 46068,12343"];
	Layer1_Device9_Stage12_RecvKV -> Layer1_Device9_Stage12_Attention	[pos="e,46671,12526 46671,12639 46671,12639 46671,12536 46671,12536"];
	Layer1_Device9_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47739,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage12_RecvKV -> Layer1_Device9_Stage13_RecvKV	[label="Ring transfer",
		lp="47667,12596",
		pos="e,47620,12553 47620,12639 47620,12639 47620,12563 47620,12563"];
	Layer1_Device9_Stage12_Attention -> Layer1_Device9_Stage12_Accumulate	[pos="e,46328,12333 46328,12473 46328,12473 46328,12343 46328,12343"];
	Layer1_Device9_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46648,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage12_Accumulate -> Layer1_Device9_Stage13_Accumulate	[pos="e,46416,12113 46359,12306 46376,12306 46387,12306 46387,12306 46387,12306 46387,12113 46387,12113 46387,12113 46406,12113 46406,\
12113"];
	Layer1_Device9_Stage13_RecvKV -> Layer1_Device9_Stage13_Attention	[pos="e,48486,12334 48486,12500 48486,12500 48486,12344 48486,12344"];
	Layer1_Device9_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47336,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage13_RecvKV -> Layer1_Device9_Stage14_RecvKV	[label="Ring transfer",
		lp="47586,12403",
		pos="e,47538,12360 47538,12446 47538,12446 47538,12370 47538,12370"];
	Layer1_Device9_Stage13_Attention -> Layer1_Device9_Stage13_Accumulate	[pos="e,46648,12140 48545,12280 48545,12250 48545,12206 48545,12206 48545,12206 46648,12206 46648,12206 46648,12206 46648,12150 46648,\
12150"];
	Layer1_Device9_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46648,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage13_Accumulate -> Layer1_Device9_Stage14_Accumulate	[pos="e,46648,11988 46648,12087 46648,12087 46648,11998 46648,11998"];
	Layer1_Device9_Stage14_RecvKV -> Layer1_Device9_Stage14_Attention	[pos="e,46127,12140 46460,12266 46271,12266 46127,12266 46127,12266 46127,12266 46127,12150 46127,12150"];
	Layer1_Device9_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47857,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device9_Stage14_RecvKV -> Layer1_Device9_Stage15_RecvKV	[label="Ring transfer",
		lp="47644,12210",
		pos="e,47596,12167 47596,12253 47596,12253 47596,12177 47596,12177"];
	Layer1_Device9_Stage14_Attention -> Layer1_Device9_Stage14_Accumulate	[pos="e,46416,11952 46127,12087 46127,12041 46127,11952 46127,11952 46127,11952 46406,11952 46406,11952"];
	Layer1_Device9_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="48617,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device9_Stage14_Accumulate -> Layer1_Device9_Stage15_Accumulate	[pos="e,48385,11835 46648,11934 46648,11897 46648,11835 46648,11835 46648,11835 48375,11835 48375,11835"];
	Layer1_Device9_Stage15_RecvKV -> Layer1_Device9_Stage15_Attention	[pos="e,48581,11988 48581,12112 48581,12112 48581,11998 48581,11998"];
	Layer1_Device9_Stage15_Attention -> Layer1_Device9_Stage15_Accumulate	[pos="e,48617,11862 48617,11934 48617,11934 48617,11872 48617,11872"];
	Layer1_Device9_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48644,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device9_Stage15_Accumulate -> Layer1_Device9_ConcatHeads	[pos="e,48634,11736 48634,11808 48634,11808 48634,11746 48634,11746"];
	Layer1_Device9_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48667,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device9_ConcatHeads -> Layer1_Device9_OutputProj	[pos="e,48662,11610 48662,11682 48662,11682 48662,11620 48662,11620"];
	Layer1_Device9_OutputProj -> Layer1_Device9_Residual1	[pos="e,48667,11484 48667,11556 48667,11556 48667,11494 48667,11494"];
	Layer1_Device9_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48616,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device9_Residual1 -> Layer1_Device9_LayerNorm2	[pos="e,48622,11358 48622,11430 48622,11430 48622,11368 48622,11368"];
	Layer1_Device9_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="48770,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device9_Residual1 -> Layer1_Device9_Residual2	[pos="e,49009,10676 49009,11430 49009,11430 49009,10686 49009,10686"];
	Layer1_Device9_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48323,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device9_LayerNorm2 -> Layer1_Device9_GateProj	[pos="e,48476,11232 48476,11304 48476,11304 48476,11242 48476,11242"];
	Layer1_Device9_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48636,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device9_LayerNorm2 -> Layer1_Device9_UpProj	[pos="e,48688,11143 48688,11304 48688,11304 48688,11153 48688,11153"];
	Layer1_Device9_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48323,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device9_GateProj -> Layer1_Device9_Activation	[pos="e,48254,11054 48254,11179 48254,11179 48254,11064 48254,11064"];
	Layer1_Device9_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48419,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device9_UpProj -> Layer1_Device9_ElemMul	[pos="e,48689,10928 48689,11089 48689,11089 48689,10938 48689,10938"];
	Layer1_Device9_Activation -> Layer1_Device9_ElemMul	[pos="e,48323,10928 48323,11000 48323,11000 48323,10938 48323,10938"];
	Layer1_Device9_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48562,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device9_ElemMul -> Layer1_Device9_DownProj	[pos="e,48562,10802 48562,10874 48562,10874 48562,10812 48562,10812"];
	Layer1_Device9_DownProj -> Layer1_Device9_Residual2	[pos="e,48582,10676 48582,10748 48582,10748 48582,10686 48582,10686"];
	Layer1_Device9_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48770,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device9_Residual2 -> Layer1_Device9_Output	[pos="e,48770,10550 48770,10622 48770,10622 48770,10560 48770,10560"];
	Layer2_Device9_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48770,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device9_Output -> Layer2_Device9_Input	[pos="e,48770,10402 48770,10475 48770,10475 48770,10412 48770,10412"];
	Layer1_Device10_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53399,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device10_Input -> Layer1_Device10_LayerNorm1	[pos="e,53546,15328 53546,15415 53546,15415 53546,15338 53546,15338"];
	Layer1_Device10_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="52916,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device10_Input -> Layer1_Device10_Residual1	[pos="e,53303,11457 53720,15401 53720,15000 53720,11457 53720,11457 53720,11457 53313,11457 53313,11457"];
	Layer1_Device10_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52809,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device10_LayerNorm1 -> Layer1_Device10_QKVProj	[pos="e,53322,15202 53322,15274 53322,15274 53322,15212 53322,15212"];
	Layer1_Device10_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51563,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage0_RecvKV	[label="Local K,V",
		lp="52064,15105",
		pos="e,52321,15062 52321,15148 52321,15148 52321,15072 52321,15072"];
	Layer1_Device10_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52353,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage0_Attention	[label=Q_local,
		lp="52625,15009",
		pos="e,52534,14843 52534,15148 52534,15148 52534,14853 52534,14853"];
	Layer1_Device10_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50023,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage1_Attention	[label=Q_local,
		lp="50087,14912",
		pos="e,50167,14649 52159,15155 51377,15155 50167,15155 50167,15155 50167,15155 50167,14659 50167,14659"];
	Layer1_Device10_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50634,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage2_Attention	[label=Q_local,
		lp="52765,14816",
		pos="e,50804,14456 52629,15148 52629,15053 52629,14736 52629,14736 52629,14736 50804,14736 50804,14736 50804,14736 50804,14466 50804,\
14466"];
	Layer1_Device10_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52649,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage3_Attention	[label=Q_local,
		lp="52905,14719",
		pos="e,52822,14264 52822,15149 52822,15149 52822,14274 52822,14274"];
	Layer1_Device10_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49855,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage4_Attention	[label=Q_local,
		lp="49707,14623",
		pos="e,49708,14070 52159,15168 51250,15168 49708,15168 49708,15168 49708,15168 49708,14080 49708,14080"];
	Layer1_Device10_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52767,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage5_Attention	[label=Q_local,
		lp="53009,14526",
		pos="e,52940,13877 52940,15148 52940,15148 52940,13887 52940,13887"];
	Layer1_Device10_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52794,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage6_Attention	[label=Q_local,
		lp="53141,14430",
		pos="e,53012,13684 53012,15149 53012,15149 53012,13694 53012,13694"];
	Layer1_Device10_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49855,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage7_Attention	[label=Q_local,
		lp="49561,14333",
		pos="e,49623,13473 52159,15181 51224,15181 49612,15181 49612,15181 49612,15181 49612,13473 49612,13473 49612,13473 49613,13473 49613,\
13473"];
	Layer1_Device10_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52692,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage8_Attention	[label=Q_local,
		lp="53259,14237",
		pos="e,52924,13271 53102,15149 53102,14919 53102,13271 53102,13271 53102,13271 52934,13271 52934,13271"];
	Layer1_Device10_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49753,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage9_Attention	[label=Q_local,
		lp="49443,14140",
		pos="e,49600,13105 52159,15188 51221,15188 49600,15188 49600,15188 49600,15188 49600,13115 49600,13115"];
	Layer1_Device10_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50046,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage10_Attention	[label=Q_local,
		lp="49303,14044",
		pos="e,49814,12885 52159,15195 51197,15195 49505,15195 49505,15195 49505,15195 49505,12885 49505,12885 49505,12885 49804,12885 49804,\
12885"];
	Layer1_Device10_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51201,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage11_Attention	[label=Q_local,
		lp="53382,13947",
		pos="e,51278,12719 53178,15149 53178,14885 53178,12760 53178,12760 53178,12760 51278,12760 51278,12760 51278,12760 51278,12729 51278,\
12729"];
	Layer1_Device10_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51420,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage12_Attention	[label=Q_local,
		lp="53523,13851",
		pos="e,51461,12526 53254,15148 53254,14885 53254,12758 53254,12758 53254,12758 51461,12758 51461,12758 51461,12758 51461,12536 51461,\
12536"];
	Layer1_Device10_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="53435,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage13_Attention	[label=Q_local,
		lp="53663,13754",
		pos="e,53580,12334 53459,15161 53532,15161 53580,15161 53580,15161 53580,15161 53580,12344 53580,12344"];
	Layer1_Device10_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51017,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage14_Attention	[label=Q_local,
		lp="49157,13658",
		pos="e,51094,12140 53459,15175 53588,15175 53684,15175 53684,15175 53684,15175 53684,12245 53684,12245 53684,12245 51094,12245 51094,\
12245 51094,12245 51094,12150 51094,12150"];
	Layer1_Device10_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52754,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_QKVProj -> Layer1_Device10_Stage15_Attention	[label=Q_local,
		lp="53781,13561",
		pos="e,52986,11970 53459,15181 53597,15181 53702,15181 53702,15181 53702,15181 53702,11970 53702,11970 53702,11970 52996,11970 52996,\
11970"];
	Layer1_Device10_Stage0_RecvKV -> Layer1_Device10_Stage0_Attention	[pos="e,52302,14843 52302,15009 52302,15009 52302,14853 52302,14853"];
	Layer1_Device10_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51144,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage0_RecvKV -> Layer1_Device10_Stage1_RecvKV	[label="Ring transfer",
		lp="51483,14912",
		pos="e,51354,14869 51354,14955 51354,14955 51354,14879 51354,14879"];
	Layer1_Device10_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="50544,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage0_Attention -> Layer1_Device10_Stage0_Accumulate	[pos="e,50544,14649 52353,14789 52353,14766 52353,14738 52353,14738 52353,14738 50544,14738 50544,14738 50544,14738 50544,14659 50544,\
14659"];
	Layer1_Device10_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50113,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage0_Accumulate -> Layer1_Device10_Stage1_Accumulate	[pos="e,50328,14456 50328,14596 50328,14596 50328,14466 50328,14466"];
	Layer1_Device10_Stage1_RecvKV -> Layer1_Device10_Stage1_Attention	[pos="e,50239,14650 50239,14816 50239,14816 50239,14660 50239,14660"];
	Layer1_Device10_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51753,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage1_RecvKV -> Layer1_Device10_Stage2_RecvKV	[label="Ring transfer",
		lp="51461,14719",
		pos="e,51448,14676 51448,14762 51448,14762 51448,14686 51448,14686"];
	Layer1_Device10_Stage1_Attention -> Layer1_Device10_Stage1_Accumulate	[pos="e,50068,14456 50068,14596 50068,14596 50068,14466 50068,14466"];
	Layer1_Device10_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50231,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage1_Accumulate -> Layer1_Device10_Stage2_Accumulate	[pos="e,50172,14263 50172,14403 50172,14403 50172,14273 50172,14273"];
	Layer1_Device10_Stage2_RecvKV -> Layer1_Device10_Stage2_Attention	[pos="e,50849,14457 50849,14623 50849,14623 50849,14467 50849,14467"];
	Layer1_Device10_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51843,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage2_RecvKV -> Layer1_Device10_Stage3_RecvKV	[label="Ring transfer",
		lp="51846,14526",
		pos="e,51798,14483 51798,14569 51798,14569 51798,14493 51798,14493"];
	Layer1_Device10_Stage2_Attention -> Layer1_Device10_Stage2_Accumulate	[pos="e,50432,14263 50432,14403 50432,14403 50432,14273 50432,14273"];
	Layer1_Device10_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50376,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage2_Accumulate -> Layer1_Device10_Stage3_Accumulate	[pos="e,50304,14070 50304,14210 50304,14210 50304,14080 50304,14080"];
	Layer1_Device10_Stage3_RecvKV -> Layer1_Device10_Stage3_Attention	[pos="e,52590,14264 52590,14430 52590,14430 52590,14274 52590,14274"];
	Layer1_Device10_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51440,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage3_RecvKV -> Layer1_Device10_Stage4_RecvKV	[label="Ring transfer",
		lp="51624,14333",
		pos="e,51642,14290 51642,14376 51642,14376 51642,14300 51642,14300"];
	Layer1_Device10_Stage3_Attention -> Layer1_Device10_Stage3_Accumulate	[pos="e,50564,14070 52461,14210 52461,14185 52461,14153 52461,14153 52461,14153 50564,14153 50564,14153 50564,14153 50564,14080 50564,\
14080"];
	Layer1_Device10_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50349,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage3_Accumulate -> Layer1_Device10_Stage4_Accumulate	[pos="e,50362,13877 50362,14017 50362,14017 50362,13887 50362,13887"];
	Layer1_Device10_Stage4_RecvKV -> Layer1_Device10_Stage4_Attention	[pos="e,50043,14070 50564,14196 50283,14196 50043,14196 50043,14196 50043,14196 50043,14080 50043,14080"];
	Layer1_Device10_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51585,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage4_RecvKV -> Layer1_Device10_Stage5_RecvKV	[label="Ring transfer",
		lp="51560,14140",
		pos="e,51512,14097 51512,14183 51512,14183 51512,14107 51512,14107"];
	Layer1_Device10_Stage4_Attention -> Layer1_Device10_Stage4_Accumulate	[pos="e,50131,13877 50087,14043 50113,14043 50131,14043 50131,14043 50131,14043 50131,13887 50131,13887"];
	Layer1_Device10_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52273,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage4_Accumulate -> Layer1_Device10_Stage5_Accumulate	[pos="e,52260,13684 50362,13824 50362,13787 50362,13726 50362,13726 50362,13726 52260,13726 52260,13726 52260,13726 52260,13694 52260,\
13694"];
	Layer1_Device10_Stage5_RecvKV -> Layer1_Device10_Stage5_Attention	[pos="e,52535,13859 52487,14044 52487,14038 52487,13859 52487,13859 52487,13859 52525,13859 52525,13859"];
	Layer1_Device10_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51558,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage5_RecvKV -> Layer1_Device10_Stage6_RecvKV	[label="Ring transfer",
		lp="51619,13947",
		pos="e,51572,13904 51572,13990 51572,13990 51572,13914 51572,13914"];
	Layer1_Device10_Stage5_Attention -> Layer1_Device10_Stage5_Accumulate	[pos="e,52496,13684 52535,13841 52511,13841 52496,13841 52496,13841 52496,13841 52496,13694 52496,13694"];
	Layer1_Device10_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52273,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage5_Accumulate -> Layer1_Device10_Stage6_Accumulate	[pos="e,52273,13491 52273,13631 52273,13631 52273,13501 52273,13501"];
	Layer1_Device10_Stage6_RecvKV -> Layer1_Device10_Stage6_Attention	[pos="e,52780,13684 52145,13810 52459,13810 52780,13810 52780,13810 52780,13810 52780,13694 52780,13694"];
	Layer1_Device10_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51064,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage6_RecvKV -> Layer1_Device10_Stage7_RecvKV	[label="Ring transfer",
		lp="51420,13754",
		pos="e,51311,13711 51311,13797 51311,13797 51311,13721 51311,13721"];
	Layer1_Device10_Stage6_Attention -> Layer1_Device10_Stage6_Accumulate	[pos="e,52505,13455 52743,13631 52743,13576 52743,13455 52743,13455 52743,13455 52515,13455 52515,13455"];
	Layer1_Device10_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52171,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage6_Accumulate -> Layer1_Device10_Stage7_Accumulate	[pos="e,52222,13298 52222,13438 52222,13438 52222,13308 52222,13308"];
	Layer1_Device10_Stage7_RecvKV -> Layer1_Device10_Stage7_Attention	[pos="e,49855,13491 50188,13617 49999,13617 49855,13617 49855,13617 49855,13617 49855,13501 49855,13501"];
	Layer1_Device10_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51064,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage7_RecvKV -> Layer1_Device10_Stage8_RecvKV	[label="Ring transfer",
		lp="51112,13561",
		pos="e,51064,13518 51064,13604 51064,13604 51064,13528 51064,13528"];
	Layer1_Device10_Stage7_Attention -> Layer1_Device10_Stage7_Accumulate	[pos="e,51962,13298 50064,13438 50064,13423 50064,13406 50064,13406 50064,13406 51962,13406 51962,13406 51962,13406 51962,13308 51962,\
13308"];
	Layer1_Device10_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52171,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage7_Accumulate -> Layer1_Device10_Stage8_Accumulate	[pos="e,52171,13105 52171,13245 52171,13245 52171,13115 52171,13115"];
	Layer1_Device10_Stage8_RecvKV -> Layer1_Device10_Stage8_Attention	[pos="e,52482,13298 51651,13424 52041,13424 52482,13424 52482,13424 52482,13424 52482,13308 52482,13308"];
	Layer1_Device10_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="50962,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage8_RecvKV -> Layer1_Device10_Stage9_RecvKV	[label="Ring transfer",
		lp="51047,13368",
		pos="e,51013,13325 51013,13411 51013,13411 51013,13335 51013,13335"];
	Layer1_Device10_Stage8_Attention -> Layer1_Device10_Stage8_Accumulate	[pos="e,52403,13078 52578,13245 52578,13192 52578,13078 52578,13078 52578,13078 52413,13078 52413,13078"];
	Layer1_Device10_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50567,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage8_Accumulate -> Layer1_Device10_Stage9_Accumulate	[pos="e,50644,12912 52171,13052 52171,13020 52171,12971 52171,12971 52171,12971 50644,12971 50644,12971 50644,12971 50644,12922 50644,\
12922"];
	Layer1_Device10_Stage9_RecvKV -> Layer1_Device10_Stage9_Attention	[pos="e,49804,13105 50086,13231 49924,13231 49804,13231 49804,13231 49804,13231 49804,13115 49804,13115"];
	Layer1_Device10_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="50962,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage9_RecvKV -> Layer1_Device10_Stage10_RecvKV	[label="Ring transfer",
		lp="51010,13175",
		pos="e,50962,13132 50962,13218 50962,13218 50962,13142 50962,13142"];
	Layer1_Device10_Stage9_Attention -> Layer1_Device10_Stage9_Accumulate	[pos="e,50490,12912 49900,13052 49900,13016 49900,12957 49900,12957 49900,12957 50490,12957 50490,12957 50490,12957 50490,12922 50490,\
12922"];
	Layer1_Device10_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50567,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage9_Accumulate -> Layer1_Device10_Stage10_Accumulate	[pos="e,50567,12719 50567,12859 50567,12859 50567,12729 50567,12729"];
	Layer1_Device10_Stage10_RecvKV -> Layer1_Device10_Stage10_Attention	[pos="e,50160,12912 50160,13025 50160,13025 50160,12922 50160,12922"];
	Layer1_Device10_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51776,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage10_RecvKV -> Layer1_Device10_Stage11_RecvKV	[label="Ring transfer",
		lp="51279,12982",
		pos="e,51369,12939 51369,13025 51369,13025 51369,12949 51369,12949"];
	Layer1_Device10_Stage10_Attention -> Layer1_Device10_Stage10_Accumulate	[pos="e,50335,12692 50070,12859 50070,12806 50070,12692 50070,12692 50070,12692 50325,12692 50325,12692"];
	Layer1_Device10_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50899,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage10_Accumulate -> Layer1_Device10_Stage11_Accumulate	[pos="e,50733,12526 50733,12666 50733,12666 50733,12536 50733,12536"];
	Layer1_Device10_Stage11_RecvKV -> Layer1_Device10_Stage11_Attention	[pos="e,51124,12719 51124,12832 51124,12832 51124,12729 51124,12729"];
	Layer1_Device10_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52410,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage11_RecvKV -> Layer1_Device10_Stage12_RecvKV	[label="Ring transfer",
		lp="51980,12789",
		pos="e,52093,12746 52093,12832 52093,12832 52093,12756 52093,12756"];
	Layer1_Device10_Stage11_Attention -> Layer1_Device10_Stage11_Accumulate	[pos="e,51050,12526 51050,12666 51050,12666 51050,12536 51050,12536"];
	Layer1_Device10_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51017,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage11_Accumulate -> Layer1_Device10_Stage12_Accumulate	[pos="e,50958,12333 50958,12473 50958,12473 50958,12343 50958,12343"];
	Layer1_Device10_Stage12_RecvKV -> Layer1_Device10_Stage12_Attention	[pos="e,51571,12527 51571,12693 51571,12693 51571,12537 51571,12537"];
	Layer1_Device10_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52629,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage12_RecvKV -> Layer1_Device10_Stage13_RecvKV	[label="Ring transfer",
		lp="52567,12596",
		pos="e,52520,12553 52520,12639 52520,12639 52520,12563 52520,12563"];
	Layer1_Device10_Stage12_Attention -> Layer1_Device10_Stage12_Accumulate	[pos="e,51218,12333 51218,12473 51218,12473 51218,12343 51218,12343"];
	Layer1_Device10_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51538,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage12_Accumulate -> Layer1_Device10_Stage13_Accumulate	[pos="e,51306,12113 51249,12306 51266,12306 51277,12306 51277,12306 51277,12306 51277,12113 51277,12113 51277,12113 51296,12113 51296,\
12113"];
	Layer1_Device10_Stage13_RecvKV -> Layer1_Device10_Stage13_Attention	[pos="e,53376,12334 53376,12500 53376,12500 53376,12344 53376,12344"];
	Layer1_Device10_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52226,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage13_RecvKV -> Layer1_Device10_Stage14_RecvKV	[label="Ring transfer",
		lp="52543,12403",
		pos="e,52428,12360 52428,12446 52428,12446 52428,12370 52428,12370"];
	Layer1_Device10_Stage13_Attention -> Layer1_Device10_Stage13_Accumulate	[pos="e,51538,12140 53435,12280 53435,12251 53435,12210 53435,12210 53435,12210 51538,12210 51538,12210 51538,12210 51538,12150 51538,\
12150"];
	Layer1_Device10_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51538,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage13_Accumulate -> Layer1_Device10_Stage14_Accumulate	[pos="e,51538,11988 51538,12087 51538,12087 51538,11998 51538,11998"];
	Layer1_Device10_Stage14_RecvKV -> Layer1_Device10_Stage14_Attention	[pos="e,50940,12140 51350,12266 51122,12266 50940,12266 50940,12266 50940,12266 50940,12150 50940,12150"];
	Layer1_Device10_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52747,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device10_Stage14_RecvKV -> Layer1_Device10_Stage15_RecvKV	[label="Ring transfer",
		lp="52534,12210",
		pos="e,52486,12167 52486,12253 52486,12253 52486,12177 52486,12177"];
	Layer1_Device10_Stage14_Attention -> Layer1_Device10_Stage14_Accumulate	[pos="e,51306,11961 51017,12087 51017,12043 51017,11961 51017,11961 51017,11961 51296,11961 51296,11961"];
	Layer1_Device10_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52754,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device10_Stage14_Accumulate -> Layer1_Device10_Stage15_Accumulate	[pos="e,52522,11835 51538,11934 51538,11897 51538,11835 51538,11835 51538,11835 52512,11835 52512,11835"];
	Layer1_Device10_Stage15_RecvKV -> Layer1_Device10_Stage15_Attention	[pos="e,52754,11988 52754,12060 52754,12060 52754,11998 52754,11998"];
	Layer1_Device10_Stage15_Attention -> Layer1_Device10_Stage15_Accumulate	[pos="e,52754,11862 52754,11934 52754,11934 52754,11872 52754,11872"];
	Layer1_Device10_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="52792,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device10_Stage15_Accumulate -> Layer1_Device10_ConcatHeads	[pos="e,52776,11736 52776,11808 52776,11808 52776,11746 52776,11746"];
	Layer1_Device10_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="52849,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device10_ConcatHeads -> Layer1_Device10_OutputProj	[pos="e,52827,11610 52827,11682 52827,11682 52827,11620 52827,11620"];
	Layer1_Device10_OutputProj -> Layer1_Device10_Residual1	[pos="e,52849,11484 52849,11556 52849,11556 52849,11494 52849,11494"];
	Layer1_Device10_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="52622,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device10_Residual1 -> Layer1_Device10_LayerNorm2	[pos="e,52682,11358 52682,11430 52682,11430 52682,11368 52682,11368"];
	Layer1_Device10_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="52920,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device10_Residual1 -> Layer1_Device10_Residual2	[pos="e,53085,10676 53085,11430 53085,11430 53085,10686 53085,10686"];
	Layer1_Device10_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="52329,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device10_LayerNorm2 -> Layer1_Device10_GateProj	[pos="e,52482,11232 52482,11304 52482,11304 52482,11242 52482,11242"];
	Layer1_Device10_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="52642,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device10_LayerNorm2 -> Layer1_Device10_UpProj	[pos="e,52694,11143 52694,11304 52694,11304 52694,11153 52694,11153"];
	Layer1_Device10_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="52329,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device10_GateProj -> Layer1_Device10_Activation	[pos="e,52260,11054 52260,11179 52260,11179 52260,11064 52260,11064"];
	Layer1_Device10_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="52425,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device10_UpProj -> Layer1_Device10_ElemMul	[pos="e,52695,10928 52695,11089 52695,11089 52695,10938 52695,10938"];
	Layer1_Device10_Activation -> Layer1_Device10_ElemMul	[pos="e,52329,10928 52329,11000 52329,11000 52329,10938 52329,10938"];
	Layer1_Device10_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="52521,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device10_ElemMul -> Layer1_Device10_DownProj	[pos="e,52521,10802 52521,10874 52521,10874 52521,10812 52521,10812"];
	Layer1_Device10_DownProj -> Layer1_Device10_Residual2	[pos="e,52637,10676 52637,10748 52637,10748 52637,10686 52637,10686"];
	Layer1_Device10_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="52920,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device10_Residual2 -> Layer1_Device10_Output	[pos="e,52920,10550 52920,10622 52920,10622 52920,10560 52920,10560"];
	Layer2_Device10_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="52920,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device10_Output -> Layer2_Device10_Input	[pos="e,52920,10402 52920,10475 52920,10475 52920,10412 52920,10412"];
	Layer1_Device11_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58378,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device11_Input -> Layer1_Device11_LayerNorm1	[pos="e,58493,15328 58493,15410 58493,15410 58493,15338 58493,15338"];
	Layer1_Device11_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="58737,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device11_Input -> Layer1_Device11_Residual1	[pos="e,58848,11484 58848,15406 58848,15406 58848,11494 58848,11494"];
	Layer1_Device11_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57818,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device11_LayerNorm1 -> Layer1_Device11_QKVProj	[pos="e,58316,15202 58316,15274 58316,15274 58316,15212 58316,15212"];
	Layer1_Device11_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56575,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage0_RecvKV	[label="Local K,V",
		lp="57076,15105",
		pos="e,57332,15062 57332,15148 57332,15148 57332,15072 57332,15072"];
	Layer1_Device11_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57365,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage0_Attention	[label=Q_local,
		lp="57637,15009",
		pos="e,57546,14843 57546,15148 57546,15148 57546,14853 57546,14853"];
	Layer1_Device11_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55035,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage1_Attention	[label=Q_local,
		lp="55099,14912",
		pos="e,55159,14649 57168,15155 56381,15155 55159,15155 55159,15155 55159,15155 55159,14659 55159,14659"];
	Layer1_Device11_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55646,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage2_Attention	[label=Q_local,
		lp="57777,14816",
		pos="e,55816,14456 57641,15149 57641,15054 57641,14741 57641,14741 57641,14741 55816,14741 55816,14741 55816,14741 55816,14466 55816,\
14466"];
	Layer1_Device11_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57661,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage3_Attention	[label=Q_local,
		lp="57917,14719",
		pos="e,57834,14264 57834,15149 57834,15149 57834,14274 57834,14274"];
	Layer1_Device11_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54867,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage4_Attention	[label=Q_local,
		lp="54719,14623",
		pos="e,54720,14070 57168,15168 56260,15168 54720,15168 54720,15168 54720,15168 54720,14080 54720,14080"];
	Layer1_Device11_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57785,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage5_Attention	[label=Q_local,
		lp="58021,14526",
		pos="e,57954,13877 57954,15148 57954,15148 57954,13887 57954,13887"];
	Layer1_Device11_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57817,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage6_Attention	[label=Q_local,
		lp="58153,14430",
		pos="e,58032,13684 58032,15149 58032,15149 58032,13694 58032,13694"];
	Layer1_Device11_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54878,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage7_Attention	[label=Q_local,
		lp="54573,14333",
		pos="e,54646,13473 57168,15188 56231,15188 54611,15188 54611,15188 54611,15188 54611,13473 54611,13473 54611,13473 54636,13473 54636,\
13473"];
	Layer1_Device11_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57552,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage8_Attention	[label=Q_local,
		lp="58277,14237",
		pos="e,57784,13271 58111,15149 58111,14919 58111,13271 58111,13271 58111,13271 57794,13271 57794,13271"];
	Layer1_Device11_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54613,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage9_Attention	[label=Q_local,
		lp="54455,14140",
		pos="e,54586,13105 57168,15195 56225,15195 54586,15195 54586,15195 54586,15195 54586,13115 54586,13115"];
	Layer1_Device11_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55018,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage10_Attention	[label=Q_local,
		lp="54315,14044",
		pos="e,55134,12913 58173,15148 58173,14901 58173,13023 58173,13023 58173,13023 55134,13023 55134,13023 55134,13023 55134,12923 55134,\
12923"];
	Layer1_Device11_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56232,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage11_Attention	[label=Q_local,
		lp="58406,13947",
		pos="e,56309,12719 58236,15148 58236,14884 58236,12755 58236,12755 58236,12755 56309,12755 56309,12755 56309,12755 56309,12729 56309,\
12729"];
	Layer1_Device11_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56438,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage12_Attention	[label=Q_local,
		lp="58541,13851",
		pos="e,56492,12526 58298,15148 58298,14884 58298,12753 58298,12753 58298,12753 56492,12753 56492,12753 56492,12753 56492,12536 56492,\
12536"];
	Layer1_Device11_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="58453,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage13_Attention	[label=Q_local,
		lp="58681,13754",
		pos="e,58579,12334 58468,15161 58535,15161 58579,15161 58579,15161 58579,15161 58579,12344 58579,12344"];
	Layer1_Device11_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56035,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage14_Attention	[label=Q_local,
		lp="54169,13658",
		pos="e,56112,12140 58468,15175 58595,15175 58689,15175 58689,15175 58689,15175 58689,12237 58689,12237 58689,12237 56112,12237 56112,\
12237 56112,12237 56112,12150 56112,12150"];
	Layer1_Device11_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="58168,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_QKVProj -> Layer1_Device11_Stage15_Attention	[label=Q_local,
		lp="58799,13561",
		pos="e,58400,11970 58468,15181 58597,15181 58694,15181 58694,15181 58694,15181 58694,11970 58694,11970 58694,11970 58410,11970 58410,\
11970"];
	Layer1_Device11_Stage0_RecvKV -> Layer1_Device11_Stage0_Attention	[pos="e,57314,14843 57314,15009 57314,15009 57314,14853 57314,14853"];
	Layer1_Device11_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56156,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage0_RecvKV -> Layer1_Device11_Stage1_RecvKV	[label="Ring transfer",
		lp="56558,14912",
		pos="e,56366,14869 56366,14955 56366,14955 56366,14879 56366,14879"];
	Layer1_Device11_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="55556,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage0_Attention -> Layer1_Device11_Stage0_Accumulate	[pos="e,55556,14649 57365,14789 57365,14769 57365,14744 57365,14744 57365,14744 55556,14744 55556,14744 55556,14744 55556,14659 55556,\
14659"];
	Layer1_Device11_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55125,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage0_Accumulate -> Layer1_Device11_Stage1_Accumulate	[pos="e,55340,14456 55340,14596 55340,14596 55340,14466 55340,14466"];
	Layer1_Device11_Stage1_RecvKV -> Layer1_Device11_Stage1_Attention	[pos="e,55251,14650 55251,14816 55251,14816 55251,14660 55251,14660"];
	Layer1_Device11_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56765,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage1_RecvKV -> Layer1_Device11_Stage2_RecvKV	[label="Ring transfer",
		lp="56463,14719",
		pos="e,56460,14676 56460,14762 56460,14762 56460,14686 56460,14686"];
	Layer1_Device11_Stage1_Attention -> Layer1_Device11_Stage1_Accumulate	[pos="e,55080,14456 55080,14596 55080,14596 55080,14466 55080,14466"];
	Layer1_Device11_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55243,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage1_Accumulate -> Layer1_Device11_Stage2_Accumulate	[pos="e,55184,14263 55184,14403 55184,14403 55184,14273 55184,14273"];
	Layer1_Device11_Stage2_RecvKV -> Layer1_Device11_Stage2_Attention	[pos="e,55861,14457 55861,14623 55861,14623 55861,14467 55861,14467"];
	Layer1_Device11_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56855,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage2_RecvKV -> Layer1_Device11_Stage3_RecvKV	[label="Ring transfer",
		lp="56858,14526",
		pos="e,56810,14483 56810,14569 56810,14569 56810,14493 56810,14493"];
	Layer1_Device11_Stage2_Attention -> Layer1_Device11_Stage2_Accumulate	[pos="e,55444,14263 55444,14403 55444,14403 55444,14273 55444,14273"];
	Layer1_Device11_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55388,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage2_Accumulate -> Layer1_Device11_Stage3_Accumulate	[pos="e,55316,14070 55316,14210 55316,14210 55316,14080 55316,14080"];
	Layer1_Device11_Stage3_RecvKV -> Layer1_Device11_Stage3_Attention	[pos="e,57602,14264 57602,14430 57602,14430 57602,14274 57602,14274"];
	Layer1_Device11_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56452,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage3_RecvKV -> Layer1_Device11_Stage4_RecvKV	[label="Ring transfer",
		lp="56769,14333",
		pos="e,56654,14290 56654,14376 56654,14376 56654,14300 56654,14300"];
	Layer1_Device11_Stage3_Attention -> Layer1_Device11_Stage3_Accumulate	[pos="e,55576,14070 57473,14210 57473,14187 57473,14158 57473,14158 57473,14158 55576,14158 55576,14158 55576,14158 55576,14080 55576,\
14080"];
	Layer1_Device11_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55367,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage3_Accumulate -> Layer1_Device11_Stage4_Accumulate	[pos="e,55378,13877 55378,14017 55378,14017 55378,13887 55378,13887"];
	Layer1_Device11_Stage4_RecvKV -> Layer1_Device11_Stage4_Attention	[pos="e,55055,14070 55576,14196 55295,14196 55055,14196 55055,14196 55055,14196 55055,14080 55055,14080"];
	Layer1_Device11_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56597,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage4_RecvKV -> Layer1_Device11_Stage5_RecvKV	[label="Ring transfer",
		lp="56572,14140",
		pos="e,56524,14097 56524,14183 56524,14183 56524,14107 56524,14107"];
	Layer1_Device11_Stage4_Attention -> Layer1_Device11_Stage4_Accumulate	[pos="e,55146,13877 55099,14043 55127,14043 55146,14043 55146,14043 55146,14043 55146,13887 55146,13887"];
	Layer1_Device11_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57296,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage4_Accumulate -> Layer1_Device11_Stage5_Accumulate	[pos="e,57280,13685 55383,13824 55383,13794 55383,13751 55383,13751 55383,13751 57280,13751 57280,13751 57280,13751 57280,13695 57280,\
13695"];
	Layer1_Device11_Stage5_RecvKV -> Layer1_Device11_Stage5_Attention	[pos="e,57553,13859 57506,14044 57506,14038 57506,13859 57506,13859 57506,13859 57543,13859 57543,13859"];
	Layer1_Device11_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56576,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage5_RecvKV -> Layer1_Device11_Stage6_RecvKV	[label="Ring transfer",
		lp="56634,13947",
		pos="e,56586,13904 56586,13990 56586,13990 56586,13914 56586,13914"];
	Layer1_Device11_Stage5_Attention -> Layer1_Device11_Stage5_Accumulate	[pos="e,57522,13684 57553,13841 57534,13841 57522,13841 57522,13841 57522,13841 57522,13694 57522,13694"];
	Layer1_Device11_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57296,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage5_Accumulate -> Layer1_Device11_Stage6_Accumulate	[pos="e,57296,13491 57296,13631 57296,13631 57296,13501 57296,13501"];
	Layer1_Device11_Stage6_RecvKV -> Layer1_Device11_Stage6_Attention	[pos="e,57801,13684 57163,13810 57478,13810 57801,13810 57801,13810 57801,13810 57801,13694 57801,13694"];
	Layer1_Device11_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56087,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage6_RecvKV -> Layer1_Device11_Stage7_RecvKV	[label="Ring transfer",
		lp="56314,13754",
		pos="e,56332,13711 56332,13797 56332,13797 56332,13721 56332,13721"];
	Layer1_Device11_Stage6_Attention -> Layer1_Device11_Stage6_Accumulate	[pos="e,57528,13455 57684,13631 57684,13576 57684,13455 57684,13455 57684,13455 57538,13455 57538,13455"];
	Layer1_Device11_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57031,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage6_Accumulate -> Layer1_Device11_Stage7_Accumulate	[pos="e,57164,13298 57164,13438 57164,13438 57164,13308 57164,13308"];
	Layer1_Device11_Stage7_RecvKV -> Layer1_Device11_Stage7_Attention	[pos="e,55104,13491 55211,13617 55145,13617 55104,13617 55104,13617 55104,13617 55104,13501 55104,13501"];
	Layer1_Device11_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56087,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage7_RecvKV -> Layer1_Device11_Stage8_RecvKV	[label="Ring transfer",
		lp="56135,13561",
		pos="e,56087,13518 56087,13604 56087,13604 56087,13528 56087,13528"];
	Layer1_Device11_Stage7_Attention -> Layer1_Device11_Stage7_Accumulate	[pos="e,56903,13298 55006,13438 55006,13398 55006,13329 55006,13329 55006,13329 56903,13329 56903,13329 56903,13329 56903,13308 56903,\
13308"];
	Layer1_Device11_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57031,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage7_Accumulate -> Layer1_Device11_Stage8_Accumulate	[pos="e,57031,13105 57031,13245 57031,13245 57031,13115 57031,13115"];
	Layer1_Device11_Stage8_RecvKV -> Layer1_Device11_Stage8_Attention	[pos="e,57424,13298 56674,13424 57033,13424 57424,13424 57424,13424 57424,13424 57424,13308 57424,13308"];
	Layer1_Device11_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="55822,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage8_RecvKV -> Layer1_Device11_Stage9_RecvKV	[label="Ring transfer",
		lp="55937,13368",
		pos="e,55954,13325 55954,13411 55954,13411 55954,13335 55954,13335"];
	Layer1_Device11_Stage8_Attention -> Layer1_Device11_Stage8_Accumulate	[pos="e,57263,13078 57494,13245 57494,13192 57494,13078 57494,13078 57494,13078 57273,13078 57273,13078"];
	Layer1_Device11_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55539,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage8_Accumulate -> Layer1_Device11_Stage9_Accumulate	[pos="e,55616,12913 57031,13052 57031,13018 57031,12965 57031,12965 57031,12965 55616,12965 55616,12965 55616,12965 55616,12923 55616,\
12923"];
	Layer1_Device11_Stage9_RecvKV -> Layer1_Device11_Stage9_Attention	[pos="e,54746,13105 54946,13231 54827,13231 54746,13231 54746,13231 54746,13231 54746,13115 54746,13115"];
	Layer1_Device11_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="55822,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage9_RecvKV -> Layer1_Device11_Stage10_RecvKV	[label="Ring transfer",
		lp="55870,13175",
		pos="e,55822,13132 55822,13218 55822,13218 55822,13142 55822,13142"];
	Layer1_Device11_Stage9_Attention -> Layer1_Device11_Stage9_Accumulate	[pos="e,55462,12913 54816,13052 54816,13017 54816,12960 54816,12960 54816,12960 55462,12960 55462,12960 55462,12960 55462,12923 55462,\
12923"];
	Layer1_Device11_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55539,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage9_Accumulate -> Layer1_Device11_Stage10_Accumulate	[pos="e,55539,12719 55539,12859 55539,12859 55539,12729 55539,12729"];
	Layer1_Device11_Stage10_RecvKV -> Layer1_Device11_Stage10_Attention	[pos="e,55018,12912 55018,13025 55018,13025 55018,12922 55018,12922"];
	Layer1_Device11_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56748,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage10_RecvKV -> Layer1_Device11_Stage11_RecvKV	[label="Ring transfer",
		lp="56046,12982",
		pos="e,56285,12939 56285,13025 56285,13025 56285,12949 56285,12949"];
	Layer1_Device11_Stage10_Attention -> Layer1_Device11_Stage10_Accumulate	[pos="e,55307,12692 55221,12859 55221,12806 55221,12692 55221,12692 55221,12692 55297,12692 55297,12692"];
	Layer1_Device11_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55917,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage10_Accumulate -> Layer1_Device11_Stage11_Accumulate	[pos="e,55728,12526 55728,12666 55728,12666 55728,12536 55728,12536"];
	Layer1_Device11_Stage11_RecvKV -> Layer1_Device11_Stage11_Attention	[pos="e,56155,12719 56155,12832 56155,12832 56155,12729 56155,12729"];
	Layer1_Device11_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57441,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage11_RecvKV -> Layer1_Device11_Stage12_RecvKV	[label="Ring transfer",
		lp="57056,12789",
		pos="e,57094,12746 57094,12832 57094,12832 57094,12756 57094,12756"];
	Layer1_Device11_Stage11_Attention -> Layer1_Device11_Stage11_Accumulate	[pos="e,56074,12526 56074,12666 56074,12666 56074,12536 56074,12536"];
	Layer1_Device11_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56035,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage11_Accumulate -> Layer1_Device11_Stage12_Accumulate	[pos="e,55976,12333 55976,12473 55976,12473 55976,12343 55976,12343"];
	Layer1_Device11_Stage12_RecvKV -> Layer1_Device11_Stage12_Attention	[pos="e,56595,12527 56595,12693 56595,12693 56595,12537 56595,12537"];
	Layer1_Device11_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57647,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage12_RecvKV -> Layer1_Device11_Stage13_RecvKV	[label="Ring transfer",
		lp="57592,12596",
		pos="e,57544,12553 57544,12639 57544,12639 57544,12563 57544,12563"];
	Layer1_Device11_Stage12_Attention -> Layer1_Device11_Stage12_Accumulate	[pos="e,56236,12333 56236,12473 56236,12473 56236,12343 56236,12343"];
	Layer1_Device11_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56556,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage12_Accumulate -> Layer1_Device11_Stage13_Accumulate	[pos="e,56324,12113 56267,12306 56284,12306 56295,12306 56295,12306 56295,12306 56295,12113 56295,12113 56295,12113 56314,12113 56314,\
12113"];
	Layer1_Device11_Stage13_RecvKV -> Layer1_Device11_Stage13_Attention	[pos="e,58394,12334 58394,12500 58394,12500 58394,12344 58394,12344"];
	Layer1_Device11_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57244,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage13_RecvKV -> Layer1_Device11_Stage14_RecvKV	[label="Ring transfer",
		lp="57494,12403",
		pos="e,57446,12360 57446,12446 57446,12446 57446,12370 57446,12370"];
	Layer1_Device11_Stage13_Attention -> Layer1_Device11_Stage13_Accumulate	[pos="e,56556,12140 58453,12280 58453,12253 58453,12214 58453,12214 58453,12214 56556,12214 56556,12214 56556,12214 56556,12150 56556,\
12150"];
	Layer1_Device11_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56556,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage13_Accumulate -> Layer1_Device11_Stage14_Accumulate	[pos="e,56556,11988 56556,12087 56556,12087 56556,11998 56556,11998"];
	Layer1_Device11_Stage14_RecvKV -> Layer1_Device11_Stage14_Attention	[pos="e,55958,12140 56368,12266 56140,12266 55958,12266 55958,12266 55958,12266 55958,12150 55958,12150"];
	Layer1_Device11_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57765,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device11_Stage14_RecvKV -> Layer1_Device11_Stage15_RecvKV	[label="Ring transfer",
		lp="57471,12210",
		pos="e,57504,12167 57504,12253 57504,12253 57504,12177 57504,12177"];
	Layer1_Device11_Stage14_Attention -> Layer1_Device11_Stage14_Accumulate	[pos="e,56324,11952 56035,12087 56035,12041 56035,11952 56035,11952 56035,11952 56314,11952 56314,11952"];
	Layer1_Device11_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="58168,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device11_Stage14_Accumulate -> Layer1_Device11_Stage15_Accumulate	[pos="e,57936,11835 56556,11934 56556,11897 56556,11835 56556,11835 56556,11835 57926,11835 57926,11835"];
	Layer1_Device11_Stage15_RecvKV -> Layer1_Device11_Stage15_Attention	[pos="e,58168,11988 58168,12060 58168,12060 58168,11998 58168,11998"];
	Layer1_Device11_Stage15_Attention -> Layer1_Device11_Stage15_Accumulate	[pos="e,58168,11862 58168,11934 58168,11934 58168,11872 58168,11872"];
	Layer1_Device11_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58374,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device11_Stage15_Accumulate -> Layer1_Device11_ConcatHeads	[pos="e,58274,11736 58274,11808 58274,11808 58274,11746 58274,11746"];
	Layer1_Device11_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58486,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device11_ConcatHeads -> Layer1_Device11_OutputProj	[pos="e,58436,11610 58436,11682 58436,11682 58436,11620 58436,11620"];
	Layer1_Device11_OutputProj -> Layer1_Device11_Residual1	[pos="e,58525,11484 58525,11556 58525,11556 58525,11494 58525,11494"];
	Layer1_Device11_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58542,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device11_Residual1 -> Layer1_Device11_LayerNorm2	[pos="e,58553,11358 58553,11430 58553,11430 58553,11368 58553,11368"];
	Layer1_Device11_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="58740,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device11_Residual1 -> Layer1_Device11_Residual2	[pos="e,58955,10676 58955,11430 58955,11430 58955,10686 58955,10686"];
	Layer1_Device11_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58249,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device11_LayerNorm2 -> Layer1_Device11_GateProj	[pos="e,58402,11232 58402,11304 58402,11304 58402,11242 58402,11242"];
	Layer1_Device11_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58562,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device11_LayerNorm2 -> Layer1_Device11_UpProj	[pos="e,58614,11143 58614,11304 58614,11304 58614,11153 58614,11153"];
	Layer1_Device11_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58249,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device11_GateProj -> Layer1_Device11_Activation	[pos="e,58180,11054 58180,11179 58180,11179 58180,11064 58180,11064"];
	Layer1_Device11_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58345,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device11_UpProj -> Layer1_Device11_ElemMul	[pos="e,58615,10928 58615,11089 58615,11089 58615,10938 58615,10938"];
	Layer1_Device11_Activation -> Layer1_Device11_ElemMul	[pos="e,58249,10928 58249,11000 58249,11000 58249,10938 58249,10938"];
	Layer1_Device11_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58488,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device11_ElemMul -> Layer1_Device11_DownProj	[pos="e,58488,10802 58488,10874 58488,10874 58488,10812 58488,10812"];
	Layer1_Device11_DownProj -> Layer1_Device11_Residual2	[pos="e,58530,10676 58530,10748 58530,10748 58530,10686 58530,10686"];
	Layer1_Device11_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58740,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device11_Residual2 -> Layer1_Device11_Output	[pos="e,58740,10550 58740,10622 58740,10622 58740,10560 58740,10560"];
	Layer2_Device11_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58740,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device11_Output -> Layer2_Device11_Input	[pos="e,58740,10402 58740,10475 58740,10475 58740,10412 58740,10412"];
	Layer1_Device12_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63506,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device12_Input -> Layer1_Device12_LayerNorm1	[pos="e,63506,15328 63506,15401 63506,15401 63506,15338 63506,15338"];
	Layer1_Device12_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="63709,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device12_Input -> Layer1_Device12_Residual1	[pos="e,63772,11484 63772,15417 63772,15417 63772,11494 63772,11494"];
	Layer1_Device12_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62740,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device12_LayerNorm1 -> Layer1_Device12_QKVProj	[pos="e,63341,15202 63341,15274 63341,15274 63341,15212 63341,15212"];
	Layer1_Device12_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61494,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage0_RecvKV	[label="Local K,V",
		lp="61994,15105",
		pos="e,62252,15062 62252,15148 62252,15148 62252,15072 62252,15072"];
	Layer1_Device12_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62284,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage0_Attention	[label=Q_local,
		lp="62556,15009",
		pos="e,62465,14843 62465,15148 62465,15148 62465,14853 62465,14853"];
	Layer1_Device12_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59954,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage1_Attention	[label=Q_local,
		lp="60018,14912",
		pos="e,60136,14649 62090,15155 61319,15155 60136,15155 60136,15155 60136,15155 60136,14659 60136,14659"];
	Layer1_Device12_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60565,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage2_Attention	[label=Q_local,
		lp="62696,14816",
		pos="e,60735,14456 62560,15149 62560,15055 62560,14746 62560,14746 62560,14746 60735,14746 60735,14746 60735,14746 60735,14466 60735,\
14466"];
	Layer1_Device12_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62580,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage3_Attention	[label=Q_local,
		lp="62836,14719",
		pos="e,62753,14264 62753,15149 62753,15149 62753,14274 62753,14274"];
	Layer1_Device12_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59786,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage4_Attention	[label=Q_local,
		lp="59638,14623",
		pos="e,59688,14070 62090,15168 61194,15168 59688,15168 59688,15168 59688,15168 59688,14080 59688,14080"];
	Layer1_Device12_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62703,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage5_Attention	[label=Q_local,
		lp="62933,14526",
		pos="e,62873,13877 62873,15148 62873,15148 62873,13887 62873,13887"];
	Layer1_Device12_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62731,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage6_Attention	[label=Q_local,
		lp="63072,14430",
		pos="e,62948,13684 62948,15149 62948,15149 62948,13694 62948,13694"];
	Layer1_Device12_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59792,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage7_Attention	[label=Q_local,
		lp="59492,14333",
		pos="e,59560,13473 62090,15188 61140,15188 59485,15188 59485,15188 59485,15188 59485,13473 59485,13473 59485,13473 59550,13473 59550,\
13473"];
	Layer1_Device12_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62516,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage8_Attention	[label=Q_local,
		lp="63195,14237",
		pos="e,62748,13271 63017,15149 63017,14919 63017,13271 63017,13271 63017,13271 62758,13271 62758,13271"];
	Layer1_Device12_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59577,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage9_Attention	[label=Q_local,
		lp="59374,14140",
		pos="e,59415,13105 62090,15195 61122,15195 59415,15195 59415,15195 59415,15195 59415,13115 59415,13115"];
	Layer1_Device12_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59969,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage10_Attention	[label=Q_local,
		lp="59234,14044",
		pos="e,60089,12912 63072,15148 63072,14900 63072,13020 63072,13020 63072,13020 60089,13020 60089,13020 60089,13020 60089,12922 60089,\
12922"];
	Layer1_Device12_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61108,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage11_Attention	[label=Q_local,
		lp="63316,13947",
		pos="e,61185,12719 63127,15148 63127,14886 63127,12785 63127,12785 63127,12785 61185,12785 61185,12785 61185,12785 61185,12729 61185,\
12729"];
	Layer1_Device12_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61356,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage12_Attention	[label=Q_local,
		lp="63459,13851",
		pos="e,61368,12526 63182,15148 63182,14886 63182,12783 63182,12783 63182,12783 61368,12783 61368,12783 61368,12783 61368,12536 61368,\
12536"];
	Layer1_Device12_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="63371,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage13_Attention	[label=Q_local,
		lp="63599,13754",
		pos="e,63544,12334 63390,15161 63480,15161 63544,15161 63544,15161 63544,15161 63544,12344 63544,12344"];
	Layer1_Device12_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60953,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage14_Attention	[label=Q_local,
		lp="59088,13658",
		pos="e,60721,12113 63313,15149 63313,14878 63313,12622 63313,12622 63313,12622 60555,12622 60555,12622 60555,12622 60555,12113 60555,\
12113 60555,12113 60711,12113 60711,12113"];
	Layer1_Device12_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="63086,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_QKVProj -> Layer1_Device12_Stage15_Attention	[label=Q_local,
		lp="63717,13561",
		pos="e,63318,11970 63390,15175 63542,15175 63660,15175 63660,15175 63660,15175 63660,11970 63660,11970 63660,11970 63328,11970 63328,\
11970"];
	Layer1_Device12_Stage0_RecvKV -> Layer1_Device12_Stage0_Attention	[pos="e,62233,14843 62233,15009 62233,15009 62233,14853 62233,14853"];
	Layer1_Device12_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61075,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage0_RecvKV -> Layer1_Device12_Stage1_RecvKV	[label="Ring transfer",
		lp="61332,14912",
		pos="e,61284,14869 61284,14955 61284,14955 61284,14879 61284,14879"];
	Layer1_Device12_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="60475,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage0_Attention -> Layer1_Device12_Stage0_Accumulate	[pos="e,60475,14650 62284,14789 62284,14770 62284,14749 62284,14749 62284,14749 60475,14749 60475,14749 60475,14749 60475,14660 60475,\
14660"];
	Layer1_Device12_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60044,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage0_Accumulate -> Layer1_Device12_Stage1_Accumulate	[pos="e,60260,14456 60260,14596 60260,14596 60260,14466 60260,14466"];
	Layer1_Device12_Stage1_RecvKV -> Layer1_Device12_Stage1_Attention	[pos="e,60170,14650 60170,14816 60170,14816 60170,14660 60170,14660"];
	Layer1_Device12_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61684,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage1_RecvKV -> Layer1_Device12_Stage2_RecvKV	[label="Ring transfer",
		lp="61382,14719",
		pos="e,61380,14676 61380,14762 61380,14762 61380,14686 61380,14686"];
	Layer1_Device12_Stage1_Attention -> Layer1_Device12_Stage1_Accumulate	[pos="e,59999,14456 59999,14596 59999,14596 59999,14466 59999,14466"];
	Layer1_Device12_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60162,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage1_Accumulate -> Layer1_Device12_Stage2_Accumulate	[pos="e,60103,14263 60103,14403 60103,14403 60103,14273 60103,14273"];
	Layer1_Device12_Stage2_RecvKV -> Layer1_Device12_Stage2_Attention	[pos="e,60780,14457 60780,14623 60780,14623 60780,14467 60780,14467"];
	Layer1_Device12_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61774,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage2_RecvKV -> Layer1_Device12_Stage3_RecvKV	[label="Ring transfer",
		lp="61777,14526",
		pos="e,61729,14483 61729,14569 61729,14569 61729,14493 61729,14493"];
	Layer1_Device12_Stage2_Attention -> Layer1_Device12_Stage2_Accumulate	[pos="e,60364,14263 60364,14403 60364,14403 60364,14273 60364,14273"];
	Layer1_Device12_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60307,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage2_Accumulate -> Layer1_Device12_Stage3_Accumulate	[pos="e,60234,14070 60234,14210 60234,14210 60234,14080 60234,14080"];
	Layer1_Device12_Stage3_RecvKV -> Layer1_Device12_Stage3_Attention	[pos="e,62521,14264 62521,14430 62521,14430 62521,14274 62521,14274"];
	Layer1_Device12_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61371,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage3_RecvKV -> Layer1_Device12_Stage4_RecvKV	[label="Ring transfer",
		lp="61555,14333",
		pos="e,61572,14290 61572,14376 61572,14376 61572,14300 61572,14300"];
	Layer1_Device12_Stage3_Attention -> Layer1_Device12_Stage3_Accumulate	[pos="e,60495,14070 62392,14210 62392,14189 62392,14163 62392,14163 62392,14163 60495,14163 60495,14163 60495,14163 60495,14080 60495,\
14080"];
	Layer1_Device12_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60285,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage3_Accumulate -> Layer1_Device12_Stage4_Accumulate	[pos="e,60296,13877 60296,14017 60296,14017 60296,13887 60296,13887"];
	Layer1_Device12_Stage4_RecvKV -> Layer1_Device12_Stage4_Attention	[pos="e,59974,14070 60495,14196 60214,14196 59974,14196 59974,14196 59974,14196 59974,14080 59974,14080"];
	Layer1_Device12_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61516,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage4_RecvKV -> Layer1_Device12_Stage5_RecvKV	[label="Ring transfer",
		lp="61491,14140",
		pos="e,61444,14097 61444,14183 61444,14183 61444,14107 61444,14107"];
	Layer1_Device12_Stage4_Attention -> Layer1_Device12_Stage4_Accumulate	[pos="e,60064,13877 60018,14043 60046,14043 60064,14043 60064,14043 60064,14043 60064,13887 60064,13887"];
	Layer1_Device12_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="62210,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage4_Accumulate -> Layer1_Device12_Stage5_Accumulate	[pos="e,62196,13685 60299,13824 60299,13792 60299,13741 60299,13741 60299,13741 62196,13741 62196,13741 62196,13741 62196,13695 62196,\
13695"];
	Layer1_Device12_Stage5_RecvKV -> Layer1_Device12_Stage5_Attention	[pos="e,62471,13850 62425,14044 62425,14038 62425,13850 62425,13850 62425,13850 62461,13850 62461,13850"];
	Layer1_Device12_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61494,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage5_RecvKV -> Layer1_Device12_Stage6_RecvKV	[label="Ring transfer",
		lp="61555,13947",
		pos="e,61505,13904 61505,13990 61505,13990 61505,13914 61505,13914"];
	Layer1_Device12_Stage5_Attention -> Layer1_Device12_Stage5_Accumulate	[pos="e,62442,13657 62486,13824 62486,13771 62486,13657 62486,13657 62486,13657 62452,13657 62452,13657"];
	Layer1_Device12_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="62210,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage5_Accumulate -> Layer1_Device12_Stage6_Accumulate	[pos="e,62210,13491 62210,13631 62210,13631 62210,13501 62210,13501"];
	Layer1_Device12_Stage6_RecvKV -> Layer1_Device12_Stage6_Attention	[pos="e,62717,13684 62081,13810 62395,13810 62717,13810 62717,13810 62717,13810 62717,13694 62717,13694"];
	Layer1_Device12_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61001,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage6_RecvKV -> Layer1_Device12_Stage7_RecvKV	[label="Ring transfer",
		lp="61386,13754",
		pos="e,61248,13711 61248,13797 61248,13797 61248,13721 61248,13721"];
	Layer1_Device12_Stage6_Attention -> Layer1_Device12_Stage6_Accumulate	[pos="e,62442,13455 62624,13631 62624,13576 62624,13455 62624,13455 62624,13455 62452,13455 62452,13455"];
	Layer1_Device12_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61995,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage6_Accumulate -> Layer1_Device12_Stage7_Accumulate	[pos="e,62102,13298 62102,13438 62102,13438 62102,13308 62102,13308"];
	Layer1_Device12_Stage7_RecvKV -> Layer1_Device12_Stage7_Attention	[pos="e,59789,13491 60125,13617 59934,13617 59789,13617 59789,13617 59789,13617 59789,13501 59789,13501"];
	Layer1_Device12_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61001,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage7_RecvKV -> Layer1_Device12_Stage8_RecvKV	[label="Ring transfer",
		lp="61049,13561",
		pos="e,61001,13518 61001,13604 61001,13604 61001,13528 61001,13528"];
	Layer1_Device12_Stage7_Attention -> Layer1_Device12_Stage7_Accumulate	[pos="e,61842,13299 59945,13438 59945,13407 59945,13360 59945,13360 59945,13360 61842,13360 61842,13360 61842,13360 61842,13309 61842,\
13309"];
	Layer1_Device12_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61995,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage7_Accumulate -> Layer1_Device12_Stage8_Accumulate	[pos="e,61995,13105 61995,13245 61995,13245 61995,13115 61995,13115"];
	Layer1_Device12_Stage8_RecvKV -> Layer1_Device12_Stage8_Attention	[pos="e,62363,13298 61588,13424 61957,13424 62363,13424 62363,13424 62363,13424 62363,13308 62363,13308"];
	Layer1_Device12_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60786,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage8_RecvKV -> Layer1_Device12_Stage9_RecvKV	[label="Ring transfer",
		lp="60941,13368",
		pos="e,60894,13325 60894,13411 60894,13411 60894,13335 60894,13335"];
	Layer1_Device12_Stage8_Attention -> Layer1_Device12_Stage8_Accumulate	[pos="e,62227,13078 62452,13245 62452,13192 62452,13078 62452,13078 62452,13078 62237,13078 62237,13078"];
	Layer1_Device12_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60490,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage8_Accumulate -> Layer1_Device12_Stage9_Accumulate	[pos="e,60567,12913 61995,13052 61995,13019 61995,12968 61995,12968 61995,12968 60567,12968 60567,12968 60567,12968 60567,12923 60567,\
12923"];
	Layer1_Device12_Stage9_RecvKV -> Layer1_Device12_Stage9_Attention	[pos="e,59684,13105 59910,13231 59778,13231 59684,13231 59684,13231 59684,13231 59684,13115 59684,13115"];
	Layer1_Device12_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="60786,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage9_RecvKV -> Layer1_Device12_Stage10_RecvKV	[label="Ring transfer",
		lp="60834,13175",
		pos="e,60786,13132 60786,13218 60786,13218 60786,13142 60786,13142"];
	Layer1_Device12_Stage9_Attention -> Layer1_Device12_Stage9_Accumulate	[pos="e,60413,12913 59773,13052 59773,13008 59773,12925 59773,12925 59773,12925 60413,12925 60413,12925 60413,12925 60413,12923 60413,\
12923"];
	Layer1_Device12_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60490,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage9_Accumulate -> Layer1_Device12_Stage10_Accumulate	[pos="e,60490,12719 60490,12859 60490,12859 60490,12729 60490,12729"];
	Layer1_Device12_Stage10_RecvKV -> Layer1_Device12_Stage10_Attention	[pos="e,59978,12912 59978,13025 59978,13025 59978,12922 59978,12922"];
	Layer1_Device12_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61699,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage10_RecvKV -> Layer1_Device12_Stage11_RecvKV	[label="Ring transfer",
		lp="61103,12982",
		pos="e,61242,12939 61242,13025 61242,13025 61242,12949 61242,12949"];
	Layer1_Device12_Stage10_Attention -> Layer1_Device12_Stage10_Accumulate	[pos="e,60258,12692 60144,12859 60144,12806 60144,12692 60144,12692 60144,12692 60248,12692 60248,12692"];
	Layer1_Device12_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60835,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage10_Accumulate -> Layer1_Device12_Stage11_Accumulate	[pos="e,60662,12526 60662,12666 60662,12666 60662,12536 60662,12536"];
	Layer1_Device12_Stage11_RecvKV -> Layer1_Device12_Stage11_Attention	[pos="e,61031,12719 61031,12832 61031,12832 61031,12729 61031,12729"];
	Layer1_Device12_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62317,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage11_RecvKV -> Layer1_Device12_Stage12_RecvKV	[label="Ring transfer",
		lp="62056,12789",
		pos="e,62008,12746 62008,12832 62008,12832 62008,12756 62008,12756"];
	Layer1_Device12_Stage11_Attention -> Layer1_Device12_Stage11_Accumulate	[pos="e,60972,12526 60972,12666 60972,12666 60972,12536 60972,12536"];
	Layer1_Device12_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60953,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage11_Accumulate -> Layer1_Device12_Stage12_Accumulate	[pos="e,60894,12333 60894,12473 60894,12473 60894,12343 60894,12343"];
	Layer1_Device12_Stage12_RecvKV -> Layer1_Device12_Stage12_Attention	[pos="e,61492,12526 61492,12639 61492,12639 61492,12536 61492,12536"];
	Layer1_Device12_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62565,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage12_RecvKV -> Layer1_Device12_Stage13_RecvKV	[label="Ring transfer",
		lp="62530,12596",
		pos="e,62441,12553 62441,12639 62441,12639 62441,12563 62441,12563"];
	Layer1_Device12_Stage12_Attention -> Layer1_Device12_Stage12_Accumulate	[pos="e,61154,12333 61154,12473 61154,12473 61154,12343 61154,12343"];
	Layer1_Device12_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61474,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage12_Accumulate -> Layer1_Device12_Stage13_Accumulate	[pos="e,61242,12113 61185,12306 61202,12306 61213,12306 61213,12306 61213,12306 61213,12113 61213,12113 61213,12113 61232,12113 61232,\
12113"];
	Layer1_Device12_Stage13_RecvKV -> Layer1_Device12_Stage13_Attention	[pos="e,63312,12334 63312,12500 63312,12500 63312,12344 63312,12344"];
	Layer1_Device12_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62162,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage13_RecvKV -> Layer1_Device12_Stage14_RecvKV	[label="Ring transfer",
		lp="62346,12403",
		pos="e,62364,12360 62364,12446 62364,12446 62364,12370 62364,12370"];
	Layer1_Device12_Stage13_Attention -> Layer1_Device12_Stage13_Accumulate	[pos="e,61474,12140 63371,12280 63371,12254 63371,12218 63371,12218 63371,12218 61474,12218 61474,12218 61474,12218 61474,12150 61474,\
12150"];
	Layer1_Device12_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61474,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage13_Accumulate -> Layer1_Device12_Stage14_Accumulate	[pos="e,61474,11988 61474,12087 61474,12087 61474,11998 61474,11998"];
	Layer1_Device12_Stage14_RecvKV -> Layer1_Device12_Stage14_Attention	[pos="e,60953,12140 61286,12266 61097,12266 60953,12266 60953,12266 60953,12266 60953,12150 60953,12150"];
	Layer1_Device12_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62683,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device12_Stage14_RecvKV -> Layer1_Device12_Stage15_RecvKV	[label="Ring transfer",
		lp="62470,12210",
		pos="e,62422,12167 62422,12253 62422,12253 62422,12177 62422,12177"];
	Layer1_Device12_Stage14_Attention -> Layer1_Device12_Stage14_Accumulate	[pos="e,61242,11952 60953,12087 60953,12041 60953,11952 60953,11952 60953,11952 61232,11952 61232,11952"];
	Layer1_Device12_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="63086,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device12_Stage14_Accumulate -> Layer1_Device12_Stage15_Accumulate	[pos="e,62854,11835 61474,11934 61474,11897 61474,11835 61474,11835 61474,11835 62844,11835 62844,11835"];
	Layer1_Device12_Stage15_RecvKV -> Layer1_Device12_Stage15_Attention	[pos="e,63086,11988 63086,12060 63086,12060 63086,11998 63086,11998"];
	Layer1_Device12_Stage15_Attention -> Layer1_Device12_Stage15_Accumulate	[pos="e,63086,11862 63086,11934 63086,11934 63086,11872 63086,11872"];
	Layer1_Device12_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63492,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device12_Stage15_Accumulate -> Layer1_Device12_ConcatHeads	[pos="e,63292,11736 63292,11808 63292,11808 63292,11746 63292,11746"];
	Layer1_Device12_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63504,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device12_ConcatHeads -> Layer1_Device12_OutputProj	[pos="e,63504,11610 63504,11682 63504,11682 63504,11620 63504,11620"];
	Layer1_Device12_OutputProj -> Layer1_Device12_Residual1	[pos="e,63520,11484 63520,11556 63520,11556 63520,11494 63520,11494"];
	Layer1_Device12_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63467,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device12_Residual1 -> Layer1_Device12_LayerNorm2	[pos="e,63501,11358 63501,11430 63501,11430 63501,11368 63501,11368"];
	Layer1_Device12_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="63658,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device12_Residual1 -> Layer1_Device12_Residual2	[pos="e,63878,10676 63878,11430 63878,11430 63878,10686 63878,10686"];
	Layer1_Device12_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63174,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device12_LayerNorm2 -> Layer1_Device12_GateProj	[pos="e,63326,11232 63326,11304 63326,11304 63326,11242 63326,11242"];
	Layer1_Device12_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63487,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device12_LayerNorm2 -> Layer1_Device12_UpProj	[pos="e,63540,11143 63540,11304 63540,11304 63540,11153 63540,11153"];
	Layer1_Device12_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63174,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device12_GateProj -> Layer1_Device12_Activation	[pos="e,63106,11054 63106,11179 63106,11179 63106,11064 63106,11064"];
	Layer1_Device12_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63270,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device12_UpProj -> Layer1_Device12_ElemMul	[pos="e,63540,10928 63540,11089 63540,11089 63540,10938 63540,10938"];
	Layer1_Device12_Activation -> Layer1_Device12_ElemMul	[pos="e,63174,10928 63174,11000 63174,11000 63174,10938 63174,10938"];
	Layer1_Device12_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63366,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device12_ElemMul -> Layer1_Device12_DownProj	[pos="e,63366,10802 63366,10874 63366,10874 63366,10812 63366,10812"];
	Layer1_Device12_DownProj -> Layer1_Device12_Residual2	[pos="e,63428,10676 63428,10748 63428,10748 63428,10686 63428,10686"];
	Layer1_Device12_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63658,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device12_Residual2 -> Layer1_Device12_Output	[pos="e,63658,10550 63658,10622 63658,10622 63658,10560 63658,10560"];
	Layer2_Device12_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63658,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device12_Output -> Layer2_Device12_Input	[pos="e,63658,10402 63658,10475 63658,10475 63658,10412 63658,10412"];
	Layer1_Device13_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68231,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device13_Input -> Layer1_Device13_LayerNorm1	[pos="e,68331,15328 68331,15409 68331,15409 68331,15338 68331,15338"];
	Layer1_Device13_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="68650,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device13_Input -> Layer1_Device13_Residual1	[pos="e,68668,11484 68668,15405 68668,15405 68668,11494 68668,11494"];
	Layer1_Device13_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67639,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device13_LayerNorm1 -> Layer1_Device13_QKVProj	[pos="e,68153,15202 68153,15274 68153,15274 68153,15212 68153,15212"];
	Layer1_Device13_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66393,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage0_RecvKV	[label="Local K,V",
		lp="66894,15105",
		pos="e,67151,15062 67151,15148 67151,15148 67151,15072 67151,15072"];
	Layer1_Device13_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67146,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage0_Attention	[label=Q_local,
		lp="67455,15009",
		pos="e,67345,14843 67345,15148 67345,15148 67345,14853 67345,14853"];
	Layer1_Device13_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64853,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage1_Attention	[label=Q_local,
		lp="64917,14912",
		pos="e,64961,14649 66989,15155 66197,15155 64961,15155 64961,15155 64961,15155 64961,14659 64961,14659"];
	Layer1_Device13_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65464,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage2_Attention	[label=Q_local,
		lp="67595,14816",
		pos="e,65634,14457 67440,15149 67440,15048 67440,14699 67440,14699 67440,14699 65634,14699 65634,14699 65634,14699 65634,14467 65634,\
14467"];
	Layer1_Device13_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67479,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage3_Attention	[label=Q_local,
		lp="67735,14719",
		pos="e,67652,14264 67652,15149 67652,15149 67652,14274 67652,14274"];
	Layer1_Device13_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64685,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage4_Attention	[label=Q_local,
		lp="64537,14623",
		pos="e,64538,14070 66989,15168 66080,15168 64538,15168 64538,15168 64538,15168 64538,14080 64538,14080"];
	Layer1_Device13_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67600,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage5_Attention	[label=Q_local,
		lp="67839,14526",
		pos="e,67771,13877 67771,15148 67771,15148 67771,13887 67771,13887"];
	Layer1_Device13_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67699,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage6_Attention	[label=Q_local,
		lp="67971,14430",
		pos="e,67881,13684 67881,15149 67881,15149 67881,13694 67881,13694"];
	Layer1_Device13_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64760,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage7_Attention	[label=Q_local,
		lp="64391,14333",
		pos="e,64528,13473 66989,15181 66051,15181 64430,15181 64430,15181 64430,15181 64430,13473 64430,13473 64430,13473 64518,13473 64518,\
13473"];
	Layer1_Device13_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67460,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage8_Attention	[label=Q_local,
		lp="68092,14237",
		pos="e,67692,13271 67955,15149 67955,14919 67955,13271 67955,13271 67955,13271 67702,13271 67702,13271"];
	Layer1_Device13_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64521,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage9_Attention	[label=Q_local,
		lp="64273,14140",
		pos="e,64407,13105 66989,15188 66046,15188 64407,15188 64407,15188 64407,15188 64407,13115 64407,13115"];
	Layer1_Device13_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64688,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage10_Attention	[label=Q_local,
		lp="64133,14044",
		pos="e,64456,12885 66989,15195 66008,15195 64262,15195 64262,15195 64262,15195 64262,12885 64262,12885 64262,12885 64446,12885 64446,\
12885"];
	Layer1_Device13_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65923,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage11_Attention	[label=Q_local,
		lp="68213,13947",
		pos="e,66000,12719 68003,15149 68003,14889 68003,12815 68003,12815 68003,12815 66000,12815 66000,12815 66000,12815 66000,12729 66000,\
12729"];
	Layer1_Device13_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="66253,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage12_Attention	[label=Q_local,
		lp="68356,13851",
		pos="e,66183,12526 68027,15149 68027,14888 68027,12807 68027,12807 68027,12807 66183,12807 66183,12807 66183,12807 66183,12536 66183,\
12536"];
	Layer1_Device13_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="68268,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage13_Attention	[label=Q_local,
		lp="68496,13754",
		pos="e,68413,12334 68289,15161 68363,15161 68413,15161 68413,15161 68413,15161 68413,12344 68413,12344"];
	Layer1_Device13_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65850,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage14_Attention	[label=Q_local,
		lp="63987,13658",
		pos="e,65618,12113 67979,15148 67979,14900 67979,12998 67979,12998 67979,12998 65469,12998 65469,12998 65469,12998 65469,12113 65469,\
12113 65469,12113 65608,12113 65608,12113"];
	Layer1_Device13_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67983,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_QKVProj -> Layer1_Device13_Stage15_Attention	[label=Q_local,
		lp="68614,13561",
		pos="e,68215,11970 68289,15175 68414,15175 68507,15175 68507,15175 68507,15175 68507,11970 68507,11970 68507,11970 68225,11970 68225,\
11970"];
	Layer1_Device13_Stage0_RecvKV -> Layer1_Device13_Stage0_Attention	[pos="e,67114,14842 67114,15005 67114,15005 67114,14852 67114,14852"];
	Layer1_Device13_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65937,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage0_RecvKV -> Layer1_Device13_Stage1_RecvKV	[label="Ring transfer",
		lp="66213,14912",
		pos="e,66165,14869 66165,14955 66165,14955 66165,14879 66165,14879"];
	Layer1_Device13_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="65374,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage0_Attention -> Layer1_Device13_Stage0_Accumulate	[pos="e,65374,14650 67146,14789 67146,14755 67146,14702 67146,14702 67146,14702 65374,14702 65374,14702 65374,14702 65374,14660 65374,\
14660"];
	Layer1_Device13_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64943,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage0_Accumulate -> Layer1_Device13_Stage1_Accumulate	[pos="e,65158,14456 65158,14596 65158,14596 65158,14466 65158,14466"];
	Layer1_Device13_Stage1_RecvKV -> Layer1_Device13_Stage1_Attention	[pos="e,65051,14650 65051,14816 65051,14816 65051,14660 65051,14660"];
	Layer1_Device13_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66583,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage1_RecvKV -> Layer1_Device13_Stage2_RecvKV	[label="Ring transfer",
		lp="66254,14719",
		pos="e,66260,14676 66260,14762 66260,14762 66260,14686 66260,14686"];
	Layer1_Device13_Stage1_Attention -> Layer1_Device13_Stage1_Accumulate	[pos="e,64898,14456 64898,14596 64898,14596 64898,14466 64898,14466"];
	Layer1_Device13_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65061,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage1_Accumulate -> Layer1_Device13_Stage2_Accumulate	[pos="e,65002,14263 65002,14403 65002,14403 65002,14273 65002,14273"];
	Layer1_Device13_Stage2_RecvKV -> Layer1_Device13_Stage2_Attention	[pos="e,65679,14457 65679,14623 65679,14623 65679,14467 65679,14467"];
	Layer1_Device13_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66673,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage2_RecvKV -> Layer1_Device13_Stage3_RecvKV	[label="Ring transfer",
		lp="66676,14526",
		pos="e,66628,14483 66628,14569 66628,14569 66628,14493 66628,14493"];
	Layer1_Device13_Stage2_Attention -> Layer1_Device13_Stage2_Accumulate	[pos="e,65262,14263 65262,14403 65262,14403 65262,14273 65262,14273"];
	Layer1_Device13_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65206,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage2_Accumulate -> Layer1_Device13_Stage3_Accumulate	[pos="e,65134,14070 65134,14210 65134,14210 65134,14080 65134,14080"];
	Layer1_Device13_Stage3_RecvKV -> Layer1_Device13_Stage3_Attention	[pos="e,67420,14264 67420,14430 67420,14430 67420,14274 67420,14274"];
	Layer1_Device13_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66270,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage3_RecvKV -> Layer1_Device13_Stage4_RecvKV	[label="Ring transfer",
		lp="66519,14333",
		pos="e,66472,14290 66472,14376 66472,14376 66472,14300 66472,14300"];
	Layer1_Device13_Stage3_Attention -> Layer1_Device13_Stage3_Accumulate	[pos="e,65394,14070 67291,14210 67291,14191 67291,14168 67291,14168 67291,14168 65394,14168 65394,14168 65394,14168 65394,14080 65394,\
14080"];
	Layer1_Device13_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65182,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage3_Accumulate -> Layer1_Device13_Stage4_Accumulate	[pos="e,65194,13877 65194,14017 65194,14017 65194,13887 65194,13887"];
	Layer1_Device13_Stage4_RecvKV -> Layer1_Device13_Stage4_Attention	[pos="e,64873,14070 65394,14196 65113,14196 64873,14196 64873,14196 64873,14196 64873,14080 64873,14080"];
	Layer1_Device13_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66415,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage4_RecvKV -> Layer1_Device13_Stage5_RecvKV	[label="Ring transfer",
		lp="66390,14140",
		pos="e,66342,14097 66342,14183 66342,14183 66342,14107 66342,14107"];
	Layer1_Device13_Stage4_Attention -> Layer1_Device13_Stage4_Accumulate	[pos="e,64962,13877 64917,14043 64944,14043 64962,14043 64962,14043 64962,14043 64962,13887 64962,13887"];
	Layer1_Device13_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="67178,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage4_Accumulate -> Layer1_Device13_Stage5_Accumulate	[pos="e,67129,13685 65231,13824 65231,13807 65231,13787 65231,13787 65231,13787 67129,13787 67129,13787 67129,13787 67129,13695 67129,\
13695"];
	Layer1_Device13_Stage5_RecvKV -> Layer1_Device13_Stage5_Attention	[pos="e,67368,13850 67323,14044 67323,14038 67323,13850 67323,13850 67323,13850 67358,13850 67358,13850"];
	Layer1_Device13_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66391,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage5_RecvKV -> Layer1_Device13_Stage6_RecvKV	[label="Ring transfer",
		lp="66451,13947",
		pos="e,66403,13904 66403,13990 66403,13990 66403,13914 66403,13914"];
	Layer1_Device13_Stage5_Attention -> Layer1_Device13_Stage5_Accumulate	[pos="e,67389,13684 67389,13824 67389,13824 67389,13694 67389,13694"];
	Layer1_Device13_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="67178,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage5_Accumulate -> Layer1_Device13_Stage6_Accumulate	[pos="e,67178,13491 67178,13631 67178,13631 67178,13501 67178,13501"];
	Layer1_Device13_Stage6_RecvKV -> Layer1_Device13_Stage6_Attention	[pos="e,67650,13684 66978,13810 67306,13810 67650,13810 67650,13810 67650,13810 67650,13694 67650,13694"];
	Layer1_Device13_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65969,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage6_RecvKV -> Layer1_Device13_Stage7_RecvKV	[label="Ring transfer",
		lp="66162,13754",
		pos="e,66180,13711 66180,13797 66180,13797 66180,13721 66180,13721"];
	Layer1_Device13_Stage6_Attention -> Layer1_Device13_Stage6_Accumulate	[pos="e,67410,13455 67580,13631 67580,13576 67580,13455 67580,13455 67580,13455 67420,13455 67420,13455"];
	Layer1_Device13_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66939,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage6_Accumulate -> Layer1_Device13_Stage7_Accumulate	[pos="e,67058,13298 67058,13438 67058,13438 67058,13308 67058,13308"];
	Layer1_Device13_Stage7_RecvKV -> Layer1_Device13_Stage7_Attention	[pos="e,64971,13491 65093,13617 65019,13617 64971,13617 64971,13617 64971,13617 64971,13501 64971,13501"];
	Layer1_Device13_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65969,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage7_RecvKV -> Layer1_Device13_Stage8_RecvKV	[label="Ring transfer",
		lp="66017,13561",
		pos="e,65969,13518 65969,13604 65969,13604 65969,13528 65969,13528"];
	Layer1_Device13_Stage7_Attention -> Layer1_Device13_Stage7_Accumulate	[pos="e,66798,13298 64901,13438 64901,13401 64901,13340 64901,13340 64901,13340 66798,13340 66798,13340 66798,13340 66798,13308 66798,\
13308"];
	Layer1_Device13_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66939,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage7_Accumulate -> Layer1_Device13_Stage8_Accumulate	[pos="e,66939,13105 66939,13245 66939,13245 66939,13115 66939,13115"];
	Layer1_Device13_Stage8_RecvKV -> Layer1_Device13_Stage8_Attention	[pos="e,67319,13298 66556,13424 66920,13424 67319,13424 67319,13424 67319,13424 67319,13308 67319,13308"];
	Layer1_Device13_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65730,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage8_RecvKV -> Layer1_Device13_Stage9_RecvKV	[label="Ring transfer",
		lp="65897,13368",
		pos="e,65850,13325 65850,13411 65850,13411 65850,13335 65850,13335"];
	Layer1_Device13_Stage8_Attention -> Layer1_Device13_Stage8_Accumulate	[pos="e,67171,13078 67283,13245 67283,13192 67283,13078 67283,13078 67283,13078 67181,13078 67181,13078"];
	Layer1_Device13_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65209,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage8_Accumulate -> Layer1_Device13_Stage9_Accumulate	[pos="e,65286,12912 66939,13052 66939,13029 66939,13001 66939,13001 66939,13001 65286,13001 65286,13001 65286,13001 65286,12922 65286,\
12922"];
	Layer1_Device13_Stage9_RecvKV -> Layer1_Device13_Stage9_Attention	[pos="e,64640,13105 64854,13231 64728,13231 64640,13231 64640,13231 64640,13231 64640,13115 64640,13115"];
	Layer1_Device13_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65730,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage9_RecvKV -> Layer1_Device13_Stage10_RecvKV	[label="Ring transfer",
		lp="65778,13175",
		pos="e,65730,13132 65730,13218 65730,13218 65730,13142 65730,13142"];
	Layer1_Device13_Stage9_Attention -> Layer1_Device13_Stage9_Accumulate	[pos="e,65132,12912 64604,13052 64604,13012 64604,12941 64604,12941 64604,12941 65132,12941 65132,12941 65132,12941 65132,12922 65132,\
12922"];
	Layer1_Device13_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65209,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage9_Accumulate -> Layer1_Device13_Stage10_Accumulate	[pos="e,65209,12719 65209,12859 65209,12859 65209,12729 65209,12729"];
	Layer1_Device13_Stage10_RecvKV -> Layer1_Device13_Stage10_Attention	[pos="e,64865,12913 64865,13079 64865,13079 64865,12923 64865,12923"];
	Layer1_Device13_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66418,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage10_RecvKV -> Layer1_Device13_Stage11_RecvKV	[label="Ring transfer",
		lp="66047,12982",
		pos="e,66074,12939 66074,13025 66074,13025 66074,12949 66074,12949"];
	Layer1_Device13_Stage10_Attention -> Layer1_Device13_Stage10_Accumulate	[pos="e,64977,12692 64732,12859 64732,12806 64732,12692 64732,12692 64732,12692 64967,12692 64967,12692"];
	Layer1_Device13_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65732,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage10_Accumulate -> Layer1_Device13_Stage11_Accumulate	[pos="e,65500,12499 65419,12666 65419,12613 65419,12499 65419,12499 65419,12499 65490,12499 65490,12499"];
	Layer1_Device13_Stage11_RecvKV -> Layer1_Device13_Stage11_Attention	[pos="e,65846,12719 65846,12832 65846,12832 65846,12729 65846,12729"];
	Layer1_Device13_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67132,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage11_RecvKV -> Layer1_Device13_Stage12_RecvKV	[label="Ring transfer",
		lp="66823,12789",
		pos="e,66775,12746 66775,12832 66775,12832 66775,12756 66775,12756"];
	Layer1_Device13_Stage11_Attention -> Layer1_Device13_Stage11_Accumulate	[pos="e,65828,12526 65828,12666 65828,12666 65828,12536 65828,12536"];
	Layer1_Device13_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65850,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage11_Accumulate -> Layer1_Device13_Stage12_Accumulate	[pos="e,65791,12333 65791,12473 65791,12473 65791,12343 65791,12343"];
	Layer1_Device13_Stage12_RecvKV -> Layer1_Device13_Stage12_Attention	[pos="e,66348,12526 66348,12639 66348,12639 66348,12536 66348,12536"];
	Layer1_Device13_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67462,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage12_RecvKV -> Layer1_Device13_Stage13_RecvKV	[label="Ring transfer",
		lp="67345,12596",
		pos="e,67297,12553 67297,12639 67297,12639 67297,12563 67297,12563"];
	Layer1_Device13_Stage12_Attention -> Layer1_Device13_Stage12_Accumulate	[pos="e,66052,12333 66052,12473 66052,12473 66052,12343 66052,12343"];
	Layer1_Device13_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66371,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage12_Accumulate -> Layer1_Device13_Stage13_Accumulate	[pos="e,66139,12113 66082,12306 66099,12306 66110,12306 66110,12306 66110,12306 66110,12113 66110,12113 66110,12113 66129,12113 66129,\
12113"];
	Layer1_Device13_Stage13_RecvKV -> Layer1_Device13_Stage13_Attention	[pos="e,68209,12334 68209,12500 68209,12500 68209,12344 68209,12344"];
	Layer1_Device13_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67059,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage13_RecvKV -> Layer1_Device13_Stage14_RecvKV	[label="Ring transfer",
		lp="67243,12403",
		pos="e,67260,12360 67260,12446 67260,12446 67260,12370 67260,12370"];
	Layer1_Device13_Stage13_Attention -> Layer1_Device13_Stage13_Accumulate	[pos="e,66371,12140 68268,12280 68268,12255 68268,12222 68268,12222 68268,12222 66371,12222 66371,12222 66371,12222 66371,12150 66371,\
12150"];
	Layer1_Device13_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66371,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage13_Accumulate -> Layer1_Device13_Stage14_Accumulate	[pos="e,66371,11988 66371,12087 66371,12087 66371,11998 66371,11998"];
	Layer1_Device13_Stage14_RecvKV -> Layer1_Device13_Stage14_Attention	[pos="e,65850,12140 66183,12266 65994,12266 65850,12266 65850,12266 65850,12266 65850,12150 65850,12150"];
	Layer1_Device13_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67580,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device13_Stage14_RecvKV -> Layer1_Device13_Stage15_RecvKV	[label="Ring transfer",
		lp="67391,12210",
		pos="e,67320,12167 67320,12253 67320,12253 67320,12177 67320,12177"];
	Layer1_Device13_Stage14_Attention -> Layer1_Device13_Stage14_Accumulate	[pos="e,66139,11952 65850,12087 65850,12041 65850,11952 65850,11952 65850,11952 66129,11952 66129,11952"];
	Layer1_Device13_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="67983,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device13_Stage14_Accumulate -> Layer1_Device13_Stage15_Accumulate	[pos="e,67751,11835 66371,11934 66371,11897 66371,11835 66371,11835 66371,11835 67741,11835 67741,11835"];
	Layer1_Device13_Stage15_RecvKV -> Layer1_Device13_Stage15_Attention	[pos="e,67983,11988 67983,12060 67983,12060 67983,11998 67983,11998"];
	Layer1_Device13_Stage15_Attention -> Layer1_Device13_Stage15_Accumulate	[pos="e,67983,11862 67983,11934 67983,11934 67983,11872 67983,11872"];
	Layer1_Device13_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68189,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device13_Stage15_Accumulate -> Layer1_Device13_ConcatHeads	[pos="e,68089,11736 68089,11808 68089,11808 68089,11746 68089,11746"];
	Layer1_Device13_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68301,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device13_ConcatHeads -> Layer1_Device13_OutputProj	[pos="e,68251,11610 68251,11682 68251,11682 68251,11620 68251,11620"];
	Layer1_Device13_OutputProj -> Layer1_Device13_Residual1	[pos="e,68389,11484 68389,11556 68389,11556 68389,11494 68389,11494"];
	Layer1_Device13_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68529,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device13_Residual1 -> Layer1_Device13_LayerNorm2	[pos="e,68529,11358 68529,11430 68529,11430 68529,11368 68529,11368"];
	Layer1_Device13_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="68581,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device13_Residual1 -> Layer1_Device13_Residual2	[pos="e,68871,10676 68871,11430 68871,11430 68871,10686 68871,10686"];
	Layer1_Device13_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68236,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device13_LayerNorm2 -> Layer1_Device13_GateProj	[pos="e,68388,11232 68388,11304 68388,11304 68388,11242 68388,11242"];
	Layer1_Device13_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68549,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device13_LayerNorm2 -> Layer1_Device13_UpProj	[pos="e,68602,11143 68602,11304 68602,11304 68602,11153 68602,11153"];
	Layer1_Device13_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68236,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device13_GateProj -> Layer1_Device13_Activation	[pos="e,68168,11054 68168,11179 68168,11179 68168,11064 68168,11064"];
	Layer1_Device13_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68332,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device13_UpProj -> Layer1_Device13_ElemMul	[pos="e,68602,10928 68602,11089 68602,11089 68602,10938 68602,10938"];
	Layer1_Device13_Activation -> Layer1_Device13_ElemMul	[pos="e,68236,10928 68236,11000 68236,11000 68236,10938 68236,10938"];
	Layer1_Device13_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68466,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device13_ElemMul -> Layer1_Device13_DownProj	[pos="e,68466,10802 68466,10874 68466,10874 68466,10812 68466,10812"];
	Layer1_Device13_DownProj -> Layer1_Device13_Residual2	[pos="e,68466,10676 68466,10748 68466,10748 68466,10686 68466,10686"];
	Layer1_Device13_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68581,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device13_Residual2 -> Layer1_Device13_Output	[pos="e,68581,10550 68581,10622 68581,10622 68581,10560 68581,10560"];
	Layer2_Device13_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68581,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device13_Output -> Layer2_Device13_Input	[pos="e,68581,10402 68581,10475 68581,10475 68581,10412 68581,10412"];
	Layer1_Device14_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73358,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device14_Input -> Layer1_Device14_LayerNorm1	[pos="e,73433,15328 73433,15406 73433,15406 73433,15338 73433,15338"];
	Layer1_Device14_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="73980,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device14_Input -> Layer1_Device14_Residual1	[pos="e,73847,11484 73847,15417 73847,15417 73847,11494 73847,11494"];
	Layer1_Device14_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72920,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device14_LayerNorm1 -> Layer1_Device14_QKVProj	[pos="e,73357,15202 73357,15274 73357,15274 73357,15212 73357,15212"];
	Layer1_Device14_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71674,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage0_RecvKV	[label="Local K,V",
		lp="72174,15105",
		pos="e,72432,15062 72432,15148 72432,15148 72432,15072 72432,15072"];
	Layer1_Device14_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72464,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage0_Attention	[label=Q_local,
		lp="72736,15009",
		pos="e,72645,14843 72645,15148 72645,15148 72645,14853 72645,14853"];
	Layer1_Device14_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70134,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage1_Attention	[label=Q_local,
		lp="70198,14912",
		pos="e,70121,14649 72270,15155 71443,15155 70121,15155 70121,15155 70121,15155 70121,14659 70121,14659"];
	Layer1_Device14_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70745,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage2_Attention	[label=Q_local,
		lp="72876,14816",
		pos="e,70915,14456 72740,15148 72740,15055 72740,14752 72740,14752 72740,14752 70915,14752 70915,14752 70915,14752 70915,14466 70915,\
14466"];
	Layer1_Device14_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72760,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage3_Attention	[label=Q_local,
		lp="73016,14719",
		pos="e,72933,14264 72933,15149 72933,15149 72933,14274 72933,14274"];
	Layer1_Device14_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69966,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage4_Attention	[label=Q_local,
		lp="69818,14623",
		pos="e,69876,14070 72270,15168 71376,15168 69876,15168 69876,15168 69876,15168 69876,14080 69876,14080"];
	Layer1_Device14_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72885,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage5_Attention	[label=Q_local,
		lp="73120,14526",
		pos="e,73054,13877 73054,15148 73054,15148 73054,13887 73054,13887"];
	Layer1_Device14_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72913,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage6_Attention	[label=Q_local,
		lp="73252,14430",
		pos="e,73130,13684 73130,15149 73130,15149 73130,13694 73130,13694"];
	Layer1_Device14_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69974,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage7_Attention	[label=Q_local,
		lp="69672,14333",
		pos="e,69742,13473 72270,15181 71324,15181 69680,15181 69680,15181 69680,15181 69680,13473 69680,13473 69680,13473 69732,13473 69732,\
13473"];
	Layer1_Device14_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72741,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage8_Attention	[label=Q_local,
		lp="73377,14237",
		pos="e,72973,13271 73191,15149 73191,14919 73191,13271 73191,13271 73191,13271 72983,13271 72983,13271"];
	Layer1_Device14_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69802,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage9_Attention	[label=Q_local,
		lp="69554,14140",
		pos="e,69625,13105 72270,15188 71310,15188 69625,15188 69625,15188 69625,15188 69625,13115 69625,13115"];
	Layer1_Device14_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69777,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage10_Attention	[label=Q_local,
		lp="69414,14044",
		pos="e,69558,12912 72270,15195 71293,15195 69558,15195 69558,15195 69558,15195 69558,12922 69558,12922"];
	Layer1_Device14_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71247,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage11_Attention	[label=Q_local,
		lp="73500,13947",
		pos="e,71324,12719 73283,15148 73283,14887 73283,12795 73283,12795 73283,12795 71324,12795 71324,12795 71324,12795 71324,12729 71324,\
12729"];
	Layer1_Device14_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71538,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage12_Attention	[label=Q_local,
		lp="73641,13851",
		pos="e,71507,12526 73329,15148 73329,14887 73329,12792 73329,12792 73329,12792 71507,12792 71507,12792 71507,12792 71507,12536 71507,\
12536"];
	Layer1_Device14_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73553,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage13_Attention	[label=Q_local,
		lp="73781,13754",
		pos="e,73726,12334 73570,15161 73662,15161 73726,15161 73726,15161 73726,15161 73726,12344 73726,12344"];
	Layer1_Device14_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71135,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage14_Attention	[label=Q_local,
		lp="69268,13658",
		pos="e,70903,12113 73237,15148 73237,14888 73237,12820 73237,12820 73237,12820 70686,12820 70686,12820 70686,12820 70686,12113 70686,\
12113 70686,12113 70893,12113 70893,12113"];
	Layer1_Device14_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73268,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_QKVProj -> Layer1_Device14_Stage15_Attention	[label=Q_local,
		lp="73899,13561",
		pos="e,73500,11970 73570,15175 73696,15175 73791,15175 73791,15175 73791,15175 73791,11970 73791,11970 73791,11970 73510,11970 73510,\
11970"];
	Layer1_Device14_Stage0_RecvKV -> Layer1_Device14_Stage0_Attention	[pos="e,72413,14843 72413,15009 72413,15009 72413,14853 72413,14853"];
	Layer1_Device14_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71255,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage0_RecvKV -> Layer1_Device14_Stage1_RecvKV	[label="Ring transfer",
		lp="71513,14912",
		pos="e,71464,14869 71464,14955 71464,14955 71464,14879 71464,14879"];
	Layer1_Device14_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="70655,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage0_Attention -> Layer1_Device14_Stage0_Accumulate	[pos="e,70655,14649 72464,14789 72464,14772 72464,14754 72464,14754 72464,14754 70655,14754 70655,14754 70655,14754 70655,14659 70655,\
14659"];
	Layer1_Device14_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70224,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage0_Accumulate -> Layer1_Device14_Stage1_Accumulate	[pos="e,70440,14456 70440,14596 70440,14596 70440,14466 70440,14466"];
	Layer1_Device14_Stage1_RecvKV -> Layer1_Device14_Stage1_Attention	[pos="e,70350,14650 70350,14816 70350,14816 70350,14660 70350,14660"];
	Layer1_Device14_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71864,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage1_RecvKV -> Layer1_Device14_Stage2_RecvKV	[label="Ring transfer",
		lp="71562,14719",
		pos="e,71560,14676 71560,14762 71560,14762 71560,14686 71560,14686"];
	Layer1_Device14_Stage1_Attention -> Layer1_Device14_Stage1_Accumulate	[pos="e,70179,14456 70179,14596 70179,14596 70179,14466 70179,14466"];
	Layer1_Device14_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70342,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage1_Accumulate -> Layer1_Device14_Stage2_Accumulate	[pos="e,70283,14263 70283,14403 70283,14403 70283,14273 70283,14273"];
	Layer1_Device14_Stage2_RecvKV -> Layer1_Device14_Stage2_Attention	[pos="e,70960,14457 70960,14623 70960,14623 70960,14467 70960,14467"];
	Layer1_Device14_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71954,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage2_RecvKV -> Layer1_Device14_Stage3_RecvKV	[label="Ring transfer",
		lp="71957,14526",
		pos="e,71909,14483 71909,14569 71909,14569 71909,14493 71909,14493"];
	Layer1_Device14_Stage2_Attention -> Layer1_Device14_Stage2_Accumulate	[pos="e,70544,14263 70544,14403 70544,14403 70544,14273 70544,14273"];
	Layer1_Device14_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70487,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage2_Accumulate -> Layer1_Device14_Stage3_Accumulate	[pos="e,70414,14070 70414,14210 70414,14210 70414,14080 70414,14080"];
	Layer1_Device14_Stage3_RecvKV -> Layer1_Device14_Stage3_Attention	[pos="e,72701,14264 72701,14430 72701,14430 72701,14274 72701,14274"];
	Layer1_Device14_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71551,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage3_RecvKV -> Layer1_Device14_Stage4_RecvKV	[label="Ring transfer",
		lp="71800,14333",
		pos="e,71752,14290 71752,14376 71752,14376 71752,14300 71752,14300"];
	Layer1_Device14_Stage3_Attention -> Layer1_Device14_Stage3_Accumulate	[pos="e,70675,14071 72572,14210 72572,14193 72572,14173 72572,14173 72572,14173 70675,14173 70675,14173 70675,14173 70675,14081 70675,\
14081"];
	Layer1_Device14_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70467,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage3_Accumulate -> Layer1_Device14_Stage4_Accumulate	[pos="e,70477,13877 70477,14017 70477,14017 70477,13887 70477,13887"];
	Layer1_Device14_Stage4_RecvKV -> Layer1_Device14_Stage4_Attention	[pos="e,70154,14070 70675,14196 70394,14196 70154,14196 70154,14196 70154,14196 70154,14080 70154,14080"];
	Layer1_Device14_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71696,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage4_RecvKV -> Layer1_Device14_Stage5_RecvKV	[label="Ring transfer",
		lp="71672,14140",
		pos="e,71624,14097 71624,14183 71624,14183 71624,14107 71624,14107"];
	Layer1_Device14_Stage4_Attention -> Layer1_Device14_Stage4_Accumulate	[pos="e,70246,13877 70198,14043 70226,14043 70246,14043 70246,14043 70246,14043 70246,13887 70246,13887"];
	Layer1_Device14_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72392,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage4_Accumulate -> Layer1_Device14_Stage5_Accumulate	[pos="e,72378,13685 70481,13824 70481,13793 70481,13746 70481,13746 70481,13746 72378,13746 72378,13746 72378,13746 72378,13695 72378,\
13695"];
	Layer1_Device14_Stage5_RecvKV -> Layer1_Device14_Stage5_Attention	[pos="e,72653,13859 72606,14044 72606,14038 72606,13859 72606,13859 72606,13859 72643,13859 72643,13859"];
	Layer1_Device14_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71676,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage5_RecvKV -> Layer1_Device14_Stage6_RecvKV	[label="Ring transfer",
		lp="71731,13947",
		pos="e,71686,13904 71686,13990 71686,13990 71686,13914 71686,13914"];
	Layer1_Device14_Stage5_Attention -> Layer1_Device14_Stage5_Accumulate	[pos="e,72620,13684 72653,13841 72633,13841 72620,13841 72620,13841 72620,13841 72620,13694 72620,13694"];
	Layer1_Device14_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72392,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage5_Accumulate -> Layer1_Device14_Stage6_Accumulate	[pos="e,72392,13491 72392,13631 72392,13631 72392,13501 72392,13501"];
	Layer1_Device14_Stage6_RecvKV -> Layer1_Device14_Stage6_Attention	[pos="e,72899,13684 72263,13810 72577,13810 72899,13810 72899,13810 72899,13810 72899,13694 72899,13694"];
	Layer1_Device14_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71183,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage6_RecvKV -> Layer1_Device14_Stage7_RecvKV	[label="Ring transfer",
		lp="71568,13754",
		pos="e,71430,13711 71430,13797 71430,13797 71430,13721 71430,13721"];
	Layer1_Device14_Stage6_Attention -> Layer1_Device14_Stage6_Accumulate	[pos="e,72624,13455 72827,13631 72827,13576 72827,13455 72827,13455 72827,13455 72634,13455 72634,13455"];
	Layer1_Device14_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72220,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage6_Accumulate -> Layer1_Device14_Stage7_Accumulate	[pos="e,72306,13298 72306,13438 72306,13438 72306,13308 72306,13308"];
	Layer1_Device14_Stage7_RecvKV -> Layer1_Device14_Stage7_Attention	[pos="e,70202,13491 70307,13617 70242,13617 70202,13617 70202,13617 70202,13617 70202,13501 70202,13501"];
	Layer1_Device14_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71183,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage7_RecvKV -> Layer1_Device14_Stage8_RecvKV	[label="Ring transfer",
		lp="71231,13561",
		pos="e,71183,13518 71183,13604 71183,13604 71183,13528 71183,13528"];
	Layer1_Device14_Stage7_Attention -> Layer1_Device14_Stage7_Accumulate	[pos="e,72046,13298 70148,13438 70148,13417 70148,13391 70148,13391 70148,13391 72046,13391 72046,13391 72046,13391 72046,13308 72046,\
13308"];
	Layer1_Device14_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72220,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage7_Accumulate -> Layer1_Device14_Stage8_Accumulate	[pos="e,72220,13105 72220,13245 72220,13245 72220,13115 72220,13115"];
	Layer1_Device14_Stage8_RecvKV -> Layer1_Device14_Stage8_Attention	[pos="e,72566,13298 71770,13424 72147,13424 72566,13424 72566,13424 72566,13424 72566,13308 72566,13308"];
	Layer1_Device14_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71011,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage8_RecvKV -> Layer1_Device14_Stage9_RecvKV	[label="Ring transfer",
		lp="71149,13368",
		pos="e,71097,13325 71097,13411 71097,13411 71097,13335 71097,13335"];
	Layer1_Device14_Stage8_Attention -> Layer1_Device14_Stage8_Accumulate	[pos="e,72452,13078 72741,13245 72741,13192 72741,13078 72741,13078 72741,13078 72462,13078 72462,13078"];
	Layer1_Device14_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70298,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage8_Accumulate -> Layer1_Device14_Stage9_Accumulate	[pos="e,70310,12912 72208,13052 72208,13031 72208,13006 72208,13006 72208,13006 70310,13006 70310,13006 70310,13006 70310,12922 70310,\
12922"];
	Layer1_Device14_Stage9_RecvKV -> Layer1_Device14_Stage9_Attention	[pos="e,69888,13105 70135,13231 69991,13231 69888,13231 69888,13231 69888,13231 69888,13115 69888,13115"];
	Layer1_Device14_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71011,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage9_RecvKV -> Layer1_Device14_Stage10_RecvKV	[label="Ring transfer",
		lp="71059,13175",
		pos="e,71011,13132 71011,13218 71011,13218 71011,13142 71011,13142"];
	Layer1_Device14_Stage9_Attention -> Layer1_Device14_Stage9_Accumulate	[pos="e,70079,12912 70034,13078 70061,13078 70079,13078 70079,13078 70079,13078 70079,12922 70079,12922"];
	Layer1_Device14_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70298,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage9_Accumulate -> Layer1_Device14_Stage10_Accumulate	[pos="e,70298,12719 70298,12859 70298,12859 70298,12729 70298,12729"];
	Layer1_Device14_Stage10_RecvKV -> Layer1_Device14_Stage10_Attention	[pos="e,69790,12912 70135,13038 69940,13038 69790,13038 69790,13038 69790,13038 69790,12922 69790,12922"];
	Layer1_Device14_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71507,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage10_RecvKV -> Layer1_Device14_Stage11_RecvKV	[label="Ring transfer",
		lp="71246,12982",
		pos="e,71259,12939 71259,13025 71259,13025 71259,12949 71259,12949"];
	Layer1_Device14_Stage10_Attention -> Layer1_Device14_Stage10_Accumulate	[pos="e,70066,12692 69943,12859 69943,12806 69943,12692 69943,12692 69943,12692 70056,12692 70056,12692"];
	Layer1_Device14_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71017,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage10_Accumulate -> Layer1_Device14_Stage11_Accumulate	[pos="e,70900,12526 70530,12692 70699,12692 70900,12692 70900,12692 70900,12692 70900,12536 70900,12536"];
	Layer1_Device14_Stage11_RecvKV -> Layer1_Device14_Stage11_Attention	[pos="e,71170,12719 71170,12832 71170,12832 71170,12729 71170,12729"];
	Layer1_Device14_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72456,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage11_RecvKV -> Layer1_Device14_Stage12_RecvKV	[label="Ring transfer",
		lp="72078,12789",
		pos="e,71982,12746 71982,12832 71982,12832 71982,12756 71982,12756"];
	Layer1_Device14_Stage11_Attention -> Layer1_Device14_Stage11_Accumulate	[pos="e,71132,12526 71132,12666 71132,12666 71132,12536 71132,12536"];
	Layer1_Device14_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71135,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage11_Accumulate -> Layer1_Device14_Stage12_Accumulate	[pos="e,71076,12333 71076,12473 71076,12473 71076,12343 71076,12343"];
	Layer1_Device14_Stage12_RecvKV -> Layer1_Device14_Stage12_Attention	[pos="e,71653,12526 71653,12639 71653,12639 71653,12536 71653,12536"];
	Layer1_Device14_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72747,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage12_RecvKV -> Layer1_Device14_Stage13_RecvKV	[label="Ring transfer",
		lp="72605,12596",
		pos="e,72602,12553 72602,12639 72602,12639 72602,12563 72602,12563"];
	Layer1_Device14_Stage12_Attention -> Layer1_Device14_Stage12_Accumulate	[pos="e,71336,12333 71336,12473 71336,12473 71336,12343 71336,12343"];
	Layer1_Device14_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71656,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage12_Accumulate -> Layer1_Device14_Stage13_Accumulate	[pos="e,71424,12113 71367,12306 71384,12306 71395,12306 71395,12306 71395,12306 71395,12113 71395,12113 71395,12113 71414,12113 71414,\
12113"];
	Layer1_Device14_Stage13_RecvKV -> Layer1_Device14_Stage13_Attention	[pos="e,73494,12334 73494,12500 73494,12500 73494,12344 73494,12344"];
	Layer1_Device14_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72344,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage13_RecvKV -> Layer1_Device14_Stage14_RecvKV	[label="Ring transfer",
		lp="72593,12403",
		pos="e,72546,12360 72546,12446 72546,12446 72546,12370 72546,12370"];
	Layer1_Device14_Stage13_Attention -> Layer1_Device14_Stage13_Accumulate	[pos="e,71656,12140 73553,12280 73553,12256 73553,12226 73553,12226 73553,12226 71656,12226 71656,12226 71656,12226 71656,12150 71656,\
12150"];
	Layer1_Device14_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71656,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage13_Accumulate -> Layer1_Device14_Stage14_Accumulate	[pos="e,71656,11988 71656,12087 71656,12087 71656,11998 71656,11998"];
	Layer1_Device14_Stage14_RecvKV -> Layer1_Device14_Stage14_Attention	[pos="e,71135,12140 71468,12266 71279,12266 71135,12266 71135,12266 71135,12266 71135,12150 71135,12150"];
	Layer1_Device14_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72865,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device14_Stage14_RecvKV -> Layer1_Device14_Stage15_RecvKV	[label="Ring transfer",
		lp="72652,12210",
		pos="e,72604,12167 72604,12253 72604,12253 72604,12177 72604,12177"];
	Layer1_Device14_Stage14_Attention -> Layer1_Device14_Stage14_Accumulate	[pos="e,71424,11952 71135,12087 71135,12041 71135,11952 71135,11952 71135,11952 71414,11952 71414,11952"];
	Layer1_Device14_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="73268,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device14_Stage14_Accumulate -> Layer1_Device14_Stage15_Accumulate	[pos="e,73036,11835 71850,11934 71850,11897 71850,11835 71850,11835 71850,11835 73026,11835 73026,11835"];
	Layer1_Device14_Stage15_RecvKV -> Layer1_Device14_Stage15_Attention	[pos="e,73268,11988 73268,12060 73268,12060 73268,11998 73268,11998"];
	Layer1_Device14_Stage15_Attention -> Layer1_Device14_Stage15_Accumulate	[pos="e,73268,11862 73268,11934 73268,11934 73268,11872 73268,11872"];
	Layer1_Device14_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73473,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device14_Stage15_Accumulate -> Layer1_Device14_ConcatHeads	[pos="e,73374,11736 73374,11808 73374,11808 73374,11746 73374,11746"];
	Layer1_Device14_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73584,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device14_ConcatHeads -> Layer1_Device14_OutputProj	[pos="e,73535,11610 73535,11682 73535,11682 73535,11620 73535,11620"];
	Layer1_Device14_OutputProj -> Layer1_Device14_Residual1	[pos="e,73695,11484 73695,11556 73695,11556 73695,11494 73695,11494"];
	Layer1_Device14_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73690,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device14_Residual1 -> Layer1_Device14_LayerNorm2	[pos="e,73748,11358 73748,11430 73748,11430 73748,11368 73748,11368"];
	Layer1_Device14_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="73528,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device14_Residual1 -> Layer1_Device14_Residual2	[pos="e,73915,10649 73962,11430 73962,11290 73962,10649 73962,10649 73962,10649 73925,10649 73925,10649"];
	Layer1_Device14_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73397,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device14_LayerNorm2 -> Layer1_Device14_GateProj	[pos="e,73550,11232 73550,11304 73550,11304 73550,11242 73550,11242"];
	Layer1_Device14_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73710,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device14_LayerNorm2 -> Layer1_Device14_UpProj	[pos="e,73762,11143 73762,11304 73762,11304 73762,11153 73762,11153"];
	Layer1_Device14_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73397,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device14_GateProj -> Layer1_Device14_Activation	[pos="e,73328,11054 73328,11179 73328,11179 73328,11064 73328,11064"];
	Layer1_Device14_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73493,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device14_UpProj -> Layer1_Device14_ElemMul	[pos="e,73763,10928 73763,11089 73763,11089 73763,10938 73763,10938"];
	Layer1_Device14_Activation -> Layer1_Device14_ElemMul	[pos="e,73397,10928 73397,11000 73397,11000 73397,10938 73397,10938"];
	Layer1_Device14_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73514,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device14_ElemMul -> Layer1_Device14_DownProj	[pos="e,73514,10802 73514,10874 73514,10874 73514,10812 73514,10812"];
	Layer1_Device14_DownProj -> Layer1_Device14_Residual2	[pos="e,73514,10676 73514,10748 73514,10748 73514,10686 73514,10686"];
	Layer1_Device14_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73528,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device14_Residual2 -> Layer1_Device14_Output	[pos="e,73528,10550 73528,10622 73528,10622 73528,10560 73528,10560"];
	Layer2_Device14_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73528,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device14_Output -> Layer2_Device14_Input	[pos="e,73528,10402 73528,10475 73528,10475 73528,10412 73528,10412"];
	Layer1_Device15_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77907,15301",
		shape=rectangle,
		width=5.9167];
	Layer1_Device15_Input -> Layer1_Device15_LayerNorm1	[pos="e,77907,15328 77907,15401 77907,15401 77907,15338 77907,15338"];
	Layer1_Device15_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="78108,11457",
		shape=rectangle,
		width=10.736];
	Layer1_Device15_Input -> Layer1_Device15_Residual1	[pos="e,78495,11457 78226,15438 78453,15438 78726,15438 78726,15438 78726,15438 78726,11457 78726,11457 78726,11457 78505,11457 78505,\
11457"];
	Layer1_Device15_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77890,15175",
		shape=rectangle,
		width=18.042];
	Layer1_Device15_LayerNorm1 -> Layer1_Device15_QKVProj	[pos="e,77907,15202 77907,15274 77907,15274 77907,15212 77907,15212"];
	Layer1_Device15_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76588,15009",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage0_RecvKV	[label="Local K,V",
		lp="77088,15105",
		pos="e,77374,15062 77374,15148 77374,15148 77374,15072 77374,15072"];
	Layer1_Device15_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77378,14816",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage0_Attention	[label=Q_local,
		lp="77650,15009",
		pos="e,77559,14843 77559,15148 77559,15148 77559,14853 77559,14853"];
	Layer1_Device15_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75048,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage1_Attention	[label=Q_local,
		lp="75112,14912",
		pos="e,75214,14649 77240,15155 76448,15155 75214,15155 75214,15155 75214,15155 75214,14659 75214,14659"];
	Layer1_Device15_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75659,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage2_Attention	[label=Q_local,
		lp="77790,14816",
		pos="e,75829,14456 77654,15148 77654,15056 77654,14757 77654,14757 77654,14757 75829,14757 75829,14757 75829,14757 75829,14466 75829,\
14466"];
	Layer1_Device15_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77674,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage3_Attention	[label=Q_local,
		lp="77930,14719",
		pos="e,77847,14264 77847,15149 77847,15149 77847,14274 77847,14274"];
	Layer1_Device15_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74880,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage4_Attention	[label=Q_local,
		lp="74732,14623",
		pos="e,74770,14070 77240,15168 76326,15168 74770,15168 74770,15168 74770,15168 74770,14080 74770,14080"];
	Layer1_Device15_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77799,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage5_Attention	[label=Q_local,
		lp="78036,14526",
		pos="e,77968,13877 77968,15148 77968,15148 77968,13887 77968,13887"];
	Layer1_Device15_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77898,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage6_Attention	[label=Q_local,
		lp="78166,14430",
		pos="e,78080,13684 78080,15149 78080,15149 78080,13694 78080,13694"];
	Layer1_Device15_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74959,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage7_Attention	[label=Q_local,
		lp="74586,14333",
		pos="e,74727,13473 77240,15181 76289,15181 74629,15181 74629,15181 74629,15181 74629,13473 74629,13473 74629,13473 74717,13473 74717,\
13473"];
	Layer1_Device15_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77760,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage8_Attention	[label=Q_local,
		lp="78291,14237",
		pos="e,77992,13271 78232,15149 78232,14919 78232,13271 78232,13271 78232,13271 78002,13271 78002,13271"];
	Layer1_Device15_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74821,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage9_Attention	[label=Q_local,
		lp="74468,14140",
		pos="e,74609,13105 77240,15188 76284,15188 74609,15188 74609,15188 74609,15188 74609,13115 74609,13115"];
	Layer1_Device15_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74477,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage10_Attention	[label=Q_local,
		lp="74328,14044",
		pos="e,74418,12912 77240,15195 76235,15195 74418,15195 74418,15195 74418,15195 74418,12922 74418,12922"];
	Layer1_Device15_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75999,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage11_Attention	[label=Q_local,
		lp="78420,13947",
		pos="e,76226,12666 78334,15149 78334,14876 78334,12604 78334,12604 78334,12604 76226,12604 76226,12604 76226,12604 76226,12656 76226,\
12656"];
	Layer1_Device15_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76452,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage12_Attention	[label=Q_local,
		lp="78555,13851",
		pos="e,76552,12526 78437,15148 78437,14874 78437,12587 78437,12587 78437,12587 76552,12587 76552,12587 76552,12587 76552,12536 76552,\
12536"];
	Layer1_Device15_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="78467,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage13_Attention	[label=Q_local,
		lp="78695,13754",
		pos="e,78592,12334 78540,15161 78572,15161 78592,15161 78592,15161 78592,15161 78592,12344 78592,12344"];
	Layer1_Device15_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76049,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage14_Attention	[label=Q_local,
		lp="74182,13658",
		pos="e,76126,12140 78540,15175 78638,15175 78708,15175 78708,15175 78708,15175 78708,12249 78708,12249 78708,12249 76126,12249 76126,\
12249 76126,12249 76126,12150 76126,12150"];
	Layer1_Device15_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77780,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_QKVProj -> Layer1_Device15_Stage15_Attention	[label=Q_local,
		lp="78813,13561",
		pos="e,78012,11961 78540,15188 78643,15188 78717,15188 78717,15188 78717,15188 78717,11961 78717,11961 78717,11961 78022,11961 78022,\
11961"];
	Layer1_Device15_Stage0_RecvKV -> Layer1_Device15_Stage0_Attention	[pos="e,77327,14843 77327,15009 77327,15009 77327,14853 77327,14853"];
	Layer1_Device15_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76169,14816",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage0_RecvKV -> Layer1_Device15_Stage1_RecvKV	[label="Ring transfer",
		lp="76508,14912",
		pos="e,76378,14869 76378,14955 76378,14955 76378,14879 76378,14879"];
	Layer1_Device15_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="75569,14623",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage0_Attention -> Layer1_Device15_Stage0_Accumulate	[pos="e,75569,14650 77378,14789 77378,14775 77378,14760 77378,14760 77378,14760 75569,14760 75569,14760 75569,14760 75569,14660 75569,\
14660"];
	Layer1_Device15_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75138,14430",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage0_Accumulate -> Layer1_Device15_Stage1_Accumulate	[pos="e,75354,14456 75354,14596 75354,14596 75354,14466 75354,14466"];
	Layer1_Device15_Stage1_RecvKV -> Layer1_Device15_Stage1_Attention	[pos="e,75264,14650 75264,14816 75264,14816 75264,14660 75264,14660"];
	Layer1_Device15_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76778,14623",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage1_RecvKV -> Layer1_Device15_Stage2_RecvKV	[label="Ring transfer",
		lp="76476,14719",
		pos="e,76474,14676 76474,14762 76474,14762 76474,14686 76474,14686"];
	Layer1_Device15_Stage1_Attention -> Layer1_Device15_Stage1_Accumulate	[pos="e,75093,14456 75093,14596 75093,14596 75093,14466 75093,14466"];
	Layer1_Device15_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75256,14237",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage1_Accumulate -> Layer1_Device15_Stage2_Accumulate	[pos="e,75197,14263 75197,14403 75197,14403 75197,14273 75197,14273"];
	Layer1_Device15_Stage2_RecvKV -> Layer1_Device15_Stage2_Attention	[pos="e,75874,14457 75874,14623 75874,14623 75874,14467 75874,14467"];
	Layer1_Device15_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76868,14430",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage2_RecvKV -> Layer1_Device15_Stage3_RecvKV	[label="Ring transfer",
		lp="76871,14526",
		pos="e,76823,14483 76823,14569 76823,14569 76823,14493 76823,14493"];
	Layer1_Device15_Stage2_Attention -> Layer1_Device15_Stage2_Accumulate	[pos="e,75458,14263 75458,14403 75458,14403 75458,14273 75458,14273"];
	Layer1_Device15_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75401,14044",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage2_Accumulate -> Layer1_Device15_Stage3_Accumulate	[pos="e,75328,14070 75328,14210 75328,14210 75328,14080 75328,14080"];
	Layer1_Device15_Stage3_RecvKV -> Layer1_Device15_Stage3_Attention	[pos="e,77615,14264 77615,14430 77615,14430 77615,14274 77615,14274"];
	Layer1_Device15_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76465,14237",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage3_RecvKV -> Layer1_Device15_Stage4_RecvKV	[label="Ring transfer",
		lp="76649,14333",
		pos="e,76666,14290 76666,14376 76666,14376 76666,14300 76666,14300"];
	Layer1_Device15_Stage3_Attention -> Layer1_Device15_Stage3_Accumulate	[pos="e,75589,14070 77486,14210 77486,14195 77486,14178 77486,14178 77486,14178 75589,14178 75589,14178 75589,14178 75589,14080 75589,\
14080"];
	Layer1_Device15_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75381,13851",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage3_Accumulate -> Layer1_Device15_Stage4_Accumulate	[pos="e,75391,13877 75391,14017 75391,14017 75391,13887 75391,13887"];
	Layer1_Device15_Stage4_RecvKV -> Layer1_Device15_Stage4_Attention	[pos="e,75068,14070 75589,14196 75308,14196 75068,14196 75068,14196 75068,14196 75068,14080 75068,14080"];
	Layer1_Device15_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76610,14044",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage4_RecvKV -> Layer1_Device15_Stage5_RecvKV	[label="Ring transfer",
		lp="76585,14140",
		pos="e,76538,14097 76538,14183 76538,14183 76538,14107 76538,14107"];
	Layer1_Device15_Stage4_Attention -> Layer1_Device15_Stage4_Accumulate	[pos="e,75160,13877 75112,14043 75140,14043 75160,14043 75160,14043 75160,14043 75160,13887 75160,13887"];
	Layer1_Device15_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="77377,13658",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage4_Accumulate -> Layer1_Device15_Stage5_Accumulate	[pos="e,77328,13684 75430,13824 75430,13809 75430,13792 75430,13792 75430,13792 77328,13792 77328,13792 77328,13792 77328,13694 77328,\
13694"];
	Layer1_Device15_Stage5_RecvKV -> Layer1_Device15_Stage5_Attention	[pos="e,77567,13850 77520,14044 77520,14038 77520,13850 77520,13850 77520,13850 77557,13850 77557,13850"];
	Layer1_Device15_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="76590,13851",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage5_RecvKV -> Layer1_Device15_Stage6_RecvKV	[label="Ring transfer",
		lp="76645,13947",
		pos="e,76600,13904 76600,13990 76600,13990 76600,13914 76600,13914"];
	Layer1_Device15_Stage5_Attention -> Layer1_Device15_Stage5_Accumulate	[pos="e,77588,13684 77588,13824 77588,13824 77588,13694 77588,13694"];
	Layer1_Device15_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="77377,13465",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage5_Accumulate -> Layer1_Device15_Stage6_Accumulate	[pos="e,77377,13491 77377,13631 77377,13631 77377,13501 77377,13501"];
	Layer1_Device15_Stage6_RecvKV -> Layer1_Device15_Stage6_Attention	[pos="e,77848,13684 77177,13810 77505,13810 77848,13810 77848,13810 77848,13810 77848,13694 77848,13694"];
	Layer1_Device15_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="76168,13658",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage6_RecvKV -> Layer1_Device15_Stage7_RecvKV	[label="Ring transfer",
		lp="76361,13754",
		pos="e,76379,13711 76379,13797 76379,13797 76379,13721 76379,13721"];
	Layer1_Device15_Stage6_Attention -> Layer1_Device15_Stage6_Accumulate	[pos="e,77609,13464 77829,13631 77829,13578 77829,13464 77829,13464 77829,13464 77619,13464 77619,13464"];
	Layer1_Device15_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="77239,13272",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage6_Accumulate -> Layer1_Device15_Stage7_Accumulate	[pos="e,77308,13298 77308,13438 77308,13438 77308,13308 77308,13308"];
	Layer1_Device15_Stage7_RecvKV -> Layer1_Device15_Stage7_Attention	[pos="e,75170,13491 75292,13617 75218,13617 75170,13617 75170,13617 75170,13617 75170,13501 75170,13501"];
	Layer1_Device15_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="76168,13465",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage7_RecvKV -> Layer1_Device15_Stage8_RecvKV	[label="Ring transfer",
		lp="76216,13561",
		pos="e,76168,13518 76168,13604 76168,13604 76168,13528 76168,13528"];
	Layer1_Device15_Stage7_Attention -> Layer1_Device15_Stage7_Accumulate	[pos="e,77048,13299 75150,13438 75150,13421 75150,13401 75150,13401 75150,13401 77048,13401 77048,13401 77048,13401 77048,13309 77048,\
13309"];
	Layer1_Device15_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="77239,13079",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage7_Accumulate -> Layer1_Device15_Stage8_Accumulate	[pos="e,77239,13105 77239,13245 77239,13245 77239,13115 77239,13115"];
	Layer1_Device15_Stage8_RecvKV -> Layer1_Device15_Stage8_Attention	[pos="e,77568,13298 76755,13424 77139,13424 77568,13424 77568,13424 77568,13424 77568,13308 77568,13308"];
	Layer1_Device15_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="76030,13272",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage8_RecvKV -> Layer1_Device15_Stage9_RecvKV	[label="Ring transfer",
		lp="76170,13368",
		pos="e,76099,13325 76099,13411 76099,13411 76099,13335 76099,13335"];
	Layer1_Device15_Stage8_Attention -> Layer1_Device15_Stage8_Accumulate	[pos="e,77471,13078 77760,13245 77760,13192 77760,13078 77760,13078 77760,13078 77481,13078 77481,13078"];
	Layer1_Device15_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74998,12886",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage8_Accumulate -> Layer1_Device15_Stage9_Accumulate	[pos="e,75170,12912 77067,13052 77067,13034 77067,13014 77067,13014 77067,13014 75170,13014 75170,13014 75170,13014 75170,12922 75170,\
12922"];
	Layer1_Device15_Stage9_RecvKV -> Layer1_Device15_Stage9_Attention	[pos="e,74890,13105 75154,13231 75001,13231 74890,13231 74890,13231 74890,13231 74890,13115 74890,13115"];
	Layer1_Device15_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76030,13079",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage9_RecvKV -> Layer1_Device15_Stage10_RecvKV	[label="Ring transfer",
		lp="76078,13175",
		pos="e,76030,13132 76030,13218 76030,13218 76030,13142 76030,13142"];
	Layer1_Device15_Stage9_Attention -> Layer1_Device15_Stage9_Accumulate	[pos="e,74910,12912 74910,13052 74910,13052 74910,12922 74910,12922"];
	Layer1_Device15_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74998,12693",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage9_Accumulate -> Layer1_Device15_Stage10_Accumulate	[pos="e,74998,12719 74998,12859 74998,12859 74998,12729 74998,12729"];
	Layer1_Device15_Stage10_RecvKV -> Layer1_Device15_Stage10_Attention	[pos="e,74649,12912 75153,13038 74880,13038 74649,13038 74649,13038 74649,13038 74649,12922 74649,12922"];
	Layer1_Device15_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76207,12886",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage10_RecvKV -> Layer1_Device15_Stage11_RecvKV	[label="Ring transfer",
		lp="76078,12982",
		pos="e,76118,12939 76118,13025 76118,13025 76118,12949 76118,12949"];
	Layer1_Device15_Stage10_Attention -> Layer1_Device15_Stage10_Accumulate	[pos="e,74766,12692 74538,12859 74538,12806 74538,12692 74538,12692 74538,12692 74756,12692 74756,12692"];
	Layer1_Device15_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75931,12500",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage10_Accumulate -> Layer1_Device15_Stage11_Accumulate	[pos="e,75699,12499 75208,12666 75208,12613 75208,12499 75208,12499 75208,12499 75689,12499 75689,12499"];
	Layer1_Device15_Stage11_RecvKV -> Layer1_Device15_Stage11_Attention	[pos="e,75999,12719 75999,12832 75999,12832 75999,12729 75999,12729"];
	Layer1_Device15_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77208,12693",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage11_RecvKV -> Layer1_Device15_Stage12_RecvKV	[label="Ring transfer",
		lp="76698,12789",
		pos="e,76708,12746 76708,12832 76708,12832 76708,12756 76708,12756"];
	Layer1_Device15_Stage11_Attention -> Layer1_Device15_Stage11_Accumulate	[pos="e,75965,12526 75965,12666 75965,12666 75965,12536 75965,12536"];
	Layer1_Device15_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76049,12307",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage11_Accumulate -> Layer1_Device15_Stage12_Accumulate	[pos="e,75990,12333 75990,12473 75990,12473 75990,12343 75990,12343"];
	Layer1_Device15_Stage12_RecvKV -> Layer1_Device15_Stage12_Attention	[pos="e,76420,12526 76420,12639 76420,12639 76420,12536 76420,12536"];
	Layer1_Device15_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77661,12500",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage12_RecvKV -> Layer1_Device15_Stage13_RecvKV	[label="Ring transfer",
		lp="77636,12596",
		pos="e,77434,12553 77434,12639 77434,12639 77434,12563 77434,12563"];
	Layer1_Device15_Stage12_Attention -> Layer1_Device15_Stage12_Accumulate	[pos="e,76250,12333 76250,12473 76250,12473 76250,12343 76250,12343"];
	Layer1_Device15_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76570,12114",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage12_Accumulate -> Layer1_Device15_Stage13_Accumulate	[pos="e,76338,12113 76281,12306 76298,12306 76309,12306 76309,12306 76309,12306 76309,12113 76309,12113 76309,12113 76328,12113 76328,\
12113"];
	Layer1_Device15_Stage13_RecvKV -> Layer1_Device15_Stage13_Attention	[pos="e,78408,12334 78408,12500 78408,12500 78408,12344 78408,12344"];
	Layer1_Device15_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77258,12307",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage13_RecvKV -> Layer1_Device15_Stage14_RecvKV	[label="Ring transfer",
		lp="77507,12403",
		pos="e,77460,12360 77460,12446 77460,12446 77460,12370 77460,12370"];
	Layer1_Device15_Stage13_Attention -> Layer1_Device15_Stage13_Accumulate	[pos="e,76570,12140 78467,12280 78467,12257 78467,12229 78467,12229 78467,12229 76570,12229 76570,12229 76570,12229 76570,12150 76570,\
12150"];
	Layer1_Device15_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76570,11961",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage13_Accumulate -> Layer1_Device15_Stage14_Accumulate	[pos="e,76570,11988 76570,12087 76570,12087 76570,11998 76570,11998"];
	Layer1_Device15_Stage14_RecvKV -> Layer1_Device15_Stage14_Attention	[pos="e,75972,12140 76382,12266 76154,12266 75972,12266 75972,12266 75972,12266 75972,12150 75972,12150"];
	Layer1_Device15_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77779,12114",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer1_Device15_Stage14_RecvKV -> Layer1_Device15_Stage15_RecvKV	[label="Ring transfer",
		lp="77567,12210",
		pos="e,77518,12167 77518,12253 77518,12253 77518,12177 77518,12177"];
	Layer1_Device15_Stage14_Attention -> Layer1_Device15_Stage14_Accumulate	[pos="e,76338,11952 76049,12087 76049,12041 76049,11952 76049,11952 76049,11952 76328,11952 76328,11952"];
	Layer1_Device15_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="77780,11835",
		shape=rectangle,
		width=6.4306];
	Layer1_Device15_Stage14_Accumulate -> Layer1_Device15_Stage15_Accumulate	[pos="e,77548,11835 76752,11934 76752,11897 76752,11835 76752,11835 76752,11835 77538,11835 77538,11835"];
	Layer1_Device15_Stage15_RecvKV -> Layer1_Device15_Stage15_Attention	[pos="e,77780,11988 77780,12060 77780,12060 77780,11998 77780,11998"];
	Layer1_Device15_Stage15_Attention -> Layer1_Device15_Stage15_Accumulate	[pos="e,77780,11862 77780,11934 77780,11934 77780,11872 77780,11872"];
	Layer1_Device15_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="78093,11709",
		shape=rectangle,
		width=6.2639];
	Layer1_Device15_Stage15_Accumulate -> Layer1_Device15_ConcatHeads	[pos="e,77940,11736 77940,11808 77940,11808 77940,11746 77940,11746"];
	Layer1_Device15_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="78097,11583",
		shape=rectangle,
		width=5.9167];
	Layer1_Device15_ConcatHeads -> Layer1_Device15_OutputProj	[pos="e,78097,11610 78097,11682 78097,11682 78097,11620 78097,11620"];
	Layer1_Device15_OutputProj -> Layer1_Device15_Residual1	[pos="e,78097,11484 78097,11556 78097,11556 78097,11494 78097,11494"];
	Layer1_Device15_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77888,11331",
		shape=rectangle,
		width=5.9167];
	Layer1_Device15_Residual1 -> Layer1_Device15_LayerNorm2	[pos="e,77911,11358 77911,11430 77911,11430 77911,11368 77911,11368"];
	Layer1_Device15_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="78096,10649",
		shape=rectangle,
		width=10.736];
	Layer1_Device15_Residual1 -> Layer1_Device15_Residual2	[pos="e,78308,10676 78308,11430 78308,11430 78308,10686 78308,10686"];
	Layer1_Device15_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77596,11205",
		shape=rectangle,
		width=6.25];
	Layer1_Device15_LayerNorm2 -> Layer1_Device15_GateProj	[pos="e,77748,11232 77748,11304 77748,11304 77748,11242 77748,11242"];
	Layer1_Device15_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77909,11116",
		shape=rectangle,
		width=6.25];
	Layer1_Device15_LayerNorm2 -> Layer1_Device15_UpProj	[pos="e,77961,11143 77961,11304 77961,11304 77961,11153 77961,11153"];
	Layer1_Device15_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77596,11027",
		shape=rectangle,
		width=6.25];
	Layer1_Device15_GateProj -> Layer1_Device15_Activation	[pos="e,77528,11054 77528,11179 77528,11179 77528,11064 77528,11064"];
	Layer1_Device15_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77691,10901",
		shape=rectangle,
		width=11.403];
	Layer1_Device15_UpProj -> Layer1_Device15_ElemMul	[pos="e,77961,10928 77961,11089 77961,11089 77961,10938 77961,10938"];
	Layer1_Device15_Activation -> Layer1_Device15_ElemMul	[pos="e,77596,10928 77596,11000 77596,11000 77596,10938 77596,10938"];
	Layer1_Device15_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77827,10775",
		shape=rectangle,
		width=6.0833];
	Layer1_Device15_ElemMul -> Layer1_Device15_DownProj	[pos="e,77827,10802 77827,10874 77827,10874 77827,10812 77827,10812"];
	Layer1_Device15_DownProj -> Layer1_Device15_Residual2	[pos="e,77878,10676 77878,10748 77878,10748 77878,10686 77878,10686"];
	Layer1_Device15_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 1 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="78096,10512",
		shape=ellipse,
		width=8.3674];
	Layer1_Device15_Residual2 -> Layer1_Device15_Output	[pos="e,78096,10550 78096,10622 78096,10622 78096,10560 78096,10560"];
	Layer2_Device15_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="78096,10364",
		shape=ellipse,
		width=8.3674];
	Layer1_Device15_Output -> Layer2_Device15_Input	[pos="e,78096,10402 78096,10475 78096,10475 78096,10412 78096,10412"];
	Layer2_Device0_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4187,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device0_Input -> Layer2_Device0_LayerNorm1	[pos="e,4400.3,10227 4512.2,10350 4512.2,10315 4512.2,10227 4512.2,10227 4512.2,10227 4410.3,10227 4410.3,10227"];
	Layer2_Device0_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="4826,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device0_Input -> Layer2_Device0_Residual1	[pos="e,4946,6410 4946,10332 4946,10332 4946,6420 4946,6420"];
	Layer2_Device0_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3786,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device0_LayerNorm1 -> Layer2_Device0_QKVProj	[pos="e,4187,10128 4187,10201 4187,10201 4187,10138 4187,10138"];
	Layer2_Device0_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2543,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage0_RecvKV	[label="Local K,V",
		lp="3043.5,10031",
		pos="e,3299.6,9988.1 3299.6,10075 3299.6,10075 3299.6,9998.1 3299.6,9998.1"];
	Layer2_Device0_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3296,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage0_Attention	[label=Q_local,
		lp="3605,9934.8",
		pos="e,3495.1,9768.7 3495.1,10075 3495.1,10075 3495.1,9778.7 3495.1,9778.7"];
	Layer2_Device0_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1003,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage1_Attention	[label=Q_local,
		lp="1067,9838.3",
		pos="e,1126.9,9575.4 3136.4,10085 2349.2,10085 1126.9,10085 1126.9,10085 1126.9,10085 1126.9,9585.4 1126.9,9585.4"];
	Layer2_Device0_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1614,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage2_Attention	[label=Q_local,
		lp="3745,9741.8",
		pos="e,1784.4,9382.4 3590.1,10075 3590.1,9971.3 3590.1,9604 3590.1,9604 3590.1,9604 1784.4,9604 1784.4,9604 1784.4,9604 1784.4,9392.4 \
1784.4,9392.4"];
	Layer2_Device0_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3629,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage3_Attention	[label=Q_local,
		lp="3885,9645.3",
		pos="e,3801.6,9189.6 3801.6,10075 3801.6,10075 3801.6,9199.6 3801.6,9199.6"];
	Layer2_Device0_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="835,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage4_Attention	[label=Q_local,
		lp="687,9548.8",
		pos="e,687.5,8996.5 3136.5,10096 2227.9,10096 687.5,10096 687.5,10096 687.5,10096 687.5,9006.5 687.5,9006.5"];
	Layer2_Device0_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3752,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage5_Attention	[label=Q_local,
		lp="3989,9452.3",
		pos="e,3922,8803.6 3922,10075 3922,10075 3922,8813.6 3922,8813.6"];
	Layer2_Device0_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3837,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage6_Attention	[label=Q_local,
		lp="4121,9355.8",
		pos="e,4026,8610.6 4026,10075 4026,10075 4026,8620.6 4026,8620.6"];
	Layer2_Device0_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="898,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage7_Attention	[label=Q_local,
		lp="541,9259.3",
		pos="e,666.37,8390 3136,10106 2201.9,10106 590.83,10106 590.83,10106 590.83,10106 590.83,8390 590.83,8390 590.83,8390 656.37,8390 656.37,\
8390"];
	Layer2_Device0_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3639,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage8_Attention	[label=Q_local,
		lp="4244,9162.8",
		pos="e,3870.7,8197 4123.7,10075 4123.7,9844.7 4123.7,8197 4123.7,8197 4123.7,8197 3880.7,8197 3880.7,8197"];
	Layer2_Device0_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="700,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage9_Attention	[label=Q_local,
		lp="423,9066.3",
		pos="e,578.17,8031.5 3136.4,10117 2199,10117 578.17,10117 578.17,10117 578.17,10117 578.17,8041.5 578.17,8041.5"];
	Layer2_Device0_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1001,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage10_Attention	[label=Q_local,
		lp="283,8969.8",
		pos="e,1151.5,7838.4 4178.9,10075 4178.9,9827.7 4178.9,7940 4178.9,7940 4178.9,7940 1151.5,7940 1151.5,7940 1151.5,7940 1151.5,7848.4 \
1151.5,7848.4"];
	Layer2_Device0_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2216,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage11_Attention	[label=Q_local,
		lp="4367,8873.3",
		pos="e,2293.2,7645.5 4234.2,10075 4234.2,9810.3 4234.2,7680 4234.2,7680 4234.2,7680 2293.2,7680 2293.2,7680 2293.2,7680 2293.2,7655.5 \
2293.2,7655.5"];
	Layer2_Device0_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2405,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage12_Attention	[label=Q_local,
		lp="4508,8776.8",
		pos="e,2476.4,7452.3 4289.4,10075 4289.4,9810.1 4289.4,7678 4289.4,7678 4289.4,7678 2476.4,7678 2476.4,7678 2476.4,7678 2476.4,7462.3 \
2476.4,7462.3"];
	Layer2_Device0_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4420,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage13_Attention	[label=Q_local,
		lp="4648,8680.3",
		pos="e,4612.2,7259.8 4435.7,10094 4538.8,10094 4612.2,10094 4612.2,10094 4612.2,10094 4612.2,7269.8 4612.2,7269.8"];
	Layer2_Device0_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2002,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage14_Attention	[label=Q_local,
		lp="137,8583.8",
		pos="e,2233.7,7048 4435.8,10088 4517.4,10088 4572.9,10088 4572.9,10088 4572.9,10088 4572.9,7314 4572.9,7314 4572.9,7314 2272.1,7314 2272.1,\
7314 2272.1,7314 2272.1,7048 2272.1,7048 2272.1,7048 2243.7,7048 2243.7,7048"];
	Layer2_Device0_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4135,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_QKVProj -> Layer2_Device0_Stage15_Attention	[label=Q_local,
		lp="4766,8487.3",
		pos="e,4366.7,6896 4435.7,10107 4633.1,10107 4798.8,10107 4798.8,10107 4798.8,10107 4798.8,6896 4798.8,6896 4798.8,6896 4376.7,6896 4376.7,\
6896"];
	Layer2_Device0_Stage0_RecvKV -> Layer2_Device0_Stage0_Attention	[pos="e,3263.6,9768.6 3263.6,9931.5 3263.6,9931.5 3263.6,9778.6 3263.6,9778.6"];
	Layer2_Device0_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2087,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage0_RecvKV -> Layer2_Device0_Stage1_RecvKV	[label="Ring transfer",
		lp="2370,9838.3",
		pos="e,2315,9794.9 2315,9881.6 2315,9881.6 2315,9804.9 2315,9804.9"];
	Layer2_Device0_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="1524,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage0_Attention -> Layer2_Device0_Stage0_Accumulate	[pos="e,1524,9575.5 3296,9715.2 3296,9675.9 3296,9607 3296,9607 3296,9607 1524,9607 1524,9607 1524,9607 1524,9585.5 1524,9585.5"];
	Layer2_Device0_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1093,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage0_Accumulate -> Layer2_Device0_Stage1_Accumulate	[pos="e,1308.5,9382.4 1308.5,9522 1308.5,9522 1308.5,9392.4 1308.5,9392.4"];
	Layer2_Device0_Stage1_RecvKV -> Layer2_Device0_Stage1_Attention	[pos="e,1200.9,9575.7 1200.9,9741.7 1200.9,9741.7 1200.9,9585.7 1200.9,9585.7"];
	Layer2_Device0_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2733,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage1_RecvKV -> Layer2_Device0_Stage2_RecvKV	[label="Ring transfer",
		lp="2404,9645.3",
		pos="e,2410,9601.9 2410,9688.6 2410,9688.6 2410,9611.9 2410,9611.9"];
	Layer2_Device0_Stage1_Attention -> Layer2_Device0_Stage1_Accumulate	[pos="e,1048,9382.4 1048,9522 1048,9522 1048,9392.4 1048,9392.4"];
	Layer2_Device0_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1211,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage1_Accumulate -> Layer2_Device0_Stage2_Accumulate	[pos="e,1152,9189.4 1152,9329 1152,9329 1152,9199.4 1152,9199.4"];
	Layer2_Device0_Stage2_RecvKV -> Layer2_Device0_Stage2_Attention	[pos="e,1829.4,9382.7 1829.4,9548.7 1829.4,9548.7 1829.4,9392.7 1829.4,9392.7"];
	Layer2_Device0_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2823,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage2_RecvKV -> Layer2_Device0_Stage3_RecvKV	[label="Ring transfer",
		lp="2826,9452.3",
		pos="e,2778,9408.9 2778,9495.6 2778,9495.6 2778,9418.9 2778,9418.9"];
	Layer2_Device0_Stage2_Attention -> Layer2_Device0_Stage2_Accumulate	[pos="e,1412.5,9189.4 1412.5,9329 1412.5,9329 1412.5,9199.4 1412.5,9199.4"];
	Layer2_Device0_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1356,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage2_Accumulate -> Layer2_Device0_Stage3_Accumulate	[pos="e,1283.5,8996.4 1283.5,9136 1283.5,9136 1283.5,9006.4 1283.5,9006.4"];
	Layer2_Device0_Stage3_RecvKV -> Layer2_Device0_Stage3_Attention	[pos="e,3570.1,9189.7 3570.1,9355.7 3570.1,9355.7 3570.1,9199.7 3570.1,9199.7"];
	Layer2_Device0_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2420,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage3_RecvKV -> Layer2_Device0_Stage4_RecvKV	[label="Ring transfer",
		lp="2604,9259.3",
		pos="e,2621.5,9215.9 2621.5,9302.6 2621.5,9302.6 2621.5,9225.9 2621.5,9225.9"];
	Layer2_Device0_Stage3_Attention -> Layer2_Device0_Stage3_Accumulate	[pos="e,1543.9,8996.4 3441.1,9136.3 3441.1,9096.8 3441.1,9027 3441.1,9027 3441.1,9027 1543.9,9027 1543.9,9027 1543.9,9027 1543.9,9006.4 \
1543.9,9006.4"];
	Layer2_Device0_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1334,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage3_Accumulate -> Layer2_Device0_Stage4_Accumulate	[pos="e,1345,8803.4 1345,8943 1345,8943 1345,8813.4 1345,8813.4"];
	Layer2_Device0_Stage4_RecvKV -> Layer2_Device0_Stage4_Attention	[pos="e,1023,8996.5 1546.7,9123 1264.5,9123 1023,9123 1023,9123 1023,9123 1023,9006.5 1023,9006.5"];
	Layer2_Device0_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2565,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage4_RecvKV -> Layer2_Device0_Stage5_RecvKV	[label="Ring transfer",
		lp="2540,9066.3",
		pos="e,2492.5,9022.9 2492.5,9109.6 2492.5,9109.6 2492.5,9032.9 2492.5,9032.9"];
	Layer2_Device0_Stage4_Attention -> Layer2_Device0_Stage4_Accumulate	[pos="e,1113.5,8803.5 1066.6,8969 1094.6,8969 1113.5,8969 1113.5,8969 1113.5,8969 1113.5,8813.5 1113.5,8813.5"];
	Layer2_Device0_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3316,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage4_Accumulate -> Layer2_Device0_Stage5_Accumulate	[pos="e,3273.6,8610.6 1376.4,8750.2 1376.4,8729 1376.4,8703 1376.4,8703 1376.4,8703 3273.6,8703 3273.6,8703 3273.6,8703 3273.6,8620.6 \
3273.6,8620.6"];
	Layer2_Device0_Stage5_RecvKV -> Layer2_Device0_Stage5_Attention	[pos="e,3520.2,8776 3473.6,8969.7 3473.6,8963.8 3473.6,8776 3473.6,8776 3473.6,8776 3510.2,8776 3510.2,8776"];
	Layer2_Device0_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2543,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage5_RecvKV -> Layer2_Device0_Stage6_RecvKV	[label="Ring transfer",
		lp="2602,8873.3",
		pos="e,2554,8829.9 2554,8916.6 2554,8916.6 2554,8839.9 2554,8839.9"];
	Layer2_Device0_Stage5_Attention -> Layer2_Device0_Stage5_Accumulate	[pos="e,3534,8610.4 3534,8750 3534,8750 3534,8620.4 3534,8620.4"];
	Layer2_Device0_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3316,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage5_Accumulate -> Layer2_Device0_Stage6_Accumulate	[pos="e,3316,8417.4 3316,8557 3316,8557 3316,8427.4 3316,8427.4"];
	Layer2_Device0_Stage6_RecvKV -> Layer2_Device0_Stage6_Attention	[pos="e,3794.5,8610.5 3133.5,8737 3457.7,8737 3794.5,8737 3794.5,8737 3794.5,8737 3794.5,8620.5 3794.5,8620.5"];
	Layer2_Device0_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2107,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage6_RecvKV -> Layer2_Device0_Stage7_RecvKV	[label="Ring transfer",
		lp="2449,8680.3",
		pos="e,2325,8636.9 2325,8723.6 2325,8723.6 2325,8646.9 2325,8646.9"];
	Layer2_Device0_Stage6_Attention -> Layer2_Device0_Stage6_Accumulate	[pos="e,3547.7,8381 3738,8557.3 3738,8502.3 3738,8381 3738,8381 3738,8381 3557.7,8381 3557.7,8381"];
	Layer2_Device0_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3118,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage6_Accumulate -> Layer2_Device0_Stage7_Accumulate	[pos="e,3217,8224.4 3217,8364 3217,8364 3217,8234.4 3217,8234.4"];
	Layer2_Device0_Stage7_RecvKV -> Layer2_Device0_Stage7_Attention	[pos="e,1116,8417.5 1372.5,8583 1225.9,8583 1116,8583 1116,8583 1116,8583 1116,8427.5 1116,8427.5"];
	Layer2_Device0_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2107,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage7_RecvKV -> Layer2_Device0_Stage8_RecvKV	[label="Ring transfer",
		lp="2155,8487.3",
		pos="e,2107,8443.9 2107,8530.6 2107,8530.6 2107,8453.9 2107,8453.9"];
	Layer2_Device0_Stage7_Attention -> Layer2_Device0_Stage7_Accumulate	[pos="e,2956.6,8224.6 1059.4,8364.2 1059.4,8331.7 1059.4,8281 1059.4,8281 1059.4,8281 2956.6,8281 2956.6,8281 2956.6,8281 2956.6,8234.6 \
2956.6,8234.6"];
	Layer2_Device0_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3118,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage7_Accumulate -> Layer2_Device0_Stage8_Accumulate	[pos="e,3118,8031.4 3118,8171 3118,8171 3118,8041.4 3118,8041.4"];
	Layer2_Device0_Stage8_RecvKV -> Layer2_Device0_Stage8_Attention	[pos="e,3477.5,8224.5 2697.2,8351 3068.4,8351 3477.5,8351 3477.5,8351 3477.5,8351 3477.5,8234.5 3477.5,8234.5"];
	Layer2_Device0_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="1909,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage8_RecvKV -> Layer2_Device0_Stage9_RecvKV	[label="Ring transfer",
		lp="1988,8294.3",
		pos="e,2008,8250.9 2008,8337.6 2008,8337.6 2008,8260.9 2008,8260.9"];
	Layer2_Device0_Stage8_Attention -> Layer2_Device0_Stage8_Accumulate	[pos="e,3349.7,8004 3529.1,8171.3 3529.1,8118.2 3529.1,8004 3529.1,8004 3529.1,8004 3359.7,8004 3359.7,8004"];
	Layer2_Device0_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1522,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage8_Accumulate -> Layer2_Device0_Stage9_Accumulate	[pos="e,1599.2,7838.7 3118,7978.1 3118,7947.9 3118,7903 3118,7903 3118,7903 1599.2,7903 1599.2,7903 1599.2,7903 1599.2,7848.7 1599.2,7848.7"];
	Layer2_Device0_Stage9_RecvKV -> Layer2_Device0_Stage9_Attention	[pos="e,799,8031.5 1174.4,8197 968.16,8197 799,8197 799,8197 799,8197 799,8041.5 799,8041.5"];
	Layer2_Device0_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="1909,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage9_RecvKV -> Layer2_Device0_Stage10_RecvKV	[label="Ring transfer",
		lp="1957,8101.3",
		pos="e,1909,8057.9 1909,8144.6 1909,8144.6 1909,8067.9 1909,8067.9"];
	Layer2_Device0_Stage9_Attention -> Layer2_Device0_Stage9_Accumulate	[pos="e,1444.8,7838.7 850.5,7978.1 850.5,7940.4 850.5,7876 850.5,7876 850.5,7876 1444.8,7876 1444.8,7876 1444.8,7876 1444.8,7848.7 1444.8,\
7848.7"];
	Layer2_Device0_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1522,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage9_Accumulate -> Layer2_Device0_Stage10_Accumulate	[pos="e,1522,7645.4 1522,7785 1522,7785 1522,7655.4 1522,7655.4"];
	Layer2_Device0_Stage10_RecvKV -> Layer2_Device0_Stage10_Attention	[pos="e,1070.4,7838.5 1070.4,7951.6 1070.4,7951.6 1070.4,7848.5 1070.4,7848.5"];
	Layer2_Device0_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2731,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage10_RecvKV -> Layer2_Device0_Stage11_RecvKV	[label="Ring transfer",
		lp="2226,7908.3",
		pos="e,2320,7864.9 2320,7951.6 2320,7951.6 2320,7874.9 2320,7874.9"];
	Layer2_Device0_Stage10_Attention -> Layer2_Device0_Stage10_Accumulate	[pos="e,1290.5,7618 1195,7785.3 1195,7732.2 1195,7618 1195,7618 1195,7618 1280.5,7618 1280.5,7618"];
	Layer2_Device0_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1884,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage10_Accumulate -> Layer2_Device0_Stage11_Accumulate	[pos="e,1703,7452.4 1703,7592 1703,7592 1703,7462.4 1703,7462.4"];
	Layer2_Device0_Stage11_RecvKV -> Layer2_Device0_Stage11_Attention	[pos="e,2138.8,7645.5 2138.8,7758.6 2138.8,7758.6 2138.8,7655.5 2138.8,7655.5"];
	Layer2_Device0_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3425,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage11_RecvKV -> Layer2_Device0_Stage12_RecvKV	[label="Ring transfer",
		lp="3087,7715.3",
		pos="e,3078,7671.9 3078,7758.6 3078,7758.6 3078,7681.9 3078,7681.9"];
	Layer2_Device0_Stage11_Attention -> Layer2_Device0_Stage11_Accumulate	[pos="e,2050,7452.4 2050,7592 2050,7592 2050,7462.4 2050,7462.4"];
	Layer2_Device0_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2002,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage11_Accumulate -> Layer2_Device0_Stage12_Accumulate	[pos="e,1943,7259.4 1943,7399 1943,7399 1943,7269.4 1943,7269.4"];
	Layer2_Device0_Stage12_RecvKV -> Layer2_Device0_Stage12_Attention	[pos="e,2570.9,7452.7 2570.9,7618.7 2570.9,7618.7 2570.9,7462.7 2570.9,7462.7"];
	Layer2_Device0_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3614,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage12_RecvKV -> Layer2_Device0_Stage13_RecvKV	[label="Ring transfer",
		lp="3567,7522.3",
		pos="e,3519.5,7478.9 3519.5,7565.6 3519.5,7565.6 3519.5,7488.9 3519.5,7488.9"];
	Layer2_Device0_Stage12_Attention -> Layer2_Device0_Stage12_Accumulate	[pos="e,2203.5,7259.4 2203.5,7399 2203.5,7399 2203.5,7269.4 2203.5,7269.4"];
	Layer2_Device0_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2523,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage12_Accumulate -> Layer2_Device0_Stage13_Accumulate	[pos="e,2291.2,7030 2233.5,7232 2245.6,7232 2252.8,7232 2252.8,7232 2252.8,7232 2252.8,7030 2252.8,7030 2252.8,7030 2281.2,7030 2281.2,\
7030"];
	Layer2_Device0_Stage13_RecvKV -> Layer2_Device0_Stage13_Attention	[pos="e,4361.1,7259.7 4361.1,7425.7 4361.1,7425.7 4361.1,7269.7 4361.1,7269.7"];
	Layer2_Device0_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3211,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage13_RecvKV -> Layer2_Device0_Stage14_RecvKV	[label="Ring transfer",
		lp="3461,7329.3",
		pos="e,3412.5,7285.9 3412.5,7372.6 3412.5,7372.6 3412.5,7295.9 3412.5,7295.9"];
	Layer2_Device0_Stage13_Attention -> Layer2_Device0_Stage13_Accumulate	[pos="e,2523,7066.6 4420,7206.1 4420,7166.3 4420,7096 4420,7096 4420,7096 2523,7096 2523,7096 2523,7096 2523,7076.6 2523,7076.6"];
	Layer2_Device0_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2523,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage13_Accumulate -> Layer2_Device0_Stage14_Accumulate	[pos="e,2523,6913.8 2523,7013.2 2523,7013.2 2523,6923.8 2523,6923.8"];
	Layer2_Device0_Stage14_RecvKV -> Layer2_Device0_Stage14_Attention	[pos="e,2002,7066.5 2338,7193 2147.7,7193 2002,7193 2002,7193 2002,7193 2002,7076.5 2002,7076.5"];
	Layer2_Device0_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3732,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device0_Stage14_RecvKV -> Layer2_Device0_Stage15_RecvKV	[label="Ring transfer",
		lp="3519,7136.3",
		pos="e,3471.5,7092.9 3471.5,7179.6 3471.5,7179.6 3471.5,7102.9 3471.5,7102.9"];
	Layer2_Device0_Stage14_Attention -> Layer2_Device0_Stage14_Accumulate	[pos="e,2291.4,6887 2205.4,7013.2 2205.4,6969.4 2205.4,6887 2205.4,6887 2205.4,6887 2281.4,6887 2281.4,6887"];
	Layer2_Device0_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="4135,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device0_Stage14_Accumulate -> Layer2_Device0_Stage15_Accumulate	[pos="e,3903.2,6761 2523,6860.5 2523,6823.4 2523,6761 2523,6761 2523,6761 3893.2,6761 3893.2,6761"];
	Layer2_Device0_Stage15_RecvKV -> Layer2_Device0_Stage15_Attention	[pos="e,4135,6913.9 4135,6986.5 4135,6986.5 4135,6923.9 4135,6923.9"];
	Layer2_Device0_Stage15_Attention -> Layer2_Device0_Stage15_Accumulate	[pos="e,4135,6788 4135,6860.6 4135,6860.6 4135,6798 4135,6798"];
	Layer2_Device0_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4139,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device0_Stage15_Accumulate -> Layer2_Device0_ConcatHeads	[pos="e,4139,6662 4139,6734.6 4139,6734.6 4139,6672 4139,6672"];
	Layer2_Device0_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4150,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device0_ConcatHeads -> Layer2_Device0_OutputProj	[pos="e,4150,6536 4150,6608.6 4150,6608.6 4150,6546 4150,6546"];
	Layer2_Device0_OutputProj -> Layer2_Device0_Residual1	[pos="e,4439.4,6383 4303.5,6482.5 4303.5,6445.4 4303.5,6383 4303.5,6383 4303.5,6383 4429.4,6383 4429.4,6383"];
	Layer2_Device0_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4761,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device0_Residual1 -> Layer2_Device0_LayerNorm2	[pos="e,4761,6284 4761,6356.6 4761,6356.6 4761,6294 4761,6294"];
	Layer2_Device0_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="5061,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device0_Residual1 -> Layer2_Device0_Residual2	[pos="e,5109.8,5601.9 5109.8,6356.4 5109.8,6356.4 5109.8,5611.9 5109.8,5611.9"];
	Layer2_Device0_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4469,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device0_LayerNorm2 -> Layer2_Device0_GateProj	[pos="e,4621,6158 4621,6230.6 4621,6230.6 4621,6168 4621,6168"];
	Layer2_Device0_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4782,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device0_LayerNorm2 -> Layer2_Device0_UpProj	[pos="e,4834,6069.1 4834,6230.5 4834,6230.5 4834,6079.1 4834,6079.1"];
	Layer2_Device0_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4469,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device0_GateProj -> Layer2_Device0_Activation	[pos="e,4400.5,5979.9 4400.5,6104.7 4400.5,6104.7 4400.5,5989.9 4400.5,5989.9"];
	Layer2_Device0_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="4564,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device0_UpProj -> Layer2_Device0_ElemMul	[pos="e,4834.2,5854.1 4834.2,6015.5 4834.2,6015.5 4834.2,5864.1 4834.2,5864.1"];
	Layer2_Device0_Activation -> Layer2_Device0_ElemMul	[pos="e,4469,5854 4469,5926.6 4469,5926.6 4469,5864 4469,5864"];
	Layer2_Device0_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4707,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device0_ElemMul -> Layer2_Device0_DownProj	[pos="e,4707,5728 4707,5800.6 4707,5800.6 4707,5738 4707,5738"];
	Layer2_Device0_DownProj -> Layer2_Device0_Residual2	[pos="e,4800.2,5602 4800.2,5674.6 4800.2,5674.6 4800.2,5612 4800.2,5612"];
	Layer2_Device0_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="5061,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device0_Residual2 -> Layer2_Device0_Output	[pos="e,5061,5475.9 5061,5548.6 5061,5548.6 5061,5485.9 5061,5485.9"];
	Layer3_Device0_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 0 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="5061,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device0_Output -> Layer3_Device0_Input	[pos="e,5061,5328 5061,5400.6 5061,5400.6 5061,5338 5061,5338"];
	Layer2_Device1_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9503,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device1_Input -> Layer2_Device1_LayerNorm1	[pos="e,9503,10254 9503,10326 9503,10326 9503,10264 9503,10264"];
	Layer2_Device1_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="9754,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device1_Input -> Layer2_Device1_Residual1	[pos="e,9763.6,6409.9 9763.6,10344 9763.6,10344 9763.6,6419.9 9763.6,6419.9"];
	Layer2_Device1_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8730,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device1_LayerNorm1 -> Layer2_Device1_QKVProj	[pos="e,9334.8,10128 9334.8,10201 9334.8,10201 9334.8,10138 9334.8,10138"];
	Layer2_Device1_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7484,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage0_RecvKV	[label="Local K,V",
		lp="7984.5,10031",
		pos="e,8242.1,9988.1 8242.1,10075 8242.1,10075 8242.1,9998.1 8242.1,9998.1"];
	Layer2_Device1_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8274,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage0_Attention	[label=Q_local,
		lp="8546,9934.8",
		pos="e,8454.6,9768.7 8454.6,10075 8454.6,10075 8454.6,9778.7 8454.6,9778.7"];
	Layer2_Device1_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5944,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage1_Attention	[label=Q_local,
		lp="6008,9838.3",
		pos="e,5953.9,9575.6 8080.5,10081 7260,10081 5953.9,10081 5953.9,10081 5953.9,10081 5953.9,9585.6 5953.9,9585.6"];
	Layer2_Device1_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6555,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage2_Attention	[label=Q_local,
		lp="8686,9741.8",
		pos="e,6725.4,9382.6 8549.6,10075 8549.6,9974.9 8549.6,9630 8549.6,9630 8549.6,9630 6725.4,9630 6725.4,9630 6725.4,9630 6725.4,9392.6 \
6725.4,9392.6"];
	Layer2_Device1_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8570,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage3_Attention	[label=Q_local,
		lp="8826,9645.3",
		pos="e,8742.6,9189.6 8742.6,10075 8742.6,10075 8742.6,9199.6 8742.6,9199.6"];
	Layer2_Device1_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5776,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage4_Attention	[label=Q_local,
		lp="5628,9548.8",
		pos="e,5708.5,8996.4 8080.4,10101 7192.4,10101 5708.5,10101 5708.5,10101 5708.5,10101 5708.5,9006.4 5708.5,9006.4"];
	Layer2_Device1_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8699,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage5_Attention	[label=Q_local,
		lp="8932,9452.3",
		pos="e,8866,8803.6 8866,10075 8866,10075 8866,8813.6 8866,8813.6"];
	Layer2_Device1_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8721,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage6_Attention	[label=Q_local,
		lp="9062,9355.8",
		pos="e,8941.5,8610.6 8941.5,10075 8941.5,10075 8941.5,8620.6 8941.5,8620.6"];
	Layer2_Device1_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5782,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage7_Attention	[label=Q_local,
		lp="5482,9259.3",
		pos="e,5550.2,8399 8080.3,10114 7137.7,10114 5502.5,10114 5502.5,10114 5502.5,10114 5502.5,8399 5502.5,8399 5502.5,8399 5540.2,8399 5540.2,\
8399"];
	Layer2_Device1_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8589,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage8_Attention	[label=Q_local,
		lp="9191,9162.8",
		pos="e,8820.5,8197 9001.9,10075 9001.9,9844.7 9001.9,8197 9001.9,8197 9001.9,8197 8830.5,8197 8830.5,8197"];
	Layer2_Device1_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5650,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage9_Attention	[label=Q_local,
		lp="5364,9066.3",
		pos="e,5460.5,8031.6 8080.1,10121 7126.7,10121 5460.5,10121 5460.5,10121 5460.5,10121 5460.5,8041.6 5460.5,8041.6"];
	Layer2_Device1_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="5932,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage10_Attention	[label=Q_local,
		lp="5224,8969.8",
		pos="e,6088.8,7838.6 9051.3,10074 9051.3,9826.3 9051.3,7944 9051.3,7944 9051.3,7944 6088.8,7944 6088.8,7944 6088.8,7944 6088.8,7848.6 \
6088.8,7848.6"];
	Layer2_Device1_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7071,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage11_Attention	[label=Q_local,
		lp="9314,8873.3",
		pos="e,7148.2,7645.5 9100.8,10075 9100.8,9812.6 9100.8,7699 9100.8,7699 9100.8,7699 7148.2,7699 7148.2,7699 7148.2,7699 7148.2,7655.5 \
7148.2,7655.5"];
	Layer2_Device1_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7352,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage12_Attention	[label=Q_local,
		lp="9455,8776.8",
		pos="e,7331.4,7452.7 9150.2,10075 9150.2,9812.2 9150.2,7696 9150.2,7696 9150.2,7696 7331.4,7696 7331.4,7696 7331.4,7696 7331.4,7462.7 \
7331.4,7462.7"];
	Layer2_Device1_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9367,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage13_Attention	[label=Q_local,
		lp="9595,8680.3",
		pos="e,9539.6,7259.7 9379.6,10086 9473.7,10086 9539.6,10086 9539.6,10086 9539.6,10086 9539.6,7269.7 9539.6,7269.7"];
	Layer2_Device1_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6949,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage14_Attention	[label=Q_local,
		lp="5078,8583.8",
		pos="e,7026.2,7066.5 9379.6,10098 9508.6,10098 9605.2,10098 9605.2,10098 9605.2,10098 9605.2,7160 9605.2,7160 9605.2,7160 7026.2,7160 \
7026.2,7160 7026.2,7160 7026.2,7076.5 7026.2,7076.5"];
	Layer2_Device1_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9292,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_QKVProj -> Layer2_Device1_Stage15_Attention	[label=Q_local,
		lp="9713,8487.3",
		pos="e,9523.6,6896 9379.6,10104 9512.1,10104 9611.9,10104 9611.9,10104 9611.9,10104 9611.9,6896 9611.9,6896 9611.9,6896 9533.6,6896 9533.6,\
6896"];
	Layer2_Device1_Stage0_RecvKV -> Layer2_Device1_Stage0_Attention	[pos="e,8223.1,9768.7 8223.1,9934.7 8223.1,9934.7 8223.1,9778.7 8223.1,9778.7"];
	Layer2_Device1_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7065,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage0_RecvKV -> Layer2_Device1_Stage1_RecvKV	[label="Ring transfer",
		lp="7322,9838.3",
		pos="e,7274.5,9794.9 7274.5,9881.6 7274.5,9881.6 7274.5,9804.9 7274.5,9804.9"];
	Layer2_Device1_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="6465,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage0_Attention -> Layer2_Device1_Stage0_Accumulate	[pos="e,6465,9575.5 8274,9715.2 8274,9683 8274,9633 8274,9633 8274,9633 6465,9633 6465,9633 6465,9633 6465,9585.5 6465,9585.5"];
	Layer2_Device1_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6034,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage0_Accumulate -> Layer2_Device1_Stage1_Accumulate	[pos="e,6249.5,9382.4 6249.5,9522 6249.5,9522 6249.5,9392.4 6249.5,9392.4"];
	Layer2_Device1_Stage1_RecvKV -> Layer2_Device1_Stage1_Attention	[pos="e,6160.4,9575.7 6160.4,9741.7 6160.4,9741.7 6160.4,9585.7 6160.4,9585.7"];
	Layer2_Device1_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7674,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage1_RecvKV -> Layer2_Device1_Stage2_RecvKV	[label="Ring transfer",
		lp="7382,9645.3",
		pos="e,7369.5,9601.9 7369.5,9688.6 7369.5,9688.6 7369.5,9611.9 7369.5,9611.9"];
	Layer2_Device1_Stage1_Attention -> Layer2_Device1_Stage1_Accumulate	[pos="e,5989,9382.4 5989,9522 5989,9522 5989,9392.4 5989,9392.4"];
	Layer2_Device1_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6152,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage1_Accumulate -> Layer2_Device1_Stage2_Accumulate	[pos="e,6093,9189.4 6093,9329 6093,9329 6093,9199.4 6093,9199.4"];
	Layer2_Device1_Stage2_RecvKV -> Layer2_Device1_Stage2_Attention	[pos="e,6770.4,9382.7 6770.4,9548.7 6770.4,9548.7 6770.4,9392.7 6770.4,9392.7"];
	Layer2_Device1_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7764,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage2_RecvKV -> Layer2_Device1_Stage3_RecvKV	[label="Ring transfer",
		lp="7767,9452.3",
		pos="e,7719,9408.9 7719,9495.6 7719,9495.6 7719,9418.9 7719,9418.9"];
	Layer2_Device1_Stage2_Attention -> Layer2_Device1_Stage2_Accumulate	[pos="e,6353.5,9189.4 6353.5,9329 6353.5,9329 6353.5,9199.4 6353.5,9199.4"];
	Layer2_Device1_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6297,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage2_Accumulate -> Layer2_Device1_Stage3_Accumulate	[pos="e,6224.5,8996.4 6224.5,9136 6224.5,9136 6224.5,9006.4 6224.5,9006.4"];
	Layer2_Device1_Stage3_RecvKV -> Layer2_Device1_Stage3_Attention	[pos="e,8511.1,9189.7 8511.1,9355.7 8511.1,9355.7 8511.1,9199.7 8511.1,9199.7"];
	Layer2_Device1_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7361,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage3_RecvKV -> Layer2_Device1_Stage4_RecvKV	[label="Ring transfer",
		lp="7678,9259.3",
		pos="e,7562.5,9215.9 7562.5,9302.6 7562.5,9302.6 7562.5,9225.9 7562.5,9225.9"];
	Layer2_Device1_Stage3_Attention -> Layer2_Device1_Stage3_Accumulate	[pos="e,6484.9,8996.6 8382.1,9136.2 8382.1,9098.3 8382.1,9033 8382.1,9033 8382.1,9033 6484.9,9033 6484.9,9033 6484.9,9033 6484.9,9006.6 \
6484.9,9006.6"];
	Layer2_Device1_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6281,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage3_Accumulate -> Layer2_Device1_Stage4_Accumulate	[pos="e,6289,8803.4 6289,8943 6289,8943 6289,8813.4 6289,8813.4"];
	Layer2_Device1_Stage4_RecvKV -> Layer2_Device1_Stage4_Attention	[pos="e,5964,8996.5 6487.7,9123 6205.5,9123 5964,9123 5964,9123 5964,9123 5964,9006.5 5964,9006.5"];
	Layer2_Device1_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7506,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage4_RecvKV -> Layer2_Device1_Stage5_RecvKV	[label="Ring transfer",
		lp="7481,9066.3",
		pos="e,7433.5,9022.9 7433.5,9109.6 7433.5,9109.6 7433.5,9032.9 7433.5,9032.9"];
	Layer2_Device1_Stage4_Attention -> Layer2_Device1_Stage4_Accumulate	[pos="e,6057.5,8803.5 6007.8,8969 6037.3,8969 6057.5,8969 6057.5,8969 6057.5,8969 6057.5,8813.5 6057.5,8813.5"];
	Layer2_Device1_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8200,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage4_Accumulate -> Layer2_Device1_Stage5_Accumulate	[pos="e,8189.1,8610.4 6291.9,8750 6291.9,8713.4 6291.9,8652 6291.9,8652 6291.9,8652 8189.1,8652 8189.1,8652 8189.1,8652 8189.1,8620.4 \
8189.1,8620.4"];
	Layer2_Device1_Stage5_RecvKV -> Layer2_Device1_Stage5_Attention	[pos="e,8467.3,8776 8417.6,8969.7 8417.6,8963.8 8417.6,8776 8417.6,8776 8417.6,8776 8457.3,8776 8457.3,8776"];
	Layer2_Device1_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7490,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage5_RecvKV -> Layer2_Device1_Stage6_RecvKV	[label="Ring transfer",
		lp="7546,8873.3",
		pos="e,7498,8829.9 7498,8916.6 7498,8916.6 7498,8839.9 7498,8839.9"];
	Layer2_Device1_Stage5_Attention -> Layer2_Device1_Stage5_Accumulate	[pos="e,8431.6,8583 8478.5,8750.3 8478.5,8697.2 8478.5,8583 8478.5,8583 8478.5,8583 8441.6,8583 8441.6,8583"];
	Layer2_Device1_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8200,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage5_Accumulate -> Layer2_Device1_Stage6_Accumulate	[pos="e,8200,8417.4 8200,8557 8200,8557 8200,8427.4 8200,8427.4"];
	Layer2_Device1_Stage6_RecvKV -> Layer2_Device1_Stage6_Attention	[pos="e,8710,8610.5 8080.3,8737 8391.8,8737 8710,8737 8710,8737 8710,8737 8710,8620.5 8710,8620.5"];
	Layer2_Device1_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="6991,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage6_RecvKV -> Layer2_Device1_Stage7_RecvKV	[label="Ring transfer",
		lp="7289,8680.3",
		pos="e,7240.5,8636.9 7240.5,8723.6 7240.5,8723.6 7240.5,8646.9 7240.5,8646.9"];
	Layer2_Device1_Stage6_Attention -> Layer2_Device1_Stage6_Accumulate	[pos="e,8431.8,8381 8655,8557.3 8655,8502.3 8655,8381 8655,8381 8655,8381 8441.8,8381 8441.8,8381"];
	Layer2_Device1_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8068,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage6_Accumulate -> Layer2_Device1_Stage7_Accumulate	[pos="e,8134,8224.4 8134,8364 8134,8364 8134,8234.4 8134,8234.4"];
	Layer2_Device1_Stage7_RecvKV -> Layer2_Device1_Stage7_Attention	[pos="e,5779,8417.5 6117.8,8544 5926.1,8544 5779,8544 5779,8544 5779,8544 5779,8427.5 5779,8427.5"];
	Layer2_Device1_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="6991,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage7_RecvKV -> Layer2_Device1_Stage8_RecvKV	[label="Ring transfer",
		lp="7039,8487.3",
		pos="e,6991,8443.9 6991,8530.6 6991,8530.6 6991,8453.9 6991,8453.9"];
	Layer2_Device1_Stage7_Attention -> Layer2_Device1_Stage7_Accumulate	[pos="e,7873.6,8224.5 5976.4,8364 5976.4,8344.7 5976.4,8322 5976.4,8322 5976.4,8322 7873.6,8322 7873.6,8322 7873.6,8322 7873.6,8234.5 \
7873.6,8234.5"];
	Layer2_Device1_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8068,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage7_Accumulate -> Layer2_Device1_Stage8_Accumulate	[pos="e,8068,8031.4 8068,8171 8068,8171 8068,8041.4 8068,8041.4"];
	Layer2_Device1_Stage8_RecvKV -> Layer2_Device1_Stage8_Attention	[pos="e,8394.5,8224.5 7581.4,8351 7965.1,8351 8394.5,8351 8394.5,8351 8394.5,8351 8394.5,8234.5 8394.5,8234.5"];
	Layer2_Device1_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="6859,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage8_RecvKV -> Layer2_Device1_Stage9_RecvKV	[label="Ring transfer",
		lp="6973,8294.3",
		pos="e,6925,8250.9 6925,8337.6 6925,8337.6 6925,8260.9 6925,8260.9"];
	Layer2_Device1_Stage8_Attention -> Layer2_Device1_Stage8_Accumulate	[pos="e,8299.5,8004 8469.6,8171.3 8469.6,8118.2 8469.6,8004 8469.6,8004 8469.6,8004 8309.5,8004 8309.5,8004"];
	Layer2_Device1_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6453,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage8_Accumulate -> Layer2_Device1_Stage9_Accumulate	[pos="e,6530.2,7838.6 8068,7978.1 8068,7948.8 8068,7906 8068,7906 8068,7906 6530.2,7906 6530.2,7906 6530.2,7906 6530.2,7848.6 6530.2,7848.6"];
	Layer2_Device1_Stage9_RecvKV -> Layer2_Device1_Stage9_Attention	[pos="e,5716,8031.5 5985.8,8158 5829.9,8158 5716,8158 5716,8158 5716,8158 5716,8041.5 5716,8041.5"];
	Layer2_Device1_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="6859,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage9_RecvKV -> Layer2_Device1_Stage10_RecvKV	[label="Ring transfer",
		lp="6907,8101.3",
		pos="e,6859,8057.9 6859,8144.6 6859,8144.6 6859,8067.9 6859,8067.9"];
	Layer2_Device1_Stage9_Attention -> Layer2_Device1_Stage9_Accumulate	[pos="e,6375.8,7838.6 5791,7978 5791,7934 5791,7851 5791,7851 5791,7851 6375.8,7851 6375.8,7851 6375.8,7851 6375.8,7848.6 6375.8,7848.6"];
	Layer2_Device1_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6453,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage9_Accumulate -> Layer2_Device1_Stage10_Accumulate	[pos="e,6453,7645.4 6453,7785 6453,7785 6453,7655.4 6453,7655.4"];
	Layer2_Device1_Stage10_RecvKV -> Layer2_Device1_Stage10_Attention	[pos="e,6014.1,7838.7 6014.1,8004.7 6014.1,8004.7 6014.1,7848.7 6014.1,7848.7"];
	Layer2_Device1_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7662,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage10_RecvKV -> Layer2_Device1_Stage11_RecvKV	[label="Ring transfer",
		lp="7176,7908.3",
		pos="e,7260.5,7864.9 7260.5,7951.6 7260.5,7951.6 7260.5,7874.9 7260.5,7874.9"];
	Layer2_Device1_Stage10_Attention -> Layer2_Device1_Stage10_Accumulate	[pos="e,6221.4,7618 6108,7785.3 6108,7732.2 6108,7618 6108,7618 6108,7618 6211.4,7618 6211.4,7618"];
	Layer2_Device1_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6831,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage10_Accumulate -> Layer2_Device1_Stage11_Accumulate	[pos="e,6642,7452.4 6642,7592 6642,7592 6642,7462.4 6642,7462.4"];
	Layer2_Device1_Stage11_RecvKV -> Layer2_Device1_Stage11_Attention	[pos="e,6993.8,7645.5 6993.8,7758.6 6993.8,7758.6 6993.8,7655.5 6993.8,7655.5"];
	Layer2_Device1_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8280,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage11_RecvKV -> Layer2_Device1_Stage12_RecvKV	[label="Ring transfer",
		lp="7881,7715.3",
		pos="e,7971,7671.9 7971,7758.6 7971,7758.6 7971,7681.9 7971,7681.9"];
	Layer2_Device1_Stage11_Attention -> Layer2_Device1_Stage11_Accumulate	[pos="e,6951,7452.4 6951,7592 6951,7592 6951,7462.4 6951,7462.4"];
	Layer2_Device1_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6949,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage11_Accumulate -> Layer2_Device1_Stage12_Accumulate	[pos="e,6890,7259.4 6890,7399 6890,7399 6890,7269.4 6890,7269.4"];
	Layer2_Device1_Stage12_RecvKV -> Layer2_Device1_Stage12_Attention	[pos="e,7471.9,7452.5 7471.9,7565.6 7471.9,7565.6 7471.9,7462.5 7471.9,7462.5"];
	Layer2_Device1_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8561,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage12_RecvKV -> Layer2_Device1_Stage13_RecvKV	[label="Ring transfer",
		lp="8469,7522.3",
		pos="e,8420.5,7478.9 8420.5,7565.6 8420.5,7565.6 8420.5,7488.9 8420.5,7488.9"];
	Layer2_Device1_Stage12_Attention -> Layer2_Device1_Stage12_Accumulate	[pos="e,7150.5,7259.4 7150.5,7399 7150.5,7399 7150.5,7269.4 7150.5,7269.4"];
	Layer2_Device1_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7470,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage12_Accumulate -> Layer2_Device1_Stage13_Accumulate	[pos="e,7238.3,7039 7180.6,7232 7198.3,7232 7209.4,7232 7209.4,7232 7209.4,7232 7209.4,7039 7209.4,7039 7209.4,7039 7228.3,7039 7228.3,\
7039"];
	Layer2_Device1_Stage13_RecvKV -> Layer2_Device1_Stage13_Attention	[pos="e,9308.1,7259.7 9308.1,7425.7 9308.1,7425.7 9308.1,7269.7 9308.1,7269.7"];
	Layer2_Device1_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8158,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage13_RecvKV -> Layer2_Device1_Stage14_RecvKV	[label="Ring transfer",
		lp="8342,7329.3",
		pos="e,8359.5,7285.9 8359.5,7372.6 8359.5,7372.6 8359.5,7295.9 8359.5,7295.9"];
	Layer2_Device1_Stage13_Attention -> Layer2_Device1_Stage13_Accumulate	[pos="e,7470,7066.4 9367,7206.3 9367,7167.5 9367,7100 9367,7100 9367,7100 7470,7100 7470,7100 7470,7100 7470,7076.4 7470,7076.4"];
	Layer2_Device1_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7470,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage13_Accumulate -> Layer2_Device1_Stage14_Accumulate	[pos="e,7470,6913.8 7470,7013.2 7470,7013.2 7470,6923.8 7470,6923.8"];
	Layer2_Device1_Stage14_RecvKV -> Layer2_Device1_Stage14_Attention	[pos="e,6871.8,7066.5 7285,7193 7056,7193 6871.8,7193 6871.8,7193 6871.8,7193 6871.8,7076.5 6871.8,7076.5"];
	Layer2_Device1_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8679,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device1_Stage14_RecvKV -> Layer2_Device1_Stage15_RecvKV	[label="Ring transfer",
		lp="8466,7136.3",
		pos="e,8418.5,7092.9 8418.5,7179.6 8418.5,7179.6 8418.5,7102.9 8418.5,7102.9"];
	Layer2_Device1_Stage14_Attention -> Layer2_Device1_Stage14_Accumulate	[pos="e,7238.5,6878 7126.4,7013.1 7126.4,6967.1 7126.4,6878 7126.4,6878 7126.4,6878 7228.5,6878 7228.5,6878"];
	Layer2_Device1_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="9292,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device1_Stage14_Accumulate -> Layer2_Device1_Stage15_Accumulate	[pos="e,9060.3,6761 7470,6860.5 7470,6823.4 7470,6761 7470,6761 7470,6761 9050.3,6761 9050.3,6761"];
	Layer2_Device1_Stage15_RecvKV -> Layer2_Device1_Stage15_Attention	[pos="e,9292,6914.1 9292,7006.3 9292,7006.3 9292,6924.1 9292,6924.1"];
	Layer2_Device1_Stage15_Attention -> Layer2_Device1_Stage15_Accumulate	[pos="e,9292,6788 9292,6860.6 9292,6860.6 9292,6798 9292,6798"];
	Layer2_Device1_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9393,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device1_Stage15_Accumulate -> Layer2_Device1_ConcatHeads	[pos="e,9345.5,6662 9345.5,6734.6 9345.5,6734.6 9345.5,6672 9345.5,6672"];
	Layer2_Device1_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9453,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device1_ConcatHeads -> Layer2_Device1_OutputProj	[pos="e,9429.2,6536 9429.2,6608.6 9429.2,6608.6 9429.2,6546 9429.2,6546"];
	Layer2_Device1_OutputProj -> Layer2_Device1_Residual1	[pos="e,9516.8,6410 9516.8,6482.6 9516.8,6482.6 9516.8,6420 9516.8,6420"];
	Layer2_Device1_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9603,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device1_Residual1 -> Layer2_Device1_LayerNorm2	[pos="e,9603,6284 9603,6356.6 9603,6356.6 9603,6294 9603,6294"];
	Layer2_Device1_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="9872,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device1_Residual1 -> Layer2_Device1_Residual2	[pos="e,9994.2,5601.9 9994.2,6356.4 9994.2,6356.4 9994.2,5611.9 9994.2,5611.9"];
	Layer2_Device1_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9310,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device1_LayerNorm2 -> Layer2_Device1_GateProj	[pos="e,9462.5,6158 9462.5,6230.6 9462.5,6230.6 9462.5,6168 9462.5,6168"];
	Layer2_Device1_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9623,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device1_LayerNorm2 -> Layer2_Device1_UpProj	[pos="e,9675.5,6069.1 9675.5,6230.5 9675.5,6230.5 9675.5,6079.1 9675.5,6079.1"];
	Layer2_Device1_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9310,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device1_GateProj -> Layer2_Device1_Activation	[pos="e,9241.5,5979.9 9241.5,6104.7 9241.5,6104.7 9241.5,5989.9 9241.5,5989.9"];
	Layer2_Device1_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="9406,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device1_UpProj -> Layer2_Device1_ElemMul	[pos="e,9675.8,5854.1 9675.8,6015.5 9675.8,6015.5 9675.8,5864.1 9675.8,5864.1"];
	Layer2_Device1_Activation -> Layer2_Device1_ElemMul	[pos="e,9310,5854 9310,5926.6 9310,5926.6 9310,5864 9310,5864"];
	Layer2_Device1_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9549,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device1_ElemMul -> Layer2_Device1_DownProj	[pos="e,9549,5728 9549,5800.6 9549,5800.6 9549,5738 9549,5738"];
	Layer2_Device1_DownProj -> Layer2_Device1_Residual2	[pos="e,9626.8,5602 9626.8,5674.6 9626.8,5674.6 9626.8,5612 9626.8,5612"];
	Layer2_Device1_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9872,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device1_Residual2 -> Layer2_Device1_Output	[pos="e,9872,5475.9 9872,5548.6 9872,5548.6 9872,5485.9 9872,5485.9"];
	Layer3_Device1_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 1 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9872,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device1_Output -> Layer3_Device1_Input	[pos="e,9872,5328 9872,5400.6 9872,5400.6 9872,5338 9872,5338"];
	Layer2_Device2_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14324,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device2_Input -> Layer2_Device2_LayerNorm1	[pos="e,14426,10254 14426,10335 14426,10335 14426,10264 14426,10264"];
	Layer2_Device2_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="14813,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device2_Input -> Layer2_Device2_Residual1	[pos="e,14819,6410 14819,10336 14819,10336 14819,6420 14819,6420"];
	Layer2_Device2_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13738,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device2_LayerNorm1 -> Layer2_Device2_QKVProj	[pos="e,14249,10128 14249,10201 14249,10201 14249,10138 14249,10138"];
	Layer2_Device2_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12492,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage0_RecvKV	[label="Local K,V",
		lp="12992,10031",
		pos="e,13250,9988.1 13250,10075 13250,10075 13250,9998.1 13250,9998.1"];
	Layer2_Device2_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13282,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage0_Attention	[label=Q_local,
		lp="13554,9934.8",
		pos="e,13463,9768.7 13463,10075 13463,10075 13463,9778.7 13463,9778.7"];
	Layer2_Device2_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10952,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage1_Attention	[label=Q_local,
		lp="11016,9838.3",
		pos="e,11104,9575.5 13088,10080 12308,10080 11104,10080 11104,10080 11104,10080 11104,9585.5 11104,9585.5"];
	Layer2_Device2_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11563,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage2_Attention	[label=Q_local,
		lp="13694,9741.8",
		pos="e,11733,9382.4 13558,10075 13558,9975.8 13558,9636 13558,9636 13558,9636 11733,9636 11733,9636 11733,9636 11733,9392.4 11733,9392.4"];
	Layer2_Device2_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13578,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage3_Attention	[label=Q_local,
		lp="13834,9645.3",
		pos="e,13751,9189.6 13751,10075 13751,10075 13751,9199.6 13751,9199.6"];
	Layer2_Device2_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10784,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage4_Attention	[label=Q_local,
		lp="10636,9548.8",
		pos="e,10636,8996.4 13088,10092 12179,10092 10636,10092 10636,10092 10636,10092 10636,9006.4 10636,9006.4"];
	Layer2_Device2_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13707,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage5_Attention	[label=Q_local,
		lp="13938,9452.3",
		pos="e,13874,8803.6 13874,10075 13874,10075 13874,8813.6 13874,8813.6"];
	Layer2_Device2_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13723,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage6_Attention	[label=Q_local,
		lp="14070,9355.8",
		pos="e,13946,8610.6 13946,10075 13946,10075 13946,8620.6 13946,8620.6"];
	Layer2_Device2_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10784,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage7_Attention	[label=Q_local,
		lp="10490,9259.3",
		pos="e,10552,8399 13088,10110 12155,10110 10546,10110 10546,10110 10546,10110 10546,8399 10546,8399 10546,8399 10547,8399 10547,8399"];
	Layer2_Device2_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13559,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage8_Attention	[label=Q_local,
		lp="14199,9162.8",
		pos="e,13791,8197 13995,10075 13995,9844.7 13995,8197 13995,8197 13995,8197 13801,8197 13801,8197"];
	Layer2_Device2_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10620,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage9_Attention	[label=Q_local,
		lp="10372,9066.3",
		pos="e,10540,8031.5 13088,10116 12154,10116 10540,10116 10540,10116 10540,10116 10540,8041.5 10540,8041.5"];
	Layer2_Device2_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10735,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage10_Attention	[label=Q_local,
		lp="10232,8969.8",
		pos="e,10503,7811 13088,10121 12106,10121 10358,10121 10358,10121 10358,10121 10358,7811 10358,7811 10358,7811 10493,7811 10493,7811"];
	Layer2_Device2_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="12030,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage11_Attention	[label=Q_local,
		lp="14323,8873.3",
		pos="e,12107,7645.5 14077,10075 14077,9814.8 14077,7735 14077,7735 14077,7735 12107,7735 12107,7735 12107,7735 12107,7655.5 12107,7655.5"];
	Layer2_Device2_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="12360,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage12_Attention	[label=Q_local,
		lp="14463,8776.8",
		pos="e,12290,7452.5 14118,10075 14118,9814 14118,7729 14118,7729 14118,7729 12290,7729 12290,7729 12290,7729 12290,7462.5 12290,7462.5"];
	Layer2_Device2_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14375,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage13_Attention	[label=Q_local,
		lp="14603,8680.3",
		pos="e,14513,7259.7 14388,10088 14463,10088 14513,10088 14513,10088 14513,10088 14513,7269.7 14513,7269.7"];
	Layer2_Device2_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11957,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage14_Attention	[label=Q_local,
		lp="10086,8583.8",
		pos="e,11725,7039 14036,10075 14036,9814.6 14036,7751 14036,7751 14036,7751 11576,7751 11576,7751 11576,7751 11576,7039 11576,7039 11576,\
7039 11715,7039 11715,7039"];
	Layer2_Device2_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14090,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_QKVProj -> Layer2_Device2_Stage15_Attention	[label=Q_local,
		lp="14721,8487.3",
		pos="e,14322,6896 14388,10101 14543,10101 14664,10101 14664,10101 14664,10101 14664,6896 14664,6896 14664,6896 14332,6896 14332,6896"];
	Layer2_Device2_Stage0_RecvKV -> Layer2_Device2_Stage0_Attention	[pos="e,13231,9768.7 13231,9934.7 13231,9934.7 13231,9778.7 13231,9778.7"];
	Layer2_Device2_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12073,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage0_RecvKV -> Layer2_Device2_Stage1_RecvKV	[label="Ring transfer",
		lp="12330,9838.3",
		pos="e,12282,9794.9 12282,9881.6 12282,9881.6 12282,9804.9 12282,9804.9"];
	Layer2_Device2_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="11473,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage0_Attention -> Layer2_Device2_Stage0_Accumulate	[pos="e,11473,9575.6 13282,9715.1 13282,9684.3 13282,9638 13282,9638 13282,9638 11473,9638 11473,9638 11473,9638 11473,9585.6 11473,9585.6"];
	Layer2_Device2_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11042,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage0_Accumulate -> Layer2_Device2_Stage1_Accumulate	[pos="e,11258,9382.4 11258,9522 11258,9522 11258,9392.4 11258,9392.4"];
	Layer2_Device2_Stage1_RecvKV -> Layer2_Device2_Stage1_Attention	[pos="e,11168,9575.7 11168,9741.7 11168,9741.7 11168,9585.7 11168,9585.7"];
	Layer2_Device2_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12682,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage1_RecvKV -> Layer2_Device2_Stage2_RecvKV	[label="Ring transfer",
		lp="12390,9645.3",
		pos="e,12378,9601.9 12378,9688.6 12378,9688.6 12378,9611.9 12378,9611.9"];
	Layer2_Device2_Stage1_Attention -> Layer2_Device2_Stage1_Accumulate	[pos="e,10997,9382.4 10997,9522 10997,9522 10997,9392.4 10997,9392.4"];
	Layer2_Device2_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11160,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage1_Accumulate -> Layer2_Device2_Stage2_Accumulate	[pos="e,11101,9189.4 11101,9329 11101,9329 11101,9199.4 11101,9199.4"];
	Layer2_Device2_Stage2_RecvKV -> Layer2_Device2_Stage2_Attention	[pos="e,11778,9382.7 11778,9548.7 11778,9548.7 11778,9392.7 11778,9392.7"];
	Layer2_Device2_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12772,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage2_RecvKV -> Layer2_Device2_Stage3_RecvKV	[label="Ring transfer",
		lp="12775,9452.3",
		pos="e,12727,9408.9 12727,9495.6 12727,9495.6 12727,9418.9 12727,9418.9"];
	Layer2_Device2_Stage2_Attention -> Layer2_Device2_Stage2_Accumulate	[pos="e,11362,9189.4 11362,9329 11362,9329 11362,9199.4 11362,9199.4"];
	Layer2_Device2_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11305,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage2_Accumulate -> Layer2_Device2_Stage3_Accumulate	[pos="e,11232,8996.4 11232,9136 11232,9136 11232,9006.4 11232,9006.4"];
	Layer2_Device2_Stage3_RecvKV -> Layer2_Device2_Stage3_Attention	[pos="e,13519,9189.7 13519,9355.7 13519,9355.7 13519,9199.7 13519,9199.7"];
	Layer2_Device2_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12369,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage3_RecvKV -> Layer2_Device2_Stage4_RecvKV	[label="Ring transfer",
		lp="12553,9259.3",
		pos="e,12570,9215.9 12570,9302.6 12570,9302.6 12570,9225.9 12570,9225.9"];
	Layer2_Device2_Stage3_Attention -> Layer2_Device2_Stage3_Accumulate	[pos="e,11493,8996.4 13390,9136 13390,9099.4 13390,9038 13390,9038 13390,9038 11493,9038 11493,9038 11493,9038 11493,9006.4 11493,9006.4"];
	Layer2_Device2_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11289,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage3_Accumulate -> Layer2_Device2_Stage4_Accumulate	[pos="e,11297,8803.4 11297,8943 11297,8943 11297,8813.4 11297,8813.4"];
	Layer2_Device2_Stage4_RecvKV -> Layer2_Device2_Stage4_Attention	[pos="e,10972,8996.5 11496,9123 11213,9123 10972,9123 10972,9123 10972,9123 10972,9006.5 10972,9006.5"];
	Layer2_Device2_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12514,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage4_RecvKV -> Layer2_Device2_Stage5_RecvKV	[label="Ring transfer",
		lp="12477,9066.3",
		pos="e,12442,9022.9 12442,9109.6 12442,9109.6 12442,9032.9 12442,9032.9"];
	Layer2_Device2_Stage4_Attention -> Layer2_Device2_Stage4_Accumulate	[pos="e,11066,8803.5 11016,8969 11045,8969 11066,8969 11066,8969 11066,8969 11066,8813.5 11066,8813.5"];
	Layer2_Device2_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13202,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage4_Accumulate -> Layer2_Device2_Stage5_Accumulate	[pos="e,13194,8610.6 11297,8750.2 11297,8712.3 11297,8647 11297,8647 11297,8647 13194,8647 13194,8647 13194,8647 13194,8620.6 13194,8620.6"];
	Layer2_Device2_Stage5_RecvKV -> Layer2_Device2_Stage5_Attention	[pos="e,13475,8776 13426,8969.7 13426,8963.8 13426,8776 13426,8776 13426,8776 13465,8776 13465,8776"];
	Layer2_Device2_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12498,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage5_RecvKV -> Layer2_Device2_Stage6_RecvKV	[label="Ring transfer",
		lp="12554,8873.3",
		pos="e,12506,8829.9 12506,8916.6 12506,8916.6 12506,8839.9 12506,8839.9"];
	Layer2_Device2_Stage5_Attention -> Layer2_Device2_Stage5_Accumulate	[pos="e,13434,8583 13484,8750.3 13484,8697.2 13484,8583 13484,8583 13484,8583 13444,8583 13444,8583"];
	Layer2_Device2_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13202,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage5_Accumulate -> Layer2_Device2_Stage6_Accumulate	[pos="e,13202,8417.4 13202,8557 13202,8557 13202,8427.4 13202,8427.4"];
	Layer2_Device2_Stage6_RecvKV -> Layer2_Device2_Stage6_Attention	[pos="e,13715,8610.5 13088,8737 13398,8737 13715,8737 13715,8737 13715,8737 13715,8620.5 13715,8620.5"];
	Layer2_Device2_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11993,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage6_RecvKV -> Layer2_Device2_Stage7_RecvKV	[label="Ring transfer",
		lp="12261,8680.3",
		pos="e,12246,8636.9 12246,8723.6 12246,8723.6 12246,8646.9 12246,8646.9"];
	Layer2_Device2_Stage6_Attention -> Layer2_Device2_Stage6_Accumulate	[pos="e,13434,8381 13641,8557.3 13641,8502.3 13641,8381 13641,8381 13641,8381 13444,8381 13444,8381"];
	Layer2_Device2_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13038,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage6_Accumulate -> Layer2_Device2_Stage7_Accumulate	[pos="e,13120,8224.4 13120,8364 13120,8364 13120,8234.4 13120,8234.4"];
	Layer2_Device2_Stage7_RecvKV -> Layer2_Device2_Stage7_Attention	[pos="e,10784,8417.5 11120,8544 10930,8544 10784,8544 10784,8544 10784,8544 10784,8427.5 10784,8427.5"];
	Layer2_Device2_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11993,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage7_RecvKV -> Layer2_Device2_Stage8_RecvKV	[label="Ring transfer",
		lp="12041,8487.3",
		pos="e,11993,8443.9 11993,8530.6 11993,8530.6 11993,8453.9 11993,8453.9"];
	Layer2_Device2_Stage7_Attention -> Layer2_Device2_Stage7_Accumulate	[pos="e,12860,8224.6 10962,8364.1 10962,8341.3 10962,8312 10962,8312 10962,8312 12860,8312 12860,8312 12860,8312 12860,8234.6 12860,8234.6"];
	Layer2_Device2_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13038,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage7_Accumulate -> Layer2_Device2_Stage8_Accumulate	[pos="e,13038,8031.4 13038,8171 13038,8171 13038,8041.4 13038,8041.4"];
	Layer2_Device2_Stage8_RecvKV -> Layer2_Device2_Stage8_Attention	[pos="e,13380,8224.5 12583,8351 12961,8351 13380,8351 13380,8351 13380,8351 13380,8234.5 13380,8234.5"];
	Layer2_Device2_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="11829,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage8_RecvKV -> Layer2_Device2_Stage9_RecvKV	[label="Ring transfer",
		lp="11947,8294.3",
		pos="e,11911,8250.9 11911,8337.6 11911,8337.6 11911,8260.9 11911,8260.9"];
	Layer2_Device2_Stage8_Attention -> Layer2_Device2_Stage8_Accumulate	[pos="e,13270,8004 13356,8171.3 13356,8118.2 13356,8004 13356,8004 13356,8004 13280,8004 13280,8004"];
	Layer2_Device2_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11256,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage8_Accumulate -> Layer2_Device2_Stage9_Accumulate	[pos="e,11333,7838.6 13038,7978.1 13038,7955.3 13038,7926 13038,7926 13038,7926 11333,7926 11333,7926 11333,7926 11333,7848.6 11333,7848.6"];
	Layer2_Device2_Stage9_RecvKV -> Layer2_Device2_Stage9_Attention	[pos="e,10702,8031.5 10956,8158 10808,8158 10702,8158 10702,8158 10702,8158 10702,8041.5 10702,8041.5"];
	Layer2_Device2_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="11829,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage9_RecvKV -> Layer2_Device2_Stage10_RecvKV	[label="Ring transfer",
		lp="11877,8101.3",
		pos="e,11829,8057.9 11829,8144.6 11829,8144.6 11829,8067.9 11829,8067.9"];
	Layer2_Device2_Stage9_Attention -> Layer2_Device2_Stage9_Accumulate	[pos="e,11179,7838.4 10678,7978.2 10678,7938.2 10678,7867 10678,7867 10678,7867 11179,7867 11179,7867 11179,7867 11179,7848.4 11179,7848.4"];
	Layer2_Device2_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11256,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage9_Accumulate -> Layer2_Device2_Stage10_Accumulate	[pos="e,11256,7645.4 11256,7785 11256,7785 11256,7655.4 11256,7655.4"];
	Layer2_Device2_Stage10_RecvKV -> Layer2_Device2_Stage10_Attention	[pos="e,10938,7838.7 10938,8004.7 10938,8004.7 10938,7848.7 10938,7848.7"];
	Layer2_Device2_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12465,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage10_RecvKV -> Layer2_Device2_Stage11_RecvKV	[label="Ring transfer",
		lp="12146,7908.3",
		pos="e,12147,7864.9 12147,7951.6 12147,7951.6 12147,7874.9 12147,7874.9"];
	Layer2_Device2_Stage10_Attention -> Layer2_Device2_Stage10_Accumulate	[pos="e,11024,7618 10952,7785.3 10952,7732.2 10952,7618 10952,7618 10952,7618 11014,7618 11014,7618"];
	Layer2_Device2_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11839,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage10_Accumulate -> Layer2_Device2_Stage11_Accumulate	[pos="e,11703,7452.5 11488,7618 11597,7618 11703,7618 11703,7618 11703,7618 11703,7462.5 11703,7462.5"];
	Layer2_Device2_Stage11_RecvKV -> Layer2_Device2_Stage11_Attention	[pos="e,11953,7645.5 11953,7758.6 11953,7758.6 11953,7655.5 11953,7655.5"];
	Layer2_Device2_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13239,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage11_RecvKV -> Layer2_Device2_Stage12_RecvKV	[label="Ring transfer",
		lp="12947,7715.3",
		pos="e,12852,7671.9 12852,7758.6 12852,7758.6 12852,7681.9 12852,7681.9"];
	Layer2_Device2_Stage11_Attention -> Layer2_Device2_Stage11_Accumulate	[pos="e,11934,7452.4 11934,7592 11934,7592 11934,7462.4 11934,7462.4"];
	Layer2_Device2_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11957,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage11_Accumulate -> Layer2_Device2_Stage12_Accumulate	[pos="e,11898,7259.4 11898,7399 11898,7399 11898,7269.4 11898,7269.4"];
	Layer2_Device2_Stage12_RecvKV -> Layer2_Device2_Stage12_Attention	[pos="e,12455,7452.5 12455,7565.6 12455,7565.6 12455,7462.5 12455,7462.5"];
	Layer2_Device2_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13569,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage12_RecvKV -> Layer2_Device2_Stage13_RecvKV	[label="Ring transfer",
		lp="13452,7522.3",
		pos="e,13404,7478.9 13404,7565.6 13404,7565.6 13404,7488.9 13404,7488.9"];
	Layer2_Device2_Stage12_Attention -> Layer2_Device2_Stage12_Accumulate	[pos="e,12158,7259.4 12158,7399 12158,7399 12158,7269.4 12158,7269.4"];
	Layer2_Device2_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12478,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage12_Accumulate -> Layer2_Device2_Stage13_Accumulate	[pos="e,12246,7039 12189,7232 12206,7232 12217,7232 12217,7232 12217,7232 12217,7039 12217,7039 12217,7039 12236,7039 12236,7039"];
	Layer2_Device2_Stage13_RecvKV -> Layer2_Device2_Stage13_Attention	[pos="e,14316,7259.7 14316,7425.7 14316,7425.7 14316,7269.7 14316,7269.7"];
	Layer2_Device2_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13166,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage13_RecvKV -> Layer2_Device2_Stage14_RecvKV	[label="Ring transfer",
		lp="13350,7329.3",
		pos="e,13368,7285.9 13368,7372.6 13368,7372.6 13368,7295.9 13368,7295.9"];
	Layer2_Device2_Stage13_Attention -> Layer2_Device2_Stage13_Accumulate	[pos="e,12478,7066.7 14375,7206.1 14375,7168.4 14375,7104 14375,7104 14375,7104 12478,7104 12478,7104 12478,7104 12478,7076.7 12478,7076.7"];
	Layer2_Device2_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12478,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage13_Accumulate -> Layer2_Device2_Stage14_Accumulate	[pos="e,12478,6913.8 12478,7013.2 12478,7013.2 12478,6923.8 12478,6923.8"];
	Layer2_Device2_Stage14_RecvKV -> Layer2_Device2_Stage14_Attention	[pos="e,11957,7066.5 12293,7193 12103,7193 11957,7193 11957,7193 11957,7193 11957,7076.5 11957,7076.5"];
	Layer2_Device2_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13687,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device2_Stage14_RecvKV -> Layer2_Device2_Stage15_RecvKV	[label="Ring transfer",
		lp="13474,7136.3",
		pos="e,13426,7092.9 13426,7179.6 13426,7179.6 13426,7102.9 13426,7102.9"];
	Layer2_Device2_Stage14_Attention -> Layer2_Device2_Stage14_Accumulate	[pos="e,12246,6878 12073,7013.1 12073,6967.1 12073,6878 12073,6878 12073,6878 12236,6878 12236,6878"];
	Layer2_Device2_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="14090,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device2_Stage14_Accumulate -> Layer2_Device2_Stage15_Accumulate	[pos="e,13858,6761 12478,6860.5 12478,6823.4 12478,6761 12478,6761 12478,6761 13848,6761 13848,6761"];
	Layer2_Device2_Stage15_RecvKV -> Layer2_Device2_Stage15_Attention	[pos="e,14090,6913.9 14090,6986.5 14090,6986.5 14090,6923.9 14090,6923.9"];
	Layer2_Device2_Stage15_Attention -> Layer2_Device2_Stage15_Accumulate	[pos="e,14090,6788 14090,6860.6 14090,6860.6 14090,6798 14090,6798"];
	Layer2_Device2_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14496,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device2_Stage15_Accumulate -> Layer2_Device2_ConcatHeads	[pos="e,14296,6662 14296,6734.6 14296,6734.6 14296,6672 14296,6672"];
	Layer2_Device2_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14507,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device2_ConcatHeads -> Layer2_Device2_OutputProj	[pos="e,14507,6536 14507,6608.6 14507,6608.6 14507,6546 14507,6546"];
	Layer2_Device2_OutputProj -> Layer2_Device2_Residual1	[pos="e,14573,6410 14573,6482.6 14573,6482.6 14573,6420 14573,6420"];
	Layer2_Device2_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14813,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device2_Residual1 -> Layer2_Device2_LayerNorm2	[pos="e,14813,6284 14813,6356.6 14813,6356.6 14813,6294 14813,6294"];
	Layer2_Device2_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="15114,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device2_Residual1 -> Layer2_Device2_Residual2	[pos="e,15129,5601.9 15129,6356.4 15129,6356.4 15129,5611.9 15129,5611.9"];
	Layer2_Device2_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14520,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device2_LayerNorm2 -> Layer2_Device2_GateProj	[pos="e,14672,6158 14672,6230.6 14672,6230.6 14672,6168 14672,6168"];
	Layer2_Device2_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14833,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device2_LayerNorm2 -> Layer2_Device2_UpProj	[pos="e,14886,6069.1 14886,6230.5 14886,6230.5 14886,6079.1 14886,6079.1"];
	Layer2_Device2_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14520,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device2_GateProj -> Layer2_Device2_Activation	[pos="e,14452,5979.9 14452,6104.7 14452,6104.7 14452,5989.9 14452,5989.9"];
	Layer2_Device2_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14616,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device2_UpProj -> Layer2_Device2_ElemMul	[pos="e,14886,5854.1 14886,6015.5 14886,6015.5 14886,5864.1 14886,5864.1"];
	Layer2_Device2_Activation -> Layer2_Device2_ElemMul	[pos="e,14520,5854 14520,5926.6 14520,5926.6 14520,5864 14520,5864"];
	Layer2_Device2_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14759,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device2_ElemMul -> Layer2_Device2_DownProj	[pos="e,14759,5728 14759,5800.6 14759,5800.6 14759,5738 14759,5738"];
	Layer2_Device2_DownProj -> Layer2_Device2_Residual2	[pos="e,14853,5602 14853,5674.6 14853,5674.6 14853,5612 14853,5612"];
	Layer2_Device2_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="15114,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device2_Residual2 -> Layer2_Device2_Output	[pos="e,15114,5475.9 15114,5548.6 15114,5548.6 15114,5485.9 15114,5485.9"];
	Layer3_Device2_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 2 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="15114,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device2_Output -> Layer3_Device2_Input	[pos="e,15114,5328 15114,5400.6 15114,5400.6 15114,5338 15114,5338"];
	Layer2_Device3_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="18783,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device3_Input -> Layer2_Device3_LayerNorm1	[pos="e,18985,10254 18985,10354 18985,10354 18985,10264 18985,10264"];
	Layer2_Device3_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="19669,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device3_Input -> Layer2_Device3_Residual1	[pos="e,19544,6409.8 19544,10347 19544,10347 19544,6419.8 19544,6419.8"];
	Layer2_Device3_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18608,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device3_LayerNorm1 -> Layer2_Device3_QKVProj	[pos="e,18783,10128 18783,10201 18783,10201 18783,10138 18783,10138"];
	Layer2_Device3_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17391,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage0_RecvKV	[label="Local K,V",
		lp="17892,10031",
		pos="e,18135,9988.1 18135,10075 18135,10075 18135,9998.1 18135,9998.1"];
	Layer2_Device3_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18181,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage0_Attention	[label=Q_local,
		lp="18453,9934.8",
		pos="e,18362,9768.7 18362,10075 18362,10075 18362,9778.7 18362,9778.7"];
	Layer2_Device3_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15851,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage1_Attention	[label=Q_local,
		lp="15915,9838.3",
		pos="e,16034,9575.6 17958,10081 17196,10081 16034,10081 16034,10081 16034,10081 16034,9585.6 16034,9585.6"];
	Layer2_Device3_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16462,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage2_Attention	[label=Q_local,
		lp="18593,9741.8",
		pos="e,16632,9382.4 18457,10075 18457,9976.3 18457,9641 18457,9641 18457,9641 16632,9641 16632,9641 16632,9641 16632,9392.4 16632,9392.4"];
	Layer2_Device3_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18477,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage3_Attention	[label=Q_local,
		lp="18733,9645.3",
		pos="e,18650,9189.6 18650,10075 18650,10075 18650,9199.6 18650,9199.6"];
	Layer2_Device3_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15683,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage4_Attention	[label=Q_local,
		lp="15535,9548.8",
		pos="e,15557,8996.5 17958,10094 17062,10094 15557,10094 15557,10094 15557,10094 15557,9006.5 15557,9006.5"];
	Layer2_Device3_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18597,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage5_Attention	[label=Q_local,
		lp="18837,9452.3",
		pos="e,18768,8803.6 18768,10075 18768,10075 18768,8813.6 18768,8813.6"];
	Layer2_Device3_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18625,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage6_Attention	[label=Q_local,
		lp="18969,9355.8",
		pos="e,18842,8610.6 18842,10075 18842,10075 18842,8620.6 18842,8620.6"];
	Layer2_Device3_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15686,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage7_Attention	[label=Q_local,
		lp="15389,9259.3",
		pos="e,15454,8399 17958,10107 17017,10107 15388,10107 15388,10107 15388,10107 15388,8399 15388,8399 15388,8399 15444,8399 15444,8399"];
	Layer2_Device3_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18430,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage8_Attention	[label=Q_local,
		lp="19089,9162.8",
		pos="e,18662,8197 18895,10075 18895,9844.7 18895,8197 18895,8197 18895,8197 18672,8197 18672,8197"];
	Layer2_Device3_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15491,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage9_Attention	[label=Q_local,
		lp="15271,9066.3",
		pos="e,15324,8031.5 17958,10114 17001,10114 15324,10114 15324,10114 15324,10114 15324,8041.5 15324,8041.5"];
	Layer2_Device3_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15706,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage10_Attention	[label=Q_local,
		lp="15131,8969.8",
		pos="e,15474,7811 17958,10121 16983,10121 15252,10121 15252,10121 15252,10121 15252,7811 15252,7811 15252,7811 15464,7811 15464,7811"];
	Layer2_Device3_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16920,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage11_Attention	[label=Q_local,
		lp="19210,8873.3",
		pos="e,16997,7645.4 18972,10075 18972,9813.2 18972,7722 18972,7722 18972,7722 16997,7722 16997,7722 16997,7722 16997,7655.4 16997,7655.4"];
	Layer2_Device3_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="17250,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage12_Attention	[label=Q_local,
		lp="19353,8776.8",
		pos="e,17180,7452.5 19010,10075 19010,9812.3 19010,7715 19010,7715 19010,7715 17180,7715 17180,7715 17180,7715 17180,7462.5 17180,7462.5"];
	Layer2_Device3_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="19265,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage13_Attention	[label=Q_local,
		lp="19493,8680.3",
		pos="e,19438,7259.7 19258,10088 19363,10088 19438,10088 19438,10088 19438,10088 19438,7269.7 19438,7269.7"];
	Layer2_Device3_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16847,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage14_Attention	[label=Q_local,
		lp="14985,8583.8",
		pos="e,16615,7039 18933,10075 18933,9826.2 18933,7912 18933,7912 18933,7912 16478,7912 16478,7912 16478,7912 16478,7039 16478,7039 16478,\
7039 16605,7039 16605,7039"];
	Layer2_Device3_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18980,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_QKVProj -> Layer2_Device3_Stage15_Attention	[label=Q_local,
		lp="19611,8487.3",
		pos="e,19212,6896 19258,10101 19397,10101 19504,10101 19504,10101 19504,10101 19504,6896 19504,6896 19504,6896 19222,6896 19222,6896"];
	Layer2_Device3_Stage0_RecvKV -> Layer2_Device3_Stage0_Attention	[pos="e,18130,9768.7 18130,9934.7 18130,9934.7 18130,9778.7 18130,9778.7"];
	Layer2_Device3_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="16972,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage0_RecvKV -> Layer2_Device3_Stage1_RecvKV	[label="Ring transfer",
		lp="17229,9838.3",
		pos="e,17182,9794.9 17182,9881.6 17182,9881.6 17182,9804.9 17182,9804.9"];
	Layer2_Device3_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="16372,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage0_Attention -> Layer2_Device3_Stage0_Accumulate	[pos="e,16372,9575.6 18181,9715.1 18181,9685.8 18181,9643 18181,9643 18181,9643 16372,9643 16372,9643 16372,9643 16372,9585.6 16372,9585.6"];
	Layer2_Device3_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="15941,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage0_Accumulate -> Layer2_Device3_Stage1_Accumulate	[pos="e,16156,9382.4 16156,9522 16156,9522 16156,9392.4 16156,9392.4"];
	Layer2_Device3_Stage1_RecvKV -> Layer2_Device3_Stage1_Attention	[pos="e,16067,9575.7 16067,9741.7 16067,9741.7 16067,9585.7 16067,9585.7"];
	Layer2_Device3_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17581,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage1_RecvKV -> Layer2_Device3_Stage2_RecvKV	[label="Ring transfer",
		lp="17289,9645.3",
		pos="e,17276,9601.9 17276,9688.6 17276,9688.6 17276,9611.9 17276,9611.9"];
	Layer2_Device3_Stage1_Attention -> Layer2_Device3_Stage1_Accumulate	[pos="e,15896,9382.4 15896,9522 15896,9522 15896,9392.4 15896,9392.4"];
	Layer2_Device3_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16059,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage1_Accumulate -> Layer2_Device3_Stage2_Accumulate	[pos="e,16000,9189.4 16000,9329 16000,9329 16000,9199.4 16000,9199.4"];
	Layer2_Device3_Stage2_RecvKV -> Layer2_Device3_Stage2_Attention	[pos="e,16677,9382.7 16677,9548.7 16677,9548.7 16677,9392.7 16677,9392.7"];
	Layer2_Device3_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17671,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage2_RecvKV -> Layer2_Device3_Stage3_RecvKV	[label="Ring transfer",
		lp="17674,9452.3",
		pos="e,17626,9408.9 17626,9495.6 17626,9495.6 17626,9418.9 17626,9418.9"];
	Layer2_Device3_Stage2_Attention -> Layer2_Device3_Stage2_Accumulate	[pos="e,16260,9189.4 16260,9329 16260,9329 16260,9199.4 16260,9199.4"];
	Layer2_Device3_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16204,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage2_Accumulate -> Layer2_Device3_Stage3_Accumulate	[pos="e,16132,8996.4 16132,9136 16132,9136 16132,9006.4 16132,9006.4"];
	Layer2_Device3_Stage3_RecvKV -> Layer2_Device3_Stage3_Attention	[pos="e,18418,9189.7 18418,9355.7 18418,9355.7 18418,9199.7 18418,9199.7"];
	Layer2_Device3_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17268,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage3_RecvKV -> Layer2_Device3_Stage4_RecvKV	[label="Ring transfer",
		lp="17518,9259.3",
		pos="e,17470,9215.9 17470,9302.6 17470,9302.6 17470,9225.9 17470,9225.9"];
	Layer2_Device3_Stage3_Attention -> Layer2_Device3_Stage3_Accumulate	[pos="e,16392,8996.6 18289,9136.2 18289,9100.9 18289,9043 18289,9043 18289,9043 16392,9043 16392,9043 16392,9043 16392,9006.6 16392,9006.6"];
	Layer2_Device3_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16179,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage3_Accumulate -> Layer2_Device3_Stage4_Accumulate	[pos="e,16192,8803.4 16192,8943 16192,8943 16192,8813.4 16192,8813.4"];
	Layer2_Device3_Stage4_RecvKV -> Layer2_Device3_Stage4_Attention	[pos="e,15871,8996.5 16395,9123 16112,9123 15871,9123 15871,9123 15871,9123 15871,9006.5 15871,9006.5"];
	Layer2_Device3_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17413,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage4_RecvKV -> Layer2_Device3_Stage5_RecvKV	[label="Ring transfer",
		lp="17388,9066.3",
		pos="e,17340,9022.9 17340,9109.6 17340,9109.6 17340,9032.9 17340,9032.9"];
	Layer2_Device3_Stage4_Attention -> Layer2_Device3_Stage4_Accumulate	[pos="e,15960,8803.5 15915,8969 15942,8969 15960,8969 15960,8969 15960,8969 15960,8813.5 15960,8813.5"];
	Layer2_Device3_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18104,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage4_Accumulate -> Layer2_Device3_Stage5_Accumulate	[pos="e,18090,8610.6 16193,8750.2 16193,8717.7 16193,8667 16193,8667 16193,8667 18090,8667 18090,8667 18090,8667 18090,8620.6 18090,8620.6"];
	Layer2_Device3_Stage5_RecvKV -> Layer2_Device3_Stage5_Attention	[pos="e,18365,8776 18320,8969.7 18320,8963.8 18320,8776 18320,8776 18320,8776 18355,8776 18355,8776"];
	Layer2_Device3_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17388,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage5_RecvKV -> Layer2_Device3_Stage6_RecvKV	[label="Ring transfer",
		lp="17448,8873.3",
		pos="e,17400,8829.9 17400,8916.6 17400,8916.6 17400,8839.9 17400,8839.9"];
	Layer2_Device3_Stage5_Attention -> Layer2_Device3_Stage5_Accumulate	[pos="e,18336,8583 18380,8750.3 18380,8697.2 18380,8583 18380,8583 18380,8583 18346,8583 18346,8583"];
	Layer2_Device3_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18104,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage5_Accumulate -> Layer2_Device3_Stage6_Accumulate	[pos="e,18104,8417.4 18104,8557 18104,8557 18104,8427.4 18104,8427.4"];
	Layer2_Device3_Stage6_RecvKV -> Layer2_Device3_Stage6_Attention	[pos="e,18611,8610.5 17978,8737 18291,8737 18611,8737 18611,8737 18611,8737 18611,8620.5 18611,8620.5"];
	Layer2_Device3_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16895,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage6_RecvKV -> Layer2_Device3_Stage7_RecvKV	[label="Ring transfer",
		lp="17124,8680.3",
		pos="e,17142,8636.9 17142,8723.6 17142,8723.6 17142,8646.9 17142,8646.9"];
	Layer2_Device3_Stage6_Attention -> Layer2_Device3_Stage6_Accumulate	[pos="e,18336,8381 18528,8557.3 18528,8502.3 18528,8381 18528,8381 18528,8381 18346,8381 18346,8381"];
	Layer2_Device3_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17909,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage6_Accumulate -> Layer2_Device3_Stage7_Accumulate	[pos="e,18006,8224.4 18006,8364 18006,8364 18006,8234.4 18006,8234.4"];
	Layer2_Device3_Stage7_RecvKV -> Layer2_Device3_Stage7_Attention	[pos="e,15684,8417.5 16022,8544 15831,8544 15684,8544 15684,8544 15684,8544 15684,8427.5 15684,8427.5"];
	Layer2_Device3_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16895,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage7_RecvKV -> Layer2_Device3_Stage8_RecvKV	[label="Ring transfer",
		lp="16943,8487.3",
		pos="e,16895,8443.9 16895,8530.6 16895,8530.6 16895,8453.9 16895,8453.9"];
	Layer2_Device3_Stage7_Attention -> Layer2_Device3_Stage7_Accumulate	[pos="e,17746,8224.8 15849,8364.1 15849,8333 15849,8286 15849,8286 15849,8286 17746,8286 17746,8286 17746,8286 17746,8234.8 17746,8234.8"];
	Layer2_Device3_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17909,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage7_Accumulate -> Layer2_Device3_Stage8_Accumulate	[pos="e,17909,8031.4 17909,8171 17909,8171 17909,8041.4 17909,8041.4"];
	Layer2_Device3_Stage8_RecvKV -> Layer2_Device3_Stage8_Attention	[pos="e,18267,8224.5 17485,8351 17857,8351 18267,8351 18267,8351 18267,8351 18267,8234.5 18267,8234.5"];
	Layer2_Device3_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16700,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage8_RecvKV -> Layer2_Device3_Stage9_RecvKV	[label="Ring transfer",
		lp="16878,8294.3",
		pos="e,16798,8250.9 16798,8337.6 16798,8337.6 16798,8260.9 16798,8260.9"];
	Layer2_Device3_Stage8_Attention -> Layer2_Device3_Stage8_Accumulate	[pos="e,18141,8004 18277,8171.3 18277,8118.2 18277,8004 18277,8004 18277,8004 18151,8004 18151,8004"];
	Layer2_Device3_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16227,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage8_Accumulate -> Layer2_Device3_Stage9_Accumulate	[pos="e,16304,7838.4 17909,7977.9 17909,7951.4 17909,7915 17909,7915 17909,7915 16304,7915 16304,7915 16304,7915 16304,7848.4 16304,7848.4"];
	Layer2_Device3_Stage9_RecvKV -> Layer2_Device3_Stage9_Attention	[pos="e,15588,8031.5 15827,8158 15688,8158 15588,8158 15588,8158 15588,8158 15588,8041.5 15588,8041.5"];
	Layer2_Device3_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="16700,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage9_RecvKV -> Layer2_Device3_Stage10_RecvKV	[label="Ring transfer",
		lp="16748,8101.3",
		pos="e,16700,8057.9 16700,8144.6 16700,8144.6 16700,8067.9 16700,8067.9"];
	Layer2_Device3_Stage9_Attention -> Layer2_Device3_Stage9_Accumulate	[pos="e,16150,7838.5 15598,7978.1 15598,7939.6 15598,7873 15598,7873 15598,7873 16150,7873 16150,7873 16150,7873 16150,7848.5 16150,7848.5"];
	Layer2_Device3_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16227,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage9_Accumulate -> Layer2_Device3_Stage10_Accumulate	[pos="e,16227,7645.4 16227,7785 16227,7785 16227,7655.4 16227,7655.4"];
	Layer2_Device3_Stage10_RecvKV -> Layer2_Device3_Stage10_Attention	[pos="e,15859,7838.7 15859,8004.7 15859,8004.7 15859,7848.7 15859,7848.7"];
	Layer2_Device3_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17436,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage10_RecvKV -> Layer2_Device3_Stage11_RecvKV	[label="Ring transfer",
		lp="17017,7908.3",
		pos="e,17068,7864.9 17068,7951.6 17068,7951.6 17068,7874.9 17068,7874.9"];
	Layer2_Device3_Stage10_Attention -> Layer2_Device3_Stage10_Accumulate	[pos="e,15995,7618 15877,7785.3 15877,7732.2 15877,7618 15877,7618 15877,7618 15985,7618 15985,7618"];
	Layer2_Device3_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16729,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage10_Accumulate -> Layer2_Device3_Stage11_Accumulate	[pos="e,16507,7452.5 16459,7618 16487,7618 16507,7618 16507,7618 16507,7618 16507,7462.5 16507,7462.5"];
	Layer2_Device3_Stage11_RecvKV -> Layer2_Device3_Stage11_Attention	[pos="e,16843,7645.5 16843,7758.6 16843,7758.6 16843,7655.5 16843,7655.5"];
	Layer2_Device3_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18129,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage11_RecvKV -> Layer2_Device3_Stage12_RecvKV	[label="Ring transfer",
		lp="17917,7715.3",
		pos="e,17782,7671.9 17782,7758.6 17782,7758.6 17782,7681.9 17782,7681.9"];
	Layer2_Device3_Stage11_Attention -> Layer2_Device3_Stage11_Accumulate	[pos="e,16824,7452.4 16824,7592 16824,7592 16824,7462.4 16824,7462.4"];
	Layer2_Device3_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16847,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage11_Accumulate -> Layer2_Device3_Stage12_Accumulate	[pos="e,16788,7259.4 16788,7399 16788,7399 16788,7269.4 16788,7269.4"];
	Layer2_Device3_Stage12_RecvKV -> Layer2_Device3_Stage12_Attention	[pos="e,17345,7452.5 17345,7565.6 17345,7565.6 17345,7462.5 17345,7462.5"];
	Layer2_Device3_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18459,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage12_RecvKV -> Layer2_Device3_Stage13_RecvKV	[label="Ring transfer",
		lp="18484,7522.3",
		pos="e,18294,7478.9 18294,7565.6 18294,7565.6 18294,7488.9 18294,7488.9"];
	Layer2_Device3_Stage12_Attention -> Layer2_Device3_Stage12_Accumulate	[pos="e,17048,7259.4 17048,7399 17048,7399 17048,7269.4 17048,7269.4"];
	Layer2_Device3_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17368,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage12_Accumulate -> Layer2_Device3_Stage13_Accumulate	[pos="e,17136,7039 17079,7232 17096,7232 17107,7232 17107,7232 17107,7232 17107,7039 17107,7039 17107,7039 17126,7039 17126,7039"];
	Layer2_Device3_Stage13_RecvKV -> Layer2_Device3_Stage13_Attention	[pos="e,19206,7259.7 19206,7425.7 19206,7425.7 19206,7269.7 19206,7269.7"];
	Layer2_Device3_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18056,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage13_RecvKV -> Layer2_Device3_Stage14_RecvKV	[label="Ring transfer",
		lp="18305,7329.3",
		pos="e,18258,7285.9 18258,7372.6 18258,7372.6 18258,7295.9 18258,7295.9"];
	Layer2_Device3_Stage13_Attention -> Layer2_Device3_Stage13_Accumulate	[pos="e,17368,7066.4 19265,7206 19265,7169.4 19265,7108 19265,7108 19265,7108 17368,7108 17368,7108 17368,7108 17368,7076.4 17368,7076.4"];
	Layer2_Device3_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17368,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage13_Accumulate -> Layer2_Device3_Stage14_Accumulate	[pos="e,17368,6913.8 17368,7013.2 17368,7013.2 17368,6923.8 17368,6923.8"];
	Layer2_Device3_Stage14_RecvKV -> Layer2_Device3_Stage14_Attention	[pos="e,16847,7066.5 17183,7193 16993,7193 16847,7193 16847,7193 16847,7193 16847,7076.5 16847,7076.5"];
	Layer2_Device3_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18577,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device3_Stage14_RecvKV -> Layer2_Device3_Stage15_RecvKV	[label="Ring transfer",
		lp="18364,7136.3",
		pos="e,18316,7092.9 18316,7179.6 18316,7179.6 18316,7102.9 18316,7102.9"];
	Layer2_Device3_Stage14_Attention -> Layer2_Device3_Stage14_Accumulate	[pos="e,17136,6878 16955,7013.1 16955,6967.1 16955,6878 16955,6878 16955,6878 17126,6878 17126,6878"];
	Layer2_Device3_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18980,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device3_Stage14_Accumulate -> Layer2_Device3_Stage15_Accumulate	[pos="e,18748,6761 17368,6860.5 17368,6823.4 17368,6761 17368,6761 17368,6761 18738,6761 18738,6761"];
	Layer2_Device3_Stage15_RecvKV -> Layer2_Device3_Stage15_Attention	[pos="e,18980,6913.9 18980,6986.5 18980,6986.5 18980,6923.9 18980,6923.9"];
	Layer2_Device3_Stage15_Attention -> Layer2_Device3_Stage15_Accumulate	[pos="e,18980,6788 18980,6860.6 18980,6860.6 18980,6798 18980,6798"];
	Layer2_Device3_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19186,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device3_Stage15_Accumulate -> Layer2_Device3_ConcatHeads	[pos="e,19086,6662 19086,6734.6 19086,6734.6 19086,6672 19086,6672"];
	Layer2_Device3_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19298,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device3_ConcatHeads -> Layer2_Device3_OutputProj	[pos="e,19248,6536 19248,6608.6 19248,6608.6 19248,6546 19248,6546"];
	Layer2_Device3_OutputProj -> Layer2_Device3_Residual1	[pos="e,19397,6410 19397,6482.6 19397,6482.6 19397,6420 19397,6420"];
	Layer2_Device3_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19478,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device3_Residual1 -> Layer2_Device3_LayerNorm2	[pos="e,19487,6284 19487,6356.6 19487,6356.6 19487,6294 19487,6294"];
	Layer2_Device3_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="19644,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device3_Residual1 -> Layer2_Device3_Residual2	[pos="e,19877,5601.9 19877,6356.4 19877,6356.4 19877,5611.9 19877,5611.9"];
	Layer2_Device3_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19185,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device3_LayerNorm2 -> Layer2_Device3_GateProj	[pos="e,19338,6158 19338,6230.6 19338,6230.6 19338,6168 19338,6168"];
	Layer2_Device3_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19498,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device3_LayerNorm2 -> Layer2_Device3_UpProj	[pos="e,19550,6069.1 19550,6230.5 19550,6230.5 19550,6079.1 19550,6079.1"];
	Layer2_Device3_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19185,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device3_GateProj -> Layer2_Device3_Activation	[pos="e,19116,5979.9 19116,6104.7 19116,6104.7 19116,5989.9 19116,5989.9"];
	Layer2_Device3_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19281,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device3_UpProj -> Layer2_Device3_ElemMul	[pos="e,19551,5854.1 19551,6015.5 19551,6015.5 19551,5864.1 19551,5864.1"];
	Layer2_Device3_Activation -> Layer2_Device3_ElemMul	[pos="e,19185,5854 19185,5926.6 19185,5926.6 19185,5864 19185,5864"];
	Layer2_Device3_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19424,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device3_ElemMul -> Layer2_Device3_DownProj	[pos="e,19424,5728 19424,5800.6 19424,5800.6 19424,5738 19424,5738"];
	Layer2_Device3_DownProj -> Layer2_Device3_Residual2	[pos="e,19450,5602 19450,5674.6 19450,5674.6 19450,5612 19450,5612"];
	Layer2_Device3_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19644,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device3_Residual2 -> Layer2_Device3_Output	[pos="e,19644,5475.9 19644,5548.6 19644,5548.6 19644,5485.9 19644,5485.9"];
	Layer3_Device3_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 3 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19644,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device3_Output -> Layer3_Device3_Input	[pos="e,19644,5328 19644,5400.6 19644,5400.6 19644,5338 19644,5338"];
	Layer2_Device4_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="23944,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device4_Input -> Layer2_Device4_LayerNorm1	[pos="e,24078,10254 24078,10339 24078,10339 24078,10264 24078,10264"];
	Layer2_Device4_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="24582,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device4_Input -> Layer2_Device4_Residual1	[pos="e,24500,6410 24500,10336 24500,10336 24500,6420 24500,6420"];
	Layer2_Device4_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23527,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device4_LayerNorm1 -> Layer2_Device4_QKVProj	[pos="e,23944,10128 23944,10201 23944,10201 23944,10138 23944,10138"];
	Layer2_Device4_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22281,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage0_RecvKV	[label="Local K,V",
		lp="22782,10031",
		pos="e,23039,9988.1 23039,10075 23039,10075 23039,9998.1 23039,9998.1"];
	Layer2_Device4_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23071,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage0_Attention	[label=Q_local,
		lp="23343,9934.8",
		pos="e,23252,9768.7 23252,10075 23252,10075 23252,9778.7 23252,9778.7"];
	Layer2_Device4_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20741,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage1_Attention	[label=Q_local,
		lp="20805,9838.3",
		pos="e,20840,9575.6 22877,10081 22082,10081 20840,10081 20840,10081 20840,10081 20840,9585.6 20840,9585.6"];
	Layer2_Device4_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21352,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage2_Attention	[label=Q_local,
		lp="23483,9741.8",
		pos="e,21522,9382.3 23347,10075 23347,9976.9 23347,9646 23347,9646 23347,9646 21522,9646 21522,9646 21522,9646 21522,9392.3 21522,9392.3"];
	Layer2_Device4_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23367,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage3_Attention	[label=Q_local,
		lp="23623,9645.3",
		pos="e,23540,9189.6 23540,10075 23540,10075 23540,9199.6 23540,9199.6"];
	Layer2_Device4_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20573,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage4_Attention	[label=Q_local,
		lp="20425,9548.8",
		pos="e,20426,8996.5 22877,10094 21968,10094 20426,10094 20426,10094 20426,10094 20426,9006.5 20426,9006.5"];
	Layer2_Device4_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23485,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage5_Attention	[label=Q_local,
		lp="23727,9452.3",
		pos="e,23658,8803.6 23658,10075 23658,10075 23658,8813.6 23658,8813.6"];
	Layer2_Device4_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23512,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage6_Attention	[label=Q_local,
		lp="23859,9355.8",
		pos="e,23730,8610.6 23730,10075 23730,10075 23730,8620.6 23730,8620.6"];
	Layer2_Device4_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20573,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage7_Attention	[label=Q_local,
		lp="20279,9259.3",
		pos="e,20341,8399 22877,10107 21935,10107 20300,10107 20300,10107 20300,10107 20300,8399 20300,8399 20300,8399 20331,8399 20331,8399"];
	Layer2_Device4_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23258,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage8_Attention	[label=Q_local,
		lp="23977,9162.8",
		pos="e,23490,8197 23782,10075 23782,9844.7 23782,8197 23782,8197 23782,8197 23500,8197 23500,8197"];
	Layer2_Device4_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20319,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage9_Attention	[label=Q_local,
		lp="20161,9066.3",
		pos="e,20259,8031.5 22877,10114 21924,10114 20259,10114 20259,10114 20259,10114 20259,8041.5 20259,8041.5"];
	Layer2_Device4_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20575,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage10_Attention	[label=Q_local,
		lp="20021,8969.8",
		pos="e,20343,7811 22877,10121 21876,10121 20072,10121 20072,10121 20072,10121 20072,7811 20072,7811 20072,7811 20333,7811 20333,7811"];
	Layer2_Device4_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21808,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage11_Attention	[label=Q_local,
		lp="24106,8873.3",
		pos="e,21885,7645.6 23859,10074 23859,9813.8 23859,7745 23859,7745 23859,7745 21885,7745 21885,7745 21885,7745 21885,7655.6 21885,7655.6"];
	Layer2_Device4_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="22138,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage12_Attention	[label=Q_local,
		lp="24241,8776.8",
		pos="e,22068,7452.3 23898,10075 23898,9815.4 23898,7740 23898,7740 23898,7740 22068,7740 22068,7740 22068,7740 22068,7462.3 22068,7462.3"];
	Layer2_Device4_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="24153,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage13_Attention	[label=Q_local,
		lp="24381,8680.3",
		pos="e,24326,7259.7 24177,10088 24265,10088 24326,10088 24326,10088 24326,10088 24326,7269.7 24326,7269.7"];
	Layer2_Device4_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21735,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage14_Attention	[label=Q_local,
		lp="19875,8583.8",
		pos="e,21503,7039 23821,10075 23821,9825.7 23821,7908 23821,7908 23821,7908 21356,7908 21356,7908 21356,7908 21356,7039 21356,7039 21356,\
7039 21493,7039 21493,7039"];
	Layer2_Device4_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23868,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_QKVProj -> Layer2_Device4_Stage15_Attention	[label=Q_local,
		lp="24499,8487.3",
		pos="e,24100,6896 24177,10101 24300,10101 24392,10101 24392,10101 24392,10101 24392,6896 24392,6896 24392,6896 24110,6896 24110,6896"];
	Layer2_Device4_Stage0_RecvKV -> Layer2_Device4_Stage0_Attention	[pos="e,23020,9768.7 23020,9934.7 23020,9934.7 23020,9778.7 23020,9778.7"];
	Layer2_Device4_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="21862,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage0_RecvKV -> Layer2_Device4_Stage1_RecvKV	[label="Ring transfer",
		lp="22119,9838.3",
		pos="e,22072,9794.9 22072,9881.6 22072,9881.6 22072,9804.9 22072,9804.9"];
	Layer2_Device4_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="21262,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage0_Attention -> Layer2_Device4_Stage0_Accumulate	[pos="e,21262,9575.4 23071,9715 23071,9687.6 23071,9649 23071,9649 23071,9649 21262,9649 21262,9649 21262,9649 21262,9585.4 21262,9585.4"];
	Layer2_Device4_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20831,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage0_Accumulate -> Layer2_Device4_Stage1_Accumulate	[pos="e,21046,9382.4 21046,9522 21046,9522 21046,9392.4 21046,9392.4"];
	Layer2_Device4_Stage1_RecvKV -> Layer2_Device4_Stage1_Attention	[pos="e,20957,9575.7 20957,9741.7 20957,9741.7 20957,9585.7 20957,9585.7"];
	Layer2_Device4_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22471,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage1_RecvKV -> Layer2_Device4_Stage2_RecvKV	[label="Ring transfer",
		lp="22169,9645.3",
		pos="e,22166,9601.9 22166,9688.6 22166,9688.6 22166,9611.9 22166,9611.9"];
	Layer2_Device4_Stage1_Attention -> Layer2_Device4_Stage1_Accumulate	[pos="e,20786,9382.4 20786,9522 20786,9522 20786,9392.4 20786,9392.4"];
	Layer2_Device4_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="20949,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage1_Accumulate -> Layer2_Device4_Stage2_Accumulate	[pos="e,20890,9189.4 20890,9329 20890,9329 20890,9199.4 20890,9199.4"];
	Layer2_Device4_Stage2_RecvKV -> Layer2_Device4_Stage2_Attention	[pos="e,21567,9382.7 21567,9548.7 21567,9548.7 21567,9392.7 21567,9392.7"];
	Layer2_Device4_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22561,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage2_RecvKV -> Layer2_Device4_Stage3_RecvKV	[label="Ring transfer",
		lp="22564,9452.3",
		pos="e,22516,9408.9 22516,9495.6 22516,9495.6 22516,9418.9 22516,9418.9"];
	Layer2_Device4_Stage2_Attention -> Layer2_Device4_Stage2_Accumulate	[pos="e,21150,9189.4 21150,9329 21150,9329 21150,9199.4 21150,9199.4"];
	Layer2_Device4_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21094,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage2_Accumulate -> Layer2_Device4_Stage3_Accumulate	[pos="e,21022,8996.4 21022,9136 21022,9136 21022,9006.4 21022,9006.4"];
	Layer2_Device4_Stage3_RecvKV -> Layer2_Device4_Stage3_Attention	[pos="e,23308,9189.7 23308,9355.7 23308,9355.7 23308,9199.7 23308,9199.7"];
	Layer2_Device4_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22158,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage3_RecvKV -> Layer2_Device4_Stage4_RecvKV	[label="Ring transfer",
		lp="22342,9259.3",
		pos="e,22360,9215.9 22360,9302.6 22360,9302.6 22360,9225.9 22360,9225.9"];
	Layer2_Device4_Stage3_Attention -> Layer2_Device4_Stage3_Accumulate	[pos="e,21282,8996.7 23179,9136.2 23179,9102.3 23179,9048 23179,9048 23179,9048 21282,9048 21282,9048 21282,9048 21282,9006.7 21282,9006.7"];
	Layer2_Device4_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21067,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage3_Accumulate -> Layer2_Device4_Stage4_Accumulate	[pos="e,21080,8803.4 21080,8943 21080,8943 21080,8813.4 21080,8813.4"];
	Layer2_Device4_Stage4_RecvKV -> Layer2_Device4_Stage4_Attention	[pos="e,20761,8996.5 21285,9123 21002,9123 20761,9123 20761,9123 20761,9123 20761,9006.5 20761,9006.5"];
	Layer2_Device4_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22303,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage4_RecvKV -> Layer2_Device4_Stage5_RecvKV	[label="Ring transfer",
		lp="22278,9066.3",
		pos="e,22230,9022.9 22230,9109.6 22230,9109.6 22230,9032.9 22230,9032.9"];
	Layer2_Device4_Stage4_Attention -> Layer2_Device4_Stage4_Accumulate	[pos="e,20849,8803.5 20805,8969 20831,8969 20849,8969 20849,8969 20849,8969 20849,8813.5 20849,8813.5"];
	Layer2_Device4_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22991,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage4_Accumulate -> Layer2_Device4_Stage5_Accumulate	[pos="e,22978,8610.6 21080,8750.2 21080,8714.9 21080,8657 21080,8657 21080,8657 22978,8657 22978,8657 22978,8657 22978,8620.6 22978,8620.6"];
	Layer2_Device4_Stage5_RecvKV -> Layer2_Device4_Stage5_Attention	[pos="e,23253,8785 23205,8969.7 23205,8964.1 23205,8785 23205,8785 23205,8785 23243,8785 23243,8785"];
	Layer2_Device4_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22276,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage5_RecvKV -> Layer2_Device4_Stage6_RecvKV	[label="Ring transfer",
		lp="22337,8873.3",
		pos="e,22290,8829.9 22290,8916.6 22290,8916.6 22290,8839.9 22290,8839.9"];
	Layer2_Device4_Stage5_Attention -> Layer2_Device4_Stage5_Accumulate	[pos="e,23214,8610.5 23253,8767 23229,8767 23214,8767 23214,8767 23214,8767 23214,8620.5 23214,8620.5"];
	Layer2_Device4_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22991,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage5_Accumulate -> Layer2_Device4_Stage6_Accumulate	[pos="e,22991,8417.4 22991,8557 22991,8557 22991,8427.4 22991,8427.4"];
	Layer2_Device4_Stage6_RecvKV -> Layer2_Device4_Stage6_Attention	[pos="e,23498,8610.5 22866,8737 23179,8737 23498,8737 23498,8737 23498,8737 23498,8620.5 23498,8620.5"];
	Layer2_Device4_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21782,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage6_RecvKV -> Layer2_Device4_Stage7_RecvKV	[label="Ring transfer",
		lp="22138,8680.3",
		pos="e,22029,8636.9 22029,8723.6 22029,8723.6 22029,8646.9 22029,8646.9"];
	Layer2_Device4_Stage6_Attention -> Layer2_Device4_Stage6_Accumulate	[pos="e,23223,8381 23385,8557.3 23385,8502.3 23385,8381 23385,8381 23385,8381 23233,8381 23233,8381"];
	Layer2_Device4_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22737,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage6_Accumulate -> Layer2_Device4_Stage7_Accumulate	[pos="e,22864,8224.4 22864,8364 22864,8364 22864,8234.4 22864,8234.4"];
	Layer2_Device4_Stage7_RecvKV -> Layer2_Device4_Stage7_Attention	[pos="e,20573,8417.5 20909,8544 20719,8544 20573,8544 20573,8544 20573,8544 20573,8427.5 20573,8427.5"];
	Layer2_Device4_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21782,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage7_RecvKV -> Layer2_Device4_Stage8_RecvKV	[label="Ring transfer",
		lp="21830,8487.3",
		pos="e,21782,8443.9 21782,8530.6 21782,8530.6 21782,8453.9 21782,8453.9"];
	Layer2_Device4_Stage7_Attention -> Layer2_Device4_Stage7_Accumulate	[pos="e,22604,8224.6 20706,8364.2 20706,8326.3 20706,8261 20706,8261 20706,8261 22604,8261 22604,8261 22604,8261 22604,8234.6 22604,8234.6"];
	Layer2_Device4_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22737,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage7_Accumulate -> Layer2_Device4_Stage8_Accumulate	[pos="e,22737,8031.4 22737,8171 22737,8171 22737,8041.4 22737,8041.4"];
	Layer2_Device4_Stage8_RecvKV -> Layer2_Device4_Stage8_Attention	[pos="e,23124,8224.5 22372,8351 22733,8351 23124,8351 23124,8351 23124,8351 23124,8234.5 23124,8234.5"];
	Layer2_Device4_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21528,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage8_RecvKV -> Layer2_Device4_Stage9_RecvKV	[label="Ring transfer",
		lp="21763,8294.3",
		pos="e,21655,8250.9 21655,8337.6 21655,8337.6 21655,8260.9 21655,8260.9"];
	Layer2_Device4_Stage8_Attention -> Layer2_Device4_Stage8_Accumulate	[pos="e,22969,8004 23126,8171.3 23126,8118.2 23126,8004 23126,8004 23126,8004 22979,8004 22979,8004"];
	Layer2_Device4_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21096,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage8_Accumulate -> Layer2_Device4_Stage9_Accumulate	[pos="e,21173,7838.4 22737,7978.2 22737,7950.1 22737,7910 22737,7910 22737,7910 21173,7910 21173,7910 21173,7910 21173,7848.4 21173,7848.4"];
	Layer2_Device4_Stage9_RecvKV -> Layer2_Device4_Stage9_Attention	[pos="e,20446,8031.5 20655,8158 20532,8158 20446,8158 20446,8158 20446,8158 20446,8041.5 20446,8041.5"];
	Layer2_Device4_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21528,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage9_RecvKV -> Layer2_Device4_Stage10_RecvKV	[label="Ring transfer",
		lp="21576,8101.3",
		pos="e,21528,8057.9 21528,8144.6 21528,8144.6 21528,8067.9 21528,8067.9"];
	Layer2_Device4_Stage9_Attention -> Layer2_Device4_Stage9_Accumulate	[pos="e,21019,7838.6 20447,7978 20447,7934 20447,7851 20447,7851 20447,7851 21019,7851 21019,7851 21019,7851 21019,7848.6 21019,7848.6"];
	Layer2_Device4_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21096,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage9_Accumulate -> Layer2_Device4_Stage10_Accumulate	[pos="e,21096,7645.4 21096,7785 21096,7785 21096,7655.4 21096,7655.4"];
	Layer2_Device4_Stage10_RecvKV -> Layer2_Device4_Stage10_Attention	[pos="e,20707,7838.5 20707,7951.6 20707,7951.6 20707,7848.5 20707,7848.5"];
	Layer2_Device4_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22305,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage10_RecvKV -> Layer2_Device4_Stage11_RecvKV	[label="Ring transfer",
		lp="21845,7908.3",
		pos="e,21916,7864.9 21916,7951.6 21916,7951.6 21916,7874.9 21916,7874.9"];
	Layer2_Device4_Stage10_Attention -> Layer2_Device4_Stage10_Accumulate	[pos="e,20864,7618 20748,7785.3 20748,7732.2 20748,7618 20748,7618 20748,7618 20854,7618 20854,7618"];
	Layer2_Device4_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21617,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage10_Accumulate -> Layer2_Device4_Stage11_Accumulate	[pos="e,21385,7425 21297,7592.3 21297,7539.2 21297,7425 21297,7425 21297,7425 21375,7425 21375,7425"];
	Layer2_Device4_Stage11_RecvKV -> Layer2_Device4_Stage11_Attention	[pos="e,21731,7645.5 21731,7758.6 21731,7758.6 21731,7655.5 21731,7655.5"];
	Layer2_Device4_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23017,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage11_RecvKV -> Layer2_Device4_Stage12_RecvKV	[label="Ring transfer",
		lp="22709,7715.3",
		pos="e,22661,7671.9 22661,7758.6 22661,7758.6 22661,7681.9 22661,7681.9"];
	Layer2_Device4_Stage11_Attention -> Layer2_Device4_Stage11_Accumulate	[pos="e,21712,7452.4 21712,7592 21712,7592 21712,7462.4 21712,7462.4"];
	Layer2_Device4_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21735,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage11_Accumulate -> Layer2_Device4_Stage12_Accumulate	[pos="e,21676,7259.4 21676,7399 21676,7399 21676,7269.4 21676,7269.4"];
	Layer2_Device4_Stage12_RecvKV -> Layer2_Device4_Stage12_Attention	[pos="e,22233,7452.5 22233,7565.6 22233,7565.6 22233,7462.5 22233,7462.5"];
	Layer2_Device4_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23347,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage12_RecvKV -> Layer2_Device4_Stage13_RecvKV	[label="Ring transfer",
		lp="23372,7522.3",
		pos="e,23182,7478.9 23182,7565.6 23182,7565.6 23182,7488.9 23182,7488.9"];
	Layer2_Device4_Stage12_Attention -> Layer2_Device4_Stage12_Accumulate	[pos="e,21936,7259.4 21936,7399 21936,7399 21936,7269.4 21936,7269.4"];
	Layer2_Device4_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22256,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage12_Accumulate -> Layer2_Device4_Stage13_Accumulate	[pos="e,22024,7039 21967,7232 21984,7232 21995,7232 21995,7232 21995,7232 21995,7039 21995,7039 21995,7039 22014,7039 22014,7039"];
	Layer2_Device4_Stage13_RecvKV -> Layer2_Device4_Stage13_Attention	[pos="e,24094,7259.7 24094,7425.7 24094,7425.7 24094,7269.7 24094,7269.7"];
	Layer2_Device4_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22944,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage13_RecvKV -> Layer2_Device4_Stage14_RecvKV	[label="Ring transfer",
		lp="23128,7329.3",
		pos="e,23146,7285.9 23146,7372.6 23146,7372.6 23146,7295.9 23146,7295.9"];
	Layer2_Device4_Stage13_Attention -> Layer2_Device4_Stage13_Accumulate	[pos="e,22256,7066.7 24153,7206.3 24153,7170.7 24153,7112 24153,7112 24153,7112 22256,7112 22256,7112 22256,7112 22256,7076.7 22256,7076.7"];
	Layer2_Device4_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22256,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage13_Accumulate -> Layer2_Device4_Stage14_Accumulate	[pos="e,22256,6913.8 22256,7013.2 22256,7013.2 22256,6923.8 22256,6923.8"];
	Layer2_Device4_Stage14_RecvKV -> Layer2_Device4_Stage14_Attention	[pos="e,21735,7066.5 22071,7193 21881,7193 21735,7193 21735,7193 21735,7193 21735,7076.5 21735,7076.5"];
	Layer2_Device4_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23465,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device4_Stage14_RecvKV -> Layer2_Device4_Stage15_RecvKV	[label="Ring transfer",
		lp="23252,7136.3",
		pos="e,23204,7092.9 23204,7179.6 23204,7179.6 23204,7102.9 23204,7102.9"];
	Layer2_Device4_Stage14_Attention -> Layer2_Device4_Stage14_Accumulate	[pos="e,22024,6878 21826,7013.1 21826,6967.1 21826,6878 21826,6878 21826,6878 22014,6878 22014,6878"];
	Layer2_Device4_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="23868,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device4_Stage14_Accumulate -> Layer2_Device4_Stage15_Accumulate	[pos="e,23636,6761 22256,6860.5 22256,6823.4 22256,6761 22256,6761 22256,6761 23626,6761 23626,6761"];
	Layer2_Device4_Stage15_RecvKV -> Layer2_Device4_Stage15_Attention	[pos="e,23868,6913.9 23868,6986.5 23868,6986.5 23868,6923.9 23868,6923.9"];
	Layer2_Device4_Stage15_Attention -> Layer2_Device4_Stage15_Accumulate	[pos="e,23868,6788 23868,6860.6 23868,6860.6 23868,6798 23868,6798"];
	Layer2_Device4_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24074,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device4_Stage15_Accumulate -> Layer2_Device4_ConcatHeads	[pos="e,23974,6662 23974,6734.6 23974,6734.6 23974,6672 23974,6672"];
	Layer2_Device4_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24186,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device4_ConcatHeads -> Layer2_Device4_OutputProj	[pos="e,24136,6536 24136,6608.6 24136,6608.6 24136,6546 24136,6546"];
	Layer2_Device4_OutputProj -> Layer2_Device4_Residual1	[pos="e,24297,6410 24297,6482.6 24297,6482.6 24297,6420 24297,6420"];
	Layer2_Device4_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24559,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device4_Residual1 -> Layer2_Device4_LayerNorm2	[pos="e,24559,6284 24559,6356.6 24559,6356.6 24559,6294 24559,6294"];
	Layer2_Device4_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="24856,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device4_Residual1 -> Layer2_Device4_Residual2	[pos="e,24886,5601.9 24886,6356.4 24886,6356.4 24886,5611.9 24886,5611.9"];
	Layer2_Device4_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24266,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device4_LayerNorm2 -> Layer2_Device4_GateProj	[pos="e,24418,6158 24418,6230.6 24418,6230.6 24418,6168 24418,6168"];
	Layer2_Device4_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24579,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device4_LayerNorm2 -> Layer2_Device4_UpProj	[pos="e,24632,6069.1 24632,6230.5 24632,6230.5 24632,6079.1 24632,6079.1"];
	Layer2_Device4_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24266,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device4_GateProj -> Layer2_Device4_Activation	[pos="e,24198,5979.9 24198,6104.7 24198,6104.7 24198,5989.9 24198,5989.9"];
	Layer2_Device4_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24362,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device4_UpProj -> Layer2_Device4_ElemMul	[pos="e,24632,5854.1 24632,6015.5 24632,6015.5 24632,5864.1 24632,5864.1"];
	Layer2_Device4_Activation -> Layer2_Device4_ElemMul	[pos="e,24266,5854 24266,5926.6 24266,5926.6 24266,5864 24266,5864"];
	Layer2_Device4_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24458,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device4_ElemMul -> Layer2_Device4_DownProj	[pos="e,24458,5728 24458,5800.6 24458,5800.6 24458,5738 24458,5738"];
	Layer2_Device4_DownProj -> Layer2_Device4_Residual2	[pos="e,24573,5602 24573,5674.6 24573,5674.6 24573,5612 24573,5612"];
	Layer2_Device4_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24856,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device4_Residual2 -> Layer2_Device4_Output	[pos="e,24856,5475.9 24856,5548.6 24856,5548.6 24856,5485.9 24856,5485.9"];
	Layer3_Device4_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 4 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24856,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device4_Output -> Layer3_Device4_Input	[pos="e,24856,5328 24856,5400.6 24856,5400.6 24856,5338 24856,5338"];
	Layer2_Device5_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="28763,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device5_Input -> Layer2_Device5_LayerNorm1	[pos="e,28976,10227 28998,10349 28998,10313 28998,10227 28998,10227 28998,10227 28986,10227 28986,10227"];
	Layer2_Device5_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="29448,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device5_Input -> Layer2_Device5_Residual1	[pos="e,29478,6410 29478,10337 29478,10337 29478,6420 29478,6420"];
	Layer2_Device5_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28370,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device5_LayerNorm1 -> Layer2_Device5_QKVProj	[pos="e,28763,10128 28763,10201 28763,10201 28763,10138 28763,10138"];
	Layer2_Device5_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27169,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage0_RecvKV	[label="Local K,V",
		lp="27670,10031",
		pos="e,27905,9988.1 27905,10075 27905,10075 27905,9998.1 27905,9998.1"];
	Layer2_Device5_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="27959,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage0_Attention	[label=Q_local,
		lp="28231,9934.8",
		pos="e,28140,9768.7 28140,10075 28140,10075 28140,9778.7 28140,9778.7"];
	Layer2_Device5_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25629,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage1_Attention	[label=Q_local,
		lp="25693,9838.3",
		pos="e,25712,9575.6 27720,10081 26933,10081 25712,10081 25712,10081 25712,10081 25712,9585.6 25712,9585.6"];
	Layer2_Device5_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26240,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage2_Attention	[label=Q_local,
		lp="28371,9741.8",
		pos="e,26410,9382.5 28235,10074 28235,9977.5 28235,9651 28235,9651 28235,9651 26410,9651 26410,9651 26410,9651 26410,9392.5 26410,9392.5"];
	Layer2_Device5_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28255,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage3_Attention	[label=Q_local,
		lp="28511,9645.3",
		pos="e,28428,9189.6 28428,10075 28428,10075 28428,9199.6 28428,9199.6"];
	Layer2_Device5_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25461,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage4_Attention	[label=Q_local,
		lp="25313,9548.8",
		pos="e,25314,8996.5 27720,10094 26823,10094 25314,10094 25314,10094 25314,10094 25314,9006.5 25314,9006.5"];
	Layer2_Device5_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28373,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage5_Attention	[label=Q_local,
		lp="28608,9452.3",
		pos="e,28546,8803.6 28546,10075 28546,10075 28546,8813.6 28546,8813.6"];
	Layer2_Device5_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28425,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage6_Attention	[label=Q_local,
		lp="28747,9355.8",
		pos="e,28630,8610.6 28630,10075 28630,10075 28630,8620.6 28630,8620.6"];
	Layer2_Device5_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25486,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage7_Attention	[label=Q_local,
		lp="25167,9259.3",
		pos="e,25254,8399 27720,10107 26794,10107 25206,10107 25206,10107 25206,10107 25206,8399 25206,8399 25206,8399 25244,8399 25244,8399"];
	Layer2_Device5_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28330,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage8_Attention	[label=Q_local,
		lp="28865,9162.8",
		pos="e,28562,8197 28690,10075 28690,9844.7 28690,8197 28690,8197 28690,8197 28572,8197 28572,8197"];
	Layer2_Device5_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25391,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage9_Attention	[label=Q_local,
		lp="25049,9066.3",
		pos="e,25183,8031.5 27720,10114 26788,10114 25183,10114 25183,10114 25183,10114 25183,8041.5 25183,8041.5"];
	Layer2_Device5_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25356,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage10_Attention	[label=Q_local,
		lp="24909,8969.8",
		pos="e,25142,7838.5 27720,10121 26777,10121 25142,10121 25142,10121 25142,10121 25142,7848.5 25142,7848.5"];
	Layer2_Device5_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26696,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage11_Attention	[label=Q_local,
		lp="28986,8873.3",
		pos="e,26773,7645.6 28757,10074 28757,9812.1 28757,7713 28757,7713 28757,7713 26773,7713 26773,7713 26773,7713 26773,7655.6 26773,7655.6"];
	Layer2_Device5_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="27026,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage12_Attention	[label=Q_local,
		lp="29129,8776.8",
		pos="e,26956,7452.5 28791,10074 28791,9811.7 28791,7710 28791,7710 28791,7710 26956,7710 26956,7710 26956,7710 26956,7462.5 26956,7462.5"];
	Layer2_Device5_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="29041,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage13_Attention	[label=Q_local,
		lp="29269,8680.3",
		pos="e,29214,7259.7 29020,10089 29132,10089 29214,10089 29214,10089 29214,10089 29214,7269.7 29214,7269.7"];
	Layer2_Device5_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26623,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage14_Attention	[label=Q_local,
		lp="24763,8583.8",
		pos="e,26391,7039 28724,10074 28724,9814.3 28724,7749 28724,7749 28724,7749 26220,7749 26220,7749 26220,7749 26220,7039 26220,7039 26220,\
7039 26381,7039 26381,7039"];
	Layer2_Device5_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28756,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_QKVProj -> Layer2_Device5_Stage15_Attention	[label=Q_local,
		lp="29387,8487.3",
		pos="e,28988,6896 29020,10105 29190,10105 29327,10105 29327,10105 29327,10105 29327,6896 29327,6896 29327,6896 28998,6896 28998,6896"];
	Layer2_Device5_Stage0_RecvKV -> Layer2_Device5_Stage0_Attention	[pos="e,27908,9768.7 27908,9934.7 27908,9934.7 27908,9778.7 27908,9778.7"];
	Layer2_Device5_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="26750,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage0_RecvKV -> Layer2_Device5_Stage1_RecvKV	[label="Ring transfer",
		lp="27027,9838.3",
		pos="e,26960,9794.9 26960,9881.6 26960,9881.6 26960,9804.9 26960,9804.9"];
	Layer2_Device5_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="26150,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage0_Attention -> Layer2_Device5_Stage0_Accumulate	[pos="e,26150,9575.4 27959,9715 27959,9689.2 27959,9654 27959,9654 27959,9654 26150,9654 26150,9654 26150,9654 26150,9585.4 26150,9585.4"];
	Layer2_Device5_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25719,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage0_Accumulate -> Layer2_Device5_Stage1_Accumulate	[pos="e,25934,9382.4 25934,9522 25934,9522 25934,9392.4 25934,9392.4"];
	Layer2_Device5_Stage1_RecvKV -> Layer2_Device5_Stage1_Attention	[pos="e,25845,9575.7 25845,9741.7 25845,9741.7 25845,9585.7 25845,9585.7"];
	Layer2_Device5_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27359,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage1_RecvKV -> Layer2_Device5_Stage2_RecvKV	[label="Ring transfer",
		lp="27057,9645.3",
		pos="e,27054,9601.9 27054,9688.6 27054,9688.6 27054,9611.9 27054,9611.9"];
	Layer2_Device5_Stage1_Attention -> Layer2_Device5_Stage1_Accumulate	[pos="e,25674,9382.4 25674,9522 25674,9522 25674,9392.4 25674,9392.4"];
	Layer2_Device5_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25837,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage1_Accumulate -> Layer2_Device5_Stage2_Accumulate	[pos="e,25778,9189.4 25778,9329 25778,9329 25778,9199.4 25778,9199.4"];
	Layer2_Device5_Stage2_RecvKV -> Layer2_Device5_Stage2_Attention	[pos="e,26455,9382.7 26455,9548.7 26455,9548.7 26455,9392.7 26455,9392.7"];
	Layer2_Device5_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27449,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage2_RecvKV -> Layer2_Device5_Stage3_RecvKV	[label="Ring transfer",
		lp="27452,9452.3",
		pos="e,27404,9408.9 27404,9495.6 27404,9495.6 27404,9418.9 27404,9418.9"];
	Layer2_Device5_Stage2_Attention -> Layer2_Device5_Stage2_Accumulate	[pos="e,26038,9189.4 26038,9329 26038,9329 26038,9199.4 26038,9199.4"];
	Layer2_Device5_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25982,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage2_Accumulate -> Layer2_Device5_Stage3_Accumulate	[pos="e,25910,8996.4 25910,9136 25910,9136 25910,9006.4 25910,9006.4"];
	Layer2_Device5_Stage3_RecvKV -> Layer2_Device5_Stage3_Attention	[pos="e,28196,9189.7 28196,9355.7 28196,9355.7 28196,9199.7 28196,9199.7"];
	Layer2_Device5_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27046,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage3_RecvKV -> Layer2_Device5_Stage4_RecvKV	[label="Ring transfer",
		lp="27230,9259.3",
		pos="e,27248,9215.9 27248,9302.6 27248,9302.6 27248,9225.9 27248,9225.9"];
	Layer2_Device5_Stage3_Attention -> Layer2_Device5_Stage3_Accumulate	[pos="e,26170,8996.6 28067,9136.2 28067,9103.7 28067,9053 28067,9053 28067,9053 26170,9053 26170,9053 26170,9053 26170,9006.6 26170,9006.6"];
	Layer2_Device5_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25955,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage3_Accumulate -> Layer2_Device5_Stage4_Accumulate	[pos="e,25968,8803.4 25968,8943 25968,8943 25968,8813.4 25968,8813.4"];
	Layer2_Device5_Stage4_RecvKV -> Layer2_Device5_Stage4_Attention	[pos="e,25649,8996.5 26173,9123 25890,9123 25649,9123 25649,9123 25649,9123 25649,9006.5 25649,9006.5"];
	Layer2_Device5_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27191,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage4_RecvKV -> Layer2_Device5_Stage5_RecvKV	[label="Ring transfer",
		lp="27166,9066.3",
		pos="e,27118,9022.9 27118,9109.6 27118,9109.6 27118,9032.9 27118,9032.9"];
	Layer2_Device5_Stage4_Attention -> Layer2_Device5_Stage4_Accumulate	[pos="e,25737,8803.5 25693,8969 25719,8969 25737,8969 25737,8969 25737,8969 25737,8813.5 25737,8813.5"];
	Layer2_Device5_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27904,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage4_Accumulate -> Layer2_Device5_Stage5_Accumulate	[pos="e,27878,8610.6 25981,8750.1 25981,8727.3 25981,8698 25981,8698 25981,8698 27878,8698 27878,8698 27878,8698 27878,8620.6 27878,8620.6"];
	Layer2_Device5_Stage5_RecvKV -> Layer2_Device5_Stage5_Attention	[pos="e,28141,8785 28097,8969.7 28097,8964.1 28097,8785 28097,8785 28097,8785 28131,8785 28131,8785"];
	Layer2_Device5_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27164,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage5_RecvKV -> Layer2_Device5_Stage6_RecvKV	[label="Ring transfer",
		lp="27225,8873.3",
		pos="e,27178,8829.9 27178,8916.6 27178,8916.6 27178,8839.9 27178,8839.9"];
	Layer2_Device5_Stage5_Attention -> Layer2_Device5_Stage5_Accumulate	[pos="e,28123,8610.5 28141,8767 28130,8767 28123,8767 28123,8767 28123,8767 28123,8620.5 28123,8620.5"];
	Layer2_Device5_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27904,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage5_Accumulate -> Layer2_Device5_Stage6_Accumulate	[pos="e,27904,8417.4 27904,8557 27904,8557 27904,8427.4 27904,8427.4"];
	Layer2_Device5_Stage6_RecvKV -> Layer2_Device5_Stage6_Attention	[pos="e,28399,8610.5 27754,8737 28072,8737 28399,8737 28399,8737 28399,8737 28399,8620.5 28399,8620.5"];
	Layer2_Device5_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26695,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage6_RecvKV -> Layer2_Device5_Stage7_RecvKV	[label="Ring transfer",
		lp="27062,8680.3",
		pos="e,26930,8636.9 26930,8723.6 26930,8723.6 26930,8646.9 26930,8646.9"];
	Layer2_Device5_Stage6_Attention -> Layer2_Device5_Stage6_Accumulate	[pos="e,28136,8381 28378,8557.3 28378,8502.3 28378,8381 28378,8381 28378,8381 28146,8381 28146,8381"];
	Layer2_Device5_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27809,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage6_Accumulate -> Layer2_Device5_Stage7_Accumulate	[pos="e,27856,8224.4 27856,8364 27856,8364 27856,8234.4 27856,8234.4"];
	Layer2_Device5_Stage7_RecvKV -> Layer2_Device5_Stage7_Attention	[pos="e,25705,8417.5 25822,8544 25750,8544 25705,8544 25705,8544 25705,8544 25705,8427.5 25705,8427.5"];
	Layer2_Device5_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26695,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage7_RecvKV -> Layer2_Device5_Stage8_RecvKV	[label="Ring transfer",
		lp="26743,8487.3",
		pos="e,26695,8443.9 26695,8530.6 26695,8530.6 26695,8453.9 26695,8453.9"];
	Layer2_Device5_Stage7_Attention -> Layer2_Device5_Stage7_Accumulate	[pos="e,27596,8224.6 25699,8364.1 25699,8346.6 25699,8327 25699,8327 25699,8327 27596,8327 27596,8327 27596,8327 27596,8234.6 27596,8234.6"];
	Layer2_Device5_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27809,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage7_Accumulate -> Layer2_Device5_Stage8_Accumulate	[pos="e,27809,8031.4 27809,8171 27809,8171 27809,8041.4 27809,8041.4"];
	Layer2_Device5_Stage8_RecvKV -> Layer2_Device5_Stage8_Attention	[pos="e,28117,8224.5 27285,8351 27676,8351 28117,8351 28117,8351 28117,8351 28117,8234.5 28117,8234.5"];
	Layer2_Device5_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26600,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage8_RecvKV -> Layer2_Device5_Stage9_RecvKV	[label="Ring transfer",
		lp="26696,8294.3",
		pos="e,26648,8250.9 26648,8337.6 26648,8337.6 26648,8260.9 26648,8260.9"];
	Layer2_Device5_Stage8_Attention -> Layer2_Device5_Stage8_Accumulate	[pos="e,28041,8004 28330,8171.3 28330,8118.2 28330,8004 28330,8004 28330,8004 28051,8004 28051,8004"];
	Layer2_Device5_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25877,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage8_Accumulate -> Layer2_Device5_Stage9_Accumulate	[pos="e,25894,7838.5 27792,7977.9 27792,7955.8 27792,7928 27792,7928 27792,7928 25894,7928 25894,7928 25894,7928 25894,7848.5 25894,7848.5"];
	Layer2_Device5_Stage9_RecvKV -> Layer2_Device5_Stage9_Attention	[pos="e,25438,8031.5 25727,8158 25561,8158 25438,8158 25438,8158 25438,8158 25438,8041.5 25438,8041.5"];
	Layer2_Device5_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26600,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage9_RecvKV -> Layer2_Device5_Stage10_RecvKV	[label="Ring transfer",
		lp="26648,8101.3",
		pos="e,26600,8057.9 26600,8144.6 26600,8144.6 26600,8067.9 26600,8067.9"];
	Layer2_Device5_Stage9_Attention -> Layer2_Device5_Stage9_Accumulate	[pos="e,25663,7838.5 25623,8004 25647,8004 25663,8004 25663,8004 25663,8004 25663,7848.5 25663,7848.5"];
	Layer2_Device5_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25877,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage9_Accumulate -> Layer2_Device5_Stage10_Accumulate	[pos="e,25877,7645.4 25877,7785 25877,7785 25877,7655.4 25877,7655.4"];
	Layer2_Device5_Stage10_RecvKV -> Layer2_Device5_Stage10_Attention	[pos="e,25374,7838.5 25727,7965 25528,7965 25374,7965 25374,7965 25374,7965 25374,7848.5 25374,7848.5"];
	Layer2_Device5_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27086,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage10_RecvKV -> Layer2_Device5_Stage11_RecvKV	[label="Ring transfer",
		lp="26860,7908.3",
		pos="e,26843,7864.9 26843,7951.6 26843,7951.6 26843,7874.9 26843,7874.9"];
	Layer2_Device5_Stage10_Attention -> Layer2_Device5_Stage10_Accumulate	[pos="e,25645,7618 25554,7785.3 25554,7732.2 25554,7618 25554,7618 25554,7618 25635,7618 25635,7618"];
	Layer2_Device5_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26505,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage10_Accumulate -> Layer2_Device5_Stage11_Accumulate	[pos="e,26273,7425 26104,7592.3 26104,7539.2 26104,7425 26104,7425 26104,7425 26263,7425 26263,7425"];
	Layer2_Device5_Stage11_RecvKV -> Layer2_Device5_Stage11_Attention	[pos="e,26619,7645.5 26619,7758.6 26619,7758.6 26619,7655.5 26619,7655.5"];
	Layer2_Device5_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27905,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage11_RecvKV -> Layer2_Device5_Stage12_RecvKV	[label="Ring transfer",
		lp="27544,7715.3",
		pos="e,27496,7671.9 27496,7758.6 27496,7758.6 27496,7681.9 27496,7681.9"];
	Layer2_Device5_Stage11_Attention -> Layer2_Device5_Stage11_Accumulate	[pos="e,26600,7452.4 26600,7592 26600,7592 26600,7462.4 26600,7462.4"];
	Layer2_Device5_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26623,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage11_Accumulate -> Layer2_Device5_Stage12_Accumulate	[pos="e,26564,7259.4 26564,7399 26564,7399 26564,7269.4 26564,7269.4"];
	Layer2_Device5_Stage12_RecvKV -> Layer2_Device5_Stage12_Attention	[pos="e,27121,7452.5 27121,7565.6 27121,7565.6 27121,7462.5 27121,7462.5"];
	Layer2_Device5_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28235,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage12_RecvKV -> Layer2_Device5_Stage13_RecvKV	[label="Ring transfer",
		lp="28118,7522.3",
		pos="e,28070,7478.9 28070,7565.6 28070,7565.6 28070,7488.9 28070,7488.9"];
	Layer2_Device5_Stage12_Attention -> Layer2_Device5_Stage12_Accumulate	[pos="e,26824,7259.4 26824,7399 26824,7399 26824,7269.4 26824,7269.4"];
	Layer2_Device5_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27144,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage12_Accumulate -> Layer2_Device5_Stage13_Accumulate	[pos="e,26912,7039 26855,7232 26872,7232 26883,7232 26883,7232 26883,7232 26883,7039 26883,7039 26883,7039 26902,7039 26902,7039"];
	Layer2_Device5_Stage13_RecvKV -> Layer2_Device5_Stage13_Attention	[pos="e,28982,7259.7 28982,7425.7 28982,7425.7 28982,7269.7 28982,7269.7"];
	Layer2_Device5_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27832,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage13_RecvKV -> Layer2_Device5_Stage14_RecvKV	[label="Ring transfer",
		lp="28149,7329.3",
		pos="e,28034,7285.9 28034,7372.6 28034,7372.6 28034,7295.9 28034,7295.9"];
	Layer2_Device5_Stage13_Attention -> Layer2_Device5_Stage13_Accumulate	[pos="e,27144,7066.5 29041,7206.3 29041,7171.8 29041,7116 29041,7116 29041,7116 27144,7116 27144,7116 27144,7116 27144,7076.5 27144,7076.5"];
	Layer2_Device5_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27144,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage13_Accumulate -> Layer2_Device5_Stage14_Accumulate	[pos="e,27144,6913.8 27144,7013.2 27144,7013.2 27144,6923.8 27144,6923.8"];
	Layer2_Device5_Stage14_RecvKV -> Layer2_Device5_Stage14_Attention	[pos="e,26623,7066.5 26959,7193 26769,7193 26623,7193 26623,7193 26623,7193 26623,7076.5 26623,7076.5"];
	Layer2_Device5_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28353,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device5_Stage14_RecvKV -> Layer2_Device5_Stage15_RecvKV	[label="Ring transfer",
		lp="28140,7136.3",
		pos="e,28092,7092.9 28092,7179.6 28092,7179.6 28092,7102.9 28092,7102.9"];
	Layer2_Device5_Stage14_Attention -> Layer2_Device5_Stage14_Accumulate	[pos="e,26912,6878 26686,7013.1 26686,6967.1 26686,6878 26686,6878 26686,6878 26902,6878 26902,6878"];
	Layer2_Device5_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28756,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device5_Stage14_Accumulate -> Layer2_Device5_Stage15_Accumulate	[pos="e,28524,6761 27144,6860.5 27144,6823.4 27144,6761 27144,6761 27144,6761 28514,6761 28514,6761"];
	Layer2_Device5_Stage15_RecvKV -> Layer2_Device5_Stage15_Attention	[pos="e,28756,6913.9 28756,6986.5 28756,6986.5 28756,6923.9 28756,6923.9"];
	Layer2_Device5_Stage15_Attention -> Layer2_Device5_Stage15_Accumulate	[pos="e,28756,6788 28756,6860.6 28756,6860.6 28756,6798 28756,6798"];
	Layer2_Device5_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="28961,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device5_Stage15_Accumulate -> Layer2_Device5_ConcatHeads	[pos="e,28862,6662 28862,6734.6 28862,6734.6 28862,6672 28862,6672"];
	Layer2_Device5_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29169,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device5_ConcatHeads -> Layer2_Device5_OutputProj	[pos="e,29071,6536 29071,6608.6 29071,6608.6 29071,6546 29071,6546"];
	Layer2_Device5_OutputProj -> Layer2_Device5_Residual1	[pos="e,29222,6410 29222,6482.6 29222,6482.6 29222,6420 29222,6420"];
	Layer2_Device5_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29298,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device5_Residual1 -> Layer2_Device5_LayerNorm2	[pos="e,29298,6284 29298,6356.6 29298,6356.6 29298,6294 29298,6294"];
	Layer2_Device5_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="29583,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device5_Residual1 -> Layer2_Device5_Residual2	[pos="e,29689,5601.9 29689,6356.4 29689,6356.4 29689,5611.9 29689,5611.9"];
	Layer2_Device5_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29006,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device5_LayerNorm2 -> Layer2_Device5_GateProj	[pos="e,29158,6158 29158,6230.6 29158,6230.6 29158,6168 29158,6168"];
	Layer2_Device5_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29319,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device5_LayerNorm2 -> Layer2_Device5_UpProj	[pos="e,29371,6069.1 29371,6230.5 29371,6230.5 29371,6079.1 29371,6079.1"];
	Layer2_Device5_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29006,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device5_GateProj -> Layer2_Device5_Activation	[pos="e,28938,5979.9 28938,6104.7 28938,6104.7 28938,5989.9 28938,5989.9"];
	Layer2_Device5_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29101,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device5_UpProj -> Layer2_Device5_ElemMul	[pos="e,29371,5854.1 29371,6015.5 29371,6015.5 29371,5864.1 29371,5864.1"];
	Layer2_Device5_Activation -> Layer2_Device5_ElemMul	[pos="e,29006,5854 29006,5926.6 29006,5926.6 29006,5864 29006,5864"];
	Layer2_Device5_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29244,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device5_ElemMul -> Layer2_Device5_DownProj	[pos="e,29244,5728 29244,5800.6 29244,5800.6 29244,5738 29244,5738"];
	Layer2_Device5_DownProj -> Layer2_Device5_Residual2	[pos="e,29330,5602 29330,5674.6 29330,5674.6 29330,5612 29330,5612"];
	Layer2_Device5_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29583,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device5_Residual2 -> Layer2_Device5_Output	[pos="e,29583,5475.9 29583,5548.6 29583,5548.6 29583,5485.9 29583,5485.9"];
	Layer3_Device5_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 5 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29583,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device5_Output -> Layer3_Device5_Input	[pos="e,29583,5328 29583,5400.6 29583,5400.6 29583,5338 29583,5338"];
	Layer2_Device6_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33700,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device6_Input -> Layer2_Device6_LayerNorm1	[pos="e,33800,10254 33800,10335 33800,10335 33800,10264 33800,10264"];
	Layer2_Device6_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="34304,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device6_Input -> Layer2_Device6_Residual1	[pos="e,34273,6409.9 34273,10352 34273,10352 34273,6419.9 34273,6419.9"];
	Layer2_Device6_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33306,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device6_LayerNorm1 -> Layer2_Device6_QKVProj	[pos="e,33700,10128 33700,10201 33700,10201 33700,10138 33700,10138"];
	Layer2_Device6_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32060,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage0_RecvKV	[label="Local K,V",
		lp="32560,10031",
		pos="e,32818,9988.1 32818,10075 32818,10075 32818,9998.1 32818,9998.1"];
	Layer2_Device6_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="32813,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage0_Attention	[label=Q_local,
		lp="33122,9934.8",
		pos="e,33012,9768.7 33012,10075 33012,10075 33012,9778.7 33012,9778.7"];
	Layer2_Device6_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30520,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage1_Attention	[label=Q_local,
		lp="30584,9838.3",
		pos="e,30490,9575.6 32656,10082 31825,10082 30490,10082 30490,10082 30490,10082 30490,9585.6 30490,9585.6"];
	Layer2_Device6_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31131,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage2_Attention	[label=Q_local,
		lp="33262,9741.8",
		pos="e,31301,9382.4 33107,10074 33107,9971.8 33107,9609 33107,9609 33107,9609 31301,9609 31301,9609 31301,9609 31301,9392.4 31301,9392.4"];
	Layer2_Device6_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33146,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage3_Attention	[label=Q_local,
		lp="33402,9645.3",
		pos="e,33319,9189.6 33319,10075 33319,10075 33319,9199.6 33319,9199.6"];
	Layer2_Device6_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30352,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage4_Attention	[label=Q_local,
		lp="30204,9548.8",
		pos="e,30204,8996.6 32656,10097 31747,10097 30204,10097 30204,10097 30204,10097 30204,9006.6 30204,9006.6"];
	Layer2_Device6_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33265,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage5_Attention	[label=Q_local,
		lp="33508,9452.3",
		pos="e,33437,8803.6 33437,10075 33437,10075 33437,8813.6 33437,8813.6"];
	Layer2_Device6_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33374,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage6_Attention	[label=Q_local,
		lp="33638,9355.8",
		pos="e,33551,8610.6 33551,10075 33551,10075 33551,8620.6 33551,8620.6"];
	Layer2_Device6_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30435,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage7_Attention	[label=Q_local,
		lp="30058,9259.3",
		pos="e,30203,8399 32656,10112 31711,10112 30066,10112 30066,10112 30066,10112 30066,8399 30066,8399 30066,8399 30193,8399 30193,8399"];
	Layer2_Device6_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33129,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage8_Attention	[label=Q_local,
		lp="33757,9162.8",
		pos="e,33361,8197 33628,10075 33628,9844.7 33628,8197 33628,8197 33628,8197 33371,8197 33371,8197"];
	Layer2_Device6_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30190,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage9_Attention	[label=Q_local,
		lp="29940,9066.3",
		pos="e,30012,8031.6 32656,10120 31697,10120 30012,10120 30012,10120 30012,10120 30012,8041.6 30012,8041.6"];
	Layer2_Device6_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30497,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage10_Attention	[label=Q_local,
		lp="29800,8969.8",
		pos="e,30645,7838.5 33650,10074 33650,9826.7 33650,7947 33650,7947 33650,7947 30645,7947 30645,7947 30645,7947 30645,7848.5 30645,7848.5"];
	Layer2_Device6_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31588,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage11_Attention	[label=Q_local,
		lp="33878,8873.3",
		pos="e,31665,7645.5 33672,10074 33672,9813.5 33672,7742 33672,7742 33672,7742 31665,7742 31665,7742 31665,7742 31665,7655.5 31665,7655.5"];
	Layer2_Device6_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31918,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage12_Attention	[label=Q_local,
		lp="34021,8776.8",
		pos="e,31848,7452.4 33694,10075 33694,9815.1 33694,7738 33694,7738 33694,7738 31848,7738 31848,7738 31848,7738 31848,7462.4 31848,7462.4"];
	Layer2_Device6_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33933,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage13_Attention	[label=Q_local,
		lp="34161,8680.3",
		pos="e,34106,7259.7 33956,10086 34044,10086 34106,10086 34106,10086 34106,10086 34106,7269.7 34106,7269.7"];
	Layer2_Device6_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31515,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage14_Attention	[label=Q_local,
		lp="29654,8583.8",
		pos="e,31592,7066.5 33956,10098 34092,10098 34195,10098 34195,10098 34195,10098 34195,7167 34195,7167 34195,7167 31592,7167 31592,7167 \
31592,7167 31592,7076.5 31592,7076.5"];
	Layer2_Device6_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33648,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_QKVProj -> Layer2_Device6_Stage15_Attention	[label=Q_local,
		lp="34279,8487.3",
		pos="e,33880,6896 33956,10104 34107,10104 34226,10104 34226,10104 34226,10104 34226,6896 34226,6896 34226,6896 33890,6896 33890,6896"];
	Layer2_Device6_Stage0_RecvKV -> Layer2_Device6_Stage0_Attention	[pos="e,32781,9768.6 32781,9931.5 32781,9931.5 32781,9778.6 32781,9778.6"];
	Layer2_Device6_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31604,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage0_RecvKV -> Layer2_Device6_Stage1_RecvKV	[label="Ring transfer",
		lp="31980,9838.3",
		pos="e,31832,9794.9 31832,9881.6 31832,9881.6 31832,9804.9 31832,9804.9"];
	Layer2_Device6_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="31041,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage0_Attention -> Layer2_Device6_Stage0_Accumulate	[pos="e,31041,9575.6 32813,9715.2 32813,9677.3 32813,9612 32813,9612 32813,9612 31041,9612 31041,9612 31041,9612 31041,9585.6 31041,9585.6"];
	Layer2_Device6_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30610,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage0_Accumulate -> Layer2_Device6_Stage1_Accumulate	[pos="e,30826,9382.4 30826,9522 30826,9522 30826,9392.4 30826,9392.4"];
	Layer2_Device6_Stage1_RecvKV -> Layer2_Device6_Stage1_Attention	[pos="e,30718,9575.7 30718,9741.7 30718,9741.7 30718,9585.7 30718,9585.7"];
	Layer2_Device6_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32250,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage1_RecvKV -> Layer2_Device6_Stage2_RecvKV	[label="Ring transfer",
		lp="31921,9645.3",
		pos="e,31927,9601.9 31927,9688.6 31927,9688.6 31927,9611.9 31927,9611.9"];
	Layer2_Device6_Stage1_Attention -> Layer2_Device6_Stage1_Accumulate	[pos="e,30565,9382.4 30565,9522 30565,9522 30565,9392.4 30565,9392.4"];
	Layer2_Device6_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30728,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage1_Accumulate -> Layer2_Device6_Stage2_Accumulate	[pos="e,30669,9189.4 30669,9329 30669,9329 30669,9199.4 30669,9199.4"];
	Layer2_Device6_Stage2_RecvKV -> Layer2_Device6_Stage2_Attention	[pos="e,31346,9382.7 31346,9548.7 31346,9548.7 31346,9392.7 31346,9392.7"];
	Layer2_Device6_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32340,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage2_RecvKV -> Layer2_Device6_Stage3_RecvKV	[label="Ring transfer",
		lp="32343,9452.3",
		pos="e,32295,9408.9 32295,9495.6 32295,9495.6 32295,9418.9 32295,9418.9"];
	Layer2_Device6_Stage2_Attention -> Layer2_Device6_Stage2_Accumulate	[pos="e,30930,9189.4 30930,9329 30930,9329 30930,9199.4 30930,9199.4"];
	Layer2_Device6_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30873,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage2_Accumulate -> Layer2_Device6_Stage3_Accumulate	[pos="e,30800,8996.4 30800,9136 30800,9136 30800,9006.4 30800,9006.4"];
	Layer2_Device6_Stage3_RecvKV -> Layer2_Device6_Stage3_Attention	[pos="e,33087,9189.7 33087,9355.7 33087,9355.7 33087,9199.7 33087,9199.7"];
	Layer2_Device6_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31937,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage3_RecvKV -> Layer2_Device6_Stage4_RecvKV	[label="Ring transfer",
		lp="32187,9259.3",
		pos="e,32138,9215.9 32138,9302.6 32138,9302.6 32138,9225.9 32138,9225.9"];
	Layer2_Device6_Stage3_Attention -> Layer2_Device6_Stage3_Accumulate	[pos="e,31061,8996.8 32958,9136.1 32958,9105 32958,9058 32958,9058 32958,9058 31061,9058 31061,9058 31061,9058 31061,9006.8 31061,9006.8"];
	Layer2_Device6_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30847,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage3_Accumulate -> Layer2_Device6_Stage4_Accumulate	[pos="e,30860,8803.4 30860,8943 30860,8943 30860,8813.4 30860,8813.4"];
	Layer2_Device6_Stage4_RecvKV -> Layer2_Device6_Stage4_Attention	[pos="e,30540,8996.5 31064,9123 30781,9123 30540,9123 30540,9123 30540,9123 30540,9006.5 30540,9006.5"];
	Layer2_Device6_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32082,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage4_RecvKV -> Layer2_Device6_Stage5_RecvKV	[label="Ring transfer",
		lp="32057,9066.3",
		pos="e,32010,9022.9 32010,9109.6 32010,9109.6 32010,9032.9 32010,9032.9"];
	Layer2_Device6_Stage4_Attention -> Layer2_Device6_Stage4_Accumulate	[pos="e,30628,8803.5 30584,8969 30611,8969 30628,8969 30628,8969 30628,8969 30628,8813.5 30628,8813.5"];
	Layer2_Device6_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32853,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage4_Accumulate -> Layer2_Device6_Stage5_Accumulate	[pos="e,32799,8610.3 30901,8750.2 30901,8734.6 30901,8718 30901,8718 30901,8718 32799,8718 32799,8718 32799,8718 32799,8620.3 32799,8620.3"];
	Layer2_Device6_Stage5_RecvKV -> Layer2_Device6_Stage5_Attention	[pos="e,33033,8776 32989,8969.7 32989,8963.8 32989,8776 32989,8776 32989,8776 33023,8776 33023,8776"];
	Layer2_Device6_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32056,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage5_RecvKV -> Layer2_Device6_Stage6_RecvKV	[label="Ring transfer",
		lp="32117,8873.3",
		pos="e,32069,8829.9 32069,8916.6 32069,8916.6 32069,8839.9 32069,8839.9"];
	Layer2_Device6_Stage5_Attention -> Layer2_Device6_Stage5_Accumulate	[pos="e,33059,8610.4 33059,8750 33059,8750 33059,8620.4 33059,8620.4"];
	Layer2_Device6_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32853,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage5_Accumulate -> Layer2_Device6_Stage6_Accumulate	[pos="e,32853,8417.4 32853,8557 32853,8557 32853,8427.4 32853,8427.4"];
	Layer2_Device6_Stage6_RecvKV -> Layer2_Device6_Stage6_Attention	[pos="e,33320,8610.5 32647,8737 32976,8737 33320,8737 33320,8737 33320,8737 33320,8620.5 33320,8620.5"];
	Layer2_Device6_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31644,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage6_RecvKV -> Layer2_Device6_Stage7_RecvKV	[label="Ring transfer",
		lp="31898,8680.3",
		pos="e,31850,8636.9 31850,8723.6 31850,8723.6 31850,8646.9 31850,8646.9"];
	Layer2_Device6_Stage6_Attention -> Layer2_Device6_Stage6_Accumulate	[pos="e,33085,8381 33252,8557.3 33252,8502.3 33252,8381 33252,8381 33252,8381 33095,8381 33095,8381"];
	Layer2_Device6_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32608,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage6_Accumulate -> Layer2_Device6_Stage7_Accumulate	[pos="e,32730,8224.4 32730,8364 32730,8364 32730,8234.4 32730,8234.4"];
	Layer2_Device6_Stage7_RecvKV -> Layer2_Device6_Stage7_Attention	[pos="e,30641,8417.5 30771,8544 30692,8544 30641,8544 30641,8544 30641,8544 30641,8427.5 30641,8427.5"];
	Layer2_Device6_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31644,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage7_RecvKV -> Layer2_Device6_Stage8_RecvKV	[label="Ring transfer",
		lp="31692,8487.3",
		pos="e,31644,8443.9 31644,8530.6 31644,8530.6 31644,8453.9 31644,8453.9"];
	Layer2_Device6_Stage7_Attention -> Layer2_Device6_Stage7_Accumulate	[pos="e,32470,8224.4 30573,8364 30573,8327.4 30573,8266 30573,8266 30573,8266 32470,8266 32470,8266 32470,8266 32470,8234.4 32470,8234.4"];
	Layer2_Device6_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32608,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage7_Accumulate -> Layer2_Device6_Stage8_Accumulate	[pos="e,32608,8031.4 32608,8171 32608,8171 32608,8041.4 32608,8041.4"];
	Layer2_Device6_Stage8_RecvKV -> Layer2_Device6_Stage8_Attention	[pos="e,32991,8224.5 32234,8351 32596,8351 32991,8351 32991,8351 32991,8351 32991,8234.5 32991,8234.5"];
	Layer2_Device6_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31399,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage8_RecvKV -> Layer2_Device6_Stage9_RecvKV	[label="Ring transfer",
		lp="31522,8294.3",
		pos="e,31522,8250.9 31522,8337.6 31522,8337.6 31522,8260.9 31522,8260.9"];
	Layer2_Device6_Stage8_Attention -> Layer2_Device6_Stage8_Accumulate	[pos="e,32840,8004 33022,8171.3 33022,8118.2 33022,8004 33022,8004 33022,8004 32850,8004 32850,8004"];
	Layer2_Device6_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31018,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage8_Accumulate -> Layer2_Device6_Stage9_Accumulate	[pos="e,31095,7838.6 32608,7978.1 32608,7947.3 32608,7901 32608,7901 32608,7901 31095,7901 31095,7901 31095,7901 31095,7848.6 31095,7848.6"];
	Layer2_Device6_Stage9_RecvKV -> Layer2_Device6_Stage9_Attention	[pos="e,30312,8031.5 30526,8158 30400,8158 30312,8158 30312,8158 30312,8158 30312,8041.5 30312,8041.5"];
	Layer2_Device6_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31399,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage9_RecvKV -> Layer2_Device6_Stage10_RecvKV	[label="Ring transfer",
		lp="31447,8101.3",
		pos="e,31399,8057.9 31399,8144.6 31399,8144.6 31399,8067.9 31399,8067.9"];
	Layer2_Device6_Stage9_Attention -> Layer2_Device6_Stage9_Accumulate	[pos="e,30941,7838.4 30344,7978.2 30344,7941 30344,7878 30344,7878 30344,7878 30941,7878 30941,7878 30941,7878 30941,7848.4 30941,7848.4"];
	Layer2_Device6_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31018,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage9_Accumulate -> Layer2_Device6_Stage10_Accumulate	[pos="e,31018,7645.4 31018,7785 31018,7785 31018,7655.4 31018,7655.4"];
	Layer2_Device6_Stage10_RecvKV -> Layer2_Device6_Stage10_Attention	[pos="e,30562,7838.5 30562,7951.6 30562,7951.6 30562,7848.5 30562,7848.5"];
	Layer2_Device6_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32227,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage10_RecvKV -> Layer2_Device6_Stage11_RecvKV	[label="Ring transfer",
		lp="31716,7908.3",
		pos="e,31813,7864.9 31813,7951.6 31813,7951.6 31813,7874.9 31813,7874.9"];
	Layer2_Device6_Stage10_Attention -> Layer2_Device6_Stage10_Accumulate	[pos="e,30786,7618 30604,7785.3 30604,7732.2 30604,7618 30604,7618 30604,7618 30776,7618 30776,7618"];
	Layer2_Device6_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31397,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage10_Accumulate -> Layer2_Device6_Stage11_Accumulate	[pos="e,31208,7452.4 31208,7592 31208,7592 31208,7462.4 31208,7462.4"];
	Layer2_Device6_Stage11_RecvKV -> Layer2_Device6_Stage11_Attention	[pos="e,31511,7645.5 31511,7758.6 31511,7758.6 31511,7655.5 31511,7655.5"];
	Layer2_Device6_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32797,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage11_RecvKV -> Layer2_Device6_Stage12_RecvKV	[label="Ring transfer",
		lp="32560,7715.3",
		pos="e,32512,7671.9 32512,7758.6 32512,7758.6 32512,7681.9 32512,7681.9"];
	Layer2_Device6_Stage11_Attention -> Layer2_Device6_Stage11_Accumulate	[pos="e,31492,7452.4 31492,7592 31492,7592 31492,7462.4 31492,7462.4"];
	Layer2_Device6_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31515,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage11_Accumulate -> Layer2_Device6_Stage12_Accumulate	[pos="e,31456,7259.4 31456,7399 31456,7399 31456,7269.4 31456,7269.4"];
	Layer2_Device6_Stage12_RecvKV -> Layer2_Device6_Stage12_Attention	[pos="e,32013,7452.5 32013,7565.6 32013,7565.6 32013,7462.5 32013,7462.5"];
	Layer2_Device6_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33127,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage12_RecvKV -> Layer2_Device6_Stage13_RecvKV	[label="Ring transfer",
		lp="33152,7522.3",
		pos="e,32962,7478.9 32962,7565.6 32962,7565.6 32962,7488.9 32962,7488.9"];
	Layer2_Device6_Stage12_Attention -> Layer2_Device6_Stage12_Accumulate	[pos="e,31716,7259.4 31716,7399 31716,7399 31716,7269.4 31716,7269.4"];
	Layer2_Device6_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32036,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage12_Accumulate -> Layer2_Device6_Stage13_Accumulate	[pos="e,31804,7039 31747,7232 31764,7232 31775,7232 31775,7232 31775,7232 31775,7039 31775,7039 31775,7039 31794,7039 31794,7039"];
	Layer2_Device6_Stage13_RecvKV -> Layer2_Device6_Stage13_Attention	[pos="e,33874,7259.7 33874,7425.7 33874,7425.7 33874,7269.7 33874,7269.7"];
	Layer2_Device6_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32724,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage13_RecvKV -> Layer2_Device6_Stage14_RecvKV	[label="Ring transfer",
		lp="33041,7329.3",
		pos="e,32926,7285.9 32926,7372.6 32926,7372.6 32926,7295.9 32926,7295.9"];
	Layer2_Device6_Stage13_Attention -> Layer2_Device6_Stage13_Accumulate	[pos="e,32036,7066.5 33933,7206.1 33933,7172.7 33933,7120 33933,7120 33933,7120 32036,7120 32036,7120 32036,7120 32036,7076.5 32036,7076.5"];
	Layer2_Device6_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32036,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage13_Accumulate -> Layer2_Device6_Stage14_Accumulate	[pos="e,32036,6913.8 32036,7013.2 32036,7013.2 32036,6923.8 32036,6923.8"];
	Layer2_Device6_Stage14_RecvKV -> Layer2_Device6_Stage14_Attention	[pos="e,31438,7066.5 31851,7193 31622,7193 31438,7193 31438,7193 31438,7193 31438,7076.5 31438,7076.5"];
	Layer2_Device6_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33245,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device6_Stage14_RecvKV -> Layer2_Device6_Stage15_RecvKV	[label="Ring transfer",
		lp="33032,7136.3",
		pos="e,32984,7092.9 32984,7179.6 32984,7179.6 32984,7102.9 32984,7102.9"];
	Layer2_Device6_Stage14_Attention -> Layer2_Device6_Stage14_Accumulate	[pos="e,31804,6878 31539,7013.1 31539,6967.1 31539,6878 31539,6878 31539,6878 31794,6878 31794,6878"];
	Layer2_Device6_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="33648,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device6_Stage14_Accumulate -> Layer2_Device6_Stage15_Accumulate	[pos="e,33416,6761 32036,6860.5 32036,6823.4 32036,6761 32036,6761 32036,6761 33406,6761 33406,6761"];
	Layer2_Device6_Stage15_RecvKV -> Layer2_Device6_Stage15_Attention	[pos="e,33648,6913.9 33648,6986.5 33648,6986.5 33648,6923.9 33648,6923.9"];
	Layer2_Device6_Stage15_Attention -> Layer2_Device6_Stage15_Accumulate	[pos="e,33648,6788 33648,6860.6 33648,6860.6 33648,6798 33648,6798"];
	Layer2_Device6_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33806,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device6_Stage15_Accumulate -> Layer2_Device6_ConcatHeads	[pos="e,33730,6662 33730,6734.6 33730,6734.6 33730,6672 33730,6672"];
	Layer2_Device6_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34043,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device6_ConcatHeads -> Layer2_Device6_OutputProj	[pos="e,33931,6536 33931,6608.6 33931,6608.6 33931,6546 33931,6546"];
	Layer2_Device6_OutputProj -> Layer2_Device6_Residual1	[pos="e,34087,6410 34087,6482.6 34087,6482.6 34087,6420 34087,6420"];
	Layer2_Device6_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34190,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device6_Residual1 -> Layer2_Device6_LayerNorm2	[pos="e,34190,6284 34190,6356.6 34190,6356.6 34190,6294 34190,6294"];
	Layer2_Device6_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="34425,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device6_Residual1 -> Layer2_Device6_Residual2	[pos="e,34563,5601.9 34563,6356.4 34563,6356.4 34563,5611.9 34563,5611.9"];
	Layer2_Device6_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33897,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device6_LayerNorm2 -> Layer2_Device6_GateProj	[pos="e,34050,6158 34050,6230.6 34050,6230.6 34050,6168 34050,6168"];
	Layer2_Device6_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="34210,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device6_LayerNorm2 -> Layer2_Device6_UpProj	[pos="e,34262,6069.1 34262,6230.5 34262,6230.5 34262,6079.1 34262,6079.1"];
	Layer2_Device6_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33897,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device6_GateProj -> Layer2_Device6_Activation	[pos="e,33828,5979.9 33828,6104.7 33828,6104.7 33828,5989.9 33828,5989.9"];
	Layer2_Device6_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="33993,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device6_UpProj -> Layer2_Device6_ElemMul	[pos="e,34263,5854.1 34263,6015.5 34263,6015.5 34263,5864.1 34263,5864.1"];
	Layer2_Device6_Activation -> Layer2_Device6_ElemMul	[pos="e,33897,5854 33897,5926.6 33897,5926.6 33897,5864 33897,5864"];
	Layer2_Device6_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34089,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device6_ElemMul -> Layer2_Device6_DownProj	[pos="e,34089,5728 34089,5800.6 34089,5800.6 34089,5738 34089,5738"];
	Layer2_Device6_DownProj -> Layer2_Device6_Residual2	[pos="e,34173,5602 34173,5674.6 34173,5674.6 34173,5612 34173,5612"];
	Layer2_Device6_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34425,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device6_Residual2 -> Layer2_Device6_Output	[pos="e,34425,5475.9 34425,5548.6 34425,5548.6 34425,5485.9 34425,5485.9"];
	Layer3_Device6_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 6 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34425,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device6_Output -> Layer3_Device6_Input	[pos="e,34425,5328 34425,5400.6 34425,5400.6 34425,5338 34425,5338"];
	Layer2_Device7_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38744,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device7_Input -> Layer2_Device7_LayerNorm1	[pos="e,38805,10254 38805,10331 38805,10331 38805,10264 38805,10264"];
	Layer2_Device7_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="39224,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device7_Input -> Layer2_Device7_Residual1	[pos="e,39168,6409.8 39168,10338 39168,10338 39168,6419.8 39168,6419.8"];
	Layer2_Device7_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38202,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device7_LayerNorm1 -> Layer2_Device7_QKVProj	[pos="e,38691,10128 38691,10201 38691,10201 38691,10138 38691,10138"];
	Layer2_Device7_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36956,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage0_RecvKV	[label="Local K,V",
		lp="37456,10031",
		pos="e,37714,9988.1 37714,10075 37714,10075 37714,9998.1 37714,9998.1"];
	Layer2_Device7_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37746,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage0_Attention	[label=Q_local,
		lp="38018,9934.8",
		pos="e,37927,9768.7 37927,10075 37927,10075 37927,9778.7 37927,9778.7"];
	Layer2_Device7_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35416,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage1_Attention	[label=Q_local,
		lp="35480,9838.3",
		pos="e,35477,9575.5 37552,10080 36746,10080 35477,10080 35477,10080 35477,10080 35477,9585.5 35477,9585.5"];
	Layer2_Device7_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36027,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage2_Attention	[label=Q_local,
		lp="38158,9741.8",
		pos="e,36197,9382.5 38022,10075 38022,9978.5 38022,9657 38022,9657 38022,9657 36197,9657 36197,9657 36197,9657 36197,9392.5 36197,9392.5"];
	Layer2_Device7_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38042,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage3_Attention	[label=Q_local,
		lp="38298,9645.3",
		pos="e,38215,9189.6 38215,10075 38215,10075 38215,9199.6 38215,9199.6"];
	Layer2_Device7_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35248,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage4_Attention	[label=Q_local,
		lp="35100,9548.8",
		pos="e,35100,8996.4 37552,10092 36643,10092 35100,10092 35100,10092 35100,10092 35100,9006.4 35100,9006.4"];
	Layer2_Device7_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38167,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage5_Attention	[label=Q_local,
		lp="38402,9452.3",
		pos="e,38336,8803.6 38336,10075 38336,10075 38336,8813.6 38336,8813.6"];
	Layer2_Device7_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38194,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage6_Attention	[label=Q_local,
		lp="38534,9355.8",
		pos="e,38412,8610.6 38412,10075 38412,10075 38412,8620.6 38412,8620.6"];
	Layer2_Device7_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35255,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage7_Attention	[label=Q_local,
		lp="34954,9259.3",
		pos="e,35023,8399 37552,10110 36605,10110 34954,10110 34954,10110 34954,10110 34954,8399 34954,8399 34954,8399 35013,8399 35013,8399"];
	Layer2_Device7_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37999,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage8_Attention	[label=Q_local,
		lp="38659,9162.8",
		pos="e,38231,8197 38473,10075 38473,9844.7 38473,8197 38473,8197 38473,8197 38241,8197 38241,8197"];
	Layer2_Device7_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35060,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage9_Attention	[label=Q_local,
		lp="34836,9066.3",
		pos="e,34891,8031.5 37552,10116 36588,10116 34891,10116 34891,10116 34891,10116 34891,8041.5 34891,8041.5"];
	Layer2_Device7_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35196,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage10_Attention	[label=Q_local,
		lp="34696,8969.8",
		pos="e,34964,7811 37552,10121 36570,10121 34822,10121 34822,10121 34822,10121 34822,7811 34822,7811 34822,7811 34954,7811 34954,7811"];
	Layer2_Device7_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36533,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage11_Attention	[label=Q_local,
		lp="38780,8873.3",
		pos="e,36610,7645.6 38567,10074 38567,9811.5 38567,7708 38567,7708 38567,7708 36610,7708 36610,7708 36610,7708 36610,7655.6 36610,7655.6"];
	Layer2_Device7_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36820,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage12_Attention	[label=Q_local,
		lp="38923,8776.8",
		pos="e,36793,7452.4 38614,10074 38614,9811.3 38614,7706 38614,7706 38614,7706 36793,7706 36793,7706 36793,7706 36793,7462.4 36793,7462.4"];
	Layer2_Device7_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38835,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage13_Attention	[label=Q_local,
		lp="39063,8680.3",
		pos="e,38953,7259.7 38852,10089 38913,10089 38953,10089 38953,10089 38953,10089 38953,7269.7 38953,7269.7"];
	Layer2_Device7_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36417,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage14_Attention	[label=Q_local,
		lp="34550,8583.8",
		pos="e,36185,7039 38520,10075 38520,9815.2 38520,7756 38520,7756 38520,7756 36037,7756 36037,7756 36037,7756 36037,7039 36037,7039 36037,\
7039 36175,7039 36175,7039"];
	Layer2_Device7_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38550,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_QKVProj -> Layer2_Device7_Stage15_Attention	[label=Q_local,
		lp="39181,8487.3",
		pos="e,38782,6896 38852,10105 38979,10105 39074,10105 39074,10105 39074,10105 39074,6896 39074,6896 39074,6896 38792,6896 38792,6896"];
	Layer2_Device7_Stage0_RecvKV -> Layer2_Device7_Stage0_Attention	[pos="e,37695,9768.7 37695,9934.7 37695,9934.7 37695,9778.7 37695,9778.7"];
	Layer2_Device7_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36537,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage0_RecvKV -> Layer2_Device7_Stage1_RecvKV	[label="Ring transfer",
		lp="36794,9838.3",
		pos="e,36746,9794.9 36746,9881.6 36746,9881.6 36746,9804.9 36746,9804.9"];
	Layer2_Device7_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="35937,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage0_Attention -> Layer2_Device7_Stage0_Accumulate	[pos="e,35937,9575.5 37746,9715.2 37746,9690.9 37746,9659 37746,9659 37746,9659 35937,9659 35937,9659 35937,9659 35937,9585.5 35937,9585.5"];
	Layer2_Device7_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35506,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage0_Accumulate -> Layer2_Device7_Stage1_Accumulate	[pos="e,35722,9382.4 35722,9522 35722,9522 35722,9392.4 35722,9392.4"];
	Layer2_Device7_Stage1_RecvKV -> Layer2_Device7_Stage1_Attention	[pos="e,35632,9575.7 35632,9741.7 35632,9741.7 35632,9585.7 35632,9585.7"];
	Layer2_Device7_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37146,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage1_RecvKV -> Layer2_Device7_Stage2_RecvKV	[label="Ring transfer",
		lp="36854,9645.3",
		pos="e,36842,9601.9 36842,9688.6 36842,9688.6 36842,9611.9 36842,9611.9"];
	Layer2_Device7_Stage1_Attention -> Layer2_Device7_Stage1_Accumulate	[pos="e,35461,9382.4 35461,9522 35461,9522 35461,9392.4 35461,9392.4"];
	Layer2_Device7_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35624,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage1_Accumulate -> Layer2_Device7_Stage2_Accumulate	[pos="e,35565,9189.4 35565,9329 35565,9329 35565,9199.4 35565,9199.4"];
	Layer2_Device7_Stage2_RecvKV -> Layer2_Device7_Stage2_Attention	[pos="e,36242,9382.7 36242,9548.7 36242,9548.7 36242,9392.7 36242,9392.7"];
	Layer2_Device7_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37236,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage2_RecvKV -> Layer2_Device7_Stage3_RecvKV	[label="Ring transfer",
		lp="37239,9452.3",
		pos="e,37191,9408.9 37191,9495.6 37191,9495.6 37191,9418.9 37191,9418.9"];
	Layer2_Device7_Stage2_Attention -> Layer2_Device7_Stage2_Accumulate	[pos="e,35826,9189.4 35826,9329 35826,9329 35826,9199.4 35826,9199.4"];
	Layer2_Device7_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35769,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage2_Accumulate -> Layer2_Device7_Stage3_Accumulate	[pos="e,35696,8996.4 35696,9136 35696,9136 35696,9006.4 35696,9006.4"];
	Layer2_Device7_Stage3_RecvKV -> Layer2_Device7_Stage3_Attention	[pos="e,37983,9189.7 37983,9355.7 37983,9355.7 37983,9199.7 37983,9199.7"];
	Layer2_Device7_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36833,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage3_RecvKV -> Layer2_Device7_Stage4_RecvKV	[label="Ring transfer",
		lp="37150,9259.3",
		pos="e,37034,9215.9 37034,9302.6 37034,9302.6 37034,9225.9 37034,9225.9"];
	Layer2_Device7_Stage3_Attention -> Layer2_Device7_Stage3_Accumulate	[pos="e,35957,8996.8 37854,9136.1 37854,9106.5 37854,9063 37854,9063 37854,9063 35957,9063 35957,9063 35957,9063 35957,9006.8 35957,9006.8"];
	Layer2_Device7_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35749,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage3_Accumulate -> Layer2_Device7_Stage4_Accumulate	[pos="e,35759,8803.4 35759,8943 35759,8943 35759,8813.4 35759,8813.4"];
	Layer2_Device7_Stage4_RecvKV -> Layer2_Device7_Stage4_Attention	[pos="e,35436,8996.5 35960,9123 35677,9123 35436,9123 35436,9123 35436,9123 35436,9006.5 35436,9006.5"];
	Layer2_Device7_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36978,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage4_RecvKV -> Layer2_Device7_Stage5_RecvKV	[label="Ring transfer",
		lp="36953,9066.3",
		pos="e,36906,9022.9 36906,9109.6 36906,9109.6 36906,9032.9 36906,9032.9"];
	Layer2_Device7_Stage4_Attention -> Layer2_Device7_Stage4_Accumulate	[pos="e,35528,8803.5 35480,8969 35508,8969 35528,8969 35528,8969 35528,8969 35528,8813.5 35528,8813.5"];
	Layer2_Device7_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37673,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage4_Accumulate -> Layer2_Device7_Stage5_Accumulate	[pos="e,37660,8610.7 35762,8750.2 35762,8716.3 35762,8662 35762,8662 35762,8662 37660,8662 37660,8662 37660,8662 37660,8620.7 37660,8620.7"];
	Layer2_Device7_Stage5_RecvKV -> Layer2_Device7_Stage5_Attention	[pos="e,37935,8776 37888,8969.7 37888,8963.8 37888,8776 37888,8776 37888,8776 37925,8776 37925,8776"];
	Layer2_Device7_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36958,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage5_RecvKV -> Layer2_Device7_Stage6_RecvKV	[label="Ring transfer",
		lp="37016,8873.3",
		pos="e,36968,8829.9 36968,8916.6 36968,8916.6 36968,8839.9 36968,8839.9"];
	Layer2_Device7_Stage5_Attention -> Layer2_Device7_Stage5_Accumulate	[pos="e,37905,8583 37949,8750.3 37949,8697.2 37949,8583 37949,8583 37949,8583 37915,8583 37915,8583"];
	Layer2_Device7_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37673,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage5_Accumulate -> Layer2_Device7_Stage6_Accumulate	[pos="e,37673,8417.4 37673,8557 37673,8557 37673,8427.4 37673,8427.4"];
	Layer2_Device7_Stage6_RecvKV -> Layer2_Device7_Stage6_Attention	[pos="e,38180,8610.5 37548,8737 37861,8737 38180,8737 38180,8737 38180,8737 38180,8620.5 38180,8620.5"];
	Layer2_Device7_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36464,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage6_RecvKV -> Layer2_Device7_Stage7_RecvKV	[label="Ring transfer",
		lp="36848,8680.3",
		pos="e,36711,8636.9 36711,8723.6 36711,8723.6 36711,8646.9 36711,8646.9"];
	Layer2_Device7_Stage6_Attention -> Layer2_Device7_Stage6_Accumulate	[pos="e,37905,8381 38096,8557.3 38096,8502.3 38096,8381 38096,8381 38096,8381 37915,8381 37915,8381"];
	Layer2_Device7_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37478,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage6_Accumulate -> Layer2_Device7_Stage7_Accumulate	[pos="e,37576,8224.4 37576,8364 37576,8364 37576,8234.4 37576,8234.4"];
	Layer2_Device7_Stage7_RecvKV -> Layer2_Device7_Stage7_Attention	[pos="e,35483,8417.5 35591,8544 35525,8544 35483,8544 35483,8544 35483,8544 35483,8427.5 35483,8427.5"];
	Layer2_Device7_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36464,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage7_RecvKV -> Layer2_Device7_Stage8_RecvKV	[label="Ring transfer",
		lp="36512,8487.3",
		pos="e,36464,8443.9 36464,8530.6 36464,8530.6 36464,8453.9 36464,8453.9"];
	Layer2_Device7_Stage7_Attention -> Layer2_Device7_Stage7_Accumulate	[pos="e,37315,8224.8 35418,8364.1 35418,8334.5 35418,8291 35418,8291 35418,8291 37315,8291 37315,8291 37315,8291 37315,8234.8 37315,8234.8"];
	Layer2_Device7_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37478,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage7_Accumulate -> Layer2_Device7_Stage8_Accumulate	[pos="e,37478,8031.4 37478,8171 37478,8171 37478,8041.4 37478,8041.4"];
	Layer2_Device7_Stage8_RecvKV -> Layer2_Device7_Stage8_Attention	[pos="e,37836,8224.5 37054,8351 37426,8351 37836,8351 37836,8351 37836,8351 37836,8234.5 37836,8234.5"];
	Layer2_Device7_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36269,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage8_RecvKV -> Layer2_Device7_Stage9_RecvKV	[label="Ring transfer",
		lp="36414,8294.3",
		pos="e,36366,8250.9 36366,8337.6 36366,8337.6 36366,8260.9 36366,8260.9"];
	Layer2_Device7_Stage8_Attention -> Layer2_Device7_Stage8_Accumulate	[pos="e,37710,8004 37807,8171.3 37807,8118.2 37807,8004 37807,8004 37807,8004 37720,8004 37720,8004"];
	Layer2_Device7_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35717,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage8_Accumulate -> Layer2_Device7_Stage9_Accumulate	[pos="e,35794,7838.4 37478,7977.9 37478,7954.4 37478,7924 37478,7924 37478,7924 35794,7924 35794,7924 35794,7924 35794,7848.4 35794,7848.4"];
	Layer2_Device7_Stage9_RecvKV -> Layer2_Device7_Stage9_Attention	[pos="e,35158,8031.5 35396,8158 35257,8158 35158,8158 35158,8158 35158,8158 35158,8041.5 35158,8041.5"];
	Layer2_Device7_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36269,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage9_RecvKV -> Layer2_Device7_Stage10_RecvKV	[label="Ring transfer",
		lp="36317,8101.3",
		pos="e,36269,8057.9 36269,8144.6 36269,8144.6 36269,8067.9 36269,8067.9"];
	Layer2_Device7_Stage9_Attention -> Layer2_Device7_Stage9_Accumulate	[pos="e,35640,7838.6 35128,7978 35128,7934 35128,7851 35128,7851 35128,7851 35640,7851 35640,7851 35640,7851 35640,7848.6 35640,7848.6"];
	Layer2_Device7_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35717,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage9_Accumulate -> Layer2_Device7_Stage10_Accumulate	[pos="e,35717,7645.4 35717,7785 35717,7785 35717,7655.4 35717,7655.4"];
	Layer2_Device7_Stage10_RecvKV -> Layer2_Device7_Stage10_Attention	[pos="e,35388,7838.7 35388,8004.7 35388,8004.7 35388,7848.7 35388,7848.7"];
	Layer2_Device7_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36926,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage10_RecvKV -> Layer2_Device7_Stage11_RecvKV	[label="Ring transfer",
		lp="36684,7908.3",
		pos="e,36598,7864.9 36598,7951.6 36598,7951.6 36598,7874.9 36598,7874.9"];
	Layer2_Device7_Stage10_Attention -> Layer2_Device7_Stage10_Accumulate	[pos="e,35485,7618 35354,7785.3 35354,7732.2 35354,7618 35354,7618 35354,7618 35475,7618 35475,7618"];
	Layer2_Device7_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36299,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage10_Accumulate -> Layer2_Device7_Stage11_Accumulate	[pos="e,36184,7452.5 35949,7618 36066,7618 36184,7618 36184,7618 36184,7618 36184,7462.5 36184,7462.5"];
	Layer2_Device7_Stage11_RecvKV -> Layer2_Device7_Stage11_Attention	[pos="e,36456,7645.5 36456,7758.6 36456,7758.6 36456,7655.5 36456,7655.5"];
	Layer2_Device7_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37742,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage11_RecvKV -> Layer2_Device7_Stage12_RecvKV	[label="Ring transfer",
		lp="37382,7715.3",
		pos="e,37334,7671.9 37334,7758.6 37334,7758.6 37334,7681.9 37334,7681.9"];
	Layer2_Device7_Stage11_Attention -> Layer2_Device7_Stage11_Accumulate	[pos="e,36416,7452.4 36416,7592 36416,7592 36416,7462.4 36416,7462.4"];
	Layer2_Device7_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36417,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage11_Accumulate -> Layer2_Device7_Stage12_Accumulate	[pos="e,36358,7259.4 36358,7399 36358,7399 36358,7269.4 36358,7269.4"];
	Layer2_Device7_Stage12_RecvKV -> Layer2_Device7_Stage12_Attention	[pos="e,36937,7452.5 36937,7565.6 36937,7565.6 36937,7462.5 36937,7462.5"];
	Layer2_Device7_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="38029,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage12_RecvKV -> Layer2_Device7_Stage13_RecvKV	[label="Ring transfer",
		lp="37934,7522.3",
		pos="e,37886,7478.9 37886,7565.6 37886,7565.6 37886,7488.9 37886,7488.9"];
	Layer2_Device7_Stage12_Attention -> Layer2_Device7_Stage12_Accumulate	[pos="e,36618,7259.4 36618,7399 36618,7399 36618,7269.4 36618,7269.4"];
	Layer2_Device7_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36938,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage12_Accumulate -> Layer2_Device7_Stage13_Accumulate	[pos="e,36706,7039 36649,7232 36666,7232 36677,7232 36677,7232 36677,7232 36677,7039 36677,7039 36677,7039 36696,7039 36696,7039"];
	Layer2_Device7_Stage13_RecvKV -> Layer2_Device7_Stage13_Attention	[pos="e,38776,7259.7 38776,7425.7 38776,7425.7 38776,7269.7 38776,7269.7"];
	Layer2_Device7_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37626,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage13_RecvKV -> Layer2_Device7_Stage14_RecvKV	[label="Ring transfer",
		lp="37810,7329.3",
		pos="e,37828,7285.9 37828,7372.6 37828,7372.6 37828,7295.9 37828,7295.9"];
	Layer2_Device7_Stage13_Attention -> Layer2_Device7_Stage13_Accumulate	[pos="e,36938,7066.5 38835,7206.2 38835,7174 38835,7124 38835,7124 38835,7124 36938,7124 36938,7124 36938,7124 36938,7076.5 36938,7076.5"];
	Layer2_Device7_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36938,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage13_Accumulate -> Layer2_Device7_Stage14_Accumulate	[pos="e,36938,6913.8 36938,7013.2 36938,7013.2 36938,6923.8 36938,6923.8"];
	Layer2_Device7_Stage14_RecvKV -> Layer2_Device7_Stage14_Attention	[pos="e,36417,7066.5 36753,7193 36563,7193 36417,7193 36417,7193 36417,7193 36417,7076.5 36417,7076.5"];
	Layer2_Device7_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="38147,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device7_Stage14_RecvKV -> Layer2_Device7_Stage15_RecvKV	[label="Ring transfer",
		lp="37934,7136.3",
		pos="e,37886,7092.9 37886,7179.6 37886,7179.6 37886,7102.9 37886,7102.9"];
	Layer2_Device7_Stage14_Attention -> Layer2_Device7_Stage14_Accumulate	[pos="e,36706,6878 36417,7013.1 36417,6967.1 36417,6878 36417,6878 36417,6878 36696,6878 36696,6878"];
	Layer2_Device7_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="38550,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device7_Stage14_Accumulate -> Layer2_Device7_Stage15_Accumulate	[pos="e,38318,6761 36938,6860.5 36938,6823.4 36938,6761 36938,6761 36938,6761 38308,6761 38308,6761"];
	Layer2_Device7_Stage15_RecvKV -> Layer2_Device7_Stage15_Attention	[pos="e,38550,6913.9 38550,6986.5 38550,6986.5 38550,6923.9 38550,6923.9"];
	Layer2_Device7_Stage15_Attention -> Layer2_Device7_Stage15_Accumulate	[pos="e,38550,6788 38550,6860.6 38550,6860.6 38550,6798 38550,6798"];
	Layer2_Device7_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38756,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device7_Stage15_Accumulate -> Layer2_Device7_ConcatHeads	[pos="e,38656,6662 38656,6734.6 38656,6734.6 38656,6672 38656,6672"];
	Layer2_Device7_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38868,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device7_ConcatHeads -> Layer2_Device7_OutputProj	[pos="e,38818,6536 38818,6608.6 38818,6608.6 38818,6546 38818,6546"];
	Layer2_Device7_OutputProj -> Layer2_Device7_Residual1	[pos="e,38959,6410 38959,6482.6 38959,6482.6 38959,6420 38959,6420"];
	Layer2_Device7_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39026,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device7_Residual1 -> Layer2_Device7_LayerNorm2	[pos="e,39038,6284 39038,6356.6 39038,6356.6 39038,6294 39038,6294"];
	Layer2_Device7_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="39279,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device7_Residual1 -> Layer2_Device7_Residual2	[pos="e,39441,5601.9 39441,6356.4 39441,6356.4 39441,5611.9 39441,5611.9"];
	Layer2_Device7_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38733,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device7_LayerNorm2 -> Layer2_Device7_GateProj	[pos="e,38886,6158 38886,6230.6 38886,6230.6 38886,6168 38886,6168"];
	Layer2_Device7_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="39046,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device7_LayerNorm2 -> Layer2_Device7_UpProj	[pos="e,39098,6069.1 39098,6230.5 39098,6230.5 39098,6079.1 39098,6079.1"];
	Layer2_Device7_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38733,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device7_GateProj -> Layer2_Device7_Activation	[pos="e,38664,5979.9 38664,6104.7 38664,6104.7 38664,5989.9 38664,5989.9"];
	Layer2_Device7_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="38829,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device7_UpProj -> Layer2_Device7_ElemMul	[pos="e,39099,5854.1 39099,6015.5 39099,6015.5 39099,5864.1 39099,5864.1"];
	Layer2_Device7_Activation -> Layer2_Device7_ElemMul	[pos="e,38733,5854 38733,5926.6 38733,5926.6 38733,5864 38733,5864"];
	Layer2_Device7_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38972,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device7_ElemMul -> Layer2_Device7_DownProj	[pos="e,38972,5728 38972,5800.6 38972,5800.6 38972,5738 38972,5738"];
	Layer2_Device7_DownProj -> Layer2_Device7_Residual2	[pos="e,39042,5602 39042,5674.6 39042,5674.6 39042,5612 39042,5612"];
	Layer2_Device7_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39279,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device7_Residual2 -> Layer2_Device7_Output	[pos="e,39279,5475.9 39279,5548.6 39279,5548.6 39279,5485.9 39279,5485.9"];
	Layer3_Device7_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 7 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39279,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device7_Output -> Layer3_Device7_Input	[pos="e,39279,5328 39279,5400.6 39279,5400.6 39279,5338 39279,5338"];
	Layer2_Device8_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43515,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device8_Input -> Layer2_Device8_LayerNorm1	[pos="e,43718,10254 43718,10354 43718,10354 43718,10264 43718,10264"];
	Layer2_Device8_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="43546,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device8_Input -> Layer2_Device8_Residual1	[pos="e,43933,6383 44192,10334 44192,9968.4 44192,6383 44192,6383 44192,6383 43943,6383 43943,6383"];
	Layer2_Device8_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43098,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device8_LayerNorm1 -> Layer2_Device8_QKVProj	[pos="e,43515,10128 43515,10201 43515,10201 43515,10138 43515,10138"];
	Layer2_Device8_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41852,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage0_RecvKV	[label="Local K,V",
		lp="42352,10031",
		pos="e,42610,9988.1 42610,10075 42610,10075 42610,9998.1 42610,9998.1"];
	Layer2_Device8_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42642,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage0_Attention	[label=Q_local,
		lp="42914,9934.8",
		pos="e,42823,9768.7 42823,10075 42823,10075 42823,9778.7 42823,9778.7"];
	Layer2_Device8_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40312,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage1_Attention	[label=Q_local,
		lp="40376,9838.3",
		pos="e,40438,9575.6 42448,10082 41661,10082 40438,10082 40438,10082 40438,10082 40438,9585.6 40438,9585.6"];
	Layer2_Device8_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40923,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage2_Attention	[label=Q_local,
		lp="43054,9741.8",
		pos="e,41093,9382.4 42918,10074 42918,9979.1 42918,9662 42918,9662 42918,9662 41093,9662 41093,9662 41093,9662 41093,9392.4 41093,9392.4"];
	Layer2_Device8_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42938,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage3_Attention	[label=Q_local,
		lp="43194,9645.3",
		pos="e,43111,9189.6 43111,10075 43111,10075 43111,9199.6 43111,9199.6"];
	Layer2_Device8_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40144,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage4_Attention	[label=Q_local,
		lp="39996,9548.8",
		pos="e,39996,8996.6 42448,10097 41539,10097 39996,10097 39996,10097 39996,10097 39996,9006.6 39996,9006.6"];
	Layer2_Device8_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43056,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage5_Attention	[label=Q_local,
		lp="43298,9452.3",
		pos="e,43228,8803.6 43228,10075 43228,10075 43228,8813.6 43228,8813.6"];
	Layer2_Device8_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43084,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage6_Attention	[label=Q_local,
		lp="43430,9355.8",
		pos="e,43302,8610.6 43302,10075 43302,10075 43302,8620.6 43302,8620.6"];
	Layer2_Device8_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40145,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage7_Attention	[label=Q_local,
		lp="39850,9259.3",
		pos="e,39913,8399 42448,10112 41510,10112 39889,10112 39889,10112 39889,10112 39889,8399 39889,8399 39889,8399 39903,8399 39903,8399"];
	Layer2_Device8_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42890,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage8_Attention	[label=Q_local,
		lp="43548,9162.8",
		pos="e,43122,8197 43382,10075 43382,9844.7 43382,8197 43382,8197 43382,8197 43132,8197 43132,8197"];
	Layer2_Device8_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="39951,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage9_Attention	[label=Q_local,
		lp="39732,9066.3",
		pos="e,39865,8031.6 42448,10120 41504,10120 39865,10120 39865,10120 39865,10120 39865,8041.6 39865,8041.6"];
	Layer2_Device8_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40378,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage10_Attention	[label=Q_local,
		lp="39592,8969.8",
		pos="e,40486,7838.6 43448,10075 43448,9827.5 43448,7938 43448,7938 43448,7938 40486,7938 40486,7938 40486,7938 40486,7848.6 40486,7848.6"];
	Layer2_Device8_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41519,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage11_Attention	[label=Q_local,
		lp="43669,8873.3",
		pos="e,41596,7645.3 43515,10075 43515,9811.5 43515,7690 43515,7690 43515,7690 41596,7690 41596,7690 41596,7690 41596,7655.3 41596,7655.3"];
	Layer2_Device8_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41709,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage12_Attention	[label=Q_local,
		lp="43812,8776.8",
		pos="e,41779,7452.8 43581,10075 43581,9811.2 43581,7687 43581,7687 43581,7687 41779,7687 41779,7687 41779,7687 41779,7462.8 41779,7462.8"];
	Layer2_Device8_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43724,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage13_Attention	[label=Q_local,
		lp="43952,8680.3",
		pos="e,43897,7259.7 43748,10089 43836,10089 43897,10089 43897,10089 43897,10089 43897,7269.7 43897,7269.7"];
	Layer2_Device8_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41306,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage14_Attention	[label=Q_local,
		lp="39446,8583.8",
		pos="e,41074,7039 43698,10075 43698,9800.8 43698,7522 43698,7522 43698,7522 40924,7522 40924,7522 40924,7522 40924,7039 40924,7039 40924,\
7039 41064,7039 41064,7039"];
	Layer2_Device8_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43242,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_QKVProj -> Layer2_Device8_Stage15_Attention	[label=Q_local,
		lp="44070,8487.3",
		pos="e,43474,6896 43748,10105 43927,10105 44074,10105 44074,10105 44074,10105 44074,6896 44074,6896 44074,6896 43484,6896 43484,6896"];
	Layer2_Device8_Stage0_RecvKV -> Layer2_Device8_Stage0_Attention	[pos="e,42591,9768.7 42591,9934.7 42591,9934.7 42591,9778.7 42591,9778.7"];
	Layer2_Device8_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41433,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage0_RecvKV -> Layer2_Device8_Stage1_RecvKV	[label="Ring transfer",
		lp="41772,9838.3",
		pos="e,41642,9794.9 41642,9881.6 41642,9881.6 41642,9804.9 41642,9804.9"];
	Layer2_Device8_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="40833,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage0_Attention -> Layer2_Device8_Stage0_Accumulate	[pos="e,40833,9575.5 42642,9714.9 42642,9692.8 42642,9665 42642,9665 42642,9665 40833,9665 40833,9665 40833,9665 40833,9585.5 40833,9585.5"];
	Layer2_Device8_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40402,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage0_Accumulate -> Layer2_Device8_Stage1_Accumulate	[pos="e,40618,9382.4 40618,9522 40618,9522 40618,9392.4 40618,9392.4"];
	Layer2_Device8_Stage1_RecvKV -> Layer2_Device8_Stage1_Attention	[pos="e,40528,9575.7 40528,9741.7 40528,9741.7 40528,9585.7 40528,9585.7"];
	Layer2_Device8_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="42042,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage1_RecvKV -> Layer2_Device8_Stage2_RecvKV	[label="Ring transfer",
		lp="41750,9645.3",
		pos="e,41738,9601.9 41738,9688.6 41738,9688.6 41738,9611.9 41738,9611.9"];
	Layer2_Device8_Stage1_Attention -> Layer2_Device8_Stage1_Accumulate	[pos="e,40357,9382.4 40357,9522 40357,9522 40357,9392.4 40357,9392.4"];
	Layer2_Device8_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40520,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage1_Accumulate -> Layer2_Device8_Stage2_Accumulate	[pos="e,40461,9189.4 40461,9329 40461,9329 40461,9199.4 40461,9199.4"];
	Layer2_Device8_Stage2_RecvKV -> Layer2_Device8_Stage2_Attention	[pos="e,41138,9382.7 41138,9548.7 41138,9548.7 41138,9392.7 41138,9392.7"];
	Layer2_Device8_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="42132,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage2_RecvKV -> Layer2_Device8_Stage3_RecvKV	[label="Ring transfer",
		lp="42135,9452.3",
		pos="e,42087,9408.9 42087,9495.6 42087,9495.6 42087,9418.9 42087,9418.9"];
	Layer2_Device8_Stage2_Attention -> Layer2_Device8_Stage2_Accumulate	[pos="e,40722,9189.4 40722,9329 40722,9329 40722,9199.4 40722,9199.4"];
	Layer2_Device8_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40665,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage2_Accumulate -> Layer2_Device8_Stage3_Accumulate	[pos="e,40592,8996.4 40592,9136 40592,9136 40592,9006.4 40592,9006.4"];
	Layer2_Device8_Stage3_RecvKV -> Layer2_Device8_Stage3_Attention	[pos="e,42879,9189.7 42879,9355.7 42879,9355.7 42879,9199.7 42879,9199.7"];
	Layer2_Device8_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41729,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage3_RecvKV -> Layer2_Device8_Stage4_RecvKV	[label="Ring transfer",
		lp="42046,9259.3",
		pos="e,41930,9215.9 41930,9302.6 41930,9302.6 41930,9225.9 41930,9225.9"];
	Layer2_Device8_Stage3_Attention -> Layer2_Device8_Stage3_Accumulate	[pos="e,40853,8996.4 42750,9136.2 42750,9108.1 42750,9068 42750,9068 42750,9068 40853,9068 40853,9068 40853,9068 40853,9006.4 40853,9006.4"];
	Layer2_Device8_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40638,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage3_Accumulate -> Layer2_Device8_Stage4_Accumulate	[pos="e,40652,8803.4 40652,8943 40652,8943 40652,8813.4 40652,8813.4"];
	Layer2_Device8_Stage4_RecvKV -> Layer2_Device8_Stage4_Attention	[pos="e,40332,8996.5 40856,9123 40573,9123 40332,9123 40332,9123 40332,9123 40332,9006.5 40332,9006.5"];
	Layer2_Device8_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41874,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage4_RecvKV -> Layer2_Device8_Stage5_RecvKV	[label="Ring transfer",
		lp="41849,9066.3",
		pos="e,41802,9022.9 41802,9109.6 41802,9109.6 41802,9032.9 41802,9032.9"];
	Layer2_Device8_Stage4_Attention -> Layer2_Device8_Stage4_Accumulate	[pos="e,40420,8803.5 40376,8969 40402,8969 40420,8969 40420,8969 40420,8969 40420,8813.5 40420,8813.5"];
	Layer2_Device8_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42563,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage4_Accumulate -> Layer2_Device8_Stage5_Accumulate	[pos="e,42549,8610.8 40652,8750.1 40652,8719 40652,8672 40652,8672 40652,8672 42549,8672 42549,8672 42549,8672 42549,8620.8 42549,8620.8"];
	Layer2_Device8_Stage5_RecvKV -> Layer2_Device8_Stage5_Attention	[pos="e,42824,8785 42776,8969.7 42776,8964.1 42776,8785 42776,8785 42776,8785 42814,8785 42814,8785"];
	Layer2_Device8_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41847,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage5_RecvKV -> Layer2_Device8_Stage6_RecvKV	[label="Ring transfer",
		lp="41908,8873.3",
		pos="e,41860,8829.9 41860,8916.6 41860,8916.6 41860,8839.9 41860,8839.9"];
	Layer2_Device8_Stage5_Attention -> Layer2_Device8_Stage5_Accumulate	[pos="e,42785,8610.5 42824,8767 42800,8767 42785,8767 42785,8767 42785,8767 42785,8620.5 42785,8620.5"];
	Layer2_Device8_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42563,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage5_Accumulate -> Layer2_Device8_Stage6_Accumulate	[pos="e,42563,8417.4 42563,8557 42563,8557 42563,8427.4 42563,8427.4"];
	Layer2_Device8_Stage6_RecvKV -> Layer2_Device8_Stage6_Attention	[pos="e,43070,8610.5 42437,8737 42750,8737 43070,8737 43070,8737 43070,8737 43070,8620.5 43070,8620.5"];
	Layer2_Device8_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41354,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage6_RecvKV -> Layer2_Device8_Stage7_RecvKV	[label="Ring transfer",
		lp="41583,8680.3",
		pos="e,41600,8636.9 41600,8723.6 41600,8723.6 41600,8646.9 41600,8646.9"];
	Layer2_Device8_Stage6_Attention -> Layer2_Device8_Stage6_Accumulate	[pos="e,42795,8390 42987,8557.3 42987,8504.2 42987,8390 42987,8390 42987,8390 42805,8390 42805,8390"];
	Layer2_Device8_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42369,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage6_Accumulate -> Layer2_Device8_Stage7_Accumulate	[pos="e,42466,8224.4 42466,8364 42466,8364 42466,8234.4 42466,8234.4"];
	Layer2_Device8_Stage7_RecvKV -> Layer2_Device8_Stage7_Attention	[pos="e,40144,8417.5 40481,8544 40290,8544 40144,8544 40144,8544 40144,8544 40144,8427.5 40144,8427.5"];
	Layer2_Device8_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41354,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage7_RecvKV -> Layer2_Device8_Stage8_RecvKV	[label="Ring transfer",
		lp="41402,8487.3",
		pos="e,41354,8443.9 41354,8530.6 41354,8530.6 41354,8453.9 41354,8453.9"];
	Layer2_Device8_Stage7_Attention -> Layer2_Device8_Stage7_Accumulate	[pos="e,42206,8224.4 40308,8363.9 40308,8337.4 40308,8301 40308,8301 40308,8301 42206,8301 42206,8301 42206,8301 42206,8234.4 42206,8234.4"];
	Layer2_Device8_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42369,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage7_Accumulate -> Layer2_Device8_Stage8_Accumulate	[pos="e,42369,8031.4 42369,8171 42369,8171 42369,8041.4 42369,8041.4"];
	Layer2_Device8_Stage8_RecvKV -> Layer2_Device8_Stage8_Attention	[pos="e,42726,8224.5 41945,8351 42316,8351 42726,8351 42726,8351 42726,8351 42726,8234.5 42726,8234.5"];
	Layer2_Device8_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41160,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage8_RecvKV -> Layer2_Device8_Stage9_RecvKV	[label="Ring transfer",
		lp="41337,8294.3",
		pos="e,41257,8250.9 41257,8337.6 41257,8337.6 41257,8260.9 41257,8260.9"];
	Layer2_Device8_Stage8_Attention -> Layer2_Device8_Stage8_Accumulate	[pos="e,42601,8004 42843,8171.3 42843,8118.2 42843,8004 42843,8004 42843,8004 42611,8004 42611,8004"];
	Layer2_Device8_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40899,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage8_Accumulate -> Layer2_Device8_Stage9_Accumulate	[pos="e,40976,7838.5 42369,7978.1 42369,7944.7 42369,7892 42369,7892 42369,7892 40976,7892 40976,7892 40976,7892 40976,7848.5 40976,7848.5"];
	Layer2_Device8_Stage9_RecvKV -> Layer2_Device8_Stage9_Attention	[pos="e,40048,8031.5 40287,8158 40147,8158 40048,8158 40048,8158 40048,8158 40048,8041.5 40048,8041.5"];
	Layer2_Device8_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41160,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage9_RecvKV -> Layer2_Device8_Stage10_RecvKV	[label="Ring transfer",
		lp="41208,8101.3",
		pos="e,41160,8057.9 41160,8144.6 41160,8144.6 41160,8067.9 41160,8067.9"];
	Layer2_Device8_Stage9_Attention -> Layer2_Device8_Stage9_Accumulate	[pos="e,40822,7838.5 40164,7978.1 40164,7943.3 40164,7887 40164,7887 40164,7887 40822,7887 40822,7887 40822,7887 40822,7848.5 40822,7848.5"];
	Layer2_Device8_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40899,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage9_Accumulate -> Layer2_Device8_Stage10_Accumulate	[pos="e,40899,7645.4 40899,7785 40899,7785 40899,7655.4 40899,7655.4"];
	Layer2_Device8_Stage10_RecvKV -> Layer2_Device8_Stage10_Attention	[pos="e,40363,7838.5 40363,7951.6 40363,7951.6 40363,7848.5 40363,7848.5"];
	Layer2_Device8_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42108,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage10_RecvKV -> Layer2_Device8_Stage11_RecvKV	[label="Ring transfer",
		lp="41477,7908.3",
		pos="e,41634,7864.9 41634,7951.6 41634,7951.6 41634,7874.9 41634,7874.9"];
	Layer2_Device8_Stage10_Attention -> Layer2_Device8_Stage10_Accumulate	[pos="e,40667,7618 40541,7785.3 40541,7732.2 40541,7618 40541,7618 40541,7618 40657,7618 40657,7618"];
	Layer2_Device8_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41188,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage10_Accumulate -> Layer2_Device8_Stage11_Accumulate	[pos="e,41044,7452.4 41044,7592 41044,7592 41044,7462.4 41044,7462.4"];
	Layer2_Device8_Stage11_RecvKV -> Layer2_Device8_Stage11_Attention	[pos="e,41442,7645.5 41442,7758.6 41442,7758.6 41442,7655.5 41442,7655.5"];
	Layer2_Device8_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42728,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage11_RecvKV -> Layer2_Device8_Stage12_RecvKV	[label="Ring transfer",
		lp="42466,7715.3",
		pos="e,42418,7671.9 42418,7758.6 42418,7758.6 42418,7681.9 42418,7681.9"];
	Layer2_Device8_Stage11_Attention -> Layer2_Device8_Stage11_Accumulate	[pos="e,41354,7452.4 41354,7592 41354,7592 41354,7462.4 41354,7462.4"];
	Layer2_Device8_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41306,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage11_Accumulate -> Layer2_Device8_Stage12_Accumulate	[pos="e,41247,7259.4 41247,7399 41247,7399 41247,7269.4 41247,7269.4"];
	Layer2_Device8_Stage12_RecvKV -> Layer2_Device8_Stage12_Attention	[pos="e,41874,7452.7 41874,7618.7 41874,7618.7 41874,7462.7 41874,7462.7"];
	Layer2_Device8_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42918,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage12_RecvKV -> Layer2_Device8_Stage13_RecvKV	[label="Ring transfer",
		lp="42871,7522.3",
		pos="e,42823,7478.9 42823,7565.6 42823,7565.6 42823,7488.9 42823,7488.9"];
	Layer2_Device8_Stage12_Attention -> Layer2_Device8_Stage12_Accumulate	[pos="e,41508,7259.4 41508,7399 41508,7399 41508,7269.4 41508,7269.4"];
	Layer2_Device8_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41827,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage12_Accumulate -> Layer2_Device8_Stage13_Accumulate	[pos="e,41595,7039 41538,7232 41555,7232 41566,7232 41566,7232 41566,7232 41566,7039 41566,7039 41566,7039 41585,7039 41585,7039"];
	Layer2_Device8_Stage13_RecvKV -> Layer2_Device8_Stage13_Attention	[pos="e,43665,7259.7 43665,7425.7 43665,7425.7 43665,7269.7 43665,7269.7"];
	Layer2_Device8_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42515,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage13_RecvKV -> Layer2_Device8_Stage14_RecvKV	[label="Ring transfer",
		lp="42699,7329.3",
		pos="e,42716,7285.9 42716,7372.6 42716,7372.6 42716,7295.9 42716,7295.9"];
	Layer2_Device8_Stage13_Attention -> Layer2_Device8_Stage13_Accumulate	[pos="e,41827,7066.8 43724,7206.1 43724,7175 43724,7128 43724,7128 43724,7128 41827,7128 41827,7128 41827,7128 41827,7076.8 41827,7076.8"];
	Layer2_Device8_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41827,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage13_Accumulate -> Layer2_Device8_Stage14_Accumulate	[pos="e,41827,6913.8 41827,7013.2 41827,7013.2 41827,6923.8 41827,6923.8"];
	Layer2_Device8_Stage14_RecvKV -> Layer2_Device8_Stage14_Attention	[pos="e,41306,7066.5 41642,7193 41452,7193 41306,7193 41306,7193 41306,7193 41306,7076.5 41306,7076.5"];
	Layer2_Device8_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="43036,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device8_Stage14_RecvKV -> Layer2_Device8_Stage15_RecvKV	[label="Ring transfer",
		lp="42823,7136.3",
		pos="e,42776,7092.9 42776,7179.6 42776,7179.6 42776,7102.9 42776,7102.9"];
	Layer2_Device8_Stage14_Attention -> Layer2_Device8_Stage14_Accumulate	[pos="e,41595,6878 41306,7013.1 41306,6967.1 41306,6878 41306,6878 41306,6878 41585,6878 41585,6878"];
	Layer2_Device8_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="43242,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device8_Stage14_Accumulate -> Layer2_Device8_Stage15_Accumulate	[pos="e,43010,6761 41827,6860.5 41827,6823.4 41827,6761 41827,6761 41827,6761 43000,6761 43000,6761"];
	Layer2_Device8_Stage15_RecvKV -> Layer2_Device8_Stage15_Attention	[pos="e,43242,6913.9 43242,6986.5 43242,6986.5 43242,6923.9 43242,6923.9"];
	Layer2_Device8_Stage15_Attention -> Layer2_Device8_Stage15_Accumulate	[pos="e,43242,6788 43242,6860.6 43242,6860.6 43242,6798 43242,6798"];
	Layer2_Device8_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43286,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device8_Stage15_Accumulate -> Layer2_Device8_ConcatHeads	[pos="e,43267,6662 43267,6734.6 43267,6734.6 43267,6672 43267,6672"];
	Layer2_Device8_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43396,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device8_ConcatHeads -> Layer2_Device8_OutputProj	[pos="e,43347,6536 43347,6608.6 43347,6608.6 43347,6546 43347,6546"];
	Layer2_Device8_OutputProj -> Layer2_Device8_Residual1	[pos="e,43396,6410 43396,6482.6 43396,6482.6 43396,6420 43396,6420"];
	Layer2_Device8_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43395,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device8_Residual1 -> Layer2_Device8_LayerNorm2	[pos="e,43395,6284 43395,6356.6 43395,6356.6 43395,6294 43395,6294"];
	Layer2_Device8_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="43489,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device8_Residual1 -> Layer2_Device8_Residual2	[pos="e,43758,5601.9 43758,6356.4 43758,6356.4 43758,5611.9 43758,5611.9"];
	Layer2_Device8_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43102,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device8_LayerNorm2 -> Layer2_Device8_GateProj	[pos="e,43254,6158 43254,6230.6 43254,6230.6 43254,6168 43254,6168"];
	Layer2_Device8_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43415,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device8_LayerNorm2 -> Layer2_Device8_UpProj	[pos="e,43468,6069.1 43468,6230.5 43468,6230.5 43468,6079.1 43468,6079.1"];
	Layer2_Device8_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43102,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device8_GateProj -> Layer2_Device8_Activation	[pos="e,43034,5979.9 43034,6104.7 43034,6104.7 43034,5989.9 43034,5989.9"];
	Layer2_Device8_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="43198,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device8_UpProj -> Layer2_Device8_ElemMul	[pos="e,43468,5854.1 43468,6015.5 43468,6015.5 43468,5864.1 43468,5864.1"];
	Layer2_Device8_Activation -> Layer2_Device8_ElemMul	[pos="e,43102,5854 43102,5926.6 43102,5926.6 43102,5864 43102,5864"];
	Layer2_Device8_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43341,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device8_ElemMul -> Layer2_Device8_DownProj	[pos="e,43341,5728 43341,5800.6 43341,5800.6 43341,5738 43341,5738"];
	Layer2_Device8_DownProj -> Layer2_Device8_Residual2	[pos="e,43341,5602 43341,5674.6 43341,5674.6 43341,5612 43341,5612"];
	Layer2_Device8_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43489,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device8_Residual2 -> Layer2_Device8_Output	[pos="e,43489,5475.9 43489,5548.6 43489,5548.6 43489,5485.9 43489,5485.9"];
	Layer3_Device8_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 8 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43489,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device8_Output -> Layer3_Device8_Input	[pos="e,43489,5328 43489,5400.6 43489,5400.6 43489,5338 43489,5338"];
	Layer2_Device9_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48740,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device9_Input -> Layer2_Device9_LayerNorm1	[pos="e,48740,10254 48740,10327 48740,10327 48740,10264 48740,10264"];
	Layer2_Device9_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="48955,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device9_Input -> Layer2_Device9_Residual1	[pos="e,49012,6410 49012,10342 49012,10342 49012,6420 49012,6420"];
	Layer2_Device9_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47988,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device9_LayerNorm1 -> Layer2_Device9_QKVProj	[pos="e,48582,10128 48582,10201 48582,10201 48582,10138 48582,10138"];
	Layer2_Device9_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46742,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage0_RecvKV	[label="Local K,V",
		lp="47242,10031",
		pos="e,47500,9988.1 47500,10075 47500,10075 47500,9998.1 47500,9998.1"];
	Layer2_Device9_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47495,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage0_Attention	[label=Q_local,
		lp="47804,9934.8",
		pos="e,47694,9768.7 47694,10075 47694,10075 47694,9778.7 47694,9778.7"];
	Layer2_Device9_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45202,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage1_Attention	[label=Q_local,
		lp="45266,9838.3",
		pos="e,45329,9575.6 47338,10082 46551,10082 45329,10082 45329,10082 45329,10082 45329,9585.6 45329,9585.6"];
	Layer2_Device9_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45813,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage2_Attention	[label=Q_local,
		lp="47944,9741.8",
		pos="e,45983,9382.5 47789,10075 47789,9973.1 47789,9614 47789,9614 47789,9614 45983,9614 45983,9614 45983,9614 45983,9392.5 45983,9392.5"];
	Layer2_Device9_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47828,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage3_Attention	[label=Q_local,
		lp="48084,9645.3",
		pos="e,48001,9189.6 48001,10075 48001,10075 48001,9199.6 48001,9199.6"];
	Layer2_Device9_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45034,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage4_Attention	[label=Q_local,
		lp="44886,9548.8",
		pos="e,44886,8996.6 47338,10097 46429,10097 44886,10097 44886,10097 44886,10097 44886,9006.6 44886,9006.6"];
	Layer2_Device9_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47946,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage5_Attention	[label=Q_local,
		lp="48188,9452.3",
		pos="e,48118,8803.6 48118,10075 48118,10075 48118,8813.6 48118,8813.6"];
	Layer2_Device9_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47923,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage6_Attention	[label=Q_local,
		lp="48320,9355.8",
		pos="e,48155,8583 48218,10075 48218,9872 48218,8583 48218,8583 48218,8583 48165,8583 48165,8583"];
	Layer2_Device9_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44984,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage7_Attention	[label=Q_local,
		lp="44740,9259.3",
		pos="e,44787,8417.5 47338,10112 46403,10112 44787,10112 44787,10112 44787,10112 44787,8427.5 44787,8427.5"];
	Layer2_Device9_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47732,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage8_Attention	[label=Q_local,
		lp="48438,9162.8",
		pos="e,47964,8197 48259,10075 48259,9844.7 48259,8197 48259,8197 48259,8197 47974,8197 47974,8197"];
	Layer2_Device9_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44793,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage9_Attention	[label=Q_local,
		lp="44622,9066.3",
		pos="e,44657,8031.6 47338,10120 46369,10120 44657,10120 44657,10120 44657,10120 44657,8041.6 44657,8041.6"];
	Layer2_Device9_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45154,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage10_Attention	[label=Q_local,
		lp="44482,8969.8",
		pos="e,45284,7838.6 48300,10074 48300,9826.9 48300,7949 48300,7949 48300,7949 45284,7949 45284,7949 45284,7949 45284,7848.6 45284,7848.6"];
	Layer2_Device9_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46294,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage11_Attention	[label=Q_local,
		lp="48559,8873.3",
		pos="e,46371,7645.6 48341,10075 48341,9813.7 48341,7726 48341,7726 48341,7726 46371,7726 46371,7726 46371,7726 46371,7655.6 46371,7655.6"];
	Layer2_Device9_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46599,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage12_Attention	[label=Q_local,
		lp="48702,8776.8",
		pos="e,46554,7452.5 48382,10075 48382,9813.4 48382,7724 48382,7724 48382,7724 46554,7724 46554,7724 46554,7724 46554,7462.5 46554,7462.5"];
	Layer2_Device9_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48614,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage13_Attention	[label=Q_local,
		lp="48842,8680.3",
		pos="e,48787,7259.7 48638,10086 48726,10086 48787,10086 48787,10086 48787,10086 48787,7269.7 48787,7269.7"];
	Layer2_Device9_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46196,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage14_Attention	[label=Q_local,
		lp="44336,8583.8",
		pos="e,46273,7066.5 48638,10098 48776,10098 48881,10098 48881,10098 48881,10098 48881,7175 48881,7175 48881,7175 46273,7175 46273,7175 \
46273,7175 46273,7076.5 46273,7076.5"];
	Layer2_Device9_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48042,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_QKVProj -> Layer2_Device9_Stage15_Attention	[label=Q_local,
		lp="48960,8487.3",
		pos="e,48274,6896 48638,10104 48794,10104 48916,10104 48916,10104 48916,10104 48916,6896 48916,6896 48916,6896 48284,6896 48284,6896"];
	Layer2_Device9_Stage0_RecvKV -> Layer2_Device9_Stage0_Attention	[pos="e,47463,9768.6 47463,9931.5 47463,9931.5 47463,9778.6 47463,9778.6"];
	Layer2_Device9_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46286,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage0_RecvKV -> Layer2_Device9_Stage1_RecvKV	[label="Ring transfer",
		lp="46643,9838.3",
		pos="e,46514,9794.9 46514,9881.6 46514,9881.6 46514,9804.9 46514,9804.9"];
	Layer2_Device9_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="45723,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage0_Attention -> Layer2_Device9_Stage0_Accumulate	[pos="e,45723,9575.4 47495,9715 47495,9678.4 47495,9617 47495,9617 47495,9617 45723,9617 45723,9617 45723,9617 45723,9585.4 45723,9585.4"];
	Layer2_Device9_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45292,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage0_Accumulate -> Layer2_Device9_Stage1_Accumulate	[pos="e,45508,9382.4 45508,9522 45508,9522 45508,9392.4 45508,9392.4"];
	Layer2_Device9_Stage1_RecvKV -> Layer2_Device9_Stage1_Attention	[pos="e,45400,9575.7 45400,9741.7 45400,9741.7 45400,9585.7 45400,9585.7"];
	Layer2_Device9_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46932,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage1_RecvKV -> Layer2_Device9_Stage2_RecvKV	[label="Ring transfer",
		lp="46603,9645.3",
		pos="e,46609,9601.9 46609,9688.6 46609,9688.6 46609,9611.9 46609,9611.9"];
	Layer2_Device9_Stage1_Attention -> Layer2_Device9_Stage1_Accumulate	[pos="e,45247,9382.4 45247,9522 45247,9522 45247,9392.4 45247,9392.4"];
	Layer2_Device9_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45410,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage1_Accumulate -> Layer2_Device9_Stage2_Accumulate	[pos="e,45351,9189.4 45351,9329 45351,9329 45351,9199.4 45351,9199.4"];
	Layer2_Device9_Stage2_RecvKV -> Layer2_Device9_Stage2_Attention	[pos="e,46028,9382.7 46028,9548.7 46028,9548.7 46028,9392.7 46028,9392.7"];
	Layer2_Device9_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="47022,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage2_RecvKV -> Layer2_Device9_Stage3_RecvKV	[label="Ring transfer",
		lp="47025,9452.3",
		pos="e,46977,9408.9 46977,9495.6 46977,9495.6 46977,9418.9 46977,9418.9"];
	Layer2_Device9_Stage2_Attention -> Layer2_Device9_Stage2_Accumulate	[pos="e,45612,9189.4 45612,9329 45612,9329 45612,9199.4 45612,9199.4"];
	Layer2_Device9_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45555,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage2_Accumulate -> Layer2_Device9_Stage3_Accumulate	[pos="e,45482,8996.4 45482,9136 45482,9136 45482,9006.4 45482,9006.4"];
	Layer2_Device9_Stage3_RecvKV -> Layer2_Device9_Stage3_Attention	[pos="e,47769,9189.7 47769,9355.7 47769,9355.7 47769,9199.7 47769,9199.7"];
	Layer2_Device9_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46619,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage3_RecvKV -> Layer2_Device9_Stage4_RecvKV	[label="Ring transfer",
		lp="46869,9259.3",
		pos="e,46820,9215.9 46820,9302.6 46820,9302.6 46820,9225.9 46820,9225.9"];
	Layer2_Device9_Stage3_Attention -> Layer2_Device9_Stage3_Accumulate	[pos="e,45743,8996.4 47640,9135.9 47640,9109.4 47640,9073 47640,9073 47640,9073 45743,9073 45743,9073 45743,9073 45743,9006.4 45743,9006.4"];
	Layer2_Device9_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45528,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage3_Accumulate -> Layer2_Device9_Stage4_Accumulate	[pos="e,45542,8803.4 45542,8943 45542,8943 45542,8813.4 45542,8813.4"];
	Layer2_Device9_Stage4_RecvKV -> Layer2_Device9_Stage4_Attention	[pos="e,45222,8996.5 45746,9123 45463,9123 45222,9123 45222,9123 45222,9123 45222,9006.5 45222,9006.5"];
	Layer2_Device9_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46764,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage4_RecvKV -> Layer2_Device9_Stage5_RecvKV	[label="Ring transfer",
		lp="46786,9066.3",
		pos="e,46692,9022.9 46692,9109.6 46692,9109.6 46692,9032.9 46692,9032.9"];
	Layer2_Device9_Stage4_Attention -> Layer2_Device9_Stage4_Accumulate	[pos="e,45310,8803.5 45266,8969 45292,8969 45310,8969 45310,8969 45310,8969 45310,8813.5 45310,8813.5"];
	Layer2_Device9_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47402,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage4_Accumulate -> Layer2_Device9_Stage5_Accumulate	[pos="e,47402,8610.4 45528,8750.3 45528,8710.8 45528,8641 45528,8641 45528,8641 47402,8641 47402,8641 47402,8641 47402,8620.4 47402,8620.4"];
	Layer2_Device9_Stage5_RecvKV -> Layer2_Device9_Stage5_Attention	[pos="e,47714,8785 47670,8969.7 47670,8964.1 47670,8785 47670,8785 47670,8785 47704,8785 47704,8785"];
	Layer2_Device9_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46737,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage5_RecvKV -> Layer2_Device9_Stage6_RecvKV	[label="Ring transfer",
		lp="46798,8873.3",
		pos="e,46750,8829.9 46750,8916.6 46750,8916.6 46750,8839.9 46750,8839.9"];
	Layer2_Device9_Stage5_Attention -> Layer2_Device9_Stage5_Accumulate	[pos="e,47634,8583 47714,8767 47698,8767 47688,8767 47688,8767 47688,8767 47688,8583 47688,8583 47688,8583 47644,8583 47644,8583"];
	Layer2_Device9_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47402,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage5_Accumulate -> Layer2_Device9_Stage6_Accumulate	[pos="e,47402,8417.4 47402,8557 47402,8557 47402,8427.4 47402,8427.4"];
	Layer2_Device9_Stage6_RecvKV -> Layer2_Device9_Stage6_Attention	[pos="e,47703,8610.5 47327,8737 47528,8737 47703,8737 47703,8737 47703,8737 47703,8620.5 47703,8620.5"];
	Layer2_Device9_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46193,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage6_RecvKV -> Layer2_Device9_Stage7_RecvKV	[label="Ring transfer",
		lp="46513,8680.3",
		pos="e,46465,8636.9 46465,8723.6 46465,8723.6 46465,8646.9 46465,8646.9"];
	Layer2_Device9_Stage6_Attention -> Layer2_Device9_Stage6_Accumulate	[pos="e,47634,8381 47828,8557.3 47828,8502.3 47828,8381 47828,8381 47828,8381 47644,8381 47644,8381"];
	Layer2_Device9_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47211,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage6_Accumulate -> Layer2_Device9_Stage7_Accumulate	[pos="e,47306,8224.4 47306,8364 47306,8364 47306,8234.4 47306,8234.4"];
	Layer2_Device9_Stage7_RecvKV -> Layer2_Device9_Stage7_Attention	[pos="e,45009,8417.5 45320,8544 45143,8544 45009,8544 45009,8544 45009,8544 45009,8427.5 45009,8427.5"];
	Layer2_Device9_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46193,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage7_RecvKV -> Layer2_Device9_Stage8_RecvKV	[label="Ring transfer",
		lp="46241,8487.3",
		pos="e,46193,8443.9 46193,8530.6 46193,8530.6 46193,8453.9 46193,8453.9"];
	Layer2_Device9_Stage7_Attention -> Layer2_Device9_Stage7_Accumulate	[pos="e,47046,8224.5 45149,8364.3 45149,8339.7 45149,8307 45149,8307 45149,8307 47046,8307 47046,8307 47046,8307 47046,8234.5 47046,8234.5"];
	Layer2_Device9_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47211,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage7_Accumulate -> Layer2_Device9_Stage8_Accumulate	[pos="e,47211,8031.4 47211,8171 47211,8171 47211,8041.4 47211,8041.4"];
	Layer2_Device9_Stage8_RecvKV -> Layer2_Device9_Stage8_Attention	[pos="e,47567,8224.5 46783,8351 47156,8351 47567,8351 47567,8351 47567,8351 47567,8234.5 47567,8234.5"];
	Layer2_Device9_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46002,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage8_RecvKV -> Layer2_Device9_Stage9_RecvKV	[label="Ring transfer",
		lp="46080,8294.3",
		pos="e,46098,8250.9 46098,8337.6 46098,8337.6 46098,8260.9 46098,8260.9"];
	Layer2_Device9_Stage8_Attention -> Layer2_Device9_Stage8_Accumulate	[pos="e,47443,7995 47652,8171.3 47652,8116.3 47652,7995 47652,7995 47652,7995 47453,7995 47453,7995"];
	Layer2_Device9_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45675,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage8_Accumulate -> Layer2_Device9_Stage9_Accumulate	[pos="e,45752,7838.5 47211,7978.2 47211,7946 47211,7896 47211,7896 47211,7896 45752,7896 45752,7896 45752,7896 45752,7848.5 45752,7848.5"];
	Layer2_Device9_Stage9_RecvKV -> Layer2_Device9_Stage9_Attention	[pos="e,44888,8031.5 45129,8158 44989,8158 44888,8158 44888,8158 44888,8158 44888,8041.5 44888,8041.5"];
	Layer2_Device9_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="46002,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage9_RecvKV -> Layer2_Device9_Stage10_RecvKV	[label="Ring transfer",
		lp="46050,8101.3",
		pos="e,46002,8057.9 46002,8144.6 46002,8144.6 46002,8067.9 46002,8067.9"];
	Layer2_Device9_Stage9_Attention -> Layer2_Device9_Stage9_Accumulate	[pos="e,45598,7838.3 44974,7978.1 44974,7942.2 44974,7883 44974,7883 44974,7883 45598,7883 45598,7883 45598,7883 45598,7848.3 45598,7848.3"];
	Layer2_Device9_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45675,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage9_Accumulate -> Layer2_Device9_Stage10_Accumulate	[pos="e,45675,7645.4 45675,7785 45675,7785 45675,7655.4 45675,7655.4"];
	Layer2_Device9_Stage10_RecvKV -> Layer2_Device9_Stage10_Attention	[pos="e,45183,7838.5 45183,7951.6 45183,7951.6 45183,7848.5 45183,7848.5"];
	Layer2_Device9_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="46884,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage10_RecvKV -> Layer2_Device9_Stage11_RecvKV	[label="Ring transfer",
		lp="46319,7908.3",
		pos="e,46443,7864.9 46443,7951.6 46443,7951.6 46443,7874.9 46443,7874.9"];
	Layer2_Device9_Stage10_Attention -> Layer2_Device9_Stage10_Accumulate	[pos="e,45443,7618 45323,7785.3 45323,7732.2 45323,7618 45323,7618 45323,7618 45433,7618 45433,7618"];
	Layer2_Device9_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46078,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage10_Accumulate -> Layer2_Device9_Stage11_Accumulate	[pos="e,45876,7452.4 45876,7592 45876,7592 45876,7462.4 45876,7462.4"];
	Layer2_Device9_Stage11_RecvKV -> Layer2_Device9_Stage11_Attention	[pos="e,46217,7645.5 46217,7758.6 46217,7758.6 46217,7655.5 46217,7655.5"];
	Layer2_Device9_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47503,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage11_RecvKV -> Layer2_Device9_Stage12_RecvKV	[label="Ring transfer",
		lp="47241,7715.3",
		pos="e,47194,7671.9 47194,7758.6 47194,7758.6 47194,7681.9 47194,7681.9"];
	Layer2_Device9_Stage11_Attention -> Layer2_Device9_Stage11_Accumulate	[pos="e,46186,7452.4 46186,7592 46186,7592 46186,7462.4 46186,7462.4"];
	Layer2_Device9_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46196,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage11_Accumulate -> Layer2_Device9_Stage12_Accumulate	[pos="e,46137,7259.4 46137,7399 46137,7399 46137,7269.4 46137,7269.4"];
	Layer2_Device9_Stage12_RecvKV -> Layer2_Device9_Stage12_Attention	[pos="e,46707,7452.5 46707,7565.6 46707,7565.6 46707,7462.5 46707,7462.5"];
	Layer2_Device9_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47808,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage12_RecvKV -> Layer2_Device9_Stage13_RecvKV	[label="Ring transfer",
		lp="47704,7522.3",
		pos="e,47656,7478.9 47656,7565.6 47656,7565.6 47656,7488.9 47656,7488.9"];
	Layer2_Device9_Stage12_Attention -> Layer2_Device9_Stage12_Accumulate	[pos="e,46398,7259.4 46398,7399 46398,7399 46398,7269.4 46398,7269.4"];
	Layer2_Device9_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46717,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage12_Accumulate -> Layer2_Device9_Stage13_Accumulate	[pos="e,46485,7039 46428,7232 46445,7232 46456,7232 46456,7232 46456,7232 46456,7039 46456,7039 46456,7039 46475,7039 46475,7039"];
	Layer2_Device9_Stage13_RecvKV -> Layer2_Device9_Stage13_Attention	[pos="e,48555,7259.7 48555,7425.7 48555,7425.7 48555,7269.7 48555,7269.7"];
	Layer2_Device9_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47405,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage13_RecvKV -> Layer2_Device9_Stage14_RecvKV	[label="Ring transfer",
		lp="47589,7329.3",
		pos="e,47606,7285.9 47606,7372.6 47606,7372.6 47606,7295.9 47606,7295.9"];
	Layer2_Device9_Stage13_Attention -> Layer2_Device9_Stage13_Accumulate	[pos="e,46717,7066.5 48614,7206.1 48614,7176.2 48614,7132 48614,7132 48614,7132 46717,7132 46717,7132 46717,7132 46717,7076.5 46717,7076.5"];
	Layer2_Device9_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46717,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage13_Accumulate -> Layer2_Device9_Stage14_Accumulate	[pos="e,46717,6913.8 46717,7013.2 46717,7013.2 46717,6923.8 46717,6923.8"];
	Layer2_Device9_Stage14_RecvKV -> Layer2_Device9_Stage14_Attention	[pos="e,46119,7066.5 46532,7193 46303,7193 46119,7193 46119,7193 46119,7193 46119,7076.5 46119,7076.5"];
	Layer2_Device9_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47926,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device9_Stage14_RecvKV -> Layer2_Device9_Stage15_RecvKV	[label="Ring transfer",
		lp="47713,7136.3",
		pos="e,47666,7092.9 47666,7179.6 47666,7179.6 47666,7102.9 47666,7102.9"];
	Layer2_Device9_Stage14_Attention -> Layer2_Device9_Stage14_Accumulate	[pos="e,46485,6878 46196,7013.1 46196,6967.1 46196,6878 46196,6878 46196,6878 46475,6878 46475,6878"];
	Layer2_Device9_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="48042,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device9_Stage14_Accumulate -> Layer2_Device9_Stage15_Accumulate	[pos="e,47810,6761 46717,6860.5 46717,6823.4 46717,6761 46717,6761 46717,6761 47800,6761 47800,6761"];
	Layer2_Device9_Stage15_RecvKV -> Layer2_Device9_Stage15_Attention	[pos="e,48042,6913.9 48042,6986.5 48042,6986.5 48042,6923.9 48042,6923.9"];
	Layer2_Device9_Stage15_Attention -> Layer2_Device9_Stage15_Accumulate	[pos="e,48042,6788 48042,6860.6 48042,6860.6 48042,6798 48042,6798"];
	Layer2_Device9_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48300,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device9_Stage15_Accumulate -> Layer2_Device9_ConcatHeads	[pos="e,48174,6662 48174,6734.6 48174,6734.6 48174,6672 48174,6672"];
	Layer2_Device9_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48738,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device9_ConcatHeads -> Layer2_Device9_OutputProj	[pos="e,48525,6509 48376,6608.5 48376,6571.4 48376,6509 48376,6509 48376,6509 48515,6509 48515,6509"];
	Layer2_Device9_OutputProj -> Layer2_Device9_Residual1	[pos="e,48760,6410 48760,6482.6 48760,6482.6 48760,6420 48760,6420"];
	Layer2_Device9_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48745,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device9_Residual1 -> Layer2_Device9_LayerNorm2	[pos="e,48763,6284 48763,6356.6 48763,6356.6 48763,6294 48763,6294"];
	Layer2_Device9_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="48753,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device9_Residual1 -> Layer2_Device9_Residual2	[pos="e,49065,5601.9 49065,6356.4 49065,6356.4 49065,5611.9 49065,5611.9"];
	Layer2_Device9_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48452,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device9_LayerNorm2 -> Layer2_Device9_GateProj	[pos="e,48604,6158 48604,6230.6 48604,6230.6 48604,6168 48604,6168"];
	Layer2_Device9_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48765,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device9_LayerNorm2 -> Layer2_Device9_UpProj	[pos="e,48818,6069.1 48818,6230.5 48818,6230.5 48818,6079.1 48818,6079.1"];
	Layer2_Device9_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48452,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device9_GateProj -> Layer2_Device9_Activation	[pos="e,48384,5979.9 48384,6104.7 48384,6104.7 48384,5989.9 48384,5989.9"];
	Layer2_Device9_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="48548,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device9_UpProj -> Layer2_Device9_ElemMul	[pos="e,48818,5854.1 48818,6015.5 48818,6015.5 48818,5864.1 48818,5864.1"];
	Layer2_Device9_Activation -> Layer2_Device9_ElemMul	[pos="e,48452,5854 48452,5926.6 48452,5926.6 48452,5864 48452,5864"];
	Layer2_Device9_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48691,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device9_ElemMul -> Layer2_Device9_DownProj	[pos="e,48691,5728 48691,5800.6 48691,5800.6 48691,5738 48691,5738"];
	Layer2_Device9_DownProj -> Layer2_Device9_Residual2	[pos="e,48691,5602 48691,5674.6 48691,5674.6 48691,5612 48691,5612"];
	Layer2_Device9_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48753,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device9_Residual2 -> Layer2_Device9_Output	[pos="e,48753,5475.9 48753,5548.6 48753,5548.6 48753,5485.9 48753,5485.9"];
	Layer3_Device9_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 9 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48753,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device9_Output -> Layer3_Device9_Input	[pos="e,48753,5328 48753,5400.6 48753,5400.6 48753,5338 48753,5338"];
	Layer2_Device10_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="52837,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device10_Input -> Layer2_Device10_LayerNorm1	[pos="e,52837,10254 52837,10328 52837,10328 52837,10264 52837,10264"];
	Layer2_Device10_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="53618,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device10_Input -> Layer2_Device10_Residual1	[pos="e,53926,6409.9 53206,10376 53504,10376 53926,10376 53926,10376 53926,10376 53926,6419.9 53926,6419.9"];
	Layer2_Device10_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52817,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device10_LayerNorm1 -> Layer2_Device10_QKVProj	[pos="e,52837,10128 52837,10201 52837,10201 52837,10138 52837,10138"];
	Layer2_Device10_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51634,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage0_RecvKV	[label="Local K,V",
		lp="52134,10031",
		pos="e,52361,9988.1 52361,10075 52361,10075 52361,9998.1 52361,9998.1"];
	Layer2_Device10_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52424,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage0_Attention	[label=Q_local,
		lp="52696,9934.8",
		pos="e,52605,9768.7 52605,10075 52605,10075 52605,9778.7 52605,9778.7"];
	Layer2_Device10_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50094,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage1_Attention	[label=Q_local,
		lp="50158,9838.3",
		pos="e,50286,9575.5 52167,10080 51418,10080 50286,10080 50286,10080 50286,10080 50286,9585.5 50286,9585.5"];
	Layer2_Device10_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50705,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage2_Attention	[label=Q_local,
		lp="52836,9741.8",
		pos="e,50875,9382.3 52700,10075 52700,9980.5 52700,9667 52700,9667 52700,9667 50875,9667 50875,9667 50875,9667 50875,9392.3 50875,9392.3"];
	Layer2_Device10_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52720,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage3_Attention	[label=Q_local,
		lp="52976,9645.3",
		pos="e,52893,9189.6 52893,10075 52893,10075 52893,9199.6 52893,9199.6"];
	Layer2_Device10_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49926,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage4_Attention	[label=Q_local,
		lp="49778,9548.8",
		pos="e,49838,8996.4 52167,10092 51291,10092 49838,10092 49838,10092 49838,10092 49838,9006.4 49838,9006.4"];
	Layer2_Device10_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52838,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage5_Attention	[label=Q_local,
		lp="53080,9452.3",
		pos="e,53010,8803.6 53010,10075 53010,10075 53010,8813.6 53010,8813.6"];
	Layer2_Device10_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52866,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage6_Attention	[label=Q_local,
		lp="53212,9355.8",
		pos="e,53084,8610.6 53084,10075 53084,10075 53084,8620.6 53084,8620.6"];
	Layer2_Device10_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49927,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage7_Attention	[label=Q_local,
		lp="49632,9259.3",
		pos="e,49695,8399 52167,10110 51248,10110 49680,10110 49680,10110 49680,10110 49680,8399 49680,8399 49680,8399 49685,8399 49685,8399"];
	Layer2_Device10_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52866,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage8_Attention	[label=Q_local,
		lp="53330,9162.8",
		pos="e,53098,8197 53155,10075 53155,9844.7 53155,8197 53155,8197 53155,8197 53108,8197 53108,8197"];
	Layer2_Device10_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49927,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage9_Attention	[label=Q_local,
		lp="49514,9066.3",
		pos="e,49695,8013 52167,10116 51245,10116 49666,10116 49666,10116 49666,10116 49666,8013 49666,8013 49666,8013 49685,8013 49685,8013"];
	Layer2_Device10_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49870,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage10_Attention	[label=Q_local,
		lp="49374,8969.8",
		pos="e,49652,7838.5 52167,10121 51241,10121 49652,10121 49652,10121 49652,10121 49652,7848.5 49652,7848.5"];
	Layer2_Device10_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51256,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage11_Attention	[label=Q_local,
		lp="53451,8873.3",
		pos="e,51333,7645.4 53270,10074 53270,9809.8 53270,7676 53270,7676 53270,7676 51333,7676 51333,7676 51333,7676 51333,7655.4 51333,7655.4"];
	Layer2_Device10_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51491,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage12_Attention	[label=Q_local,
		lp="53594,8776.8",
		pos="e,51516,7452.4 53327,10074 53327,9809.6 53327,7674 53327,7674 53327,7674 51516,7674 51516,7674 51516,7674 51516,7462.4 51516,7462.4"];
	Layer2_Device10_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="53506,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage13_Attention	[label=Q_local,
		lp="53734,8680.3",
		pos="e,53643,7259.7 53467,10089 53570,10089 53643,10089 53643,10089 53643,10089 53643,7269.7 53643,7269.7"];
	Layer2_Device10_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51088,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage14_Attention	[label=Q_local,
		lp="49228,8583.8",
		pos="e,50856,7039 53212,10074 53212,9814.1 53212,7747 53212,7747 53212,7747 50709,7747 50709,7747 50709,7747 50709,7039 50709,7039 50709,\
7039 50846,7039 50846,7039"];
	Layer2_Device10_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="53615,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_QKVProj -> Layer2_Device10_Stage15_Attention	[label=Q_local,
		lp="53852,8487.3",
		pos="e,53792,6914.2 53467,10105 53646,10105 53792,10105 53792,10105 53792,10105 53792,6924.2 53792,6924.2"];
	Layer2_Device10_Stage0_RecvKV -> Layer2_Device10_Stage0_Attention	[pos="e,52373,9768.7 52373,9934.7 52373,9934.7 52373,9778.7 52373,9778.7"];
	Layer2_Device10_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51215,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage0_RecvKV -> Layer2_Device10_Stage1_RecvKV	[label="Ring transfer",
		lp="51492,9838.3",
		pos="e,51424,9794.9 51424,9881.6 51424,9881.6 51424,9804.9 51424,9804.9"];
	Layer2_Device10_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="50615,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage0_Attention -> Layer2_Device10_Stage0_Accumulate	[pos="e,50615,9575.4 52424,9715.1 52424,9694.7 52424,9670 52424,9670 52424,9670 50615,9670 50615,9670 50615,9670 50615,9585.4 50615,9585.4"];
	Layer2_Device10_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50184,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage0_Accumulate -> Layer2_Device10_Stage1_Accumulate	[pos="e,50400,9382.4 50400,9522 50400,9522 50400,9392.4 50400,9392.4"];
	Layer2_Device10_Stage1_RecvKV -> Layer2_Device10_Stage1_Attention	[pos="e,50310,9575.7 50310,9741.7 50310,9741.7 50310,9585.7 50310,9585.7"];
	Layer2_Device10_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51824,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage1_RecvKV -> Layer2_Device10_Stage2_RecvKV	[label="Ring transfer",
		lp="51532,9645.3",
		pos="e,51520,9601.9 51520,9688.6 51520,9688.6 51520,9611.9 51520,9611.9"];
	Layer2_Device10_Stage1_Attention -> Layer2_Device10_Stage1_Accumulate	[pos="e,50139,9382.4 50139,9522 50139,9522 50139,9392.4 50139,9392.4"];
	Layer2_Device10_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50302,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage1_Accumulate -> Layer2_Device10_Stage2_Accumulate	[pos="e,50243,9189.4 50243,9329 50243,9329 50243,9199.4 50243,9199.4"];
	Layer2_Device10_Stage2_RecvKV -> Layer2_Device10_Stage2_Attention	[pos="e,50920,9382.7 50920,9548.7 50920,9548.7 50920,9392.7 50920,9392.7"];
	Layer2_Device10_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51914,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage2_RecvKV -> Layer2_Device10_Stage3_RecvKV	[label="Ring transfer",
		lp="51917,9452.3",
		pos="e,51869,9408.9 51869,9495.6 51869,9495.6 51869,9418.9 51869,9418.9"];
	Layer2_Device10_Stage2_Attention -> Layer2_Device10_Stage2_Accumulate	[pos="e,50504,9189.4 50504,9329 50504,9329 50504,9199.4 50504,9199.4"];
	Layer2_Device10_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50447,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage2_Accumulate -> Layer2_Device10_Stage3_Accumulate	[pos="e,50374,8996.4 50374,9136 50374,9136 50374,9006.4 50374,9006.4"];
	Layer2_Device10_Stage3_RecvKV -> Layer2_Device10_Stage3_Attention	[pos="e,52661,9189.7 52661,9355.7 52661,9355.7 52661,9199.7 52661,9199.7"];
	Layer2_Device10_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51511,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage3_RecvKV -> Layer2_Device10_Stage4_RecvKV	[label="Ring transfer",
		lp="51695,9259.3",
		pos="e,51712,9215.9 51712,9302.6 51712,9302.6 51712,9225.9 51712,9225.9"];
	Layer2_Device10_Stage3_Attention -> Layer2_Device10_Stage3_Accumulate	[pos="e,50635,8996.5 52532,9136.3 52532,9111.7 52532,9079 52532,9079 52532,9079 50635,9079 50635,9079 50635,9079 50635,9006.5 50635,9006.5"];
	Layer2_Device10_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50420,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage3_Accumulate -> Layer2_Device10_Stage4_Accumulate	[pos="e,50434,8803.4 50434,8943 50434,8943 50434,8813.4 50434,8813.4"];
	Layer2_Device10_Stage4_RecvKV -> Layer2_Device10_Stage4_Attention	[pos="e,50114,8996.5 50638,9123 50355,9123 50114,9123 50114,9123 50114,9123 50114,9006.5 50114,9006.5"];
	Layer2_Device10_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51656,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage4_RecvKV -> Layer2_Device10_Stage5_RecvKV	[label="Ring transfer",
		lp="51631,9066.3",
		pos="e,51584,9022.9 51584,9109.6 51584,9109.6 51584,9032.9 51584,9032.9"];
	Layer2_Device10_Stage4_Attention -> Layer2_Device10_Stage4_Accumulate	[pos="e,50202,8803.5 50158,8969 50184,8969 50202,8969 50202,8969 50202,8969 50202,8813.5 50202,8813.5"];
	Layer2_Device10_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52345,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage4_Accumulate -> Layer2_Device10_Stage5_Accumulate	[pos="e,52331,8610.8 50434,8750.1 50434,8720.5 50434,8677 50434,8677 50434,8677 52331,8677 52331,8677 52331,8677 52331,8620.8 52331,8620.8"];
	Layer2_Device10_Stage5_RecvKV -> Layer2_Device10_Stage5_Attention	[pos="e,52606,8785 52558,8969.7 52558,8964.1 52558,8785 52558,8785 52558,8785 52596,8785 52596,8785"];
	Layer2_Device10_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51629,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage5_RecvKV -> Layer2_Device10_Stage6_RecvKV	[label="Ring transfer",
		lp="51690,8873.3",
		pos="e,51642,8829.9 51642,8916.6 51642,8916.6 51642,8839.9 51642,8839.9"];
	Layer2_Device10_Stage5_Attention -> Layer2_Device10_Stage5_Accumulate	[pos="e,52567,8610.5 52606,8767 52582,8767 52567,8767 52567,8767 52567,8767 52567,8620.5 52567,8620.5"];
	Layer2_Device10_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52345,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage5_Accumulate -> Layer2_Device10_Stage6_Accumulate	[pos="e,52345,8417.4 52345,8557 52345,8557 52345,8427.4 52345,8427.4"];
	Layer2_Device10_Stage6_RecvKV -> Layer2_Device10_Stage6_Attention	[pos="e,52852,8610.5 52219,8737 52532,8737 52852,8737 52852,8737 52852,8737 52852,8620.5 52852,8620.5"];
	Layer2_Device10_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51136,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage6_RecvKV -> Layer2_Device10_Stage7_RecvKV	[label="Ring transfer",
		lp="51430,8680.3",
		pos="e,51382,8636.9 51382,8723.6 51382,8723.6 51382,8646.9 51382,8646.9"];
	Layer2_Device10_Stage6_Attention -> Layer2_Device10_Stage6_Accumulate	[pos="e,52577,8381 52943,8557.3 52943,8502.3 52943,8381 52943,8381 52943,8381 52587,8381 52587,8381"];
	Layer2_Device10_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52345,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage6_Accumulate -> Layer2_Device10_Stage7_Accumulate	[pos="e,52422,8224.4 52422,8364 52422,8364 52422,8234.4 52422,8234.4"];
	Layer2_Device10_Stage7_RecvKV -> Layer2_Device10_Stage7_Attention	[pos="e,49926,8417.5 50263,8544 50072,8544 49926,8544 49926,8544 49926,8544 49926,8427.5 49926,8427.5"];
	Layer2_Device10_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51136,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage7_RecvKV -> Layer2_Device10_Stage8_RecvKV	[label="Ring transfer",
		lp="51184,8487.3",
		pos="e,51136,8443.9 51136,8530.6 51136,8530.6 51136,8453.9 51136,8453.9"];
	Layer2_Device10_Stage7_Attention -> Layer2_Device10_Stage7_Accumulate	[pos="e,52268,8224.3 49850,8364.2 49850,8348.6 49850,8332 49850,8332 49850,8332 52268,8332 52268,8332 52268,8332 52268,8234.3 52268,8234.3"];
	Layer2_Device10_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52345,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage7_Accumulate -> Layer2_Device10_Stage8_Accumulate	[pos="e,52345,8031.4 52345,8171 52345,8171 52345,8041.4 52345,8041.4"];
	Layer2_Device10_Stage8_RecvKV -> Layer2_Device10_Stage8_Attention	[pos="e,52789,8224.5 51726,8351 52201,8351 52789,8351 52789,8351 52789,8351 52789,8234.5 52789,8234.5"];
	Layer2_Device10_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51136,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage8_RecvKV -> Layer2_Device10_Stage9_RecvKV	[label="Ring transfer",
		lp="51184,8294.3",
		pos="e,51136,8250.9 51136,8337.6 51136,8337.6 51136,8260.9 51136,8260.9"];
	Layer2_Device10_Stage8_Attention -> Layer2_Device10_Stage8_Accumulate	[pos="e,52577,8004 52866,8171.3 52866,8118.2 52866,8004 52866,8004 52866,8004 52587,8004 52587,8004"];
	Layer2_Device10_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50391,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage8_Accumulate -> Layer2_Device10_Stage9_Accumulate	[pos="e,50419,7838.6 52317,7978.2 52317,7957 52317,7931 52317,7931 52317,7931 50419,7931 50419,7931 50419,7931 50419,7848.6 50419,7848.6"];
	Layer2_Device10_Stage9_RecvKV -> Layer2_Device10_Stage9_Attention	[pos="e,50004,8031.5 50263,8158 50113,8158 50004,8158 50004,8158 50004,8158 50004,8041.5 50004,8041.5"];
	Layer2_Device10_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51136,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage9_RecvKV -> Layer2_Device10_Stage10_RecvKV	[label="Ring transfer",
		lp="51184,8101.3",
		pos="e,51136,8057.9 51136,8144.6 51136,8144.6 51136,8067.9 51136,8067.9"];
	Layer2_Device10_Stage9_Attention -> Layer2_Device10_Stage9_Accumulate	[pos="e,50174,7838.5 50159,8004 50168,8004 50174,8004 50174,8004 50174,8004 50174,7848.5 50174,7848.5"];
	Layer2_Device10_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50391,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage9_Accumulate -> Layer2_Device10_Stage10_Accumulate	[pos="e,50391,7645.4 50391,7785 50391,7785 50391,7655.4 50391,7655.4"];
	Layer2_Device10_Stage10_RecvKV -> Layer2_Device10_Stage10_Attention	[pos="e,49898,7838.5 50263,7965 50058,7965 49898,7965 49898,7965 49898,7965 49898,7848.5 49898,7848.5"];
	Layer2_Device10_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51600,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage10_RecvKV -> Layer2_Device10_Stage11_RecvKV	[label="Ring transfer",
		lp="51453,7908.3",
		pos="e,51368,7864.9 51368,7951.6 51368,7951.6 51368,7874.9 51368,7874.9"];
	Layer2_Device10_Stage10_Attention -> Layer2_Device10_Stage10_Accumulate	[pos="e,50159,7618 50084,7785.3 50084,7732.2 50084,7618 50084,7618 50084,7618 50149,7618 50149,7618"];
	Layer2_Device10_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50970,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage10_Accumulate -> Layer2_Device10_Stage11_Accumulate	[pos="e,50738,7425 50554,7592.3 50554,7539.2 50554,7425 50554,7425 50554,7425 50728,7425 50728,7425"];
	Layer2_Device10_Stage11_RecvKV -> Layer2_Device10_Stage11_Attention	[pos="e,51179,7645.5 51179,7758.6 51179,7758.6 51179,7655.5 51179,7655.5"];
	Layer2_Device10_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52465,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage11_RecvKV -> Layer2_Device10_Stage12_RecvKV	[label="Ring transfer",
		lp="52081,7715.3",
		pos="e,52032,7671.9 52032,7758.6 52032,7758.6 52032,7681.9 52032,7681.9"];
	Layer2_Device10_Stage11_Attention -> Layer2_Device10_Stage11_Accumulate	[pos="e,51113,7452.4 51113,7592 51113,7592 51113,7462.4 51113,7462.4"];
	Layer2_Device10_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51088,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage11_Accumulate -> Layer2_Device10_Stage12_Accumulate	[pos="e,51029,7259.4 51029,7399 51029,7399 51029,7269.4 51029,7269.4"];
	Layer2_Device10_Stage12_RecvKV -> Layer2_Device10_Stage12_Attention	[pos="e,51634,7452.5 51634,7565.6 51634,7565.6 51634,7462.5 51634,7462.5"];
	Layer2_Device10_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52700,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage12_RecvKV -> Layer2_Device10_Stage13_RecvKV	[label="Ring transfer",
		lp="52678,7522.3",
		pos="e,52582,7478.9 52582,7565.6 52582,7565.6 52582,7488.9 52582,7488.9"];
	Layer2_Device10_Stage12_Attention -> Layer2_Device10_Stage12_Accumulate	[pos="e,51290,7259.4 51290,7399 51290,7399 51290,7269.4 51290,7269.4"];
	Layer2_Device10_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51609,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage12_Accumulate -> Layer2_Device10_Stage13_Accumulate	[pos="e,51377,7039 51320,7232 51337,7232 51348,7232 51348,7232 51348,7232 51348,7039 51348,7039 51348,7039 51367,7039 51367,7039"];
	Layer2_Device10_Stage13_RecvKV -> Layer2_Device10_Stage13_Attention	[pos="e,53447,7259.7 53447,7425.7 53447,7425.7 53447,7269.7 53447,7269.7"];
	Layer2_Device10_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52297,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage13_RecvKV -> Layer2_Device10_Stage14_RecvKV	[label="Ring transfer",
		lp="52481,7329.3",
		pos="e,52498,7285.9 52498,7372.6 52498,7372.6 52498,7295.9 52498,7295.9"];
	Layer2_Device10_Stage13_Attention -> Layer2_Device10_Stage13_Accumulate	[pos="e,51609,7066.4 53506,7206.1 53506,7177.4 53506,7136 53506,7136 53506,7136 51609,7136 51609,7136 51609,7136 51609,7076.4 51609,7076.4"];
	Layer2_Device10_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51609,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage13_Accumulate -> Layer2_Device10_Stage14_Accumulate	[pos="e,51609,6913.8 51609,7013.2 51609,7013.2 51609,6923.8 51609,6923.8"];
	Layer2_Device10_Stage14_RecvKV -> Layer2_Device10_Stage14_Attention	[pos="e,51088,7066.5 51424,7193 51234,7193 51088,7193 51088,7193 51088,7193 51088,7076.5 51088,7076.5"];
	Layer2_Device10_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52818,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device10_Stage14_RecvKV -> Layer2_Device10_Stage15_RecvKV	[label="Ring transfer",
		lp="52605,7136.3",
		pos="e,52558,7092.9 52558,7179.6 52558,7179.6 52558,7102.9 52558,7102.9"];
	Layer2_Device10_Stage14_Attention -> Layer2_Device10_Stage14_Accumulate	[pos="e,51377,6878 51088,7013.1 51088,6967.1 51088,6878 51088,6878 51088,6878 51367,6878 51367,6878"];
	Layer2_Device10_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="53615,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device10_Stage14_Accumulate -> Layer2_Device10_Stage15_Accumulate	[pos="e,53383,6761 51609,6860.5 51609,6823.4 51609,6761 51609,6761 51609,6761 53373,6761 53373,6761"];
	Layer2_Device10_Stage15_RecvKV -> Layer2_Device10_Stage15_Attention	[pos="e,53561,6913.8 53561,7039.7 53561,7039.7 53561,6923.8 53561,6923.8"];
	Layer2_Device10_Stage15_Attention -> Layer2_Device10_Stage15_Accumulate	[pos="e,53615,6788 53615,6860.6 53615,6860.6 53615,6798 53615,6798"];
	Layer2_Device10_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53616,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device10_Stage15_Accumulate -> Layer2_Device10_ConcatHeads	[pos="e,53616,6662 53616,6734.6 53616,6734.6 53616,6672 53616,6672"];
	Layer2_Device10_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53618,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device10_ConcatHeads -> Layer2_Device10_OutputProj	[pos="e,53618,6536 53618,6608.6 53618,6608.6 53618,6546 53618,6546"];
	Layer2_Device10_OutputProj -> Layer2_Device10_Residual1	[pos="e,53618,6410 53618,6482.6 53618,6482.6 53618,6420 53618,6420"];
	Layer2_Device10_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53381,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device10_Residual1 -> Layer2_Device10_LayerNorm2	[pos="e,53413,6284 53413,6356.6 53413,6356.6 53413,6294 53413,6294"];
	Layer2_Device10_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="53678,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device10_Residual1 -> Layer2_Device10_Residual2	[pos="e,53815,5601.9 53815,6356.4 53815,6356.4 53815,5611.9 53815,5611.9"];
	Layer2_Device10_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53088,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device10_LayerNorm2 -> Layer2_Device10_GateProj	[pos="e,53240,6158 53240,6230.6 53240,6230.6 53240,6168 53240,6168"];
	Layer2_Device10_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53401,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device10_LayerNorm2 -> Layer2_Device10_UpProj	[pos="e,53454,6069.1 53454,6230.5 53454,6230.5 53454,6079.1 53454,6079.1"];
	Layer2_Device10_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53088,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device10_GateProj -> Layer2_Device10_Activation	[pos="e,53020,5979.9 53020,6104.7 53020,6104.7 53020,5989.9 53020,5989.9"];
	Layer2_Device10_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="53184,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device10_UpProj -> Layer2_Device10_ElemMul	[pos="e,53454,5854.1 53454,6015.5 53454,6015.5 53454,5864.1 53454,5864.1"];
	Layer2_Device10_Activation -> Layer2_Device10_ElemMul	[pos="e,53088,5854 53088,5926.6 53088,5926.6 53088,5864 53088,5864"];
	Layer2_Device10_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53327,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device10_ElemMul -> Layer2_Device10_DownProj	[pos="e,53327,5728 53327,5800.6 53327,5800.6 53327,5738 53327,5738"];
	Layer2_Device10_DownProj -> Layer2_Device10_Residual2	[pos="e,53419,5602 53419,5674.6 53419,5674.6 53419,5612 53419,5612"];
	Layer2_Device10_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53678,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device10_Residual2 -> Layer2_Device10_Output	[pos="e,53678,5475.9 53678,5548.6 53678,5548.6 53678,5485.9 53678,5485.9"];
	Layer3_Device10_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 10 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53678,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device10_Output -> Layer3_Device10_Input	[pos="e,53678,5328 53678,5400.6 53678,5400.6 53678,5338 53678,5338"];
	Layer2_Device11_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58196,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device11_Input -> Layer2_Device11_LayerNorm1	[pos="e,58381,10254 58458,10351 58413,10351 58381,10351 58381,10351 58381,10351 58381,10264 58381,10264"];
	Layer2_Device11_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="58678,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device11_Input -> Layer2_Device11_Residual1	[pos="e,58892,6410 58892,10332 58892,10332 58892,6420 58892,6420"];
	Layer2_Device11_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57778,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device11_LayerNorm1 -> Layer2_Device11_QKVProj	[pos="e,58196,10128 58196,10201 58196,10201 58196,10138 58196,10138"];
	Layer2_Device11_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56532,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage0_RecvKV	[label="Local K,V",
		lp="57032,10031",
		pos="e,57290,9988.1 57290,10075 57290,10075 57290,9998.1 57290,9998.1"];
	Layer2_Device11_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57322,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage0_Attention	[label=Q_local,
		lp="57594,9934.8",
		pos="e,57503,9768.7 57503,10075 57503,10075 57503,9778.7 57503,9778.7"];
	Layer2_Device11_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54992,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage1_Attention	[label=Q_local,
		lp="55056,9838.3",
		pos="e,54990,9575.6 57128,10082 56304,10082 54990,10082 54990,10082 54990,10082 54990,9585.6 54990,9585.6"];
	Layer2_Device11_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55603,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage2_Attention	[label=Q_local,
		lp="57734,9741.8",
		pos="e,55773,9382.5 57598,10075 57598,9981.2 57598,9672 57598,9672 57598,9672 55773,9672 55773,9672 55773,9672 55773,9392.5 55773,9392.5"];
	Layer2_Device11_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57618,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage3_Attention	[label=Q_local,
		lp="57874,9645.3",
		pos="e,57791,9189.6 57791,10075 57791,10075 57791,9199.6 57791,9199.6"];
	Layer2_Device11_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54824,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage4_Attention	[label=Q_local,
		lp="54676,9548.8",
		pos="e,54676,8996.6 57128,10097 56219,10097 54676,10097 54676,10097 54676,10097 54676,9006.6 54676,9006.6"];
	Layer2_Device11_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57738,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage5_Attention	[label=Q_local,
		lp="57978,9452.3",
		pos="e,57910,8803.6 57910,10075 57910,10075 57910,8813.6 57910,8813.6"];
	Layer2_Device11_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57766,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage6_Attention	[label=Q_local,
		lp="58110,9355.8",
		pos="e,57984,8610.6 57984,10075 57984,10075 57984,8620.6 57984,8620.6"];
	Layer2_Device11_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54827,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage7_Attention	[label=Q_local,
		lp="54530,9259.3",
		pos="e,54595,8399 57128,10112 56180,10112 54528,10112 54528,10112 54528,10112 54528,8399 54528,8399 54528,8399 54585,8399 54585,8399"];
	Layer2_Device11_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57571,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage8_Attention	[label=Q_local,
		lp="58230,9162.8",
		pos="e,57803,8197 58058,10075 58058,9844.7 58058,8197 58058,8197 58058,8197 57813,8197 57813,8197"];
	Layer2_Device11_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54632,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage9_Attention	[label=Q_local,
		lp="54412,9066.3",
		pos="e,54464,8031.6 57128,10120 56163,10120 54464,10120 54464,10120 54464,10120 54464,8041.6 54464,8041.6"];
	Layer2_Device11_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55070,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage10_Attention	[label=Q_local,
		lp="54272,8969.8",
		pos="e,55175,7838.4 58119,10075 58119,9826.9 58119,7933 58119,7933 58119,7933 55175,7933 55175,7933 55175,7933 55175,7848.4 55175,7848.4"];
	Layer2_Device11_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56172,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage11_Attention	[label=Q_local,
		lp="58353,8873.3",
		pos="e,56249,7645.5 58179,10075 58179,9812 58179,7694 58179,7694 58179,7694 56249,7694 56249,7694 56249,7694 56249,7655.5 56249,7655.5"];
	Layer2_Device11_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56391,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage12_Attention	[label=Q_local,
		lp="58494,8776.8",
		pos="e,56432,7452.8 58240,10075 58240,9811.8 58240,7692 58240,7692 58240,7692 56432,7692 56432,7692 56432,7692 56432,7462.8 56432,7462.8"];
	Layer2_Device11_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="58406,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage13_Attention	[label=Q_local,
		lp="58634,8680.3",
		pos="e,58579,7259.7 58428,10088 58517,10088 58579,10088 58579,10088 58579,10088 58579,7269.7 58579,7269.7"];
	Layer2_Device11_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55988,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage14_Attention	[label=Q_local,
		lp="54126,8583.8",
		pos="e,56065,7066.5 58428,10101 58567,10101 58673,10101 58673,10101 58673,10101 58673,7163 58673,7163 58673,7163 56065,7163 56065,7163 \
56065,7163 56065,7076.5 56065,7076.5"];
	Layer2_Device11_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57783,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_QKVProj -> Layer2_Device11_Stage15_Attention	[label=Q_local,
		lp="58752,8487.3",
		pos="e,58015,6896 58428,10107 58584,10107 58708,10107 58708,10107 58708,10107 58708,6896 58708,6896 58708,6896 58025,6896 58025,6896"];
	Layer2_Device11_Stage0_RecvKV -> Layer2_Device11_Stage0_Attention	[pos="e,57271,9768.7 57271,9934.7 57271,9934.7 57271,9778.7 57271,9778.7"];
	Layer2_Device11_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56113,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage0_RecvKV -> Layer2_Device11_Stage1_RecvKV	[label="Ring transfer",
		lp="56370,9838.3",
		pos="e,56322,9794.9 56322,9881.6 56322,9881.6 56322,9804.9 56322,9804.9"];
	Layer2_Device11_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="55513,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage0_Attention -> Layer2_Device11_Stage0_Accumulate	[pos="e,55513,9575.6 57322,9715 57322,9696.4 57322,9675 57322,9675 57322,9675 55513,9675 55513,9675 55513,9675 55513,9585.6 55513,9585.6"];
	Layer2_Device11_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55082,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage0_Accumulate -> Layer2_Device11_Stage1_Accumulate	[pos="e,55298,9382.4 55298,9522 55298,9522 55298,9392.4 55298,9392.4"];
	Layer2_Device11_Stage1_RecvKV -> Layer2_Device11_Stage1_Attention	[pos="e,55208,9575.7 55208,9741.7 55208,9741.7 55208,9585.7 55208,9585.7"];
	Layer2_Device11_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56722,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage1_RecvKV -> Layer2_Device11_Stage2_RecvKV	[label="Ring transfer",
		lp="56447,9645.3",
		pos="e,56418,9601.9 56418,9688.6 56418,9688.6 56418,9611.9 56418,9611.9"];
	Layer2_Device11_Stage1_Attention -> Layer2_Device11_Stage1_Accumulate	[pos="e,55037,9382.4 55037,9522 55037,9522 55037,9392.4 55037,9392.4"];
	Layer2_Device11_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55200,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage1_Accumulate -> Layer2_Device11_Stage2_Accumulate	[pos="e,55141,9189.4 55141,9329 55141,9329 55141,9199.4 55141,9199.4"];
	Layer2_Device11_Stage2_RecvKV -> Layer2_Device11_Stage2_Attention	[pos="e,55818,9382.7 55818,9548.7 55818,9548.7 55818,9392.7 55818,9392.7"];
	Layer2_Device11_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56812,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage2_RecvKV -> Layer2_Device11_Stage3_RecvKV	[label="Ring transfer",
		lp="56815,9452.3",
		pos="e,56767,9408.9 56767,9495.6 56767,9495.6 56767,9418.9 56767,9418.9"];
	Layer2_Device11_Stage2_Attention -> Layer2_Device11_Stage2_Accumulate	[pos="e,55402,9189.4 55402,9329 55402,9329 55402,9199.4 55402,9199.4"];
	Layer2_Device11_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55345,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage2_Accumulate -> Layer2_Device11_Stage3_Accumulate	[pos="e,55272,8996.4 55272,9136 55272,9136 55272,9006.4 55272,9006.4"];
	Layer2_Device11_Stage3_RecvKV -> Layer2_Device11_Stage3_Attention	[pos="e,57559,9189.7 57559,9355.7 57559,9355.7 57559,9199.7 57559,9199.7"];
	Layer2_Device11_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56409,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage3_RecvKV -> Layer2_Device11_Stage4_RecvKV	[label="Ring transfer",
		lp="56726,9259.3",
		pos="e,56610,9215.9 56610,9302.6 56610,9302.6 56610,9225.9 56610,9225.9"];
	Layer2_Device11_Stage3_Attention -> Layer2_Device11_Stage3_Accumulate	[pos="e,55533,8996.6 57430,9136.1 57430,9113.3 57430,9084 57430,9084 57430,9084 55533,9084 55533,9084 55533,9084 55533,9006.6 55533,9006.6"];
	Layer2_Device11_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55320,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage3_Accumulate -> Layer2_Device11_Stage4_Accumulate	[pos="e,55332,8803.4 55332,8943 55332,8943 55332,8813.4 55332,8813.4"];
	Layer2_Device11_Stage4_RecvKV -> Layer2_Device11_Stage4_Attention	[pos="e,55012,8996.5 55536,9123 55253,9123 55012,9123 55012,9123 55012,9123 55012,9006.5 55012,9006.5"];
	Layer2_Device11_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56554,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage4_RecvKV -> Layer2_Device11_Stage5_RecvKV	[label="Ring transfer",
		lp="56529,9066.3",
		pos="e,56482,9022.9 56482,9109.6 56482,9109.6 56482,9032.9 56482,9032.9"];
	Layer2_Device11_Stage4_Attention -> Layer2_Device11_Stage4_Accumulate	[pos="e,55101,8803.5 55056,8969 55083,8969 55101,8969 55101,8969 55101,8969 55101,8813.5 55101,8813.5"];
	Layer2_Device11_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57245,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage4_Accumulate -> Layer2_Device11_Stage5_Accumulate	[pos="e,57231,8610.4 55334,8750.2 55334,8722.1 55334,8682 55334,8682 55334,8682 57231,8682 57231,8682 57231,8682 57231,8620.4 57231,8620.4"];
	Layer2_Device11_Stage5_RecvKV -> Layer2_Device11_Stage5_Attention	[pos="e,57506,8776 57461,8969.7 57461,8963.8 57461,8776 57461,8776 57461,8776 57496,8776 57496,8776"];
	Layer2_Device11_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56529,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage5_RecvKV -> Layer2_Device11_Stage6_RecvKV	[label="Ring transfer",
		lp="56590,8873.3",
		pos="e,56542,8829.9 56542,8916.6 56542,8916.6 56542,8839.9 56542,8839.9"];
	Layer2_Device11_Stage5_Attention -> Layer2_Device11_Stage5_Accumulate	[pos="e,57477,8583 57520,8750.3 57520,8697.2 57520,8583 57520,8583 57520,8583 57487,8583 57487,8583"];
	Layer2_Device11_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57245,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage5_Accumulate -> Layer2_Device11_Stage6_Accumulate	[pos="e,57245,8417.4 57245,8557 57245,8557 57245,8427.4 57245,8427.4"];
	Layer2_Device11_Stage6_RecvKV -> Layer2_Device11_Stage6_Attention	[pos="e,57752,8610.5 57119,8737 57432,8737 57752,8737 57752,8737 57752,8737 57752,8620.5 57752,8620.5"];
	Layer2_Device11_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56036,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage6_RecvKV -> Layer2_Device11_Stage7_RecvKV	[label="Ring transfer",
		lp="56331,8680.3",
		pos="e,56282,8636.9 56282,8723.6 56282,8723.6 56282,8646.9 56282,8646.9"];
	Layer2_Device11_Stage6_Attention -> Layer2_Device11_Stage6_Accumulate	[pos="e,57477,8381 57668,8557.3 57668,8502.3 57668,8381 57668,8381 57668,8381 57487,8381 57487,8381"];
	Layer2_Device11_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57050,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage6_Accumulate -> Layer2_Device11_Stage7_Accumulate	[pos="e,57148,8224.4 57148,8364 57148,8364 57148,8234.4 57148,8234.4"];
	Layer2_Device11_Stage7_RecvKV -> Layer2_Device11_Stage7_Attention	[pos="e,54826,8417.5 55163,8544 54972,8544 54826,8544 54826,8544 54826,8544 54826,8427.5 54826,8427.5"];
	Layer2_Device11_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56036,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage7_RecvKV -> Layer2_Device11_Stage8_RecvKV	[label="Ring transfer",
		lp="56084,8487.3",
		pos="e,56036,8443.9 56036,8530.6 56036,8530.6 56036,8453.9 56036,8453.9"];
	Layer2_Device11_Stage7_Attention -> Layer2_Device11_Stage7_Accumulate	[pos="e,56887,8224.4 54990,8364.2 54990,8336.1 54990,8296 54990,8296 54990,8296 56887,8296 56887,8296 56887,8296 56887,8234.4 56887,8234.4"];
	Layer2_Device11_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57050,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage7_Accumulate -> Layer2_Device11_Stage8_Accumulate	[pos="e,57050,8031.4 57050,8171 57050,8171 57050,8041.4 57050,8041.4"];
	Layer2_Device11_Stage8_RecvKV -> Layer2_Device11_Stage8_Attention	[pos="e,57408,8224.5 56626,8351 56998,8351 57408,8351 57408,8351 57408,8351 57408,8234.5 57408,8234.5"];
	Layer2_Device11_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="55841,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage8_RecvKV -> Layer2_Device11_Stage9_RecvKV	[label="Ring transfer",
		lp="55954,8294.3",
		pos="e,55938,8250.9 55938,8337.6 55938,8337.6 55938,8260.9 55938,8260.9"];
	Layer2_Device11_Stage8_Attention -> Layer2_Device11_Stage8_Accumulate	[pos="e,57282,8004 57530,8171.3 57530,8118.2 57530,8004 57530,8004 57530,8004 57292,8004 57292,8004"];
	Layer2_Device11_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55591,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage8_Accumulate -> Layer2_Device11_Stage9_Accumulate	[pos="e,55668,7838.4 57050,7978.2 57050,7944.1 57050,7889 57050,7889 57050,7889 55668,7889 55668,7889 55668,7889 55668,7848.4 55668,7848.4"];
	Layer2_Device11_Stage9_RecvKV -> Layer2_Device11_Stage9_Attention	[pos="e,54730,8031.5 54968,8158 54829,8158 54730,8158 54730,8158 54730,8158 54730,8041.5 54730,8041.5"];
	Layer2_Device11_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="55841,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage9_RecvKV -> Layer2_Device11_Stage10_RecvKV	[label="Ring transfer",
		lp="55889,8101.3",
		pos="e,55841,8057.9 55841,8144.6 55841,8144.6 55841,8067.9 55841,8067.9"];
	Layer2_Device11_Stage9_Attention -> Layer2_Device11_Stage9_Accumulate	[pos="e,55514,7838.6 54851,7978 54851,7934 54851,7851 54851,7851 54851,7851 55514,7851 55514,7851 55514,7851 55514,7848.6 55514,7848.6"];
	Layer2_Device11_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55591,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage9_Accumulate -> Layer2_Device11_Stage10_Accumulate	[pos="e,55591,7645.4 55591,7785 55591,7785 55591,7655.4 55591,7655.4"];
	Layer2_Device11_Stage10_RecvKV -> Layer2_Device11_Stage10_Attention	[pos="e,55048,7838.5 55048,7951.6 55048,7951.6 55048,7848.5 55048,7848.5"];
	Layer2_Device11_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56800,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage10_RecvKV -> Layer2_Device11_Stage11_RecvKV	[label="Ring transfer",
		lp="56158,7908.3",
		pos="e,56320,7864.9 56320,7951.6 56320,7951.6 56320,7874.9 56320,7874.9"];
	Layer2_Device11_Stage10_Attention -> Layer2_Device11_Stage10_Accumulate	[pos="e,55359,7618 55096,7785.3 55096,7732.2 55096,7618 55096,7618 55096,7618 55349,7618 55349,7618"];
	Layer2_Device11_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55870,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage10_Accumulate -> Layer2_Device11_Stage11_Accumulate	[pos="e,55730,7452.4 55730,7592 55730,7592 55730,7462.4 55730,7462.4"];
	Layer2_Device11_Stage11_RecvKV -> Layer2_Device11_Stage11_Attention	[pos="e,56095,7645.5 56095,7758.6 56095,7758.6 56095,7655.5 56095,7655.5"];
	Layer2_Device11_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57381,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage11_RecvKV -> Layer2_Device11_Stage12_RecvKV	[label="Ring transfer",
		lp="57028,7715.3",
		pos="e,57090,7671.9 57090,7758.6 57090,7758.6 57090,7681.9 57090,7681.9"];
	Layer2_Device11_Stage11_Attention -> Layer2_Device11_Stage11_Accumulate	[pos="e,56021,7452.4 56021,7592 56021,7592 56021,7462.4 56021,7462.4"];
	Layer2_Device11_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55988,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage11_Accumulate -> Layer2_Device11_Stage12_Accumulate	[pos="e,55929,7259.4 55929,7399 55929,7399 55929,7269.4 55929,7269.4"];
	Layer2_Device11_Stage12_RecvKV -> Layer2_Device11_Stage12_Attention	[pos="e,56542,7452.7 56542,7618.7 56542,7618.7 56542,7462.7 56542,7462.7"];
	Layer2_Device11_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57600,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage12_RecvKV -> Layer2_Device11_Stage13_RecvKV	[label="Ring transfer",
		lp="57539,7522.3",
		pos="e,57490,7478.9 57490,7565.6 57490,7565.6 57490,7488.9 57490,7488.9"];
	Layer2_Device11_Stage12_Attention -> Layer2_Device11_Stage12_Accumulate	[pos="e,56190,7259.4 56190,7399 56190,7399 56190,7269.4 56190,7269.4"];
	Layer2_Device11_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56509,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage12_Accumulate -> Layer2_Device11_Stage13_Accumulate	[pos="e,56277,7039 56220,7232 56237,7232 56248,7232 56248,7232 56248,7232 56248,7039 56248,7039 56248,7039 56267,7039 56267,7039"];
	Layer2_Device11_Stage13_RecvKV -> Layer2_Device11_Stage13_Attention	[pos="e,58347,7259.7 58347,7425.7 58347,7425.7 58347,7269.7 58347,7269.7"];
	Layer2_Device11_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57197,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage13_RecvKV -> Layer2_Device11_Stage14_RecvKV	[label="Ring transfer",
		lp="57446,7329.3",
		pos="e,57398,7285.9 57398,7372.6 57398,7372.6 57398,7295.9 57398,7295.9"];
	Layer2_Device11_Stage13_Attention -> Layer2_Device11_Stage13_Accumulate	[pos="e,56509,7066.4 58406,7206 58406,7178.6 58406,7140 58406,7140 58406,7140 56509,7140 56509,7140 56509,7140 56509,7076.4 56509,7076.4"];
	Layer2_Device11_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56509,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage13_Accumulate -> Layer2_Device11_Stage14_Accumulate	[pos="e,56509,6913.8 56509,7013.2 56509,7013.2 56509,6923.8 56509,6923.8"];
	Layer2_Device11_Stage14_RecvKV -> Layer2_Device11_Stage14_Attention	[pos="e,55911,7066.5 56324,7193 56095,7193 55911,7193 55911,7193 55911,7193 55911,7076.5 55911,7076.5"];
	Layer2_Device11_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57718,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device11_Stage14_RecvKV -> Layer2_Device11_Stage15_RecvKV	[label="Ring transfer",
		lp="57505,7136.3",
		pos="e,57458,7092.9 57458,7179.6 57458,7179.6 57458,7102.9 57458,7102.9"];
	Layer2_Device11_Stage14_Attention -> Layer2_Device11_Stage14_Accumulate	[pos="e,56277,6887 55988,7013.2 55988,6969.4 55988,6887 55988,6887 55988,6887 56267,6887 56267,6887"];
	Layer2_Device11_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="57783,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device11_Stage14_Accumulate -> Layer2_Device11_Stage15_Accumulate	[pos="e,57551,6761 56509,6860.5 56509,6823.4 56509,6761 56509,6761 56509,6761 57541,6761 57541,6761"];
	Layer2_Device11_Stage15_RecvKV -> Layer2_Device11_Stage15_Attention	[pos="e,57783,6913.9 57783,6986.5 57783,6986.5 57783,6923.9 57783,6923.9"];
	Layer2_Device11_Stage15_Attention -> Layer2_Device11_Stage15_Accumulate	[pos="e,57783,6788 57783,6860.6 57783,6860.6 57783,6798 57783,6798"];
	Layer2_Device11_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58015,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device11_Stage15_Accumulate -> Layer2_Device11_ConcatHeads	[pos="e,57902,6662 57902,6734.6 57902,6734.6 57902,6672 57902,6672"];
	Layer2_Device11_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58530,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device11_ConcatHeads -> Layer2_Device11_OutputProj	[pos="e,58317,6509 58232,6608.5 58232,6571.4 58232,6509 58232,6509 58232,6509 58307,6509 58307,6509"];
	Layer2_Device11_OutputProj -> Layer2_Device11_Residual1	[pos="e,58530,6410 58530,6482.6 58530,6482.6 58530,6420 58530,6420"];
	Layer2_Device11_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58436,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device11_Residual1 -> Layer2_Device11_LayerNorm2	[pos="e,58470,6284 58470,6356.6 58470,6356.6 58470,6294 58470,6294"];
	Layer2_Device11_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="58392,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device11_Residual1 -> Layer2_Device11_Residual2	[pos="e,58730,5601.9 58730,6356.4 58730,6356.4 58730,5611.9 58730,5611.9"];
	Layer2_Device11_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58143,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device11_LayerNorm2 -> Layer2_Device11_GateProj	[pos="e,58296,6158 58296,6230.6 58296,6230.6 58296,6168 58296,6168"];
	Layer2_Device11_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58456,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device11_LayerNorm2 -> Layer2_Device11_UpProj	[pos="e,58508,6069.1 58508,6230.5 58508,6230.5 58508,6079.1 58508,6079.1"];
	Layer2_Device11_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58143,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device11_GateProj -> Layer2_Device11_Activation	[pos="e,58074,5979.9 58074,6104.7 58074,6104.7 58074,5989.9 58074,5989.9"];
	Layer2_Device11_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="58239,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device11_UpProj -> Layer2_Device11_ElemMul	[pos="e,58509,5854.1 58509,6015.5 58509,6015.5 58509,5864.1 58509,5864.1"];
	Layer2_Device11_Activation -> Layer2_Device11_ElemMul	[pos="e,58143,5854 58143,5926.6 58143,5926.6 58143,5864 58143,5864"];
	Layer2_Device11_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58362,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device11_ElemMul -> Layer2_Device11_DownProj	[pos="e,58362,5728 58362,5800.6 58362,5800.6 58362,5738 58362,5738"];
	Layer2_Device11_DownProj -> Layer2_Device11_Residual2	[pos="e,58362,5602 58362,5674.6 58362,5674.6 58362,5612 58362,5612"];
	Layer2_Device11_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58392,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device11_Residual2 -> Layer2_Device11_Output	[pos="e,58392,5475.9 58392,5548.6 58392,5548.6 58392,5485.9 58392,5485.9"];
	Layer3_Device11_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 11 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="58392,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device11_Output -> Layer3_Device11_Input	[pos="e,58392,5328 58392,5400.6 58392,5400.6 58392,5338 58392,5338"];
	Layer2_Device12_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63090,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device12_Input -> Layer2_Device12_LayerNorm1	[pos="e,63287,10254 63357,10364 63316,10364 63287,10364 63287,10364 63287,10364 63287,10264 63287,10264"];
	Layer2_Device12_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="63513,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device12_Input -> Layer2_Device12_Residual1	[pos="e,63773,6409.9 63773,10330 63773,10330 63773,6419.9 63773,6419.9"];
	Layer2_Device12_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62673,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device12_LayerNorm1 -> Layer2_Device12_QKVProj	[pos="e,63090,10128 63090,10201 63090,10201 63090,10138 63090,10138"];
	Layer2_Device12_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61427,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage0_RecvKV	[label="Local K,V",
		lp="61928,10031",
		pos="e,62185,9988.1 62185,10075 62185,10075 62185,9998.1 62185,9998.1"];
	Layer2_Device12_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62217,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage0_Attention	[label=Q_local,
		lp="62489,9934.8",
		pos="e,62398,9768.7 62398,10075 62398,10075 62398,9778.7 62398,9778.7"];
	Layer2_Device12_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59887,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage1_Attention	[label=Q_local,
		lp="59951,9838.3",
		pos="e,59913,9575.6 62023,10081 61207,10081 59913,10081 59913,10081 59913,10081 59913,9585.6 59913,9585.6"];
	Layer2_Device12_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60498,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage2_Attention	[label=Q_local,
		lp="62629,9741.8",
		pos="e,60668,9382.5 62493,10075 62493,9981.5 62493,9678 62493,9678 62493,9678 60668,9678 60668,9678 60668,9678 60668,9392.5 60668,9392.5"];
	Layer2_Device12_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62513,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage3_Attention	[label=Q_local,
		lp="62769,9645.3",
		pos="e,62686,9189.6 62686,10075 62686,10075 62686,9199.6 62686,9199.6"];
	Layer2_Device12_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59719,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage4_Attention	[label=Q_local,
		lp="59571,9548.8",
		pos="e,59572,8996.5 62023,10094 61114,10094 59572,10094 59572,10094 59572,10094 59572,9006.5 59572,9006.5"];
	Layer2_Device12_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62632,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage5_Attention	[label=Q_local,
		lp="62866,9452.3",
		pos="e,62804,8803.6 62804,10075 62804,10075 62804,8813.6 62804,8813.6"];
	Layer2_Device12_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62660,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage6_Attention	[label=Q_local,
		lp="63005,9355.8",
		pos="e,62878,8610.6 62878,10075 62878,10075 62878,8620.6 62878,8620.6"];
	Layer2_Device12_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59721,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage7_Attention	[label=Q_local,
		lp="59425,9259.3",
		pos="e,59489,8399 62023,10114 61079,10114 59440,10114 59440,10114 59440,10114 59440,8399 59440,8399 59440,8399 59479,8399 59479,8399"];
	Layer2_Device12_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62498,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage8_Attention	[label=Q_local,
		lp="63124,9162.8",
		pos="e,62730,8197 62958,10075 62958,9844.7 62958,8197 62958,8197 62958,8197 62740,8197 62740,8197"];
	Layer2_Device12_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59559,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage9_Attention	[label=Q_local,
		lp="59307,9066.3",
		pos="e,59393,8031.6 62023,10121 61067,10121 59393,10121 59393,10121 59393,10121 59393,8041.6 59393,8041.6"];
	Layer2_Device12_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59954,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage10_Attention	[label=Q_local,
		lp="59167,8969.8",
		pos="e,60073,7838.5 63024,10075 63024,9827.1 63024,7935 63024,7935 63024,7935 60073,7935 60073,7935 60073,7935 60073,7848.5 60073,7848.5"];
	Layer2_Device12_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61095,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage11_Attention	[label=Q_local,
		lp="63245,8873.3",
		pos="e,61172,7645.4 63091,10075 63091,9810.9 63091,7685 63091,7685 63091,7685 61172,7685 61172,7685 61172,7685 61172,7655.4 61172,7655.4"];
	Layer2_Device12_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61285,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage12_Attention	[label=Q_local,
		lp="63388,8776.8",
		pos="e,61355,7452.4 63157,10075 63157,9810.7 63157,7683 63157,7683 63157,7683 61355,7683 61355,7683 61355,7683 61355,7462.4 61355,7462.4"];
	Layer2_Device12_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="63300,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage13_Attention	[label=Q_local,
		lp="63528,8680.3",
		pos="e,63473,7259.7 63323,10086 63411,10086 63473,10086 63473,10086 63473,10086 63473,7269.7 63473,7269.7"];
	Layer2_Device12_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60882,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage14_Attention	[label=Q_local,
		lp="59021,8583.8",
		pos="e,60959,7066.4 63323,10098 63463,10098 63570,10098 63570,10098 63570,10098 63570,7171 63570,7171 63570,7171 60959,7171 60959,7171 \
60959,7171 60959,7076.4 60959,7076.4"];
	Layer2_Device12_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="63015,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_QKVProj -> Layer2_Device12_Stage15_Attention	[label=Q_local,
		lp="63646,8487.3",
		pos="e,63247,6896 63323,10104 63482,10104 63608,10104 63608,10104 63608,10104 63608,6896 63608,6896 63608,6896 63257,6896 63257,6896"];
	Layer2_Device12_Stage0_RecvKV -> Layer2_Device12_Stage0_Attention	[pos="e,62166,9768.7 62166,9934.7 62166,9934.7 62166,9778.7 62166,9778.7"];
	Layer2_Device12_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61008,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage0_RecvKV -> Layer2_Device12_Stage1_RecvKV	[label="Ring transfer",
		lp="61265,9838.3",
		pos="e,61218,9794.9 61218,9881.6 61218,9881.6 61218,9804.9 61218,9804.9"];
	Layer2_Device12_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="60408,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage0_Attention -> Layer2_Device12_Stage0_Accumulate	[pos="e,60408,9575.4 62217,9715.2 62217,9698.5 62217,9680 62217,9680 62217,9680 60408,9680 60408,9680 60408,9680 60408,9585.4 60408,9585.4"];
	Layer2_Device12_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="59977,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage0_Accumulate -> Layer2_Device12_Stage1_Accumulate	[pos="e,60192,9382.4 60192,9522 60192,9522 60192,9392.4 60192,9392.4"];
	Layer2_Device12_Stage1_RecvKV -> Layer2_Device12_Stage1_Attention	[pos="e,60103,9575.7 60103,9741.7 60103,9741.7 60103,9585.7 60103,9585.7"];
	Layer2_Device12_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61617,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage1_RecvKV -> Layer2_Device12_Stage2_RecvKV	[label="Ring transfer",
		lp="61325,9645.3",
		pos="e,61312,9601.9 61312,9688.6 61312,9688.6 61312,9611.9 61312,9611.9"];
	Layer2_Device12_Stage1_Attention -> Layer2_Device12_Stage1_Accumulate	[pos="e,59932,9382.4 59932,9522 59932,9522 59932,9392.4 59932,9392.4"];
	Layer2_Device12_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60095,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage1_Accumulate -> Layer2_Device12_Stage2_Accumulate	[pos="e,60036,9189.4 60036,9329 60036,9329 60036,9199.4 60036,9199.4"];
	Layer2_Device12_Stage2_RecvKV -> Layer2_Device12_Stage2_Attention	[pos="e,60713,9382.7 60713,9548.7 60713,9548.7 60713,9392.7 60713,9392.7"];
	Layer2_Device12_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61707,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage2_RecvKV -> Layer2_Device12_Stage3_RecvKV	[label="Ring transfer",
		lp="61710,9452.3",
		pos="e,61662,9408.9 61662,9495.6 61662,9495.6 61662,9418.9 61662,9418.9"];
	Layer2_Device12_Stage2_Attention -> Layer2_Device12_Stage2_Accumulate	[pos="e,60296,9189.4 60296,9329 60296,9329 60296,9199.4 60296,9199.4"];
	Layer2_Device12_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60240,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage2_Accumulate -> Layer2_Device12_Stage3_Accumulate	[pos="e,60168,8996.4 60168,9136 60168,9136 60168,9006.4 60168,9006.4"];
	Layer2_Device12_Stage3_RecvKV -> Layer2_Device12_Stage3_Attention	[pos="e,62454,9189.7 62454,9355.7 62454,9355.7 62454,9199.7 62454,9199.7"];
	Layer2_Device12_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61304,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage3_RecvKV -> Layer2_Device12_Stage4_RecvKV	[label="Ring transfer",
		lp="61553,9259.3",
		pos="e,61506,9215.9 61506,9302.6 61506,9302.6 61506,9225.9 61506,9225.9"];
	Layer2_Device12_Stage3_Attention -> Layer2_Device12_Stage3_Accumulate	[pos="e,60428,8996.6 62325,9136.2 62325,9115 62325,9089 62325,9089 62325,9089 60428,9089 60428,9089 60428,9089 60428,9006.6 60428,9006.6"];
	Layer2_Device12_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60214,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage3_Accumulate -> Layer2_Device12_Stage4_Accumulate	[pos="e,60227,8803.4 60227,8943 60227,8943 60227,8813.4 60227,8813.4"];
	Layer2_Device12_Stage4_RecvKV -> Layer2_Device12_Stage4_Attention	[pos="e,59907,8996.5 60431,9123 60148,9123 59907,9123 59907,9123 59907,9123 59907,9006.5 59907,9006.5"];
	Layer2_Device12_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61449,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage4_RecvKV -> Layer2_Device12_Stage5_RecvKV	[label="Ring transfer",
		lp="61424,9066.3",
		pos="e,61376,9022.9 61376,9109.6 61376,9109.6 61376,9032.9 61376,9032.9"];
	Layer2_Device12_Stage4_Attention -> Layer2_Device12_Stage4_Accumulate	[pos="e,59996,8803.5 59951,8969 59978,8969 59996,8969 59996,8969 59996,8969 59996,8813.5 59996,8813.5"];
	Layer2_Device12_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="62139,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage4_Accumulate -> Layer2_Device12_Stage5_Accumulate	[pos="e,62125,8610.4 60228,8749.9 60228,8723.4 60228,8687 60228,8687 60228,8687 62125,8687 62125,8687 62125,8687 62125,8620.4 62125,8620.4"];
	Layer2_Device12_Stage5_RecvKV -> Layer2_Device12_Stage5_Attention	[pos="e,62400,8785 62351,8969.7 62351,8964.1 62351,8785 62351,8785 62351,8785 62390,8785 62390,8785"];
	Layer2_Device12_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61423,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage5_RecvKV -> Layer2_Device12_Stage6_RecvKV	[label="Ring transfer",
		lp="61484,8873.3",
		pos="e,61436,8829.9 61436,8916.6 61436,8916.6 61436,8839.9 61436,8839.9"];
	Layer2_Device12_Stage5_Attention -> Layer2_Device12_Stage5_Accumulate	[pos="e,62360,8610.5 62400,8767 62376,8767 62360,8767 62360,8767 62360,8767 62360,8620.5 62360,8620.5"];
	Layer2_Device12_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="62139,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage5_Accumulate -> Layer2_Device12_Stage6_Accumulate	[pos="e,62139,8417.4 62139,8557 62139,8557 62139,8427.4 62139,8427.4"];
	Layer2_Device12_Stage6_RecvKV -> Layer2_Device12_Stage6_Attention	[pos="e,62646,8610.5 62013,8737 62326,8737 62646,8737 62646,8737 62646,8737 62646,8620.5 62646,8620.5"];
	Layer2_Device12_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60930,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage6_RecvKV -> Layer2_Device12_Stage7_RecvKV	[label="Ring transfer",
		lp="61159,8680.3",
		pos="e,61176,8636.9 61176,8723.6 61176,8723.6 61176,8646.9 61176,8646.9"];
	Layer2_Device12_Stage6_Attention -> Layer2_Device12_Stage6_Accumulate	[pos="e,62371,8381 62579,8557.3 62579,8502.3 62579,8381 62579,8381 62579,8381 62381,8381 62381,8381"];
	Layer2_Device12_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61977,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage6_Accumulate -> Layer2_Device12_Stage7_Accumulate	[pos="e,62058,8224.4 62058,8364 62058,8364 62058,8234.4 62058,8234.4"];
	Layer2_Device12_Stage7_RecvKV -> Layer2_Device12_Stage7_Attention	[pos="e,59720,8417.5 60057,8544 59866,8544 59720,8544 59720,8544 59720,8544 59720,8427.5 59720,8427.5"];
	Layer2_Device12_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60930,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage7_RecvKV -> Layer2_Device12_Stage8_RecvKV	[label="Ring transfer",
		lp="60978,8487.3",
		pos="e,60930,8443.9 60930,8530.6 60930,8530.6 60930,8453.9 60930,8453.9"];
	Layer2_Device12_Stage7_Attention -> Layer2_Device12_Stage7_Accumulate	[pos="e,61798,8224.6 59900,8364.2 59900,8343 59900,8317 59900,8317 59900,8317 61798,8317 61798,8317 61798,8317 61798,8234.6 61798,8234.6"];
	Layer2_Device12_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61977,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage7_Accumulate -> Layer2_Device12_Stage8_Accumulate	[pos="e,61977,8031.4 61977,8171 61977,8171 61977,8041.4 61977,8041.4"];
	Layer2_Device12_Stage8_RecvKV -> Layer2_Device12_Stage8_Attention	[pos="e,62318,8224.5 61520,8351 61898,8351 62318,8351 62318,8351 62318,8351 62318,8234.5 62318,8234.5"];
	Layer2_Device12_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60768,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage8_RecvKV -> Layer2_Device12_Stage9_RecvKV	[label="Ring transfer",
		lp="60897,8294.3",
		pos="e,60849,8250.9 60849,8337.6 60849,8337.6 60849,8260.9 60849,8260.9"];
	Layer2_Device12_Stage8_Attention -> Layer2_Device12_Stage8_Accumulate	[pos="e,62209,8004 62435,8171.3 62435,8118.2 62435,8004 62435,8004 62435,8004 62219,8004 62219,8004"];
	Layer2_Device12_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60475,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage8_Accumulate -> Layer2_Device12_Stage9_Accumulate	[pos="e,60552,7838.7 61977,7978.3 61977,7945.5 61977,7894 61977,7894 61977,7894 60552,7894 60552,7894 60552,7894 60552,7848.7 60552,7848.7"];
	Layer2_Device12_Stage9_RecvKV -> Layer2_Device12_Stage9_Attention	[pos="e,59640,8031.5 59895,8158 59747,8158 59640,8158 59640,8158 59640,8158 59640,8041.5 59640,8041.5"];
	Layer2_Device12_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="60768,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage9_RecvKV -> Layer2_Device12_Stage10_RecvKV	[label="Ring transfer",
		lp="60816,8101.3",
		pos="e,60768,8057.9 60768,8144.6 60768,8144.6 60768,8067.9 60768,8067.9"];
	Layer2_Device12_Stage9_Attention -> Layer2_Device12_Stage9_Accumulate	[pos="e,60398,7838.6 59756,7978.2 59756,7942.9 59756,7885 59756,7885 59756,7885 60398,7885 60398,7885 60398,7885 60398,7848.6 60398,7848.6"];
	Layer2_Device12_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60475,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage9_Accumulate -> Layer2_Device12_Stage10_Accumulate	[pos="e,60475,7645.4 60475,7785 60475,7785 60475,7655.4 60475,7655.4"];
	Layer2_Device12_Stage10_RecvKV -> Layer2_Device12_Stage10_Attention	[pos="e,59961,7838.5 59961,7951.6 59961,7951.6 59961,7848.5 59961,7848.5"];
	Layer2_Device12_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61684,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage10_RecvKV -> Layer2_Device12_Stage11_RecvKV	[label="Ring transfer",
		lp="61085,7908.3",
		pos="e,61226,7864.9 61226,7951.6 61226,7951.6 61226,7874.9 61226,7874.9"];
	Layer2_Device12_Stage10_Attention -> Layer2_Device12_Stage10_Accumulate	[pos="e,60243,7618 60163,7785.3 60163,7732.2 60163,7618 60163,7618 60163,7618 60233,7618 60233,7618"];
	Layer2_Device12_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60764,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage10_Accumulate -> Layer2_Device12_Stage11_Accumulate	[pos="e,60620,7452.4 60620,7592 60620,7592 60620,7462.4 60620,7462.4"];
	Layer2_Device12_Stage11_RecvKV -> Layer2_Device12_Stage11_Attention	[pos="e,61018,7645.5 61018,7758.6 61018,7758.6 61018,7655.5 61018,7655.5"];
	Layer2_Device12_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62304,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage11_RecvKV -> Layer2_Device12_Stage12_RecvKV	[label="Ring transfer",
		lp="62089,7715.3",
		pos="e,61994,7671.9 61994,7758.6 61994,7758.6 61994,7681.9 61994,7681.9"];
	Layer2_Device12_Stage11_Attention -> Layer2_Device12_Stage11_Accumulate	[pos="e,60930,7452.4 60930,7592 60930,7592 60930,7462.4 60930,7462.4"];
	Layer2_Device12_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60882,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage11_Accumulate -> Layer2_Device12_Stage12_Accumulate	[pos="e,60823,7259.4 60823,7399 60823,7399 60823,7269.4 60823,7269.4"];
	Layer2_Device12_Stage12_RecvKV -> Layer2_Device12_Stage12_Attention	[pos="e,61450,7452.7 61450,7618.7 61450,7618.7 61450,7462.7 61450,7462.7"];
	Layer2_Device12_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62494,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage12_RecvKV -> Layer2_Device12_Stage13_RecvKV	[label="Ring transfer",
		lp="62447,7522.3",
		pos="e,62399,7478.9 62399,7565.6 62399,7565.6 62399,7488.9 62399,7488.9"];
	Layer2_Device12_Stage12_Attention -> Layer2_Device12_Stage12_Accumulate	[pos="e,61084,7259.4 61084,7399 61084,7399 61084,7269.4 61084,7269.4"];
	Layer2_Device12_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61403,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage12_Accumulate -> Layer2_Device12_Stage13_Accumulate	[pos="e,61171,7039 61114,7232 61131,7232 61142,7232 61142,7232 61142,7232 61142,7039 61142,7039 61142,7039 61161,7039 61161,7039"];
	Layer2_Device12_Stage13_RecvKV -> Layer2_Device12_Stage13_Attention	[pos="e,63241,7259.7 63241,7425.7 63241,7425.7 63241,7269.7 63241,7269.7"];
	Layer2_Device12_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62091,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage13_RecvKV -> Layer2_Device12_Stage14_RecvKV	[label="Ring transfer",
		lp="62340,7329.3",
		pos="e,62292,7285.9 62292,7372.6 62292,7372.6 62292,7295.9 62292,7295.9"];
	Layer2_Device12_Stage13_Attention -> Layer2_Device12_Stage13_Accumulate	[pos="e,61403,7066.4 63300,7206.2 63300,7180 63300,7144 63300,7144 63300,7144 61403,7144 61403,7144 61403,7144 61403,7076.4 61403,7076.4"];
	Layer2_Device12_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61403,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage13_Accumulate -> Layer2_Device12_Stage14_Accumulate	[pos="e,61403,6913.8 61403,7013.2 61403,7013.2 61403,6923.8 61403,6923.8"];
	Layer2_Device12_Stage14_RecvKV -> Layer2_Device12_Stage14_Attention	[pos="e,60805,7066.5 61218,7193 60989,7193 60805,7193 60805,7193 60805,7193 60805,7076.5 60805,7076.5"];
	Layer2_Device12_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62612,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device12_Stage14_RecvKV -> Layer2_Device12_Stage15_RecvKV	[label="Ring transfer",
		lp="62399,7136.3",
		pos="e,62352,7092.9 62352,7179.6 62352,7179.6 62352,7102.9 62352,7102.9"];
	Layer2_Device12_Stage14_Attention -> Layer2_Device12_Stage14_Accumulate	[pos="e,61171,6878 60882,7013.1 60882,6967.1 60882,6878 60882,6878 60882,6878 61161,6878 61161,6878"];
	Layer2_Device12_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="63015,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device12_Stage14_Accumulate -> Layer2_Device12_Stage15_Accumulate	[pos="e,62783,6761 61403,6860.5 61403,6823.4 61403,6761 61403,6761 61403,6761 62773,6761 62773,6761"];
	Layer2_Device12_Stage15_RecvKV -> Layer2_Device12_Stage15_Attention	[pos="e,63015,6913.9 63015,6986.5 63015,6986.5 63015,6923.9 63015,6923.9"];
	Layer2_Device12_Stage15_Attention -> Layer2_Device12_Stage15_Accumulate	[pos="e,63015,6788 63015,6860.6 63015,6860.6 63015,6798 63015,6798"];
	Layer2_Device12_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63421,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device12_Stage15_Accumulate -> Layer2_Device12_ConcatHeads	[pos="e,63221,6662 63221,6734.6 63221,6734.6 63221,6672 63221,6672"];
	Layer2_Device12_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63430,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device12_ConcatHeads -> Layer2_Device12_OutputProj	[pos="e,63430,6536 63430,6608.6 63430,6608.6 63430,6546 63430,6546"];
	Layer2_Device12_OutputProj -> Layer2_Device12_Residual1	[pos="e,63430,6410 63430,6482.6 63430,6482.6 63430,6420 63430,6420"];
	Layer2_Device12_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63251,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device12_Residual1 -> Layer2_Device12_LayerNorm2	[pos="e,63295,6284 63295,6356.6 63295,6356.6 63295,6294 63295,6294"];
	Layer2_Device12_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="63184,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device12_Residual1 -> Layer2_Device12_Residual2	[pos="e,63533,5601.9 63533,6356.4 63533,6356.4 63533,5611.9 63533,5611.9"];
	Layer2_Device12_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="62958,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device12_LayerNorm2 -> Layer2_Device12_GateProj	[pos="e,63110,6158 63110,6230.6 63110,6230.6 63110,6168 63110,6168"];
	Layer2_Device12_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63271,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device12_LayerNorm2 -> Layer2_Device12_UpProj	[pos="e,63324,6069.1 63324,6230.5 63324,6230.5 63324,6079.1 63324,6079.1"];
	Layer2_Device12_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="62958,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device12_GateProj -> Layer2_Device12_Activation	[pos="e,62890,5979.9 62890,6104.7 62890,6104.7 62890,5989.9 62890,5989.9"];
	Layer2_Device12_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="63054,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device12_UpProj -> Layer2_Device12_ElemMul	[pos="e,63324,5854.1 63324,6015.5 63324,6015.5 63324,5864.1 63324,5864.1"];
	Layer2_Device12_Activation -> Layer2_Device12_ElemMul	[pos="e,62958,5854 62958,5926.6 62958,5926.6 62958,5864 62958,5864"];
	Layer2_Device12_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63169,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device12_ElemMul -> Layer2_Device12_DownProj	[pos="e,63169,5728 63169,5800.6 63169,5800.6 63169,5738 63169,5738"];
	Layer2_Device12_DownProj -> Layer2_Device12_Residual2	[pos="e,63169,5602 63169,5674.6 63169,5674.6 63169,5612 63169,5612"];
	Layer2_Device12_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63184,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device12_Residual2 -> Layer2_Device12_Output	[pos="e,63184,5475.9 63184,5548.6 63184,5548.6 63184,5485.9 63184,5485.9"];
	Layer3_Device12_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 12 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="63184,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device12_Output -> Layer3_Device12_Input	[pos="e,63184,5328 63184,5400.6 63184,5400.6 63184,5338 63184,5338"];
	Layer2_Device13_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="67980,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device13_Input -> Layer2_Device13_LayerNorm1	[pos="e,68193,10227 68291,10354 68291,10322 68291,10227 68291,10227 68291,10227 68203,10227 68203,10227"];
	Layer2_Device13_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="68477,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device13_Input -> Layer2_Device13_Residual1	[pos="e,68696,6409.9 68696,10330 68696,10330 68696,6419.9 68696,6419.9"];
	Layer2_Device13_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67563,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device13_LayerNorm1 -> Layer2_Device13_QKVProj	[pos="e,67980,10128 67980,10201 67980,10201 67980,10138 67980,10138"];
	Layer2_Device13_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66317,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage0_RecvKV	[label="Local K,V",
		lp="66818,10031",
		pos="e,67075,9988.1 67075,10075 67075,10075 67075,9998.1 67075,9998.1"];
	Layer2_Device13_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67070,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage0_Attention	[label=Q_local,
		lp="67379,9934.8",
		pos="e,67269,9768.7 67269,10075 67269,10075 67269,9778.7 67269,9778.7"];
	Layer2_Device13_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64777,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage1_Attention	[label=Q_local,
		lp="64841,9838.3",
		pos="e,64930,9575.5 66913,10080 66134,10080 64930,10080 64930,10080 64930,10080 64930,9585.5 64930,9585.5"];
	Layer2_Device13_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65388,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage2_Attention	[label=Q_local,
		lp="67519,9741.8",
		pos="e,65558,9382.6 67364,10075 67364,9973.8 67364,9620 67364,9620 67364,9620 65558,9620 65558,9620 65558,9620 65558,9392.6 65558,9392.6"];
	Layer2_Device13_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67403,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage3_Attention	[label=Q_local,
		lp="67659,9645.3",
		pos="e,67576,9189.6 67576,10075 67576,10075 67576,9199.6 67576,9199.6"];
	Layer2_Device13_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64609,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage4_Attention	[label=Q_local,
		lp="64461,9548.8",
		pos="e,64501,8996.4 66913,10092 66014,10092 64501,10092 64501,10092 64501,10092 64501,9006.4 64501,9006.4"];
	Layer2_Device13_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67521,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage5_Attention	[label=Q_local,
		lp="67763,9452.3",
		pos="e,67694,8803.6 67694,10075 67694,10075 67694,8813.6 67694,8813.6"];
	Layer2_Device13_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67606,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage6_Attention	[label=Q_local,
		lp="67895,9355.8",
		pos="e,67795,8610.6 67795,10075 67795,10075 67795,8620.6 67795,8620.6"];
	Layer2_Device13_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64667,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage7_Attention	[label=Q_local,
		lp="64315,9259.3",
		pos="e,64435,8399 66913,10110 65974,10110 64348,10110 64348,10110 64348,10110 64348,8399 64348,8399 64348,8399 64425,8399 64425,8399"];
	Layer2_Device13_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67400,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage8_Attention	[label=Q_local,
		lp="68013,9162.8",
		pos="e,67632,8197 67871,10075 67871,9844.7 67871,8197 67871,8197 67871,8197 67642,8197 67642,8197"];
	Layer2_Device13_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64461,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage9_Attention	[label=Q_local,
		lp="64197,9066.3",
		pos="e,64319,8031.5 66913,10116 65966,10116 64319,10116 64319,10116 64319,10116 64319,8041.5 64319,8041.5"];
	Layer2_Device13_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64656,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage10_Attention	[label=Q_local,
		lp="64057,8969.8",
		pos="e,64424,7811 66913,10121 65934,10121 64195,10121 64195,10121 64195,10121 64195,7811 64195,7811 64195,7811 64414,7811 64414,7811"];
	Layer2_Device13_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65876,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage11_Attention	[label=Q_local,
		lp="68134,8873.3",
		pos="e,65953,7645.4 67938,10075 67938,9812.8 67938,7719 67938,7719 67938,7719 65953,7719 65953,7719 65953,7719 65953,7655.4 65953,7655.4"];
	Layer2_Device13_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="66174,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage12_Attention	[label=Q_local,
		lp="68277,8776.8",
		pos="e,66136,7452.4 67971,10075 67971,9812.6 67971,7717 67971,7717 67971,7717 66136,7717 66136,7717 66136,7717 66136,7462.4 66136,7462.4"];
	Layer2_Device13_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="68189,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage13_Attention	[label=Q_local,
		lp="68417,8680.3",
		pos="e,68362,7259.7 68213,10089 68301,10089 68362,10089 68362,10089 68362,10089 68362,7269.7 68362,7269.7"];
	Layer2_Device13_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65771,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage14_Attention	[label=Q_local,
		lp="63911,8583.8",
		pos="e,65539,7039 67904,10074 67904,9824.8 67904,7917 67904,7917 67904,7917 65415,7917 65415,7917 65415,7917 65415,7039 65415,7039 65415,\
7039 65529,7039 65529,7039"];
	Layer2_Device13_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="68270,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_QKVProj -> Layer2_Device13_Stage15_Attention	[label=Q_local,
		lp="68535,8487.3",
		pos="e,68461,6914.2 68213,10105 68353,10105 68461,10105 68461,10105 68461,10105 68461,6924.2 68461,6924.2"];
	Layer2_Device13_Stage0_RecvKV -> Layer2_Device13_Stage0_Attention	[pos="e,67038,9768.6 67038,9931.5 67038,9931.5 67038,9778.6 67038,9778.6"];
	Layer2_Device13_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65861,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage0_RecvKV -> Layer2_Device13_Stage1_RecvKV	[label="Ring transfer",
		lp="66218,9838.3",
		pos="e,66089,9794.9 66089,9881.6 66089,9881.6 66089,9804.9 66089,9804.9"];
	Layer2_Device13_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="65298,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage0_Attention -> Layer2_Device13_Stage0_Accumulate	[pos="e,65298,9575.6 67070,9715.2 67070,9679.9 67070,9622 67070,9622 67070,9622 65298,9622 65298,9622 65298,9622 65298,9585.6 65298,9585.6"];
	Layer2_Device13_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64867,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage0_Accumulate -> Layer2_Device13_Stage1_Accumulate	[pos="e,65082,9382.4 65082,9522 65082,9522 65082,9392.4 65082,9392.4"];
	Layer2_Device13_Stage1_RecvKV -> Layer2_Device13_Stage1_Attention	[pos="e,64975,9575.7 64975,9741.7 64975,9741.7 64975,9585.7 64975,9585.7"];
	Layer2_Device13_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66507,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage1_RecvKV -> Layer2_Device13_Stage2_RecvKV	[label="Ring transfer",
		lp="66178,9645.3",
		pos="e,66184,9601.9 66184,9688.6 66184,9688.6 66184,9611.9 66184,9611.9"];
	Layer2_Device13_Stage1_Attention -> Layer2_Device13_Stage1_Accumulate	[pos="e,64822,9382.4 64822,9522 64822,9522 64822,9392.4 64822,9392.4"];
	Layer2_Device13_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64985,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage1_Accumulate -> Layer2_Device13_Stage2_Accumulate	[pos="e,64926,9189.4 64926,9329 64926,9329 64926,9199.4 64926,9199.4"];
	Layer2_Device13_Stage2_RecvKV -> Layer2_Device13_Stage2_Attention	[pos="e,65603,9382.7 65603,9548.7 65603,9548.7 65603,9392.7 65603,9392.7"];
	Layer2_Device13_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66597,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage2_RecvKV -> Layer2_Device13_Stage3_RecvKV	[label="Ring transfer",
		lp="66600,9452.3",
		pos="e,66552,9408.9 66552,9495.6 66552,9495.6 66552,9418.9 66552,9418.9"];
	Layer2_Device13_Stage2_Attention -> Layer2_Device13_Stage2_Accumulate	[pos="e,65186,9189.4 65186,9329 65186,9329 65186,9199.4 65186,9199.4"];
	Layer2_Device13_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65130,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage2_Accumulate -> Layer2_Device13_Stage3_Accumulate	[pos="e,65058,8996.4 65058,9136 65058,9136 65058,9006.4 65058,9006.4"];
	Layer2_Device13_Stage3_RecvKV -> Layer2_Device13_Stage3_Attention	[pos="e,67344,9189.7 67344,9355.7 67344,9355.7 67344,9199.7 67344,9199.7"];
	Layer2_Device13_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66194,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage3_RecvKV -> Layer2_Device13_Stage4_RecvKV	[label="Ring transfer",
		lp="66443,9259.3",
		pos="e,66396,9215.9 66396,9302.6 66396,9302.6 66396,9225.9 66396,9225.9"];
	Layer2_Device13_Stage3_Attention -> Layer2_Device13_Stage3_Accumulate	[pos="e,65318,8996.5 67215,9136 67215,9116.7 67215,9094 67215,9094 67215,9094 65318,9094 65318,9094 65318,9094 65318,9006.5 65318,9006.5"];
	Layer2_Device13_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65103,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage3_Accumulate -> Layer2_Device13_Stage4_Accumulate	[pos="e,65116,8803.4 65116,8943 65116,8943 65116,8813.4 65116,8813.4"];
	Layer2_Device13_Stage4_RecvKV -> Layer2_Device13_Stage4_Attention	[pos="e,64797,8996.5 65321,9123 65038,9123 64797,9123 64797,9123 64797,9123 64797,9006.5 64797,9006.5"];
	Layer2_Device13_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66339,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage4_RecvKV -> Layer2_Device13_Stage5_RecvKV	[label="Ring transfer",
		lp="66314,9066.3",
		pos="e,66266,9022.9 66266,9109.6 66266,9109.6 66266,9032.9 66266,9032.9"];
	Layer2_Device13_Stage4_Attention -> Layer2_Device13_Stage4_Accumulate	[pos="e,64885,8803.5 64841,8969 64867,8969 64885,8969 64885,8969 64885,8969 64885,8813.5 64885,8813.5"];
	Layer2_Device13_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="67085,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage4_Accumulate -> Layer2_Device13_Stage5_Accumulate	[pos="e,67043,8610.5 65145,8750 65145,8730.7 65145,8708 65145,8708 65145,8708 67043,8708 67043,8708 67043,8708 67043,8620.5 67043,8620.5"];
	Layer2_Device13_Stage5_RecvKV -> Layer2_Device13_Stage5_Attention	[pos="e,67289,8776 67245,8969.7 67245,8963.8 67245,8776 67245,8776 67245,8776 67279,8776 67279,8776"];
	Layer2_Device13_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="66312,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage5_RecvKV -> Layer2_Device13_Stage6_RecvKV	[label="Ring transfer",
		lp="66373,8873.3",
		pos="e,66326,8829.9 66326,8916.6 66326,8916.6 66326,8839.9 66326,8839.9"];
	Layer2_Device13_Stage5_Attention -> Layer2_Device13_Stage5_Accumulate	[pos="e,67303,8610.4 67303,8750 67303,8750 67303,8620.4 67303,8620.4"];
	Layer2_Device13_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="67085,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage5_Accumulate -> Layer2_Device13_Stage6_Accumulate	[pos="e,67085,8417.4 67085,8557 67085,8557 67085,8427.4 67085,8427.4"];
	Layer2_Device13_Stage6_RecvKV -> Layer2_Device13_Stage6_Attention	[pos="e,67564,8610.5 66902,8737 67227,8737 67564,8737 67564,8737 67564,8737 67564,8620.5 67564,8620.5"];
	Layer2_Device13_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65876,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage6_RecvKV -> Layer2_Device13_Stage7_RecvKV	[label="Ring transfer",
		lp="66076,8680.3",
		pos="e,66094,8636.9 66094,8723.6 66094,8723.6 66094,8646.9 66094,8646.9"];
	Layer2_Device13_Stage6_Attention -> Layer2_Device13_Stage6_Accumulate	[pos="e,67317,8381 67503,8557.3 67503,8502.3 67503,8381 67503,8381 67503,8381 67327,8381 67327,8381"];
	Layer2_Device13_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66879,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage6_Accumulate -> Layer2_Device13_Stage7_Accumulate	[pos="e,66982,8224.4 66982,8364 66982,8364 66982,8234.4 66982,8234.4"];
	Layer2_Device13_Stage7_RecvKV -> Layer2_Device13_Stage7_Attention	[pos="e,64885,8417.5 65003,8544 64931,8544 64885,8544 64885,8544 64885,8544 64885,8427.5 64885,8427.5"];
	Layer2_Device13_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65876,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage7_RecvKV -> Layer2_Device13_Stage8_RecvKV	[label="Ring transfer",
		lp="65924,8487.3",
		pos="e,65876,8443.9 65876,8530.6 65876,8530.6 65876,8453.9 65876,8453.9"];
	Layer2_Device13_Stage7_Attention -> Layer2_Device13_Stage7_Accumulate	[pos="e,66722,8224.6 64824,8364.2 64824,8328.9 64824,8271 64824,8271 64824,8271 66722,8271 66722,8271 66722,8271 66722,8234.6 66722,8234.6"];
	Layer2_Device13_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66879,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage7_Accumulate -> Layer2_Device13_Stage8_Accumulate	[pos="e,66879,8031.4 66879,8171 66879,8171 66879,8041.4 66879,8041.4"];
	Layer2_Device13_Stage8_RecvKV -> Layer2_Device13_Stage8_Attention	[pos="e,67242,8224.5 66466,8351 66836,8351 67242,8351 67242,8351 67242,8351 67242,8234.5 67242,8234.5"];
	Layer2_Device13_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65670,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage8_RecvKV -> Layer2_Device13_Stage9_RecvKV	[label="Ring transfer",
		lp="65749,8294.3",
		pos="e,65773,8250.9 65773,8337.6 65773,8337.6 65773,8260.9 65773,8260.9"];
	Layer2_Device13_Stage8_Attention -> Layer2_Device13_Stage8_Accumulate	[pos="e,67111,8004 67237,8171.3 67237,8118.2 67237,8004 67237,8004 67237,8004 67121,8004 67121,8004"];
	Layer2_Device13_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65177,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage8_Accumulate -> Layer2_Device13_Stage9_Accumulate	[pos="e,65254,7838.6 66879,7978.1 66879,7952.9 66879,7919 66879,7919 66879,7919 65254,7919 65254,7919 65254,7919 65254,7848.6 65254,7848.6"];
	Layer2_Device13_Stage9_RecvKV -> Layer2_Device13_Stage9_Attention	[pos="e,64564,8031.5 64797,8158 64661,8158 64564,8158 64564,8158 64564,8158 64564,8041.5 64564,8041.5"];
	Layer2_Device13_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65670,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage9_RecvKV -> Layer2_Device13_Stage10_RecvKV	[label="Ring transfer",
		lp="65718,8101.3",
		pos="e,65670,8057.9 65670,8144.6 65670,8144.6 65670,8067.9 65670,8067.9"];
	Layer2_Device13_Stage9_Attention -> Layer2_Device13_Stage9_Accumulate	[pos="e,65100,7838.6 64558,7978.1 64558,7939 64558,7871 64558,7871 64558,7871 65100,7871 65100,7871 65100,7871 65100,7848.6 65100,7848.6"];
	Layer2_Device13_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65177,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage9_Accumulate -> Layer2_Device13_Stage10_Accumulate	[pos="e,65177,7645.4 65177,7785 65177,7785 65177,7655.4 65177,7655.4"];
	Layer2_Device13_Stage10_RecvKV -> Layer2_Device13_Stage10_Attention	[pos="e,64819,7838.7 64819,8004.7 64819,8004.7 64819,7848.7 64819,7848.7"];
	Layer2_Device13_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66386,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage10_RecvKV -> Layer2_Device13_Stage11_RecvKV	[label="Ring transfer",
		lp="65987,7908.3",
		pos="e,66028,7864.9 66028,7951.6 66028,7951.6 66028,7874.9 66028,7874.9"];
	Layer2_Device13_Stage10_Attention -> Layer2_Device13_Stage10_Accumulate	[pos="e,64945,7618 64704,7785.3 64704,7732.2 64704,7618 64704,7618 64704,7618 64935,7618 64935,7618"];
	Layer2_Device13_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65653,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage10_Accumulate -> Layer2_Device13_Stage11_Accumulate	[pos="e,65444,7452.5 65409,7618 65430,7618 65444,7618 65444,7618 65444,7618 65444,7462.5 65444,7462.5"];
	Layer2_Device13_Stage11_RecvKV -> Layer2_Device13_Stage11_Attention	[pos="e,65799,7645.5 65799,7758.6 65799,7758.6 65799,7655.5 65799,7655.5"];
	Layer2_Device13_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67085,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage11_RecvKV -> Layer2_Device13_Stage12_RecvKV	[label="Ring transfer",
		lp="66784,7715.3",
		pos="e,66736,7671.9 66736,7758.6 66736,7758.6 66736,7681.9 66736,7681.9"];
	Layer2_Device13_Stage11_Attention -> Layer2_Device13_Stage11_Accumulate	[pos="e,65764,7452.4 65764,7592 65764,7592 65764,7462.4 65764,7462.4"];
	Layer2_Device13_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65771,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage11_Accumulate -> Layer2_Device13_Stage12_Accumulate	[pos="e,65712,7259.4 65712,7399 65712,7399 65712,7269.4 65712,7269.4"];
	Layer2_Device13_Stage12_RecvKV -> Layer2_Device13_Stage12_Attention	[pos="e,66285,7452.5 66285,7565.6 66285,7565.6 66285,7462.5 66285,7462.5"];
	Layer2_Device13_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67383,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage12_RecvKV -> Layer2_Device13_Stage13_RecvKV	[label="Ring transfer",
		lp="67408,7522.3",
		pos="e,67234,7478.9 67234,7565.6 67234,7565.6 67234,7488.9 67234,7488.9"];
	Layer2_Device13_Stage12_Attention -> Layer2_Device13_Stage12_Accumulate	[pos="e,65972,7259.4 65972,7399 65972,7399 65972,7269.4 65972,7269.4"];
	Layer2_Device13_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66292,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage12_Accumulate -> Layer2_Device13_Stage13_Accumulate	[pos="e,66060,7039 66003,7232 66020,7232 66031,7232 66031,7232 66031,7232 66031,7039 66031,7039 66031,7039 66050,7039 66050,7039"];
	Layer2_Device13_Stage13_RecvKV -> Layer2_Device13_Stage13_Attention	[pos="e,68130,7259.7 68130,7425.7 68130,7425.7 68130,7269.7 68130,7269.7"];
	Layer2_Device13_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66980,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage13_RecvKV -> Layer2_Device13_Stage14_RecvKV	[label="Ring transfer",
		lp="67164,7329.3",
		pos="e,67182,7285.9 67182,7372.6 67182,7372.6 67182,7295.9 67182,7295.9"];
	Layer2_Device13_Stage13_Attention -> Layer2_Device13_Stage13_Accumulate	[pos="e,66292,7066.6 68189,7206 68189,7181.1 68189,7148 68189,7148 68189,7148 66292,7148 66292,7148 66292,7148 66292,7076.6 66292,7076.6"];
	Layer2_Device13_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66292,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage13_Accumulate -> Layer2_Device13_Stage14_Accumulate	[pos="e,66292,6913.8 66292,7013.2 66292,7013.2 66292,6923.8 66292,6923.8"];
	Layer2_Device13_Stage14_RecvKV -> Layer2_Device13_Stage14_Attention	[pos="e,65771,7066.5 66107,7193 65917,7193 65771,7193 65771,7193 65771,7193 65771,7076.5 65771,7076.5"];
	Layer2_Device13_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="67501,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device13_Stage14_RecvKV -> Layer2_Device13_Stage15_RecvKV	[label="Ring transfer",
		lp="67288,7136.3",
		pos="e,67240,7092.9 67240,7179.6 67240,7179.6 67240,7102.9 67240,7102.9"];
	Layer2_Device13_Stage14_Attention -> Layer2_Device13_Stage14_Accumulate	[pos="e,66060,6878 65771,7013.1 65771,6967.1 65771,6878 65771,6878 65771,6878 66050,6878 66050,6878"];
	Layer2_Device13_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="68270,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device13_Stage14_Accumulate -> Layer2_Device13_Stage15_Accumulate	[pos="e,68038,6761 66508,6860.5 66508,6823.4 66508,6761 66508,6761 66508,6761 68028,6761 68028,6761"];
	Layer2_Device13_Stage15_RecvKV -> Layer2_Device13_Stage15_Attention	[pos="e,68230,6914 68230,7038.8 68230,7038.8 68230,6924 68230,6924"];
	Layer2_Device13_Stage15_Attention -> Layer2_Device13_Stage15_Accumulate	[pos="e,68270,6788 68270,6860.6 68270,6860.6 68270,6798 68270,6798"];
	Layer2_Device13_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68286,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device13_Stage15_Accumulate -> Layer2_Device13_ConcatHeads	[pos="e,68281,6662 68281,6734.6 68281,6734.6 68281,6672 68281,6672"];
	Layer2_Device13_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68315,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device13_ConcatHeads -> Layer2_Device13_OutputProj	[pos="e,68307,6536 68307,6608.6 68307,6608.6 68307,6546 68307,6546"];
	Layer2_Device13_OutputProj -> Layer2_Device13_Residual1	[pos="e,68315,6410 68315,6482.6 68315,6482.6 68315,6420 68315,6420"];
	Layer2_Device13_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68258,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device13_Residual1 -> Layer2_Device13_LayerNorm2	[pos="e,68281,6284 68281,6356.6 68281,6356.6 68281,6294 68281,6294"];
	Layer2_Device13_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="68171,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device13_Residual1 -> Layer2_Device13_Residual2	[pos="e,68530,5601.9 68530,6356.4 68530,6356.4 68530,5611.9 68530,5611.9"];
	Layer2_Device13_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="67965,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device13_LayerNorm2 -> Layer2_Device13_GateProj	[pos="e,68118,6158 68118,6230.6 68118,6230.6 68118,6168 68118,6168"];
	Layer2_Device13_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68278,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device13_LayerNorm2 -> Layer2_Device13_UpProj	[pos="e,68330,6069.1 68330,6230.5 68330,6230.5 68330,6079.1 68330,6079.1"];
	Layer2_Device13_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="67965,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device13_GateProj -> Layer2_Device13_Activation	[pos="e,67896,5979.9 67896,6104.7 67896,6104.7 67896,5989.9 67896,5989.9"];
	Layer2_Device13_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="68061,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device13_UpProj -> Layer2_Device13_ElemMul	[pos="e,68331,5854.1 68331,6015.5 68331,6015.5 68331,5864.1 68331,5864.1"];
	Layer2_Device13_Activation -> Layer2_Device13_ElemMul	[pos="e,67965,5854 67965,5926.6 67965,5926.6 67965,5864 67965,5864"];
	Layer2_Device13_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68165,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device13_ElemMul -> Layer2_Device13_DownProj	[pos="e,68165,5728 68165,5800.6 68165,5800.6 68165,5738 68165,5738"];
	Layer2_Device13_DownProj -> Layer2_Device13_Residual2	[pos="e,68165,5602 68165,5674.6 68165,5674.6 68165,5612 68165,5612"];
	Layer2_Device13_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68171,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device13_Residual2 -> Layer2_Device13_Output	[pos="e,68171,5475.9 68171,5548.6 68171,5548.6 68171,5485.9 68171,5485.9"];
	Layer3_Device13_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 13 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="68171,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device13_Output -> Layer3_Device13_Input	[pos="e,68171,5328 68171,5400.6 68171,5400.6 68171,5338 68171,5338"];
	Layer2_Device14_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="72956,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device14_Input -> Layer2_Device14_LayerNorm1	[pos="e,73155,10254 73227,10364 73185,10364 73155,10364 73155,10364 73155,10364 73155,10264 73155,10264"];
	Layer2_Device14_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="73372,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device14_Input -> Layer2_Device14_Residual1	[pos="e,73613,6409.9 73613,10328 73613,10328 73613,6419.9 73613,6419.9"];
	Layer2_Device14_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72463,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device14_LayerNorm1 -> Layer2_Device14_QKVProj	[pos="e,72928,10128 72928,10201 72928,10201 72928,10138 72928,10138"];
	Layer2_Device14_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71217,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage0_RecvKV	[label="Local K,V",
		lp="71718,10031",
		pos="e,71975,9988.1 71975,10075 71975,10075 71975,9998.1 71975,9998.1"];
	Layer2_Device14_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72007,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage0_Attention	[label=Q_local,
		lp="72279,9934.8",
		pos="e,72188,9768.7 72188,10075 72188,10075 72188,9778.7 72188,9778.7"];
	Layer2_Device14_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69677,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage1_Attention	[label=Q_local,
		lp="69741,9838.3",
		pos="e,69712,9575.6 71813,10082 71000,10082 69712,10082 69712,10082 69712,10082 69712,9585.6 69712,9585.6"];
	Layer2_Device14_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70288,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage2_Attention	[label=Q_local,
		lp="72419,9741.8",
		pos="e,70458,9382.6 72283,10075 72283,9982.2 72283,9683 72283,9683 72283,9683 70458,9683 70458,9683 70458,9683 70458,9392.6 70458,9392.6"];
	Layer2_Device14_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72303,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage3_Attention	[label=Q_local,
		lp="72559,9645.3",
		pos="e,72476,9189.6 72476,10075 72476,10075 72476,9199.6 72476,9199.6"];
	Layer2_Device14_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69509,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage4_Attention	[label=Q_local,
		lp="69361,9548.8",
		pos="e,69416,8996.6 71813,10097 70918,10097 69416,10097 69416,10097 69416,10097 69416,9006.6 69416,9006.6"];
	Layer2_Device14_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72422,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage5_Attention	[label=Q_local,
		lp="72665,9452.3",
		pos="e,72594,8803.6 72594,10075 72594,10075 72594,8813.6 72594,8813.6"];
	Layer2_Device14_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72527,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage6_Attention	[label=Q_local,
		lp="72795,9355.8",
		pos="e,72706,8610.6 72706,10075 72706,10075 72706,8620.6 72706,8620.6"];
	Layer2_Device14_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69588,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage7_Attention	[label=Q_local,
		lp="69215,9259.3",
		pos="e,69356,8399 71813,10112 70877,10112 69260,10112 69260,10112 69260,10112 69260,8399 69260,8399 69260,8399 69346,8399 69346,8399"];
	Layer2_Device14_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72272,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage8_Attention	[label=Q_local,
		lp="72914,9162.8",
		pos="e,72504,8197 72785,10075 72785,9844.7 72785,8197 72785,8197 72785,8197 72514,8197 72514,8197"];
	Layer2_Device14_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69333,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage9_Attention	[label=Q_local,
		lp="69097,9066.3",
		pos="e,69243,8031.6 71813,10120 70873,10120 69243,10120 69243,10120 69243,10120 69243,8041.6 69243,8041.6"];
	Layer2_Device14_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69676,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage10_Attention	[label=Q_local,
		lp="68957,8969.8",
		pos="e,69812,7838.5 72811,10075 72811,9828 72811,7942 72811,7942 72811,7942 69812,7942 69812,7942 69812,7942 69812,7848.5 69812,7848.5"];
	Layer2_Device14_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70760,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage11_Attention	[label=Q_local,
		lp="73037,8873.3",
		pos="e,70837,7645.6 72837,10075 72837,9814.5 72837,7733 72837,7733 72837,7733 70837,7733 70837,7733 70837,7733 70837,7655.6 70837,7655.6"];
	Layer2_Device14_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71075,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage12_Attention	[label=Q_local,
		lp="73178,8776.8",
		pos="e,71020,7452.4 72863,10075 72863,9814.3 72863,7731 72863,7731 72863,7731 71020,7731 71020,7731 71020,7731 71020,7462.4 71020,7462.4"];
	Layer2_Device14_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73090,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage13_Attention	[label=Q_local,
		lp="73318,8680.3",
		pos="e,73219,7259.7 73113,10092 73177,10092 73219,10092 73219,10092 73219,10092 73219,7269.7 73219,7269.7"];
	Layer2_Device14_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70672,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage14_Attention	[label=Q_local,
		lp="68811,8583.8",
		pos="e,70904,7048 73113,10086 73173,10086 73211,10086 73211,10086 73211,10086 73211,7343 73211,7343 73211,7343 70942,7343 70942,7343 \
70942,7343 70942,7048 70942,7048 70942,7048 70914,7048 70914,7048"];
	Layer2_Device14_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72805,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_QKVProj -> Layer2_Device14_Stage15_Attention	[label=Q_local,
		lp="73436,8487.3",
		pos="e,73037,6896 73113,10104 73306,10104 73467,10104 73467,10104 73467,10104 73467,6896 73467,6896 73467,6896 73047,6896 73047,6896"];
	Layer2_Device14_Stage0_RecvKV -> Layer2_Device14_Stage0_Attention	[pos="e,71956,9768.7 71956,9934.7 71956,9934.7 71956,9778.7 71956,9778.7"];
	Layer2_Device14_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70798,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage0_RecvKV -> Layer2_Device14_Stage1_RecvKV	[label="Ring transfer",
		lp="71118,9838.3",
		pos="e,71008,9794.9 71008,9881.6 71008,9881.6 71008,9804.9 71008,9804.9"];
	Layer2_Device14_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="70198,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage0_Attention -> Layer2_Device14_Stage0_Accumulate	[pos="e,70198,9575.6 72007,9715.2 72007,9700.8 72007,9686 72007,9686 72007,9686 70198,9686 70198,9686 70198,9686 70198,9585.6 70198,9585.6"];
	Layer2_Device14_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69767,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage0_Accumulate -> Layer2_Device14_Stage1_Accumulate	[pos="e,69982,9382.4 69982,9522 69982,9522 69982,9392.4 69982,9392.4"];
	Layer2_Device14_Stage1_RecvKV -> Layer2_Device14_Stage1_Attention	[pos="e,69893,9575.7 69893,9741.7 69893,9741.7 69893,9585.7 69893,9585.7"];
	Layer2_Device14_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71407,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage1_RecvKV -> Layer2_Device14_Stage2_RecvKV	[label="Ring transfer",
		lp="71094,9645.3",
		pos="e,71102,9601.9 71102,9688.6 71102,9688.6 71102,9611.9 71102,9611.9"];
	Layer2_Device14_Stage1_Attention -> Layer2_Device14_Stage1_Accumulate	[pos="e,69722,9382.4 69722,9522 69722,9522 69722,9392.4 69722,9392.4"];
	Layer2_Device14_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69885,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage1_Accumulate -> Layer2_Device14_Stage2_Accumulate	[pos="e,69826,9189.4 69826,9329 69826,9329 69826,9199.4 69826,9199.4"];
	Layer2_Device14_Stage2_RecvKV -> Layer2_Device14_Stage2_Attention	[pos="e,70503,9382.7 70503,9548.7 70503,9548.7 70503,9392.7 70503,9392.7"];
	Layer2_Device14_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71497,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage2_RecvKV -> Layer2_Device14_Stage3_RecvKV	[label="Ring transfer",
		lp="71500,9452.3",
		pos="e,71452,9408.9 71452,9495.6 71452,9495.6 71452,9418.9 71452,9418.9"];
	Layer2_Device14_Stage2_Attention -> Layer2_Device14_Stage2_Accumulate	[pos="e,70086,9189.4 70086,9329 70086,9329 70086,9199.4 70086,9199.4"];
	Layer2_Device14_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70030,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage2_Accumulate -> Layer2_Device14_Stage3_Accumulate	[pos="e,69958,8996.4 69958,9136 69958,9136 69958,9006.4 69958,9006.4"];
	Layer2_Device14_Stage3_RecvKV -> Layer2_Device14_Stage3_Attention	[pos="e,72244,9189.7 72244,9355.7 72244,9355.7 72244,9199.7 72244,9199.7"];
	Layer2_Device14_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71094,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage3_RecvKV -> Layer2_Device14_Stage4_RecvKV	[label="Ring transfer",
		lp="71411,9259.3",
		pos="e,71296,9215.9 71296,9302.6 71296,9302.6 71296,9225.9 71296,9225.9"];
	Layer2_Device14_Stage3_Attention -> Layer2_Device14_Stage3_Accumulate	[pos="e,70218,8996.6 72115,9136.1 72115,9118.6 72115,9099 72115,9099 72115,9099 70218,9099 70218,9099 70218,9099 70218,9006.6 70218,9006.6"];
	Layer2_Device14_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70004,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage3_Accumulate -> Layer2_Device14_Stage4_Accumulate	[pos="e,70017,8803.4 70017,8943 70017,8943 70017,8813.4 70017,8813.4"];
	Layer2_Device14_Stage4_RecvKV -> Layer2_Device14_Stage4_Attention	[pos="e,69697,8996.5 70221,9123 69938,9123 69697,9123 69697,9123 69697,9123 69697,9006.5 69697,9006.5"];
	Layer2_Device14_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71239,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage4_RecvKV -> Layer2_Device14_Stage5_RecvKV	[label="Ring transfer",
		lp="71214,9066.3",
		pos="e,71166,9022.9 71166,9109.6 71166,9109.6 71166,9032.9 71166,9032.9"];
	Layer2_Device14_Stage4_Attention -> Layer2_Device14_Stage4_Accumulate	[pos="e,69786,8803.5 69741,8969 69768,8969 69786,8969 69786,8969 69786,8969 69786,8813.5 69786,8813.5"];
	Layer2_Device14_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72006,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage4_Accumulate -> Layer2_Device14_Stage5_Accumulate	[pos="e,71954,8610.6 70056,8750.1 70056,8732.6 70056,8713 70056,8713 70056,8713 71954,8713 71954,8713 71954,8713 71954,8620.6 71954,8620.6"];
	Layer2_Device14_Stage5_RecvKV -> Layer2_Device14_Stage5_Attention	[pos="e,72190,8776 72146,8969.7 72146,8963.8 72146,8776 72146,8776 72146,8776 72180,8776 72180,8776"];
	Layer2_Device14_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="71213,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage5_RecvKV -> Layer2_Device14_Stage6_RecvKV	[label="Ring transfer",
		lp="71274,8873.3",
		pos="e,71226,8829.9 71226,8916.6 71226,8916.6 71226,8839.9 71226,8839.9"];
	Layer2_Device14_Stage5_Attention -> Layer2_Device14_Stage5_Accumulate	[pos="e,72214,8610.4 72214,8750 72214,8750 72214,8620.4 72214,8620.4"];
	Layer2_Device14_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72006,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage5_Accumulate -> Layer2_Device14_Stage6_Accumulate	[pos="e,72006,8417.4 72006,8557 72006,8557 72006,8427.4 72006,8427.4"];
	Layer2_Device14_Stage6_RecvKV -> Layer2_Device14_Stage6_Attention	[pos="e,72474,8610.5 71804,8737 72132,8737 72474,8737 72474,8737 72474,8737 72474,8620.5 72474,8620.5"];
	Layer2_Device14_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70797,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage6_RecvKV -> Layer2_Device14_Stage7_RecvKV	[label="Ring transfer",
		lp="71124,8680.3",
		pos="e,71005,8636.9 71005,8723.6 71005,8723.6 71005,8646.9 71005,8646.9"];
	Layer2_Device14_Stage6_Attention -> Layer2_Device14_Stage6_Accumulate	[pos="e,72238,8381 72400,8557.3 72400,8502.3 72400,8381 72400,8381 72400,8381 72248,8381 72248,8381"];
	Layer2_Device14_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71751,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage6_Accumulate -> Layer2_Device14_Stage7_Accumulate	[pos="e,71878,8224.4 71878,8364 71878,8364 71878,8234.4 71878,8234.4"];
	Layer2_Device14_Stage7_RecvKV -> Layer2_Device14_Stage7_Attention	[pos="e,69796,8417.5 69924,8544 69846,8544 69796,8544 69796,8544 69796,8544 69796,8427.5 69796,8427.5"];
	Layer2_Device14_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70797,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage7_RecvKV -> Layer2_Device14_Stage8_RecvKV	[label="Ring transfer",
		lp="70845,8487.3",
		pos="e,70797,8443.9 70797,8530.6 70797,8530.6 70797,8453.9 70797,8453.9"];
	Layer2_Device14_Stage7_Attention -> Layer2_Device14_Stage7_Accumulate	[pos="e,71618,8224.4 69721,8364.3 69721,8324.8 69721,8255 69721,8255 69721,8255 71618,8255 71618,8255 71618,8255 71618,8234.4 71618,8234.4"];
	Layer2_Device14_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71751,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage7_Accumulate -> Layer2_Device14_Stage8_Accumulate	[pos="e,71751,8031.4 71751,8171 71751,8171 71751,8041.4 71751,8041.4"];
	Layer2_Device14_Stage8_RecvKV -> Layer2_Device14_Stage8_Attention	[pos="e,72139,8224.5 71388,8351 71748,8351 72139,8351 72139,8351 72139,8351 72139,8234.5 72139,8234.5"];
	Layer2_Device14_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70542,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage8_RecvKV -> Layer2_Device14_Stage9_RecvKV	[label="Ring transfer",
		lp="70738,8294.3",
		pos="e,70670,8250.9 70670,8337.6 70670,8337.6 70670,8260.9 70670,8260.9"];
	Layer2_Device14_Stage8_Attention -> Layer2_Device14_Stage8_Accumulate	[pos="e,71983,8004 72183,8171.3 72183,8118.2 72183,8004 72183,8004 72183,8004 71993,8004 71993,8004"];
	Layer2_Device14_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70197,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage8_Accumulate -> Layer2_Device14_Stage9_Accumulate	[pos="e,70274,7838.4 71751,7978.1 71751,7946.8 71751,7899 71751,7899 71751,7899 70274,7899 70274,7899 70274,7899 70274,7848.4 70274,7848.4"];
	Layer2_Device14_Stage9_RecvKV -> Layer2_Device14_Stage9_Attention	[pos="e,69460,8031.5 69669,8158 69546,8158 69460,8158 69460,8158 69460,8158 69460,8041.5 69460,8041.5"];
	Layer2_Device14_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70542,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage9_RecvKV -> Layer2_Device14_Stage10_RecvKV	[label="Ring transfer",
		lp="70590,8101.3",
		pos="e,70542,8057.9 70542,8144.6 70542,8144.6 70542,8067.9 70542,8067.9"];
	Layer2_Device14_Stage9_Attention -> Layer2_Device14_Stage9_Accumulate	[pos="e,70120,7838.4 69504,7978 69504,7941.4 69504,7880 69504,7880 69504,7880 70120,7880 70120,7880 70120,7880 70120,7848.4 70120,7848.4"];
	Layer2_Device14_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70197,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage9_Accumulate -> Layer2_Device14_Stage10_Accumulate	[pos="e,70197,7645.4 70197,7785 70197,7785 70197,7655.4 70197,7655.4"];
	Layer2_Device14_Stage10_RecvKV -> Layer2_Device14_Stage10_Attention	[pos="e,69717,7838.5 69717,7951.6 69717,7951.6 69717,7848.5 69717,7848.5"];
	Layer2_Device14_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71406,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage10_RecvKV -> Layer2_Device14_Stage11_RecvKV	[label="Ring transfer",
		lp="71412,7908.3",
		pos="e,70974,7864.9 70974,7951.6 70974,7951.6 70974,7874.9 70974,7874.9"];
	Layer2_Device14_Stage10_Attention -> Layer2_Device14_Stage10_Accumulate	[pos="e,69965,7618 69878,7785.3 69878,7732.2 69878,7618 69878,7618 69878,7618 69955,7618 69955,7618"];
	Layer2_Device14_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70554,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage10_Accumulate -> Layer2_Device14_Stage11_Accumulate	[pos="e,70376,7452.4 70376,7592 70376,7592 70376,7462.4 70376,7462.4"];
	Layer2_Device14_Stage11_RecvKV -> Layer2_Device14_Stage11_Attention	[pos="e,70683,7645.5 70683,7758.6 70683,7758.6 70683,7655.5 70683,7655.5"];
	Layer2_Device14_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71969,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage11_RecvKV -> Layer2_Device14_Stage12_RecvKV	[label="Ring transfer",
		lp="71618,7715.3",
		pos="e,71688,7671.9 71688,7758.6 71688,7758.6 71688,7681.9 71688,7681.9"];
	Layer2_Device14_Stage11_Attention -> Layer2_Device14_Stage11_Accumulate	[pos="e,70657,7452.4 70657,7592 70657,7592 70657,7462.4 70657,7462.4"];
	Layer2_Device14_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70672,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage11_Accumulate -> Layer2_Device14_Stage12_Accumulate	[pos="e,70613,7259.4 70613,7399 70613,7399 70613,7269.4 70613,7269.4"];
	Layer2_Device14_Stage12_RecvKV -> Layer2_Device14_Stage12_Attention	[pos="e,71178,7452.5 71178,7565.6 71178,7565.6 71178,7462.5 71178,7462.5"];
	Layer2_Device14_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72284,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage12_RecvKV -> Layer2_Device14_Stage13_RecvKV	[label="Ring transfer",
		lp="72129,7522.3",
		pos="e,72126,7478.9 72126,7565.6 72126,7565.6 72126,7488.9 72126,7488.9"];
	Layer2_Device14_Stage12_Attention -> Layer2_Device14_Stage12_Accumulate	[pos="e,70874,7259.4 70874,7399 70874,7399 70874,7269.4 70874,7269.4"];
	Layer2_Device14_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71193,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage12_Accumulate -> Layer2_Device14_Stage13_Accumulate	[pos="e,70961,7030 70904,7232 70916,7232 70923,7232 70923,7232 70923,7232 70923,7030 70923,7030 70923,7030 70951,7030 70951,7030"];
	Layer2_Device14_Stage13_RecvKV -> Layer2_Device14_Stage13_Attention	[pos="e,73031,7259.7 73031,7425.7 73031,7425.7 73031,7269.7 73031,7269.7"];
	Layer2_Device14_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71881,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage13_RecvKV -> Layer2_Device14_Stage14_RecvKV	[label="Ring transfer",
		lp="72065,7329.3",
		pos="e,72082,7285.9 72082,7372.6 72082,7372.6 72082,7295.9 72082,7295.9"];
	Layer2_Device14_Stage13_Attention -> Layer2_Device14_Stage13_Accumulate	[pos="e,71193,7066.4 73090,7205.9 73090,7182.4 73090,7152 73090,7152 73090,7152 71193,7152 71193,7152 71193,7152 71193,7076.4 71193,7076.4"];
	Layer2_Device14_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71193,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage13_Accumulate -> Layer2_Device14_Stage14_Accumulate	[pos="e,71193,6913.8 71193,7013.2 71193,7013.2 71193,6923.8 71193,6923.8"];
	Layer2_Device14_Stage14_RecvKV -> Layer2_Device14_Stage14_Attention	[pos="e,70672,7066.5 71008,7193 70818,7193 70672,7193 70672,7193 70672,7193 70672,7076.5 70672,7076.5"];
	Layer2_Device14_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="72402,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device14_Stage14_RecvKV -> Layer2_Device14_Stage15_RecvKV	[label="Ring transfer",
		lp="72189,7136.3",
		pos="e,72142,7092.9 72142,7179.6 72142,7179.6 72142,7102.9 72142,7102.9"];
	Layer2_Device14_Stage14_Attention -> Layer2_Device14_Stage14_Accumulate	[pos="e,70961,6887 70672,7013.2 70672,6969.4 70672,6887 70672,6887 70672,6887 70951,6887 70951,6887"];
	Layer2_Device14_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="72805,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device14_Stage14_Accumulate -> Layer2_Device14_Stage15_Accumulate	[pos="e,72573,6761 71394,6860.5 71394,6823.4 71394,6761 71394,6761 71394,6761 72563,6761 72563,6761"];
	Layer2_Device14_Stage15_RecvKV -> Layer2_Device14_Stage15_Attention	[pos="e,72805,6913.9 72805,6986.5 72805,6986.5 72805,6923.9 72805,6923.9"];
	Layer2_Device14_Stage15_Attention -> Layer2_Device14_Stage15_Accumulate	[pos="e,72805,6788 72805,6860.6 72805,6860.6 72805,6798 72805,6798"];
	Layer2_Device14_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="72963,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device14_Stage15_Accumulate -> Layer2_Device14_ConcatHeads	[pos="e,72887,6662 72887,6734.6 72887,6734.6 72887,6672 72887,6672"];
	Layer2_Device14_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73016,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device14_ConcatHeads -> Layer2_Device14_OutputProj	[pos="e,72996,6536 72996,6608.6 72996,6608.6 72996,6546 72996,6546"];
	Layer2_Device14_OutputProj -> Layer2_Device14_Residual1	[pos="e,73107,6410 73107,6482.6 73107,6482.6 73107,6420 73107,6420"];
	Layer2_Device14_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73150,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device14_Residual1 -> Layer2_Device14_LayerNorm2	[pos="e,73174,6284 73174,6356.6 73174,6356.6 73174,6294 73174,6294"];
	Layer2_Device14_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="73072,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device14_Residual1 -> Layer2_Device14_Residual2	[pos="e,73427,5601.9 73427,6356.4 73427,6356.4 73427,5611.9 73427,5611.9"];
	Layer2_Device14_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="72857,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device14_LayerNorm2 -> Layer2_Device14_GateProj	[pos="e,73010,6158 73010,6230.6 73010,6230.6 73010,6168 73010,6168"];
	Layer2_Device14_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="73170,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device14_LayerNorm2 -> Layer2_Device14_UpProj	[pos="e,73222,6069.1 73222,6230.5 73222,6230.5 73222,6079.1 73222,6079.1"];
	Layer2_Device14_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="72857,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device14_GateProj -> Layer2_Device14_Activation	[pos="e,72788,5979.9 72788,6104.7 72788,6104.7 72788,5989.9 72788,5989.9"];
	Layer2_Device14_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="72953,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device14_UpProj -> Layer2_Device14_ElemMul	[pos="e,73223,5854.1 73223,6015.5 73223,6015.5 73223,5864.1 73223,5864.1"];
	Layer2_Device14_Activation -> Layer2_Device14_ElemMul	[pos="e,72857,5854 72857,5926.6 72857,5926.6 72857,5864 72857,5864"];
	Layer2_Device14_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="72983,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device14_ElemMul -> Layer2_Device14_DownProj	[pos="e,72983,5728 72983,5800.6 72983,5800.6 72983,5738 72983,5738"];
	Layer2_Device14_DownProj -> Layer2_Device14_Residual2	[pos="e,72983,5602 72983,5674.6 72983,5674.6 72983,5612 72983,5612"];
	Layer2_Device14_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73072,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device14_Residual2 -> Layer2_Device14_Output	[pos="e,73072,5475.9 73072,5548.6 73072,5548.6 73072,5485.9 73072,5485.9"];
	Layer3_Device14_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 14 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="73072,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device14_Output -> Layer3_Device14_Input	[pos="e,73072,5328 73072,5400.6 73072,5400.6 73072,5338 73072,5338"];
	Layer2_Device15_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77978,10227",
		shape=rectangle,
		width=5.9167];
	Layer2_Device15_Input -> Layer2_Device15_LayerNorm1	[pos="e,77993,10254 77993,10329 77993,10329 77993,10264 77993,10264"];
	Layer2_Device15_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="77920,6383.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device15_Input -> Layer2_Device15_Residual1	[pos="e,78278,6409.9 78278,10334 78278,10334 78278,6419.9 78278,6419.9"];
	Layer2_Device15_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77352,10101",
		shape=rectangle,
		width=18.042];
	Layer2_Device15_LayerNorm1 -> Layer2_Device15_QKVProj	[pos="e,77883,10128 77883,10201 77883,10201 77883,10138 77883,10138"];
	Layer2_Device15_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76106,9934.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage0_RecvKV	[label="Local K,V",
		lp="76606,10031",
		pos="e,76864,9988.1 76864,10075 76864,10075 76864,9998.1 76864,9998.1"];
	Layer2_Device15_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76859,9741.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage0_Attention	[label=Q_local,
		lp="77168,9934.8",
		pos="e,77058,9768.7 77058,10075 77058,10075 77058,9778.7 77058,9778.7"];
	Layer2_Device15_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74566,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage1_Attention	[label=Q_local,
		lp="74630,9838.3",
		pos="e,74719,9575.5 76702,10080 75923,10080 74719,10080 74719,10080 74719,10080 74719,9585.5 74719,9585.5"];
	Layer2_Device15_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75177,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage2_Attention	[label=Q_local,
		lp="77308,9741.8",
		pos="e,75347,9382.6 77153,10075 77153,9974.4 77153,9625 77153,9625 77153,9625 75347,9625 75347,9625 75347,9625 75347,9392.6 75347,9392.6"];
	Layer2_Device15_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77192,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage3_Attention	[label=Q_local,
		lp="77448,9645.3",
		pos="e,77365,9189.6 77365,10075 77365,10075 77365,9199.6 77365,9199.6"];
	Layer2_Device15_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74398,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage4_Attention	[label=Q_local,
		lp="74250,9548.8",
		pos="e,74250,8996.3 76702,10098 75793,10098 74250,10098 74250,10098 74250,10098 74250,9006.3 74250,9006.3"];
	Layer2_Device15_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77321,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage5_Attention	[label=Q_local,
		lp="77554,9452.3",
		pos="e,77488,8803.6 77488,10075 77488,10075 77488,8813.6 77488,8813.6"];
	Layer2_Device15_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77349,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage6_Attention	[label=Q_local,
		lp="77684,9355.8",
		pos="e,77566,8610.6 77566,10075 77566,10075 77566,8620.6 77566,8620.6"];
	Layer2_Device15_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74410,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage7_Attention	[label=Q_local,
		lp="74104,9259.3",
		pos="e,74178,8399 76702,10110 75754,10110 74103,10110 74103,10110 74103,10110 74103,8399 74103,8399 74103,8399 74168,8399 74168,8399"];
	Layer2_Device15_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77146,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage8_Attention	[label=Q_local,
		lp="77813,9162.8",
		pos="e,77378,8197 77628,10075 77628,9844.7 77628,8197 77628,8197 77628,8197 77388,8197 77388,8197"];
	Layer2_Device15_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74207,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage9_Attention	[label=Q_local,
		lp="73986,9066.3",
		pos="e,74039,8031.5 76702,10116 75738,10116 74039,10116 74039,10116 74039,10116 74039,8041.5 74039,8041.5"];
	Layer2_Device15_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74360,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage10_Attention	[label=Q_local,
		lp="73846,8969.8",
		pos="e,74128,7811 76702,10121 75715,10121 73948,10121 73948,10121 73948,10121 73948,7811 73948,7811 73948,7811 74118,7811 74118,7811"];
	Layer2_Device15_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75690,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage11_Attention	[label=Q_local,
		lp="77934,8873.3",
		pos="e,75767,7645.5 77723,10075 77723,9813.1 77723,7703 77723,7703 77723,7703 75767,7703 75767,7703 75767,7703 75767,7655.5 75767,7655.5"];
	Layer2_Device15_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75974,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage12_Attention	[label=Q_local,
		lp="78077,8776.8",
		pos="e,75950,7452.4 77771,10075 77771,9812.8 77771,7701 77771,7701 77771,7701 75950,7701 75950,7701 75950,7701 75950,7462.4 75950,7462.4"];
	Layer2_Device15_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77989,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage13_Attention	[label=Q_local,
		lp="78217,8680.3",
		pos="e,78147,7259.7 78002,10092 78088,10092 78147,10092 78147,10092 78147,10092 78147,7269.7 78147,7269.7"];
	Layer2_Device15_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75571,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage14_Attention	[label=Q_local,
		lp="73700,8583.8",
		pos="e,75339,7039 77676,10075 77676,9814.9 77676,7754 77676,7754 77676,7754 75196,7754 75196,7754 75196,7754 75196,7039 75196,7039 75196,\
7039 75329,7039 75329,7039"];
	Layer2_Device15_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77832,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_QKVProj -> Layer2_Device15_Stage15_Attention	[label=Q_local,
		lp="78335,8487.3",
		pos="e,78064,6887 78002,10110 78142,10110 78249,10110 78249,10110 78249,10110 78249,6887 78249,6887 78249,6887 78074,6887 78074,6887"];
	Layer2_Device15_Stage0_RecvKV -> Layer2_Device15_Stage0_Attention	[pos="e,76827,9768.6 76827,9931.5 76827,9931.5 76827,9778.6 76827,9778.6"];
	Layer2_Device15_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75650,9741.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage0_RecvKV -> Layer2_Device15_Stage1_RecvKV	[label="Ring transfer",
		lp="76007,9838.3",
		pos="e,75878,9794.9 75878,9881.6 75878,9881.6 75878,9804.9 75878,9804.9"];
	Layer2_Device15_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="75087,9548.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage0_Attention -> Layer2_Device15_Stage0_Accumulate	[pos="e,75087,9575.6 76859,9715.1 76859,9681.5 76859,9628 76859,9628 76859,9628 75087,9628 75087,9628 75087,9628 75087,9585.6 75087,9585.6"];
	Layer2_Device15_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74656,9355.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage0_Accumulate -> Layer2_Device15_Stage1_Accumulate	[pos="e,74872,9382.4 74872,9522 74872,9522 74872,9392.4 74872,9392.4"];
	Layer2_Device15_Stage1_RecvKV -> Layer2_Device15_Stage1_Attention	[pos="e,74764,9575.7 74764,9741.7 74764,9741.7 74764,9585.7 74764,9585.7"];
	Layer2_Device15_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76296,9548.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage1_RecvKV -> Layer2_Device15_Stage2_RecvKV	[label="Ring transfer",
		lp="75967,9645.3",
		pos="e,75973,9601.9 75973,9688.6 75973,9688.6 75973,9611.9 75973,9611.9"];
	Layer2_Device15_Stage1_Attention -> Layer2_Device15_Stage1_Accumulate	[pos="e,74611,9382.4 74611,9522 74611,9522 74611,9392.4 74611,9392.4"];
	Layer2_Device15_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74774,9162.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage1_Accumulate -> Layer2_Device15_Stage2_Accumulate	[pos="e,74715,9189.4 74715,9329 74715,9329 74715,9199.4 74715,9199.4"];
	Layer2_Device15_Stage2_RecvKV -> Layer2_Device15_Stage2_Attention	[pos="e,75392,9382.7 75392,9548.7 75392,9548.7 75392,9392.7 75392,9392.7"];
	Layer2_Device15_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76386,9355.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage2_RecvKV -> Layer2_Device15_Stage3_RecvKV	[label="Ring transfer",
		lp="76389,9452.3",
		pos="e,76341,9408.9 76341,9495.6 76341,9495.6 76341,9418.9 76341,9418.9"];
	Layer2_Device15_Stage2_Attention -> Layer2_Device15_Stage2_Accumulate	[pos="e,74976,9189.4 74976,9329 74976,9329 74976,9199.4 74976,9199.4"];
	Layer2_Device15_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74919,8969.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage2_Accumulate -> Layer2_Device15_Stage3_Accumulate	[pos="e,74846,8996.4 74846,9136 74846,9136 74846,9006.4 74846,9006.4"];
	Layer2_Device15_Stage3_RecvKV -> Layer2_Device15_Stage3_Attention	[pos="e,77133,9189.7 77133,9355.7 77133,9355.7 77133,9199.7 77133,9199.7"];
	Layer2_Device15_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75983,9162.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage3_RecvKV -> Layer2_Device15_Stage4_RecvKV	[label="Ring transfer",
		lp="76232,9259.3",
		pos="e,76184,9215.9 76184,9302.6 76184,9302.6 76184,9225.9 76184,9225.9"];
	Layer2_Device15_Stage3_Attention -> Layer2_Device15_Stage3_Accumulate	[pos="e,75107,8996.3 77004,9136.2 77004,9120.6 77004,9104 77004,9104 77004,9104 75107,9104 75107,9104 75107,9104 75107,9006.3 75107,9006.3"];
	Layer2_Device15_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74903,8776.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage3_Accumulate -> Layer2_Device15_Stage4_Accumulate	[pos="e,74911,8803.4 74911,8943 74911,8943 74911,8813.4 74911,8813.4"];
	Layer2_Device15_Stage4_RecvKV -> Layer2_Device15_Stage4_Attention	[pos="e,74586,8996.5 75110,9123 74827,9123 74586,9123 74586,9123 74586,9123 74586,9006.5 74586,9006.5"];
	Layer2_Device15_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76128,8969.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage4_RecvKV -> Layer2_Device15_Stage5_RecvKV	[label="Ring transfer",
		lp="76103,9066.3",
		pos="e,76056,9022.9 76056,9109.6 76056,9109.6 76056,9032.9 76056,9032.9"];
	Layer2_Device15_Stage4_Attention -> Layer2_Device15_Stage4_Accumulate	[pos="e,74680,8803.5 74630,8969 74659,8969 74680,8969 74680,8969 74680,8969 74680,8813.5 74680,8813.5"];
	Layer2_Device15_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76828,8583.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage4_Accumulate -> Layer2_Device15_Stage5_Accumulate	[pos="e,76814,8610.5 74917,8750.3 74917,8725.7 74917,8693 74917,8693 74917,8693 76814,8693 76814,8693 76814,8693 76814,8620.5 76814,8620.5"];
	Layer2_Device15_Stage5_RecvKV -> Layer2_Device15_Stage5_Attention	[pos="e,77089,8785 77040,8969.7 77040,8964.1 77040,8785 77040,8785 77040,8785 77079,8785 77079,8785"];
	Layer2_Device15_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="76112,8776.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage5_RecvKV -> Layer2_Device15_Stage6_RecvKV	[label="Ring transfer",
		lp="76168,8873.3",
		pos="e,76120,8829.9 76120,8916.6 76120,8916.6 76120,8839.9 76120,8839.9"];
	Layer2_Device15_Stage5_Attention -> Layer2_Device15_Stage5_Accumulate	[pos="e,77054,8610.5 77089,8767 77068,8767 77054,8767 77054,8767 77054,8767 77054,8620.5 77054,8620.5"];
	Layer2_Device15_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76828,8390.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage5_Accumulate -> Layer2_Device15_Stage6_Accumulate	[pos="e,76828,8417.4 76828,8557 76828,8557 76828,8427.4 76828,8427.4"];
	Layer2_Device15_Stage6_RecvKV -> Layer2_Device15_Stage6_Attention	[pos="e,77335,8610.5 76702,8737 77015,8737 77335,8737 77335,8737 77335,8737 77335,8620.5 77335,8620.5"];
	Layer2_Device15_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75619,8583.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage6_RecvKV -> Layer2_Device15_Stage7_RecvKV	[label="Ring transfer",
		lp="75848,8680.3",
		pos="e,75866,8636.9 75866,8723.6 75866,8723.6 75866,8646.9 75866,8646.9"];
	Layer2_Device15_Stage6_Attention -> Layer2_Device15_Stage6_Accumulate	[pos="e,77060,8390 77248,8557.3 77248,8504.2 77248,8390 77248,8390 77248,8390 77070,8390 77070,8390"];
	Layer2_Device15_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76625,8197.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage6_Accumulate -> Layer2_Device15_Stage7_Accumulate	[pos="e,76726,8224.4 76726,8364 76726,8364 76726,8234.4 76726,8234.4"];
	Layer2_Device15_Stage7_RecvKV -> Layer2_Device15_Stage7_Attention	[pos="e,74636,8417.5 74746,8544 74678,8544 74636,8544 74636,8544 74636,8544 74636,8427.5 74636,8427.5"];
	Layer2_Device15_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75619,8390.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage7_RecvKV -> Layer2_Device15_Stage8_RecvKV	[label="Ring transfer",
		lp="75667,8487.3",
		pos="e,75619,8443.9 75619,8530.6 75619,8530.6 75619,8453.9 75619,8453.9"];
	Layer2_Device15_Stage7_Attention -> Layer2_Device15_Stage7_Accumulate	[pos="e,76466,8224.7 74569,8364.2 74569,8330.3 74569,8276 74569,8276 74569,8276 76466,8276 76466,8276 76466,8276 76466,8234.7 76466,8234.7"];
	Layer2_Device15_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76625,8004.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage7_Accumulate -> Layer2_Device15_Stage8_Accumulate	[pos="e,76625,8031.4 76625,8171 76625,8171 76625,8041.4 76625,8041.4"];
	Layer2_Device15_Stage8_RecvKV -> Layer2_Device15_Stage8_Attention	[pos="e,76987,8224.5 76210,8351 76580,8351 76987,8351 76987,8351 76987,8351 76987,8234.5 76987,8234.5"];
	Layer2_Device15_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75416,8197.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage8_RecvKV -> Layer2_Device15_Stage9_RecvKV	[label="Ring transfer",
		lp="75533,8294.3",
		pos="e,75518,8250.9 75518,8337.6 75518,8337.6 75518,8260.9 75518,8260.9"];
	Layer2_Device15_Stage8_Attention -> Layer2_Device15_Stage8_Accumulate	[pos="e,76857,8004 76962,8171.3 76962,8118.2 76962,8004 76962,8004 76962,8004 76867,8004 76867,8004"];
	Layer2_Device15_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74881,7811.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage8_Accumulate -> Layer2_Device15_Stage9_Accumulate	[pos="e,74958,7838.5 76625,7978.2 76625,7953.9 76625,7922 76625,7922 76625,7922 74958,7922 74958,7922 74958,7922 74958,7848.5 74958,7848.5"];
	Layer2_Device15_Stage9_RecvKV -> Layer2_Device15_Stage9_Attention	[pos="e,74308,8031.5 74543,8158 74406,8158 74308,8158 74308,8158 74308,8158 74308,8041.5 74308,8041.5"];
	Layer2_Device15_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75416,8004.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage9_RecvKV -> Layer2_Device15_Stage10_RecvKV	[label="Ring transfer",
		lp="75464,8101.3",
		pos="e,75416,8057.9 75416,8144.6 75416,8144.6 75416,8067.9 75416,8067.9"];
	Layer2_Device15_Stage9_Attention -> Layer2_Device15_Stage9_Accumulate	[pos="e,74804,7838.4 74284,7978.3 74284,7938.8 74284,7869 74284,7869 74284,7869 74804,7869 74804,7869 74804,7869 74804,7848.4 74804,7848.4"];
	Layer2_Device15_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74881,7618.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage9_Accumulate -> Layer2_Device15_Stage10_Accumulate	[pos="e,74881,7645.4 74881,7785 74881,7785 74881,7655.4 74881,7655.4"];
	Layer2_Device15_Stage10_RecvKV -> Layer2_Device15_Stage10_Attention	[pos="e,74544,7838.7 74544,8004.7 74544,8004.7 74544,7848.7 74544,7848.7"];
	Layer2_Device15_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76090,7811.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage10_RecvKV -> Layer2_Device15_Stage11_RecvKV	[label="Ring transfer",
		lp="75733,7908.3",
		pos="e,75753,7864.9 75753,7951.6 75753,7951.6 75753,7874.9 75753,7874.9"];
	Layer2_Device15_Stage10_Attention -> Layer2_Device15_Stage10_Accumulate	[pos="e,74649,7618 74433,7785.3 74433,7732.2 74433,7618 74433,7618 74433,7618 74639,7618 74639,7618"];
	Layer2_Device15_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75453,7425.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage10_Accumulate -> Layer2_Device15_Stage11_Accumulate	[pos="e,75340,7452.5 75113,7618 75227,7618 75340,7618 75340,7618 75340,7618 75340,7462.5 75340,7462.5"];
	Layer2_Device15_Stage11_RecvKV -> Layer2_Device15_Stage11_Attention	[pos="e,75613,7645.5 75613,7758.6 75613,7758.6 75613,7655.5 75613,7655.5"];
	Layer2_Device15_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76899,7618.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage11_RecvKV -> Layer2_Device15_Stage12_RecvKV	[label="Ring transfer",
		lp="76542,7715.3",
		pos="e,76494,7671.9 76494,7758.6 76494,7758.6 76494,7681.9 76494,7681.9"];
	Layer2_Device15_Stage11_Attention -> Layer2_Device15_Stage11_Accumulate	[pos="e,75572,7452.4 75572,7592 75572,7592 75572,7462.4 75572,7462.4"];
	Layer2_Device15_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75571,7232.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage11_Accumulate -> Layer2_Device15_Stage12_Accumulate	[pos="e,75512,7259.4 75512,7399 75512,7399 75512,7269.4 75512,7269.4"];
	Layer2_Device15_Stage12_RecvKV -> Layer2_Device15_Stage12_Attention	[pos="e,76092,7452.5 76092,7565.6 76092,7565.6 76092,7462.5 76092,7462.5"];
	Layer2_Device15_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77183,7425.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage12_RecvKV -> Layer2_Device15_Stage13_RecvKV	[label="Ring transfer",
		lp="77089,7522.3",
		pos="e,77041,7478.9 77041,7565.6 77041,7565.6 77041,7488.9 77041,7488.9"];
	Layer2_Device15_Stage12_Attention -> Layer2_Device15_Stage12_Accumulate	[pos="e,75772,7259.4 75772,7399 75772,7399 75772,7269.4 75772,7269.4"];
	Layer2_Device15_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76092,7039.8",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage12_Accumulate -> Layer2_Device15_Stage13_Accumulate	[pos="e,75860,7039 75803,7232 75820,7232 75831,7232 75831,7232 75831,7232 75831,7039 75831,7039 75831,7039 75850,7039 75850,7039"];
	Layer2_Device15_Stage13_RecvKV -> Layer2_Device15_Stage13_Attention	[pos="e,77930,7259.7 77930,7425.7 77930,7425.7 77930,7269.7 77930,7269.7"];
	Layer2_Device15_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76780,7232.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage13_RecvKV -> Layer2_Device15_Stage14_RecvKV	[label="Ring transfer",
		lp="77029,7329.3",
		pos="e,76982,7285.9 76982,7372.6 76982,7372.6 76982,7295.9 76982,7295.9"];
	Layer2_Device15_Stage13_Attention -> Layer2_Device15_Stage13_Accumulate	[pos="e,76092,7066.5 77989,7205.9 77989,7183.8 77989,7156 77989,7156 77989,7156 76092,7156 76092,7156 76092,7156 76092,7076.5 76092,7076.5"];
	Layer2_Device15_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76092,6887.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage13_Accumulate -> Layer2_Device15_Stage14_Accumulate	[pos="e,76092,6913.8 76092,7013.2 76092,7013.2 76092,6923.8 76092,6923.8"];
	Layer2_Device15_Stage14_RecvKV -> Layer2_Device15_Stage14_Attention	[pos="e,75571,7066.5 75907,7193 75717,7193 75571,7193 75571,7193 75571,7193 75571,7076.5 75571,7076.5"];
	Layer2_Device15_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="77301,7039.8",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer2_Device15_Stage14_RecvKV -> Layer2_Device15_Stage15_RecvKV	[label="Ring transfer",
		lp="77088,7136.3",
		pos="e,77040,7092.9 77040,7179.6 77040,7179.6 77040,7102.9 77040,7102.9"];
	Layer2_Device15_Stage14_Attention -> Layer2_Device15_Stage14_Accumulate	[pos="e,75860,6878 75571,7013.1 75571,6967.1 75571,6878 75571,6878 75571,6878 75850,6878 75850,6878"];
	Layer2_Device15_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="77832,6761.3",
		shape=rectangle,
		width=6.4306];
	Layer2_Device15_Stage14_Accumulate -> Layer2_Device15_Stage15_Accumulate	[pos="e,77600,6761 76285,6860.5 76285,6823.4 76285,6761 76285,6761 76285,6761 77590,6761 77590,6761"];
	Layer2_Device15_Stage15_RecvKV -> Layer2_Device15_Stage15_Attention	[pos="e,77832,6913.9 77832,6986.5 77832,6986.5 77832,6923.9 77832,6923.9"];
	Layer2_Device15_Stage15_Attention -> Layer2_Device15_Stage15_Accumulate	[pos="e,77832,6788 77832,6860.6 77832,6860.6 77832,6798 77832,6798"];
	Layer2_Device15_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77855,6635.3",
		shape=rectangle,
		width=6.2639];
	Layer2_Device15_Stage15_Accumulate -> Layer2_Device15_ConcatHeads	[pos="e,77846,6662 77846,6734.6 77846,6734.6 77846,6672 77846,6672"];
	Layer2_Device15_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77874,6509.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device15_ConcatHeads -> Layer2_Device15_OutputProj	[pos="e,77871,6536 77871,6608.6 77871,6608.6 77871,6546 77871,6546"];
	Layer2_Device15_OutputProj -> Layer2_Device15_Residual1	[pos="e,77874,6410 77874,6482.6 77874,6482.6 77874,6420 77874,6420"];
	Layer2_Device15_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77769,6257.3",
		shape=rectangle,
		width=5.9167];
	Layer2_Device15_Residual1 -> Layer2_Device15_LayerNorm2	[pos="e,77769,6284 77769,6356.6 77769,6356.6 77769,6294 77769,6294"];
	Layer2_Device15_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="77949,5575.3",
		shape=rectangle,
		width=10.736];
	Layer2_Device15_Residual1 -> Layer2_Device15_Residual2	[pos="e,78161,5601.9 78161,6356.4 78161,6356.4 78161,5611.9 78161,5611.9"];
	Layer2_Device15_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77477,6131.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device15_LayerNorm2 -> Layer2_Device15_GateProj	[pos="e,77629,6158 77629,6230.6 77629,6230.6 77629,6168 77629,6168"];
	Layer2_Device15_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77790,6042.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device15_LayerNorm2 -> Layer2_Device15_UpProj	[pos="e,77842,6069.1 77842,6230.5 77842,6230.5 77842,6079.1 77842,6079.1"];
	Layer2_Device15_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77477,5953.3",
		shape=rectangle,
		width=6.25];
	Layer2_Device15_GateProj -> Layer2_Device15_Activation	[pos="e,77408,5979.9 77408,6104.7 77408,6104.7 77408,5989.9 77408,5989.9"];
	Layer2_Device15_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="77572,5827.3",
		shape=rectangle,
		width=11.403];
	Layer2_Device15_UpProj -> Layer2_Device15_ElemMul	[pos="e,77842,5854.1 77842,6015.5 77842,6015.5 77842,5864.1 77842,5864.1"];
	Layer2_Device15_Activation -> Layer2_Device15_ElemMul	[pos="e,77477,5854 77477,5926.6 77477,5926.6 77477,5864 77477,5864"];
	Layer2_Device15_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77715,5701.3",
		shape=rectangle,
		width=6.0833];
	Layer2_Device15_ElemMul -> Layer2_Device15_DownProj	[pos="e,77715,5728 77715,5800.6 77715,5800.6 77715,5738 77715,5738"];
	Layer2_Device15_DownProj -> Layer2_Device15_Residual2	[pos="e,77748,5602 77748,5674.6 77748,5674.6 77748,5612 77748,5612"];
	Layer2_Device15_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 2 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77949,5438.3",
		shape=ellipse,
		width=8.3674];
	Layer2_Device15_Residual2 -> Layer2_Device15_Output	[pos="e,77949,5475.9 77949,5548.6 77949,5548.6 77949,5485.9 77949,5485.9"];
	Layer3_Device15_Input	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 15 Input\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77949,5290.4",
		shape=ellipse,
		width=8.3674];
	Layer2_Device15_Output -> Layer3_Device15_Input	[pos="e,77949,5328 77949,5400.6 77949,5400.6 77949,5338 77949,5338"];
	Layer3_Device0_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="5039,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device0_Input -> Layer3_Device0_LayerNorm1	[pos="e,5039,5179.9 5039,5253 5039,5253 5039,5189.9 5039,5189.9"];
	Layer3_Device0_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="5414,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device0_Input -> Layer3_Device0_Residual1	[pos="e,5307.1,1336.2 5307.1,5268.5 5307.1,5268.5 5307.1,1346.2 5307.1,1346.2"];
	Layer3_Device0_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4343,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device0_LayerNorm1 -> Layer3_Device0_QKVProj	[pos="e,4909.2,5054.1 4909.2,5126.7 4909.2,5126.7 4909.2,5064.1 4909.2,5064.1"];
	Layer3_Device0_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="3097,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage0_RecvKV	[label="Local K,V",
		lp="3597.5,4957.4",
		pos="e,3855.1,4914.2 3855.1,5000.7 3855.1,5000.7 3855.1,4924.2 3855.1,4924.2"];
	Layer3_Device0_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="3850,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage0_Attention	[label=Q_local,
		lp="4159,4860.9",
		pos="e,4049.1,4694.8 4049.1,5000.7 4049.1,5000.7 4049.1,4704.8 4049.1,4704.8"];
	Layer3_Device0_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1557,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage1_Attention	[label=Q_local,
		lp="1621,4764.4",
		pos="e,1686.9,4501.5 3693.3,5011 2906.9,5011 1686.9,5011 1686.9,5011 1686.9,5011 1686.9,4511.5 1686.9,4511.5"];
	Layer3_Device0_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2168,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage2_Attention	[label=Q_local,
		lp="4299,4667.9",
		pos="e,2338.4,4308.5 4144.1,5000.7 4144.1,4897.3 4144.1,4530 4144.1,4530 4144.1,4530 2338.4,4530 2338.4,4530 2338.4,4530 2338.4,4318.5 \
2338.4,4318.5"];
	Layer3_Device0_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4183,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage3_Attention	[label=Q_local,
		lp="4439,4571.4",
		pos="e,4355.6,4115.7 4355.6,5000.8 4355.6,5000.8 4355.6,4125.7 4355.6,4125.7"];
	Layer3_Device0_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1389,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage4_Attention	[label=Q_local,
		lp="1241,4474.9",
		pos="e,1308,3922.6 3693.3,5022 2801.7,5022 1308,5022 1308,5022 1308,5022 1308,3932.6 1308,3932.6"];
	Layer3_Device0_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4308,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage5_Attention	[label=Q_local,
		lp="4536,4378.4",
		pos="e,4477,3729.7 4477,5000.6 4477,5000.6 4477,3739.7 4477,3739.7"];
	Layer3_Device0_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4332,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage6_Attention	[label=Q_local,
		lp="4675,4281.9",
		pos="e,4551.5,3536.6 4551.5,5000.7 4551.5,5000.7 4551.5,3546.6 4551.5,3546.6"];
	Layer3_Device0_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1393,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage7_Attention	[label=Q_local,
		lp="1095,4185.4",
		pos="e,1161.2,3316 3693.5,5032 2739.9,5032 1072.8,5032 1072.8,5032 1072.8,5032 1072.8,3316 1072.8,3316 1072.8,3316 1151.2,3316 1151.2,\
3316"];
	Layer3_Device0_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4074,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage8_Attention	[label=Q_local,
		lp="4800,4088.9",
		pos="e,4305.6,3123 4603.9,5000.8 4603.9,4770.7 4603.9,3123 4603.9,3123 4603.9,3123 4315.6,3123 4315.6,3123"];
	Layer3_Device0_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1135,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage9_Attention	[label=Q_local,
		lp="977,3992.4",
		pos="e,988.17,2957.6 3693.2,5043 2717.7,5043 988.17,5043 988.17,5043 988.17,5043 988.17,2967.6 988.17,2967.6"];
	Layer3_Device0_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="1347,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage10_Attention	[label=Q_local,
		lp="837,3895.9",
		pos="e,1527.1,2764.7 4644.2,5000.6 4644.2,4753 4644.2,2875 4644.2,2875 4644.2,2875 1527.1,2875 1527.1,2875 1527.1,2875 1527.1,2774.7 \
1527.1,2774.7"];
	Layer3_Device0_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2677,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage11_Attention	[label=Q_local,
		lp="4923,3799.4",
		pos="e,2754.2,2571.4 4724.9,5000.8 4724.9,4738.3 4724.9,2622 4724.9,2622 4724.9,2622 2754.2,2622 2754.2,2622 2754.2,2622 2754.2,2581.4 \
2754.2,2581.4"];
	Layer3_Device0_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2961,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage12_Attention	[label=Q_local,
		lp="5064,3702.9",
		pos="e,2937.4,2378.5 4765.3,5000.8 4765.3,4738 4765.3,2619 4765.3,2619 4765.3,2619 2937.4,2619 2937.4,2619 2937.4,2619 2937.4,2388.5 \
2937.4,2388.5"];
	Layer3_Device0_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4976,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage13_Attention	[label=Q_local,
		lp="5204,3606.4",
		pos="e,5148.6,2185.8 4992.8,5014 5084.6,5014 5148.6,5014 5148.6,5014 5148.6,5014 5148.6,2195.8 5148.6,2195.8"];
	Layer3_Device0_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="2558,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage14_Attention	[label=Q_local,
		lp="691,3509.9",
		pos="e,2326.3,1965 4684.6,5000.6 4684.6,4740.6 4684.6,2677 4684.6,2677 4684.6,2677 2182.9,2677 2182.9,2677 2182.9,2677 2182.9,1965 2182.9,\
1965 2182.9,1965 2316.3,1965 2316.3,1965"];
	Layer3_Device0_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="4691,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_QKVProj -> Layer3_Device0_Stage15_Attention	[label=Q_local,
		lp="5322,3413.4",
		pos="e,4922.5,1822 4992.8,5027 5118.9,5027 5212.8,5027 5212.8,5027 5212.8,5027 5212.8,1822 5212.8,1822 5212.8,1822 4932.5,1822 4932.5,\
1822"];
	Layer3_Device0_Stage0_RecvKV -> Layer3_Device0_Stage0_Attention	[pos="e,3817.6,4694.7 3817.6,4857.6 3817.6,4857.6 3817.6,4704.7 3817.6,4704.7"];
	Layer3_Device0_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2641,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage0_RecvKV -> Layer3_Device0_Stage1_RecvKV	[label="Ring transfer",
		lp="2917,4764.4",
		pos="e,2869,4721 2869,4807.7 2869,4807.7 2869,4731 2869,4731"];
	Layer3_Device0_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="2078,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage0_Attention -> Layer3_Device0_Stage0_Accumulate	[pos="e,2078,4501.6 3850,4641.3 3850,4601.9 3850,4533 3850,4533 3850,4533 2078,4533 2078,4533 2078,4533 2078,4511.6 2078,4511.6"];
	Layer3_Device0_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1647,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage0_Accumulate -> Layer3_Device0_Stage1_Accumulate	[pos="e,1862.5,4308.5 1862.5,4448.1 1862.5,4448.1 1862.5,4318.5 1862.5,4318.5"];
	Layer3_Device0_Stage1_RecvKV -> Layer3_Device0_Stage1_Attention	[pos="e,1754.9,4501.8 1754.9,4667.8 1754.9,4667.8 1754.9,4511.8 1754.9,4511.8"];
	Layer3_Device0_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3287,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage1_RecvKV -> Layer3_Device0_Stage2_RecvKV	[label="Ring transfer",
		lp="2958,4571.4",
		pos="e,2964,4528 2964,4614.7 2964,4614.7 2964,4538 2964,4538"];
	Layer3_Device0_Stage1_Attention -> Layer3_Device0_Stage1_Accumulate	[pos="e,1602,4308.5 1602,4448.1 1602,4448.1 1602,4318.5 1602,4318.5"];
	Layer3_Device0_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1765,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage1_Accumulate -> Layer3_Device0_Stage2_Accumulate	[pos="e,1706,4115.5 1706,4255.1 1706,4255.1 1706,4125.5 1706,4125.5"];
	Layer3_Device0_Stage2_RecvKV -> Layer3_Device0_Stage2_Attention	[pos="e,2383.4,4308.8 2383.4,4474.8 2383.4,4474.8 2383.4,4318.8 2383.4,4318.8"];
	Layer3_Device0_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3377,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage2_RecvKV -> Layer3_Device0_Stage3_RecvKV	[label="Ring transfer",
		lp="3380,4378.4",
		pos="e,3332,4335 3332,4421.7 3332,4421.7 3332,4345 3332,4345"];
	Layer3_Device0_Stage2_Attention -> Layer3_Device0_Stage2_Accumulate	[pos="e,1966.5,4115.5 1966.5,4255.1 1966.5,4255.1 1966.5,4125.5 1966.5,4125.5"];
	Layer3_Device0_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1910,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage2_Accumulate -> Layer3_Device0_Stage3_Accumulate	[pos="e,1837.5,3922.5 1837.5,4062.1 1837.5,4062.1 1837.5,3932.5 1837.5,3932.5"];
	Layer3_Device0_Stage3_RecvKV -> Layer3_Device0_Stage3_Attention	[pos="e,4124.1,4115.8 4124.1,4281.8 4124.1,4281.8 4124.1,4125.8 4124.1,4125.8"];
	Layer3_Device0_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2974,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage3_RecvKV -> Layer3_Device0_Stage4_RecvKV	[label="Ring transfer",
		lp="3224,4185.4",
		pos="e,3175.5,4142 3175.5,4228.7 3175.5,4228.7 3175.5,4152 3175.5,4152"];
	Layer3_Device0_Stage3_Attention -> Layer3_Device0_Stage3_Accumulate	[pos="e,2097.9,3922.7 3995.1,4062.2 3995.1,4039.3 3995.1,4010 3995.1,4010 3995.1,4010 2097.9,4010 2097.9,4010 2097.9,4010 2097.9,3932.7 \
2097.9,3932.7"];
	Layer3_Device0_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1890,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage3_Accumulate -> Layer3_Device0_Stage4_Accumulate	[pos="e,1900,3729.5 1900,3869.1 1900,3869.1 1900,3739.5 1900,3739.5"];
	Layer3_Device0_Stage4_RecvKV -> Layer3_Device0_Stage4_Attention	[pos="e,1577,3922.6 2100.7,4049 1818.5,4049 1577,4049 1577,4049 1577,4049 1577,3932.6 1577,3932.6"];
	Layer3_Device0_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3119,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage4_RecvKV -> Layer3_Device0_Stage5_RecvKV	[label="Ring transfer",
		lp="3094,3992.4",
		pos="e,3046.5,3949 3046.5,4035.7 3046.5,4035.7 3046.5,3959 3046.5,3959"];
	Layer3_Device0_Stage4_Attention -> Layer3_Device0_Stage4_Accumulate	[pos="e,1668.5,3729.6 1620.8,3895 1649.2,3895 1668.5,3895 1668.5,3895 1668.5,3895 1668.5,3739.6 1668.5,3739.6"];
	Layer3_Device0_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3811,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage4_Accumulate -> Layer3_Device0_Stage5_Accumulate	[pos="e,3799.1,3536.4 1901.9,3676.1 1901.9,3639.4 1901.9,3578 1901.9,3578 1901.9,3578 3799.1,3578 3799.1,3578 3799.1,3578 3799.1,3546.4 \
3799.1,3546.4"];
	Layer3_Device0_Stage5_RecvKV -> Layer3_Device0_Stage5_Attention	[pos="e,4076.3,3702 4028.6,3895.8 4028.6,3889.9 4028.6,3702 4028.6,3702 4028.6,3702 4066.3,3702 4066.3,3702"];
	Layer3_Device0_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3099,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage5_RecvKV -> Layer3_Device0_Stage6_RecvKV	[label="Ring transfer",
		lp="3157,3799.4",
		pos="e,3109,3756 3109,3842.7 3109,3842.7 3109,3766 3109,3766"];
	Layer3_Device0_Stage5_Attention -> Layer3_Device0_Stage5_Accumulate	[pos="e,4042.7,3509 4088.5,3676.4 4088.5,3623.2 4088.5,3509 4088.5,3509 4088.5,3509 4052.7,3509 4052.7,3509"];
	Layer3_Device0_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3811,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage5_Accumulate -> Layer3_Device0_Stage6_Accumulate	[pos="e,3811,3343.5 3811,3483.1 3811,3483.1 3811,3353.5 3811,3353.5"];
	Layer3_Device0_Stage6_RecvKV -> Layer3_Device0_Stage6_Attention	[pos="e,4320,3536.6 3688.9,3663 4000.9,3663 4320,3663 4320,3663 4320,3663 4320,3546.6 4320,3546.6"];
	Layer3_Device0_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2602,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage6_RecvKV -> Layer3_Device0_Stage7_RecvKV	[label="Ring transfer",
		lp="2833,3606.4",
		pos="e,2850.5,3563 2850.5,3649.7 2850.5,3649.7 2850.5,3573 2850.5,3573"];
	Layer3_Device0_Stage6_Attention -> Layer3_Device0_Stage6_Accumulate	[pos="e,4042.7,3308 4203,3483.1 4203,3428.2 4203,3308 4203,3308 4203,3308 4052.7,3308 4052.7,3308"];
	Layer3_Device0_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3553,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage6_Accumulate -> Layer3_Device0_Stage7_Accumulate	[pos="e,3682,3150.5 3682,3290.1 3682,3290.1 3682,3160.5 3682,3160.5"];
	Layer3_Device0_Stage7_RecvKV -> Layer3_Device0_Stage7_Attention	[pos="e,1391,3343.6 1867,3509 1613.1,3509 1391,3509 1391,3509 1391,3509 1391,3353.6 1391,3353.6"];
	Layer3_Device0_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2602,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage7_RecvKV -> Layer3_Device0_Stage8_RecvKV	[label="Ring transfer",
		lp="2650,3413.4",
		pos="e,2602,3370 2602,3456.7 2602,3456.7 2602,3380 2602,3380"];
	Layer3_Device0_Stage7_Attention -> Layer3_Device0_Stage7_Accumulate	[pos="e,3421.6,3150.6 1524.4,3290.3 1524.4,3250.9 1524.4,3182 1524.4,3182 1524.4,3182 3421.6,3182 3421.6,3182 3421.6,3182 3421.6,3160.6 \
3421.6,3160.6"];
	Layer3_Device0_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3553,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage7_Accumulate -> Layer3_Device0_Stage8_Accumulate	[pos="e,3553,2957.5 3553,3097.1 3553,3097.1 3553,2967.5 3553,2967.5"];
	Layer3_Device0_Stage8_RecvKV -> Layer3_Device0_Stage8_Attention	[pos="e,3942.5,3150.6 3191.9,3277 3551.6,3277 3942.5,3277 3942.5,3277 3942.5,3277 3942.5,3160.6 3942.5,3160.6"];
	Layer3_Device0_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="2344,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage8_RecvKV -> Layer3_Device0_Stage9_RecvKV	[label="Ring transfer",
		lp="2394,3220.4",
		pos="e,2473,3177 2473,3263.7 2473,3263.7 2473,3187 2473,3187"];
	Layer3_Device0_Stage8_Attention -> Layer3_Device0_Stage8_Accumulate	[pos="e,3784.6,2930 3919.6,3097.4 3919.6,3044.2 3919.6,2930 3919.6,2930 3919.6,2930 3794.6,2930 3794.6,2930"];
	Layer3_Device0_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1868,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage8_Accumulate -> Layer3_Device0_Stage9_Accumulate	[pos="e,1945.2,2764.5 3553,2904.4 3553,2874.5 3553,2830 3553,2830 3553,2830 1945.2,2830 1945.2,2830 1945.2,2830 1945.2,2774.5 1945.2,2774.5"];
	Layer3_Device0_Stage9_RecvKV -> Layer3_Device0_Stage9_Attention	[pos="e,1264,2957.6 1609.1,3123 1417.7,3123 1264,3123 1264,3123 1264,3123 1264,2967.6 1264,2967.6"];
	Layer3_Device0_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="2344,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage9_RecvKV -> Layer3_Device0_Stage10_RecvKV	[label="Ring transfer",
		lp="2392,3027.4",
		pos="e,2344,2984 2344,3070.7 2344,3070.7 2344,2994 2344,2994"];
	Layer3_Device0_Stage9_Attention -> Layer3_Device0_Stage9_Accumulate	[pos="e,1790.8,2764.6 1241,2904.1 1241,2860 1241,2777 1241,2777 1241,2777 1790.8,2777 1790.8,2777 1790.8,2777 1790.8,2774.6 1790.8,2774.6"];
	Layer3_Device0_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="1868,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage9_Accumulate -> Layer3_Device0_Stage10_Accumulate	[pos="e,1868,2571.5 1868,2711.1 1868,2711.1 1868,2581.5 1868,2581.5"];
	Layer3_Device0_Stage10_RecvKV -> Layer3_Device0_Stage10_Attention	[pos="e,1475.8,2764.8 1475.8,2930.8 1475.8,2930.8 1475.8,2774.8 1475.8,2774.8"];
	Layer3_Device0_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3077,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage10_RecvKV -> Layer3_Device0_Stage11_RecvKV	[label="Ring transfer",
		lp="2661,2834.4",
		pos="e,2710.5,2791 2710.5,2877.7 2710.5,2877.7 2710.5,2801 2710.5,2801"];
	Layer3_Device0_Stage10_Attention -> Layer3_Device0_Stage10_Accumulate	[pos="e,1636.4,2544 1347,2711.4 1347,2658.2 1347,2544 1347,2544 1347,2544 1626.4,2544 1626.4,2544"];
	Layer3_Device0_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2440,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage10_Accumulate -> Layer3_Device0_Stage11_Accumulate	[pos="e,2327,2378.6 2099.5,2544 2213.6,2544 2327,2544 2327,2544 2327,2544 2327,2388.6 2327,2388.6"];
	Layer3_Device0_Stage11_RecvKV -> Layer3_Device0_Stage11_Attention	[pos="e,2599.8,2571.6 2599.8,2684.7 2599.8,2684.7 2599.8,2581.6 2599.8,2581.6"];
	Layer3_Device0_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3886,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage11_RecvKV -> Layer3_Device0_Stage12_RecvKV	[label="Ring transfer",
		lp="3248,2641.4",
		pos="e,3481.5,2598 3481.5,2684.7 3481.5,2684.7 3481.5,2608 3481.5,2608"];
	Layer3_Device0_Stage11_Attention -> Layer3_Device0_Stage11_Accumulate	[pos="e,2558.5,2378.5 2558.5,2518.1 2558.5,2518.1 2558.5,2388.5 2558.5,2388.5"];
	Layer3_Device0_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="2558,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage11_Accumulate -> Layer3_Device0_Stage12_Accumulate	[pos="e,2499,2185.5 2499,2325.1 2499,2325.1 2499,2195.5 2499,2195.5"];
	Layer3_Device0_Stage12_RecvKV -> Layer3_Device0_Stage12_Attention	[pos="e,3079.4,2378.6 3079.4,2491.7 3079.4,2491.7 3079.4,2388.6 3079.4,2388.6"];
	Layer3_Device0_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="4170,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage12_RecvKV -> Layer3_Device0_Stage13_RecvKV	[label="Ring transfer",
		lp="4148,2448.4",
		pos="e,4028,2405 4028,2491.7 4028,2491.7 4028,2415 4028,2415"];
	Layer3_Device0_Stage12_Attention -> Layer3_Device0_Stage12_Accumulate	[pos="e,2759.5,2185.5 2759.5,2325.1 2759.5,2325.1 2759.5,2195.5 2759.5,2195.5"];
	Layer3_Device0_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3079,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage12_Accumulate -> Layer3_Device0_Stage13_Accumulate	[pos="e,2847.3,1965 2789.6,2158 2807.3,2158 2818.4,2158 2818.4,2158 2818.4,2158 2818.4,1965 2818.4,1965 2818.4,1965 2837.3,1965 2837.3,\
1965"];
	Layer3_Device0_Stage13_RecvKV -> Layer3_Device0_Stage13_Attention	[pos="e,4917.1,2185.8 4917.1,2351.8 4917.1,2351.8 4917.1,2195.8 4917.1,2195.8"];
	Layer3_Device0_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="3767,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage13_RecvKV -> Layer3_Device0_Stage14_RecvKV	[label="Ring transfer",
		lp="3951,2255.4",
		pos="e,3968.5,2212 3968.5,2298.7 3968.5,2298.7 3968.5,2222 3968.5,2222"];
	Layer3_Device0_Stage13_Attention -> Layer3_Device0_Stage13_Accumulate	[pos="e,3079,1992.4 4976,2132.4 4976,2092.8 4976,2023 4976,2023 4976,2023 3079,2023 3079,2023 3079,2023 3079,2002.4 3079,2002.4"];
	Layer3_Device0_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="3079,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage13_Accumulate -> Layer3_Device0_Stage14_Accumulate	[pos="e,3079,1839.9 3079,1939.3 3079,1939.3 3079,1849.9 3079,1849.9"];
	Layer3_Device0_Stage14_RecvKV -> Layer3_Device0_Stage14_Attention	[pos="e,2558,1992.6 2893.6,2119 2703.5,2119 2558,2119 2558,2119 2558,2119 2558,2002.6 2558,2002.6"];
	Layer3_Device0_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="4288,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device0_Stage14_RecvKV -> Layer3_Device0_Stage15_RecvKV	[label="Ring transfer",
		lp="3969,2062.4",
		pos="e,4027.5,2019 4027.5,2105.7 4027.5,2105.7 4027.5,2029 4027.5,2029"];
	Layer3_Device0_Stage14_Attention -> Layer3_Device0_Stage14_Accumulate	[pos="e,2847.4,1813 2558,1939.3 2558,1895.5 2558,1813 2558,1813 2558,1813 2837.4,1813 2837.4,1813"];
	Layer3_Device0_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="4691,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device0_Stage14_Accumulate -> Layer3_Device0_Stage15_Accumulate	[pos="e,4459.2,1687 3079,1786.6 3079,1749.5 3079,1687 3079,1687 3079,1687 4449.2,1687 4449.2,1687"];
	Layer3_Device0_Stage15_RecvKV -> Layer3_Device0_Stage15_Attention	[pos="e,4691,1840 4691,1912.6 4691,1912.6 4691,1850 4691,1850"];
	Layer3_Device0_Stage15_Attention -> Layer3_Device0_Stage15_Accumulate	[pos="e,4691,1714.1 4691,1786.7 4691,1786.7 4691,1724.1 4691,1724.1"];
	Layer3_Device0_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="4896,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device0_Stage15_Accumulate -> Layer3_Device0_ConcatHeads	[pos="e,4796.5,1588.1 4796.5,1660.7 4796.5,1660.7 4796.5,1598.1 4796.5,1598.1"];
	Layer3_Device0_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="5005,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device0_ConcatHeads -> Layer3_Device0_OutputProj	[pos="e,4956.8,1462.1 4956.8,1534.7 4956.8,1534.7 4956.8,1472.1 4956.8,1472.1"];
	Layer3_Device0_OutputProj -> Layer3_Device0_Residual1	[pos="e,5122.8,1336.1 5122.8,1408.7 5122.8,1408.7 5122.8,1346.1 5122.8,1346.1"];
	Layer3_Device0_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="5414,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device0_Residual1 -> Layer3_Device0_LayerNorm2	[pos="e,5414,1210.1 5414,1282.7 5414,1282.7 5414,1220.1 5414,1220.1"];
	Layer3_Device0_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="5799,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device0_Residual1 -> Layer3_Device0_Residual2	[pos="e,5772.2,527.98 5772.2,1282.5 5772.2,1282.5 5772.2,537.98 5772.2,537.98"];
	Layer3_Device0_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="5206,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device0_LayerNorm2 -> Layer3_Device0_GateProj	[pos="e,5316,1084.1 5316,1156.7 5316,1156.7 5316,1094.1 5316,1094.1"];
	Layer3_Device0_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="5519,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device0_LayerNorm2 -> Layer3_Device0_UpProj	[pos="e,5529,995.2 5529,1156.6 5529,1156.6 5529,1005.2 5529,1005.2"];
	Layer3_Device0_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="5206,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device0_GateProj -> Layer3_Device0_Activation	[pos="e,5137.5,905.95 5137.5,1030.8 5137.5,1030.8 5137.5,915.95 5137.5,915.95"];
	Layer3_Device0_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="5301,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device0_UpProj -> Layer3_Device0_ElemMul	[pos="e,5571.2,780.2 5571.2,941.61 5571.2,941.61 5571.2,790.2 5571.2,790.2"];
	Layer3_Device0_Activation -> Layer3_Device0_ElemMul	[pos="e,5206,780.09 5206,852.69 5206,852.69 5206,790.09 5206,790.09"];
	Layer3_Device0_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="5444,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device0_ElemMul -> Layer3_Device0_DownProj	[pos="e,5444,654.09 5444,726.69 5444,726.69 5444,664.09 5444,664.09"];
	Layer3_Device0_DownProj -> Layer3_Device0_Residual2	[pos="e,5537.8,528.09 5537.8,600.69 5537.8,600.69 5537.8,538.09 5537.8,538.09"];
	Layer3_Device0_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 0 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="12176,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device0_Residual2 -> Layer3_Device0_Output	[pos="e,11875,364 5799,474.59 5799,434.6 5799,364 5799,364 5799,364 11865,364 11865,364"];
	Sequence_Aggregate	[fillcolor=lightyellow,
		height=1.4722,
		label="Sequence Aggregate\nInput: 16Ã—[batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=10000, d_model=8192]",
		pos="40613,200.95",
		shape=parallelogram,
		width=12.787];
	Layer3_Device0_Output -> Sequence_Aggregate	[pos="e,40176,161 12176,326.64 12176,267.81 12176,161 12176,161 12176,161 40166,161 40166,161"];
	Layer3_Device1_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9342,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device1_Input -> Layer3_Device1_LayerNorm1	[pos="e,9520.2,5180.1 9570.6,5290 9540.2,5290 9520.2,5290 9520.2,5290 9520.2,5290 9520.2,5190.1 9520.2,5190.1"];
	Layer3_Device1_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="10299,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device1_Input -> Layer3_Device1_Residual1	[pos="e,10146,1336 10146,5274.8 10146,5274.8 10146,1346 10146,1346"];
	Layer3_Device1_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9184,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device1_LayerNorm1 -> Layer3_Device1_QKVProj	[pos="e,9342,5054.1 9342,5126.7 9342,5126.7 9342,5064.1 9342,5064.1"];
	Layer3_Device1_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7992,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage0_RecvKV	[label="Local K,V",
		lp="8492.5,4957.4",
		pos="e,8723.1,4914.2 8723.1,5000.7 8723.1,5000.7 8723.1,4924.2 8723.1,4924.2"];
	Layer3_Device1_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="8745,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage0_Attention	[label=Q_local,
		lp="9054,4860.9",
		pos="e,8944.1,4694.8 8944.1,5000.7 8944.1,5000.7 8944.1,4704.8 8944.1,4704.8"];
	Layer3_Device1_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6452,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage1_Attention	[label=Q_local,
		lp="6516,4764.4",
		pos="e,6607.9,4501.7 8534.4,5007 7771.2,5007 6607.9,5007 6607.9,5007 6607.9,5007 6607.9,4511.7 6607.9,4511.7"];
	Layer3_Device1_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7063,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage2_Attention	[label=Q_local,
		lp="9194,4667.9",
		pos="e,7233.4,4308.5 9039.1,5000.6 9039.1,4897.8 9039.1,4535 9039.1,4535 9039.1,4535 7233.4,4535 7233.4,4535 7233.4,4535 7233.4,4318.5 \
7233.4,4318.5"];
	Layer3_Device1_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9078,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage3_Attention	[label=Q_local,
		lp="9334,4571.4",
		pos="e,9250.6,4115.7 9250.6,5000.8 9250.6,5000.8 9250.6,4125.7 9250.6,4125.7"];
	Layer3_Device1_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6284,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage4_Attention	[label=Q_local,
		lp="6136,4474.9",
		pos="e,6192,3922.6 8534.3,5020 7654.3,5020 6192,5020 6192,5020 6192,5020 6192,3932.6 6192,3932.6"];
	Layer3_Device1_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9196,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage5_Attention	[label=Q_local,
		lp="9438,4378.4",
		pos="e,9368.5,3729.7 9368.5,5000.6 9368.5,5000.6 9368.5,3739.7 9368.5,3739.7"];
	Layer3_Device1_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9220,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage6_Attention	[label=Q_local,
		lp="9570,4281.9",
		pos="e,9439.5,3536.6 9439.5,5000.7 9439.5,5000.7 9439.5,3546.6 9439.5,3546.6"];
	Layer3_Device1_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6281,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage7_Attention	[label=Q_local,
		lp="5990,4185.4",
		pos="e,6049.4,3325 8534.3,5034 7605,5034 6006.8,5034 6006.8,5034 6006.8,5034 6006.8,3325 6006.8,3325 6006.8,3325 6039.4,3325 6039.4,3325"];
	Layer3_Device1_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9092,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage8_Attention	[label=Q_local,
		lp="9688,4088.9",
		pos="e,9323.7,3123 9509.1,5000.8 9509.1,4770.7 9509.1,3123 9509.1,3123 9509.1,3123 9333.7,3123 9333.7,3123"];
	Layer3_Device1_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6153,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage9_Attention	[label=Q_local,
		lp="5872,3992.4",
		pos="e,5964.2,2957.6 8534.4,5040 7593.8,5040 5964.2,5040 5964.2,5040 5964.2,5040 5964.2,2967.6 5964.2,2967.6"];
	Layer3_Device1_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="6284,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage10_Attention	[label=Q_local,
		lp="5732,3895.9",
		pos="e,6052.4,2737 8534.4,5047 7567.2,5047 5861,5047 5861,5047 5861,5047 5861,2737 5861,2737 5861,2737 6042.4,2737 6042.4,2737"];
	Layer3_Device1_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7611,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage11_Attention	[label=Q_local,
		lp="9809,3799.4",
		pos="e,7688.2,2571.4 9624.4,5000.6 9624.4,4735.9 9624.4,2602 9624.4,2602 9624.4,2602 7688.2,2602 7688.2,2602 7688.2,2602 7688.2,2581.4 \
7688.2,2581.4"];
	Layer3_Device1_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7846,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage12_Attention	[label=Q_local,
		lp="9949,3702.9",
		pos="e,7871.4,2378.5 9682,5000.6 9682,4735.7 9682,2600 9682,2600 9682,2600 7871.4,2600 7871.4,2600 7871.4,2600 7871.4,2388.5 7871.4,2388.5"];
	Layer3_Device1_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9861,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage13_Attention	[label=Q_local,
		lp="10089,3606.4",
		pos="e,10034,2185.8 9833.6,5016 9949.2,5016 10034,5016 10034,5016 10034,5016 10034,2195.8 10034,2195.8"];
	Layer3_Device1_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="7443,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage14_Attention	[label=Q_local,
		lp="5586,3509.9",
		pos="e,7211.4,1965 9566.7,5000.5 9566.7,4750.5 9566.7,2840 9566.7,2840 9566.7,2840 7065,2840 7065,2840 7065,2840 7065,1965 7065,1965 \
7065,1965 7201.4,1965 7201.4,1965"];
	Layer3_Device1_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="9325,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_QKVProj -> Layer3_Device1_Stage15_Attention	[label=Q_local,
		lp="10207,3413.4",
		pos="e,9556.7,1822 9833.7,5031 9993.3,5031 10119,5031 10119,5031 10119,5031 10119,1822 10119,1822 10119,1822 9566.7,1822 9566.7,1822"];
	Layer3_Device1_Stage0_RecvKV -> Layer3_Device1_Stage0_Attention	[pos="e,8712.6,4694.7 8712.6,4857.6 8712.6,4857.6 8712.6,4704.7 8712.6,4704.7"];
	Layer3_Device1_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7536,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage0_RecvKV -> Layer3_Device1_Stage1_RecvKV	[label="Ring transfer",
		lp="7893,4764.4",
		pos="e,7764,4721 7764,4807.7 7764,4807.7 7764,4731 7764,4731"];
	Layer3_Device1_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="6973,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage0_Attention -> Layer3_Device1_Stage0_Accumulate	[pos="e,6973,4501.7 8745,4641.3 8745,4603.3 8745,4538 8745,4538 8745,4538 6973,4538 6973,4538 6973,4538 6973,4511.7 6973,4511.7"];
	Layer3_Device1_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6542,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage0_Accumulate -> Layer3_Device1_Stage1_Accumulate	[pos="e,6757.5,4308.5 6757.5,4448.1 6757.5,4448.1 6757.5,4318.5 6757.5,4318.5"];
	Layer3_Device1_Stage1_RecvKV -> Layer3_Device1_Stage1_Attention	[pos="e,6649.9,4501.8 6649.9,4667.8 6649.9,4667.8 6649.9,4511.8 6649.9,4511.8"];
	Layer3_Device1_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8182,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage1_RecvKV -> Layer3_Device1_Stage2_RecvKV	[label="Ring transfer",
		lp="7907,4571.4",
		pos="e,7859,4528 7859,4614.7 7859,4614.7 7859,4538 7859,4538"];
	Layer3_Device1_Stage1_Attention -> Layer3_Device1_Stage1_Accumulate	[pos="e,6497,4308.5 6497,4448.1 6497,4448.1 6497,4318.5 6497,4318.5"];
	Layer3_Device1_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6660,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage1_Accumulate -> Layer3_Device1_Stage2_Accumulate	[pos="e,6601,4115.5 6601,4255.1 6601,4255.1 6601,4125.5 6601,4125.5"];
	Layer3_Device1_Stage2_RecvKV -> Layer3_Device1_Stage2_Attention	[pos="e,7278.4,4308.8 7278.4,4474.8 7278.4,4474.8 7278.4,4318.8 7278.4,4318.8"];
	Layer3_Device1_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8272,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage2_RecvKV -> Layer3_Device1_Stage3_RecvKV	[label="Ring transfer",
		lp="8275,4378.4",
		pos="e,8227,4335 8227,4421.7 8227,4421.7 8227,4345 8227,4345"];
	Layer3_Device1_Stage2_Attention -> Layer3_Device1_Stage2_Accumulate	[pos="e,6861.5,4115.5 6861.5,4255.1 6861.5,4255.1 6861.5,4125.5 6861.5,4125.5"];
	Layer3_Device1_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6805,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage2_Accumulate -> Layer3_Device1_Stage3_Accumulate	[pos="e,6732.5,3922.5 6732.5,4062.1 6732.5,4062.1 6732.5,3932.5 6732.5,3932.5"];
	Layer3_Device1_Stage3_RecvKV -> Layer3_Device1_Stage3_Attention	[pos="e,9019.1,4115.8 9019.1,4281.8 9019.1,4281.8 9019.1,4125.8 9019.1,4125.8"];
	Layer3_Device1_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7869,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage3_RecvKV -> Layer3_Device1_Stage4_RecvKV	[label="Ring transfer",
		lp="8186,4185.4",
		pos="e,8070.5,4142 8070.5,4228.7 8070.5,4228.7 8070.5,4152 8070.5,4152"];
	Layer3_Device1_Stage3_Attention -> Layer3_Device1_Stage3_Accumulate	[pos="e,6992.9,3922.6 8890.1,4062.2 8890.1,4041.1 8890.1,4015 8890.1,4015 8890.1,4015 6992.9,4015 6992.9,4015 6992.9,4015 6992.9,3932.6 \
6992.9,3932.6"];
	Layer3_Device1_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6778,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage3_Accumulate -> Layer3_Device1_Stage4_Accumulate	[pos="e,6791.5,3729.5 6791.5,3869.1 6791.5,3869.1 6791.5,3739.5 6791.5,3739.5"];
	Layer3_Device1_Stage4_RecvKV -> Layer3_Device1_Stage4_Attention	[pos="e,6472,3922.6 6995.7,4049 6713.5,4049 6472,4049 6472,4049 6472,4049 6472,3932.6 6472,3932.6"];
	Layer3_Device1_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8014,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage4_RecvKV -> Layer3_Device1_Stage5_RecvKV	[label="Ring transfer",
		lp="7989,3992.4",
		pos="e,7941.5,3949 7941.5,4035.7 7941.5,4035.7 7941.5,3959 7941.5,3959"];
	Layer3_Device1_Stage4_Attention -> Layer3_Device1_Stage4_Accumulate	[pos="e,6560,3729.6 6515.7,3895 6542.2,3895 6560,3895 6560,3895 6560,3895 6560,3739.6 6560,3739.6"];
	Layer3_Device1_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8699,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage4_Accumulate -> Layer3_Device1_Stage5_Accumulate	[pos="e,8687.1,3536.7 6789.9,3676.3 6789.9,3641 6789.9,3583 6789.9,3583 6789.9,3583 8687.1,3583 8687.1,3583 8687.1,3583 8687.1,3546.7 \
8687.1,3546.7"];
	Layer3_Device1_Stage5_RecvKV -> Layer3_Device1_Stage5_Attention	[pos="e,8964.4,3702 8918.6,3895.8 8918.6,3889.9 8918.6,3702 8918.6,3702 8918.6,3702 8954.4,3702 8954.4,3702"];
	Layer3_Device1_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7987,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage5_RecvKV -> Layer3_Device1_Stage6_RecvKV	[label="Ring transfer",
		lp="8048,3799.4",
		pos="e,8000.5,3756 8000.5,3842.7 8000.5,3842.7 8000.5,3766 8000.5,3766"];
	Layer3_Device1_Stage5_Attention -> Layer3_Device1_Stage5_Accumulate	[pos="e,8930.7,3509 8976.5,3676.4 8976.5,3623.2 8976.5,3509 8976.5,3509 8976.5,3509 8940.7,3509 8940.7,3509"];
	Layer3_Device1_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8699,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage5_Accumulate -> Layer3_Device1_Stage6_Accumulate	[pos="e,8699,3343.5 8699,3483.1 8699,3483.1 8699,3353.5 8699,3353.5"];
	Layer3_Device1_Stage6_RecvKV -> Layer3_Device1_Stage6_Attention	[pos="e,9208,3536.6 8576.9,3663 8888.9,3663 9208,3663 9208,3663 9208,3663 9208,3546.6 9208,3546.6"];
	Layer3_Device1_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7490,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage6_RecvKV -> Layer3_Device1_Stage7_RecvKV	[label="Ring transfer",
		lp="7721,3606.4",
		pos="e,7738.5,3563 7738.5,3649.7 7738.5,3649.7 7738.5,3573 7738.5,3573"];
	Layer3_Device1_Stage6_Attention -> Layer3_Device1_Stage6_Accumulate	[pos="e,8930.5,3316 9156,3483.4 9156,3430.2 9156,3316 9156,3316 9156,3316 8940.5,3316 8940.5,3316"];
	Layer3_Device1_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8571,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage6_Accumulate -> Layer3_Device1_Stage7_Accumulate	[pos="e,8635,3150.5 8635,3290.1 8635,3290.1 8635,3160.5 8635,3160.5"];
	Layer3_Device1_Stage7_RecvKV -> Layer3_Device1_Stage7_Attention	[pos="e,6282.5,3343.6 6616.5,3470 6427.2,3470 6282.5,3470 6282.5,3470 6282.5,3470 6282.5,3353.6 6282.5,3353.6"];
	Layer3_Device1_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7490,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage7_RecvKV -> Layer3_Device1_Stage8_RecvKV	[label="Ring transfer",
		lp="7538,3413.4",
		pos="e,7490,3370 7490,3456.7 7490,3456.7 7490,3380 7490,3380"];
	Layer3_Device1_Stage7_Attention -> Layer3_Device1_Stage7_Accumulate	[pos="e,8374.6,3150.6 6477.4,3290.2 6477.4,3269.1 6477.4,3243 6477.4,3243 6477.4,3243 8374.6,3243 8374.6,3243 8374.6,3243 8374.6,3160.6 \
8374.6,3160.6"];
	Layer3_Device1_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="8571,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage7_Accumulate -> Layer3_Device1_Stage8_Accumulate	[pos="e,8571,2957.5 8571,3097.1 8571,3097.1 8571,2967.5 8571,2967.5"];
	Layer3_Device1_Stage8_RecvKV -> Layer3_Device1_Stage8_Attention	[pos="e,8895.5,3150.6 8079.9,3277 8464.6,3277 8895.5,3277 8895.5,3277 8895.5,3277 8895.5,3160.6 8895.5,3160.6"];
	Layer3_Device1_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="7362,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage8_RecvKV -> Layer3_Device1_Stage9_RecvKV	[label="Ring transfer",
		lp="7474,3220.4",
		pos="e,7426,3177 7426,3263.7 7426,3263.7 7426,3187 7426,3187"];
	Layer3_Device1_Stage8_Attention -> Layer3_Device1_Stage8_Accumulate	[pos="e,8802.7,2930 8897.1,3097.4 8897.1,3044.2 8897.1,2930 8897.1,2930 8897.1,2930 8812.7,2930 8812.7,2930"];
	Layer3_Device1_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6805,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage8_Accumulate -> Layer3_Device1_Stage9_Accumulate	[pos="e,6882.2,2764.4 8571,2904.2 8571,2878 8571,2842 8571,2842 8571,2842 6882.2,2842 6882.2,2842 6882.2,2842 6882.2,2774.4 6882.2,2774.4"];
	Layer3_Device1_Stage9_RecvKV -> Layer3_Device1_Stage9_Attention	[pos="e,6217,2957.6 6488.7,3084 6331.8,3084 6217,3084 6217,3084 6217,3084 6217,2967.6 6217,2967.6"];
	Layer3_Device1_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="7362,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage9_RecvKV -> Layer3_Device1_Stage10_RecvKV	[label="Ring transfer",
		lp="7410,3027.4",
		pos="e,7362,2984 7362,3070.7 7362,3070.7 7362,2994 7362,2994"];
	Layer3_Device1_Stage9_Attention -> Layer3_Device1_Stage9_Accumulate	[pos="e,6727.8,2764.7 6218.5,2904.2 6218.5,2866.4 6218.5,2802 6218.5,2802 6218.5,2802 6727.8,2802 6727.8,2802 6727.8,2802 6727.8,2774.7 \
6727.8,2774.7"];
	Layer3_Device1_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="6805,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage9_Accumulate -> Layer3_Device1_Stage10_Accumulate	[pos="e,6805,2571.5 6805,2711.1 6805,2711.1 6805,2581.5 6805,2581.5"];
	Layer3_Device1_Stage10_RecvKV -> Layer3_Device1_Stage10_Attention	[pos="e,6478.9,2764.8 6478.9,2930.8 6478.9,2930.8 6478.9,2774.8 6478.9,2774.8"];
	Layer3_Device1_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8014,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage10_RecvKV -> Layer3_Device1_Stage11_RecvKV	[label="Ring transfer",
		lp="7783,2834.4",
		pos="e,7688,2791 7688,2877.7 7688,2877.7 7688,2801 7688,2801"];
	Layer3_Device1_Stage10_Attention -> Layer3_Device1_Stage10_Accumulate	[pos="e,6573.4,2544 6350.5,2711.4 6350.5,2658.2 6350.5,2544 6350.5,2544 6350.5,2544 6563.4,2544 6563.4,2544"];
	Layer3_Device1_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7325,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage10_Accumulate -> Layer3_Device1_Stage11_Accumulate	[pos="e,7236.9,2378.6 7036.8,2544 7139.3,2544 7236.9,2544 7236.9,2544 7236.9,2544 7236.9,2388.6 7236.9,2388.6"];
	Layer3_Device1_Stage11_RecvKV -> Layer3_Device1_Stage11_Attention	[pos="e,7533.8,2571.6 7533.8,2684.7 7533.8,2684.7 7533.8,2581.6 7533.8,2581.6"];
	Layer3_Device1_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8820,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage11_RecvKV -> Layer3_Device1_Stage12_RecvKV	[label="Ring transfer",
		lp="8465,2641.4",
		pos="e,8417,2598 8417,2684.7 8417,2684.7 8417,2608 8417,2608"];
	Layer3_Device1_Stage11_Attention -> Layer3_Device1_Stage11_Accumulate	[pos="e,7468,2378.5 7468,2518.1 7468,2518.1 7468,2388.5 7468,2388.5"];
	Layer3_Device1_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7443,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage11_Accumulate -> Layer3_Device1_Stage12_Accumulate	[pos="e,7384,2185.5 7384,2325.1 7384,2325.1 7384,2195.5 7384,2195.5"];
	Layer3_Device1_Stage12_RecvKV -> Layer3_Device1_Stage12_Attention	[pos="e,7988.9,2378.6 7988.9,2491.7 7988.9,2491.7 7988.9,2388.6 7988.9,2388.6"];
	Layer3_Device1_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="9055,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage12_RecvKV -> Layer3_Device1_Stage13_RecvKV	[label="Ring transfer",
		lp="9033,2448.4",
		pos="e,8937.5,2405 8937.5,2491.7 8937.5,2491.7 8937.5,2415 8937.5,2415"];
	Layer3_Device1_Stage12_Attention -> Layer3_Device1_Stage12_Accumulate	[pos="e,7644.5,2185.5 7644.5,2325.1 7644.5,2325.1 7644.5,2195.5 7644.5,2195.5"];
	Layer3_Device1_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7964,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage12_Accumulate -> Layer3_Device1_Stage13_Accumulate	[pos="e,7732.3,1965 7674.6,2158 7692.3,2158 7703.4,2158 7703.4,2158 7703.4,2158 7703.4,1965 7703.4,1965 7703.4,1965 7722.3,1965 7722.3,\
1965"];
	Layer3_Device1_Stage13_RecvKV -> Layer3_Device1_Stage13_Attention	[pos="e,9802.1,2185.8 9802.1,2351.8 9802.1,2351.8 9802.1,2195.8 9802.1,2195.8"];
	Layer3_Device1_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="8652,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage13_RecvKV -> Layer3_Device1_Stage14_RecvKV	[label="Ring transfer",
		lp="8836,2255.4",
		pos="e,8853.5,2212 8853.5,2298.7 8853.5,2298.7 8853.5,2222 8853.5,2222"];
	Layer3_Device1_Stage13_Attention -> Layer3_Device1_Stage13_Accumulate	[pos="e,7964,1992.6 9861,2132.1 9861,2093.8 9861,2028 9861,2028 9861,2028 7964,2028 7964,2028 7964,2028 7964,2002.6 7964,2002.6"];
	Layer3_Device1_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="7964,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage13_Accumulate -> Layer3_Device1_Stage14_Accumulate	[pos="e,7964,1839.9 7964,1939.3 7964,1939.3 7964,1849.9 7964,1849.9"];
	Layer3_Device1_Stage14_RecvKV -> Layer3_Device1_Stage14_Attention	[pos="e,7443,1992.6 7778.6,2119 7588.5,2119 7443,2119 7443,2119 7443,2119 7443,2002.6 7443,2002.6"];
	Layer3_Device1_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="9173,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device1_Stage14_RecvKV -> Layer3_Device1_Stage15_RecvKV	[label="Ring transfer",
		lp="8960,2062.4",
		pos="e,8912.5,2019 8912.5,2105.7 8912.5,2105.7 8912.5,2029 8912.5,2029"];
	Layer3_Device1_Stage14_Attention -> Layer3_Device1_Stage14_Accumulate	[pos="e,7732.4,1804 7443,1939.2 7443,1893.2 7443,1804 7443,1804 7443,1804 7722.4,1804 7722.4,1804"];
	Layer3_Device1_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="9325,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device1_Stage14_Accumulate -> Layer3_Device1_Stage15_Accumulate	[pos="e,9093.4,1687 7964,1786.6 7964,1749.5 7964,1687 7964,1687 7964,1687 9083.4,1687 9083.4,1687"];
	Layer3_Device1_Stage15_RecvKV -> Layer3_Device1_Stage15_Attention	[pos="e,9325,1840 9325,1912.6 9325,1912.6 9325,1850 9325,1850"];
	Layer3_Device1_Stage15_Attention -> Layer3_Device1_Stage15_Accumulate	[pos="e,9325,1714.1 9325,1786.7 9325,1786.7 9325,1724.1 9325,1724.1"];
	Layer3_Device1_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9579,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device1_Stage15_Accumulate -> Layer3_Device1_ConcatHeads	[pos="e,9455,1588.1 9455,1660.7 9455,1660.7 9455,1598.1 9455,1598.1"];
	Layer3_Device1_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="9791,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device1_ConcatHeads -> Layer3_Device1_OutputProj	[pos="e,9691.2,1462.1 9691.2,1534.7 9691.2,1534.7 9691.2,1472.1 9691.2,1472.1"];
	Layer3_Device1_OutputProj -> Layer3_Device1_Residual1	[pos="e,9958.2,1336.1 9958.2,1408.7 9958.2,1408.7 9958.2,1346.1 9958.2,1346.1"];
	Layer3_Device1_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="10299,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device1_Residual1 -> Layer3_Device1_LayerNorm2	[pos="e,10299,1210.1 10299,1282.7 10299,1282.7 10299,1220.1 10299,1220.1"];
	Layer3_Device1_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="10746,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device1_Residual1 -> Layer3_Device1_Residual2	[pos="e,10711,528.28 10686,1309 10701,1309 10711,1309 10711,1309 10711,1309 10711,538.28 10711,538.28"];
	Layer3_Device1_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="10153,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device1_LayerNorm2 -> Layer3_Device1_GateProj	[pos="e,10232,1084.1 10232,1156.7 10232,1156.7 10232,1094.1 10232,1094.1"];
	Layer3_Device1_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="10466,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device1_LayerNorm2 -> Layer3_Device1_UpProj	[pos="e,10445,995.2 10445,1156.6 10445,1156.6 10445,1005.2 10445,1005.2"];
	Layer3_Device1_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="10153,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device1_GateProj -> Layer3_Device1_Activation	[pos="e,10084,905.95 10084,1030.8 10084,1030.8 10084,915.95 10084,915.95"];
	Layer3_Device1_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="10248,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device1_UpProj -> Layer3_Device1_ElemMul	[pos="e,10518,780.2 10518,941.61 10518,941.61 10518,790.2 10518,790.2"];
	Layer3_Device1_Activation -> Layer3_Device1_ElemMul	[pos="e,10153,780.09 10153,852.69 10153,852.69 10153,790.09 10153,790.09"];
	Layer3_Device1_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="10344,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device1_ElemMul -> Layer3_Device1_DownProj	[pos="e,10344,654.09 10344,726.69 10344,726.69 10344,664.09 10344,664.09"];
	Layer3_Device1_DownProj -> Layer3_Device1_Residual2	[pos="e,10461,528.09 10461,600.69 10461,600.69 10461,538.09 10461,538.09"];
	Layer3_Device1_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 1 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="13891,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device1_Residual2 -> Layer3_Device1_Output	[pos="e,13765,398.54 11133,501 11947,501 13765,501 13765,501 13765,501 13765,408.54 13765,408.54"];
	Layer3_Device1_Output -> Sequence_Aggregate	[pos="e,40199,174 13891,326.81 13891,271.31 13891,174 13891,174 13891,174 40189,174 40189,174"];
	Layer3_Device2_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14467,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device2_Input -> Layer3_Device2_LayerNorm1	[pos="e,14680,5153 14835,5276.2 14835,5241.1 14835,5153 14835,5153 14835,5153 14690,5153 14690,5153"];
	Layer3_Device2_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="15181,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device2_Input -> Layer3_Device2_Residual1	[pos="e,15248,1336.1 15248,5256.6 15248,5256.6 15248,1346.1 15248,1346.1"];
	Layer3_Device2_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14123,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device2_LayerNorm1 -> Layer3_Device2_QKVProj	[pos="e,14467,5054.1 14467,5126.7 14467,5126.7 14467,5064.1 14467,5064.1"];
	Layer3_Device2_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12877,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage0_RecvKV	[label="Local K,V",
		lp="13378,4957.4",
		pos="e,13635,4914.2 13635,5000.7 13635,5000.7 13635,4924.2 13635,4924.2"];
	Layer3_Device2_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13667,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage0_Attention	[label=Q_local,
		lp="13939,4860.9",
		pos="e,13848,4694.8 13848,5000.7 13848,5000.7 13848,4704.8 13848,4704.8"];
	Layer3_Device2_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11337,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage1_Attention	[label=Q_local,
		lp="11401,4764.4",
		pos="e,11513,4501.7 13473,5008 12700,5008 11513,5008 11513,5008 11513,5008 11513,4511.7 11513,4511.7"];
	Layer3_Device2_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11948,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage2_Attention	[label=Q_local,
		lp="14079,4667.9",
		pos="e,12118,4308.5 13943,5000.7 13943,4902.4 13943,4567 13943,4567 13943,4567 12118,4567 12118,4567 12118,4567 12118,4318.5 12118,4318.5"];
	Layer3_Device2_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13963,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage3_Attention	[label=Q_local,
		lp="14219,4571.4",
		pos="e,14136,4115.7 14136,5000.8 14136,5000.8 14136,4125.7 14136,4125.7"];
	Layer3_Device2_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11169,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage4_Attention	[label=Q_local,
		lp="11021,4474.9",
		pos="e,11065,3922.7 13473,5023 12575,5023 11065,5023 11065,5023 11065,5023 11065,3932.7 11065,3932.7"];
	Layer3_Device2_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14081,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage5_Attention	[label=Q_local,
		lp="14323,4378.4",
		pos="e,14254,3729.7 14254,5000.6 14254,5000.6 14254,3739.7 14254,3739.7"];
	Layer3_Device2_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14048,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage6_Attention	[label=Q_local,
		lp="14455,4281.9",
		pos="e,14280,3509 14353,5000.7 14353,4798.1 14353,3509 14353,3509 14353,3509 14290,3509 14290,3509"];
	Layer3_Device2_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11109,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage7_Attention	[label=Q_local,
		lp="10875,4185.4",
		pos="e,10908,3343.6 13473,5038 12534,5038 10908,5038 10908,5038 10908,5038 10908,3353.6 10908,3353.6"];
	Layer3_Device2_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="13901,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage8_Attention	[label=Q_local,
		lp="14573,4088.9",
		pos="e,14133,3123 14393,5000.8 14393,4770.7 14393,3123 14393,3123 14393,3123 14143,3123 14143,3123"];
	Layer3_Device2_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="10962,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage9_Attention	[label=Q_local,
		lp="10757,3992.4",
		pos="e,10804,2957.6 13473,5046 12507,5046 10804,5046 10804,5046 10804,5046 10804,2967.6 10804,2967.6"];
	Layer3_Device2_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="11355,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage10_Attention	[label=Q_local,
		lp="10617,3895.9",
		pos="e,11475,2764.5 14434,5000.9 14434,4754.1 14434,2868 14434,2868 14434,2868 11475,2868 11475,2868 11475,2868 11475,2774.5 11475,2774.5"];
	Layer3_Device2_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="12426,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage11_Attention	[label=Q_local,
		lp="14690,3799.4",
		pos="e,12503,2571.4 14474,5000.6 14474,4738.4 14474,2641 14474,2641 14474,2641 12503,2641 12503,2641 12503,2641 12503,2581.4 12503,2581.4"];
	Layer3_Device2_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="12728,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage12_Attention	[label=Q_local,
		lp="14831,3702.9",
		pos="e,12686,2378.6 14514,5000.6 14514,4738.1 14514,2638 14514,2638 14514,2638 12686,2638 12686,2638 12686,2638 12686,2388.6 12686,2388.6"];
	Layer3_Device2_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14743,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage13_Attention	[label=Q_local,
		lp="14971,3606.4",
		pos="e,14916,2185.8 14773,5014 14858,5014 14916,5014 14916,5014 14916,5014 14916,2195.8 14916,2195.8"];
	Layer3_Device2_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="12325,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage14_Attention	[label=Q_local,
		lp="10471,3509.9",
		pos="e,12093,1965 14664,5000.7 14664,4725.9 14664,2419 14664,2419 14664,2419 11925,2419 11925,2419 11925,2419 11925,1965 11925,1965 11925,\
1965 12083,1965 12083,1965"];
	Layer3_Device2_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="14171,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_QKVProj -> Layer3_Device2_Stage15_Attention	[label=Q_local,
		lp="15089,3413.4",
		pos="e,14403,1822 14773,5027 14917,5027 15027,5027 15027,5027 15027,5027 15027,1822 15027,1822 15027,1822 14413,1822 14413,1822"];
	Layer3_Device2_Stage0_RecvKV -> Layer3_Device2_Stage0_Attention	[pos="e,13616,4694.8 13616,4860.8 13616,4860.8 13616,4704.8 13616,4704.8"];
	Layer3_Device2_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12458,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage0_RecvKV -> Layer3_Device2_Stage1_RecvKV	[label="Ring transfer",
		lp="12715,4764.4",
		pos="e,12668,4721 12668,4807.7 12668,4807.7 12668,4731 12668,4731"];
	Layer3_Device2_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="11858,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage0_Attention -> Layer3_Device2_Stage0_Accumulate	[pos="e,11858,4501.6 13667,4641.2 13667,4612.2 13667,4570 13667,4570 13667,4570 11858,4570 11858,4570 11858,4570 11858,4511.6 11858,4511.6"];
	Layer3_Device2_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11427,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage0_Accumulate -> Layer3_Device2_Stage1_Accumulate	[pos="e,11642,4308.5 11642,4448.1 11642,4448.1 11642,4318.5 11642,4318.5"];
	Layer3_Device2_Stage1_RecvKV -> Layer3_Device2_Stage1_Attention	[pos="e,11553,4501.8 11553,4667.8 11553,4667.8 11553,4511.8 11553,4511.8"];
	Layer3_Device2_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="13067,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage1_RecvKV -> Layer3_Device2_Stage2_RecvKV	[label="Ring transfer",
		lp="12775,4571.4",
		pos="e,12762,4528 12762,4614.7 12762,4614.7 12762,4538 12762,4538"];
	Layer3_Device2_Stage1_Attention -> Layer3_Device2_Stage1_Accumulate	[pos="e,11382,4308.5 11382,4448.1 11382,4448.1 11382,4318.5 11382,4318.5"];
	Layer3_Device2_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11545,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage1_Accumulate -> Layer3_Device2_Stage2_Accumulate	[pos="e,11486,4115.5 11486,4255.1 11486,4255.1 11486,4125.5 11486,4125.5"];
	Layer3_Device2_Stage2_RecvKV -> Layer3_Device2_Stage2_Attention	[pos="e,12163,4308.8 12163,4474.8 12163,4474.8 12163,4318.8 12163,4318.8"];
	Layer3_Device2_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13157,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage2_RecvKV -> Layer3_Device2_Stage3_RecvKV	[label="Ring transfer",
		lp="13160,4378.4",
		pos="e,13112,4335 13112,4421.7 13112,4421.7 13112,4345 13112,4345"];
	Layer3_Device2_Stage2_Attention -> Layer3_Device2_Stage2_Accumulate	[pos="e,11746,4115.5 11746,4255.1 11746,4255.1 11746,4125.5 11746,4125.5"];
	Layer3_Device2_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11690,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage2_Accumulate -> Layer3_Device2_Stage3_Accumulate	[pos="e,11618,3922.5 11618,4062.1 11618,4062.1 11618,3932.5 11618,3932.5"];
	Layer3_Device2_Stage3_RecvKV -> Layer3_Device2_Stage3_Attention	[pos="e,13904,4115.8 13904,4281.8 13904,4281.8 13904,4125.8 13904,4125.8"];
	Layer3_Device2_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12754,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage3_RecvKV -> Layer3_Device2_Stage4_RecvKV	[label="Ring transfer",
		lp="12938,4185.4",
		pos="e,12956,4142 12956,4228.7 12956,4228.7 12956,4152 12956,4152"];
	Layer3_Device2_Stage3_Attention -> Layer3_Device2_Stage3_Accumulate	[pos="e,11878,3922.5 13775,4062.1 13775,4042.7 13775,4020 13775,4020 13775,4020 11878,4020 11878,4020 11878,4020 11878,3932.5 11878,3932.5"];
	Layer3_Device2_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11663,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage3_Accumulate -> Layer3_Device2_Stage4_Accumulate	[pos="e,11676,3729.5 11676,3869.1 11676,3869.1 11676,3739.5 11676,3739.5"];
	Layer3_Device2_Stage4_RecvKV -> Layer3_Device2_Stage4_Attention	[pos="e,11357,3922.6 11881,4049 11598,4049 11357,4049 11357,4049 11357,4049 11357,3932.6 11357,3932.6"];
	Layer3_Device2_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12899,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage4_RecvKV -> Layer3_Device2_Stage5_RecvKV	[label="Ring transfer",
		lp="12921,3992.4",
		pos="e,12826,3949 12826,4035.7 12826,4035.7 12826,3959 12826,3959"];
	Layer3_Device2_Stage4_Attention -> Layer3_Device2_Stage4_Accumulate	[pos="e,11445,3729.6 11401,3895 11427,3895 11445,3895 11445,3895 11445,3895 11445,3739.6 11445,3739.6"];
	Layer3_Device2_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13527,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage4_Accumulate -> Layer3_Device2_Stage5_Accumulate	[pos="e,13527,3536.6 11663,3676.3 11663,3636.9 11663,3568 11663,3568 11663,3568 13527,3568 13527,3568 13527,3568 13527,3546.6 13527,3546.6"];
	Layer3_Device2_Stage5_RecvKV -> Layer3_Device2_Stage5_Attention	[pos="e,13849,3711 13800,3895.8 13800,3890.2 13800,3711 13800,3711 13800,3711 13839,3711 13839,3711"];
	Layer3_Device2_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12872,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage5_RecvKV -> Layer3_Device2_Stage6_RecvKV	[label="Ring transfer",
		lp="12933,3799.4",
		pos="e,12886,3756 12886,3842.7 12886,3842.7 12886,3766 12886,3766"];
	Layer3_Device2_Stage5_Attention -> Layer3_Device2_Stage5_Accumulate	[pos="e,13759,3509 13849,3694 13825,3694 13808,3694 13808,3694 13808,3694 13808,3509 13808,3509 13808,3509 13769,3509 13769,3509"];
	Layer3_Device2_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13527,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage5_Accumulate -> Layer3_Device2_Stage6_Accumulate	[pos="e,13527,3343.5 13527,3483.1 13527,3483.1 13527,3353.5 13527,3353.5"];
	Layer3_Device2_Stage6_RecvKV -> Layer3_Device2_Stage6_Attention	[pos="e,13834,3536.6 13462,3663 13661,3663 13834,3663 13834,3663 13834,3663 13834,3546.6 13834,3546.6"];
	Layer3_Device2_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12318,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage6_RecvKV -> Layer3_Device2_Stage7_RecvKV	[label="Ring transfer",
		lp="12614,3606.4",
		pos="e,12595,3563 12595,3649.7 12595,3649.7 12595,3573 12595,3573"];
	Layer3_Device2_Stage6_Attention -> Layer3_Device2_Stage6_Accumulate	[pos="e,13759,3308 13974,3483.1 13974,3428.2 13974,3308 13974,3308 13974,3308 13769,3308 13769,3308"];
	Layer3_Device2_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13380,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage6_Accumulate -> Layer3_Device2_Stage7_Accumulate	[pos="e,13454,3150.5 13454,3290.1 13454,3290.1 13454,3160.5 13454,3160.5"];
	Layer3_Device2_Stage7_RecvKV -> Layer3_Device2_Stage7_Attention	[pos="e,11139,3343.6 11445,3470 11270,3470 11139,3470 11139,3470 11139,3470 11139,3353.6 11139,3353.6"];
	Layer3_Device2_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12318,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage7_RecvKV -> Layer3_Device2_Stage8_RecvKV	[label="Ring transfer",
		lp="12366,3413.4",
		pos="e,12318,3370 12318,3456.7 12318,3456.7 12318,3380 12318,3380"];
	Layer3_Device2_Stage7_Attention -> Layer3_Device2_Stage7_Accumulate	[pos="e,13193,3150.6 11296,3290.4 11296,3265.8 11296,3233 11296,3233 11296,3233 13193,3233 13193,3233 13193,3233 13193,3160.6 13193,3160.6"];
	Layer3_Device2_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="13380,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage7_Accumulate -> Layer3_Device2_Stage8_Accumulate	[pos="e,13380,2957.5 13380,3097.1 13380,3097.1 13380,2967.5 13380,2967.5"];
	Layer3_Device2_Stage8_RecvKV -> Layer3_Device2_Stage8_Attention	[pos="e,13714,3150.6 12908,3277 13289,3277 13714,3277 13714,3277 13714,3277 13714,3160.6 13714,3160.6"];
	Layer3_Device2_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="12171,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage8_RecvKV -> Layer3_Device2_Stage9_RecvKV	[label="Ring transfer",
		lp="12263,3220.4",
		pos="e,12244,3177 12244,3263.7 12244,3263.7 12244,3187 12244,3187"];
	Layer3_Device2_Stage8_Attention -> Layer3_Device2_Stage8_Accumulate	[pos="e,13612,2930 13837,3097.4 13837,3044.2 13837,2930 13837,2930 13837,2930 13622,2930 13622,2930"];
	Layer3_Device2_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11876,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage8_Accumulate -> Layer3_Device2_Stage9_Accumulate	[pos="e,11953,2764.8 13380,2904.3 13380,2870.3 13380,2816 13380,2816 13380,2816 11953,2816 11953,2816 11953,2816 11953,2774.8 11953,2774.8"];
	Layer3_Device2_Stage9_RecvKV -> Layer3_Device2_Stage9_Attention	[pos="e,11036,2957.6 11297,3084 11146,3084 11036,3084 11036,3084 11036,3084 11036,2967.6 11036,2967.6"];
	Layer3_Device2_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="12171,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage9_RecvKV -> Layer3_Device2_Stage10_RecvKV	[label="Ring transfer",
		lp="12219,3027.4",
		pos="e,12171,2984 12171,3070.7 12171,3070.7 12171,2994 12171,2994"];
	Layer3_Device2_Stage9_Attention -> Layer3_Device2_Stage9_Accumulate	[pos="e,11799,2764.5 11158,2904.4 11158,2869.9 11158,2814 11158,2814 11158,2814 11799,2814 11799,2814 11799,2814 11799,2774.5 11799,2774.5"];
	Layer3_Device2_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="11876,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage9_Accumulate -> Layer3_Device2_Stage10_Accumulate	[pos="e,11876,2571.5 11876,2711.1 11876,2711.1 11876,2581.5 11876,2581.5"];
	Layer3_Device2_Stage10_RecvKV -> Layer3_Device2_Stage10_Attention	[pos="e,11363,2764.6 11363,2877.7 11363,2877.7 11363,2774.6 11363,2774.6"];
	Layer3_Device2_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13085,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage10_RecvKV -> Layer3_Device2_Stage11_RecvKV	[label="Ring transfer",
		lp="13053,2834.4",
		pos="e,12628,2791 12628,2877.7 12628,2877.7 12628,2801 12628,2801"];
	Layer3_Device2_Stage10_Attention -> Layer3_Device2_Stage10_Accumulate	[pos="e,11644,2544 11360,2711.4 11360,2658.2 11360,2544 11360,2544 11360,2544 11634,2544 11634,2544"];
	Layer3_Device2_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12207,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage10_Accumulate -> Layer3_Device2_Stage11_Accumulate	[pos="e,12042,2378.5 12042,2518.1 12042,2518.1 12042,2388.5 12042,2388.5"];
	Layer3_Device2_Stage11_RecvKV -> Layer3_Device2_Stage11_Attention	[pos="e,12349,2571.6 12349,2684.7 12349,2684.7 12349,2581.6 12349,2581.6"];
	Layer3_Device2_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13635,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage11_RecvKV -> Layer3_Device2_Stage12_RecvKV	[label="Ring transfer",
		lp="13408,2641.4",
		pos="e,13360,2598 13360,2684.7 13360,2684.7 13360,2608 13360,2608"];
	Layer3_Device2_Stage11_Attention -> Layer3_Device2_Stage11_Accumulate	[pos="e,12316,2378.5 12316,2518.1 12316,2518.1 12316,2388.5 12316,2388.5"];
	Layer3_Device2_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12325,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage11_Accumulate -> Layer3_Device2_Stage12_Accumulate	[pos="e,12266,2185.5 12266,2325.1 12266,2325.1 12266,2195.5 12266,2195.5"];
	Layer3_Device2_Stage12_RecvKV -> Layer3_Device2_Stage12_Attention	[pos="e,12837,2378.6 12837,2491.7 12837,2491.7 12837,2388.6 12837,2388.6"];
	Layer3_Device2_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13937,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage12_RecvKV -> Layer3_Device2_Stage13_RecvKV	[label="Ring transfer",
		lp="13856,2448.4",
		pos="e,13786,2405 13786,2491.7 13786,2491.7 13786,2415 13786,2415"];
	Layer3_Device2_Stage12_Attention -> Layer3_Device2_Stage12_Accumulate	[pos="e,12526,2185.5 12526,2325.1 12526,2325.1 12526,2195.5 12526,2195.5"];
	Layer3_Device2_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12846,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage12_Accumulate -> Layer3_Device2_Stage13_Accumulate	[pos="e,12614,1965 12557,2158 12574,2158 12585,2158 12585,2158 12585,2158 12585,1965 12585,1965 12585,1965 12604,1965 12604,1965"];
	Layer3_Device2_Stage13_RecvKV -> Layer3_Device2_Stage13_Attention	[pos="e,14684,2185.8 14684,2351.8 14684,2351.8 14684,2195.8 14684,2195.8"];
	Layer3_Device2_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="13534,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage13_RecvKV -> Layer3_Device2_Stage14_RecvKV	[label="Ring transfer",
		lp="13851,2255.4",
		pos="e,13736,2212 13736,2298.7 13736,2298.7 13736,2222 13736,2222"];
	Layer3_Device2_Stage13_Attention -> Layer3_Device2_Stage13_Accumulate	[pos="e,12846,1992.4 14743,2132.3 14743,2095.1 14743,2032 14743,2032 14743,2032 12846,2032 12846,2032 12846,2032 12846,2002.4 12846,2002.4"];
	Layer3_Device2_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="12846,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage13_Accumulate -> Layer3_Device2_Stage14_Accumulate	[pos="e,12846,1839.9 12846,1939.3 12846,1939.3 12846,1849.9 12846,1849.9"];
	Layer3_Device2_Stage14_RecvKV -> Layer3_Device2_Stage14_Attention	[pos="e,12325,1992.6 12661,2119 12471,2119 12325,2119 12325,2119 12325,2119 12325,2002.6 12325,2002.6"];
	Layer3_Device2_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="14055,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device2_Stage14_RecvKV -> Layer3_Device2_Stage15_RecvKV	[label="Ring transfer",
		lp="13842,2062.4",
		pos="e,13794,2019 13794,2105.7 13794,2105.7 13794,2029 13794,2029"];
	Layer3_Device2_Stage14_Attention -> Layer3_Device2_Stage14_Accumulate	[pos="e,12614,1804 12517,1939.2 12517,1893.2 12517,1804 12517,1804 12517,1804 12604,1804 12604,1804"];
	Layer3_Device2_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="14171,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device2_Stage14_Accumulate -> Layer3_Device2_Stage15_Accumulate	[pos="e,13939,1687 12846,1786.6 12846,1749.5 12846,1687 12846,1687 12846,1687 13929,1687 13929,1687"];
	Layer3_Device2_Stage15_RecvKV -> Layer3_Device2_Stage15_Attention	[pos="e,14171,1840 14171,1912.6 14171,1912.6 14171,1850 14171,1850"];
	Layer3_Device2_Stage15_Attention -> Layer3_Device2_Stage15_Accumulate	[pos="e,14171,1714.1 14171,1786.7 14171,1786.7 14171,1724.1 14171,1724.1"];
	Layer3_Device2_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14429,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device2_Stage15_Accumulate -> Layer3_Device2_ConcatHeads	[pos="e,14303,1588.1 14303,1660.7 14303,1660.7 14303,1598.1 14303,1598.1"];
	Layer3_Device2_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="14867,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device2_ConcatHeads -> Layer3_Device2_OutputProj	[pos="e,14815,1462 14655,1561 14739,1561 14815,1561 14815,1561 14815,1561 14815,1472 14815,1472"];
	Layer3_Device2_OutputProj -> Layer3_Device2_Residual1	[pos="e,14937,1336.1 14937,1408.7 14937,1408.7 14937,1346.1 14937,1346.1"];
	Layer3_Device2_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="15181,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device2_Residual1 -> Layer3_Device2_LayerNorm2	[pos="e,15181,1210.1 15181,1282.7 15181,1282.7 15181,1220.1 15181,1220.1"];
	Layer3_Device2_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="15664,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device2_Residual1 -> Layer3_Device2_Residual2	[pos="e,15690,528.28 15568,1309 15639,1309 15690,1309 15690,1309 15690,1309 15690,538.28 15690,538.28"];
	Layer3_Device2_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14947,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device2_LayerNorm2 -> Layer3_Device2_GateProj	[pos="e,15070,1084.1 15070,1156.7 15070,1156.7 15070,1094.1 15070,1094.1"];
	Layer3_Device2_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="15260,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device2_LayerNorm2 -> Layer3_Device2_UpProj	[pos="e,15283,995.2 15283,1156.6 15283,1156.6 15283,1005.2 15283,1005.2"];
	Layer3_Device2_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="14947,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device2_GateProj -> Layer3_Device2_Activation	[pos="e,14878,905.95 14878,1030.8 14878,1030.8 14878,915.95 14878,915.95"];
	Layer3_Device2_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="15166,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device2_UpProj -> Layer3_Device2_ElemMul	[pos="e,15328,780.2 15328,941.61 15328,941.61 15328,790.2 15328,790.2"];
	Layer3_Device2_Activation -> Layer3_Device2_ElemMul	[pos="e,14964,780.09 14964,852.69 14964,852.69 14964,790.09 14964,790.09"];
	Layer3_Device2_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="15309,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device2_ElemMul -> Layer3_Device2_DownProj	[pos="e,15309,654.09 15309,726.69 15309,726.69 15309,664.09 15309,664.09"];
	Layer3_Device2_DownProj -> Layer3_Device2_Residual2	[pos="e,15403,528.09 15403,600.69 15403,600.69 15403,538.09 15403,538.09"];
	Layer3_Device2_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 2 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="18755,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device2_Residual2 -> Layer3_Device2_Output	[pos="e,18703,401.39 16051,501 16870,501 18703,501 18703,501 18703,501 18703,411.39 18703,411.39"];
	Layer3_Device2_Output -> Sequence_Aggregate	[pos="e,40222,187 18755,326.8 18755,274.72 18755,187 18755,187 18755,187 40212,187 40212,187"];
	Layer3_Device3_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19342,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device3_Input -> Layer3_Device3_LayerNorm1	[pos="e,19449,5180.2 19449,5261.7 19449,5261.7 19449,5190.2 19449,5190.2"];
	Layer3_Device3_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="20056,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device3_Input -> Layer3_Device3_Residual1	[pos="e,19921,1336.1 19921,5275.6 19921,5275.6 19921,1346.1 19921,1346.1"];
	Layer3_Device3_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18998,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device3_LayerNorm1 -> Layer3_Device3_QKVProj	[pos="e,19342,5054.1 19342,5126.7 19342,5126.7 19342,5064.1 19342,5064.1"];
	Layer3_Device3_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17752,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage0_RecvKV	[label="Local K,V",
		lp="18252,4957.4",
		pos="e,18510,4914.2 18510,5000.7 18510,5000.7 18510,4924.2 18510,4924.2"];
	Layer3_Device3_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18542,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage0_Attention	[label=Q_local,
		lp="18814,4860.9",
		pos="e,18723,4694.8 18723,5000.7 18723,5000.7 18723,4704.8 18723,4704.8"];
	Layer3_Device3_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16212,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage1_Attention	[label=Q_local,
		lp="16276,4764.4",
		pos="e,16204,4501.7 18348,5007 17523,5007 16204,5007 16204,5007 16204,5007 16204,4511.7 16204,4511.7"];
	Layer3_Device3_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16823,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage2_Attention	[label=Q_local,
		lp="18954,4667.9",
		pos="e,16993,4308.4 18818,5000.6 18818,4903 18818,4572 18818,4572 18818,4572 16993,4572 16993,4572 16993,4572 16993,4318.4 16993,4318.4"];
	Layer3_Device3_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18838,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage3_Attention	[label=Q_local,
		lp="19094,4571.4",
		pos="e,19011,4115.7 19011,5000.8 19011,5000.8 19011,4125.7 19011,4125.7"];
	Layer3_Device3_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16048,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage4_Attention	[label=Q_local,
		lp="15896,4474.9",
		pos="e,15959,3922.6 18348,5020 17456,5020 15959,5020 15959,5020 15959,5020 15959,3932.6 15959,3932.6"];
	Layer3_Device3_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18956,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage5_Attention	[label=Q_local,
		lp="19200,4378.4",
		pos="e,19128,3729.7 19128,5000.6 19128,5000.6 19128,3739.7 19128,3739.7"];
	Layer3_Device3_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="19067,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage6_Attention	[label=Q_local,
		lp="19330,4281.9",
		pos="e,19243,3536.6 19243,5000.7 19243,5000.7 19243,3546.6 19243,3546.6"];
	Layer3_Device3_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="16128,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage7_Attention	[label=Q_local,
		lp="15753,4185.4",
		pos="e,15896,3325 18348,5034 17405,5034 15769,5034 15769,5034 15769,5034 15769,3325 15769,3325 15769,3325 15886,3325 15886,3325"];
	Layer3_Device3_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="18844,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage8_Attention	[label=Q_local,
		lp="19448,4088.9",
		pos="e,19076,3123 19329,5000.8 19329,4770.7 19329,3123 19329,3123 19329,3123 19086,3123 19086,3123"];
	Layer3_Device3_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15905,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage9_Attention	[label=Q_local,
		lp="15635,3992.4",
		pos="e,15721,2957.6 18348,5040 17393,5040 15721,5040 15721,5040 15721,5040 15721,2967.6 15721,2967.6"];
	Layer3_Device3_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="15687,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage10_Attention	[label=Q_local,
		lp="15496,3895.9",
		pos="e,15587,2764.6 18348,5047 17359,5047 15587,5047 15587,5047 15587,5047 15587,2774.6 15587,2774.6"];
	Layer3_Device3_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="17293,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage11_Attention	[label=Q_local,
		lp="19566,3799.4",
		pos="e,17370,2571.7 19360,5000.7 19360,4739.6 19360,2651 19360,2651 19360,2651 17370,2651 17370,2651 17370,2651 17370,2581.7 17370,2581.7"];
	Layer3_Device3_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="17603,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage12_Attention	[label=Q_local,
		lp="19706,3702.9",
		pos="e,17553,2378.4 19391,5000.7 19391,4739.3 19391,2648 19391,2648 19391,2648 17553,2648 17553,2648 17553,2648 17553,2388.4 17553,2388.4"];
	Layer3_Device3_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="19618,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage13_Attention	[label=Q_local,
		lp="19846,3606.4",
		pos="e,19791,2185.8 19648,5012 19733,5012 19791,5012 19791,5012 19791,5012 19791,2195.8 19791,2195.8"];
	Layer3_Device3_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="17200,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage14_Attention	[label=Q_local,
		lp="15353,3509.9",
		pos="e,17277,1992.5 19648,5024 19777,5024 19874,5024 19874,5024 19874,5024 19874,2096 19874,2096 19874,2096 17277,2096 17277,2096 17277,\
2096 17277,2002.5 17277,2002.5"];
	Layer3_Device3_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="19184,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_QKVProj -> Layer3_Device3_Stage15_Attention	[label=Q_local,
		lp="19964,3413.4",
		pos="e,19416,1822 19648,5030 19789,5030 19897,5030 19897,5030 19897,5030 19897,1822 19897,1822 19897,1822 19426,1822 19426,1822"];
	Layer3_Device3_Stage0_RecvKV -> Layer3_Device3_Stage0_Attention	[pos="e,18491,4694.8 18491,4860.8 18491,4860.8 18491,4704.8 18491,4704.8"];
	Layer3_Device3_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17333,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage0_RecvKV -> Layer3_Device3_Stage1_RecvKV	[label="Ring transfer",
		lp="17590,4764.4",
		pos="e,17542,4721 17542,4807.7 17542,4807.7 17542,4731 17542,4731"];
	Layer3_Device3_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="16733,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage0_Attention -> Layer3_Device3_Stage0_Accumulate	[pos="e,16733,4501.5 18542,4641 18542,4613.6 18542,4575 18542,4575 18542,4575 16733,4575 16733,4575 16733,4575 16733,4511.5 16733,4511.5"];
	Layer3_Device3_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16302,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage0_Accumulate -> Layer3_Device3_Stage1_Accumulate	[pos="e,16518,4308.5 16518,4448.1 16518,4448.1 16518,4318.5 16518,4318.5"];
	Layer3_Device3_Stage1_RecvKV -> Layer3_Device3_Stage1_Attention	[pos="e,16428,4501.8 16428,4667.8 16428,4667.8 16428,4511.8 16428,4511.8"];
	Layer3_Device3_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="17942,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage1_RecvKV -> Layer3_Device3_Stage2_RecvKV	[label="Ring transfer",
		lp="17650,4571.4",
		pos="e,17638,4528 17638,4614.7 17638,4614.7 17638,4538 17638,4538"];
	Layer3_Device3_Stage1_Attention -> Layer3_Device3_Stage1_Accumulate	[pos="e,16257,4308.5 16257,4448.1 16257,4448.1 16257,4318.5 16257,4318.5"];
	Layer3_Device3_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16420,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage1_Accumulate -> Layer3_Device3_Stage2_Accumulate	[pos="e,16361,4115.5 16361,4255.1 16361,4255.1 16361,4125.5 16361,4125.5"];
	Layer3_Device3_Stage2_RecvKV -> Layer3_Device3_Stage2_Attention	[pos="e,17038,4308.8 17038,4474.8 17038,4474.8 17038,4318.8 17038,4318.8"];
	Layer3_Device3_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="18032,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage2_RecvKV -> Layer3_Device3_Stage3_RecvKV	[label="Ring transfer",
		lp="18035,4378.4",
		pos="e,17987,4335 17987,4421.7 17987,4421.7 17987,4345 17987,4345"];
	Layer3_Device3_Stage2_Attention -> Layer3_Device3_Stage2_Accumulate	[pos="e,16622,4115.5 16622,4255.1 16622,4255.1 16622,4125.5 16622,4125.5"];
	Layer3_Device3_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16569,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage2_Accumulate -> Layer3_Device3_Stage3_Accumulate	[pos="e,16494,3922.5 16494,4062.1 16494,4062.1 16494,3932.5 16494,3932.5"];
	Layer3_Device3_Stage3_RecvKV -> Layer3_Device3_Stage3_Attention	[pos="e,18779,4115.8 18779,4281.8 18779,4281.8 18779,4125.8 18779,4125.8"];
	Layer3_Device3_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17629,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage3_RecvKV -> Layer3_Device3_Stage4_RecvKV	[label="Ring transfer",
		lp="17878,4185.4",
		pos="e,17830,4142 17830,4228.7 17830,4228.7 17830,4152 17830,4152"];
	Layer3_Device3_Stage3_Attention -> Layer3_Device3_Stage3_Accumulate	[pos="e,16755,3922.6 18652,4062.4 18652,4037.8 18652,4005 18652,4005 18652,4005 16755,4005 16755,4005 16755,4005 16755,3932.6 16755,3932.6"];
	Layer3_Device3_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16538,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage3_Accumulate -> Layer3_Device3_Stage4_Accumulate	[pos="e,16554,3729.5 16554,3869.1 16554,3869.1 16554,3739.5 16554,3739.5"];
	Layer3_Device3_Stage4_RecvKV -> Layer3_Device3_Stage4_Attention	[pos="e,16234,3922.6 16755,4049 16474,4049 16234,4049 16234,4049 16234,4049 16234,3932.6 16234,3932.6"];
	Layer3_Device3_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17778,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage4_RecvKV -> Layer3_Device3_Stage5_RecvKV	[label="Ring transfer",
		lp="17751,3992.4",
		pos="e,17704,3949 17704,4035.7 17704,4035.7 17704,3959 17704,3959"];
	Layer3_Device3_Stage4_Attention -> Layer3_Device3_Stage4_Accumulate	[pos="e,16322,3729.6 16280,3895 16305,3895 16322,3895 16322,3895 16322,3895 16322,3739.6 16322,3739.6"];
	Layer3_Device3_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18546,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage4_Accumulate -> Layer3_Device3_Stage5_Accumulate	[pos="e,18491,3536.7 16593,3676.2 16593,3660.6 16593,3644 16593,3644 16593,3644 18491,3644 18491,3644 18491,3644 18491,3546.7 18491,3546.7"];
	Layer3_Device3_Stage5_RecvKV -> Layer3_Device3_Stage5_Attention	[pos="e,18724,3702 18682,3895.8 18682,3889.9 18682,3702 18682,3702 18682,3702 18714,3702 18714,3702"];
	Layer3_Device3_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17747,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage5_RecvKV -> Layer3_Device3_Stage6_RecvKV	[label="Ring transfer",
		lp="17810,3799.4",
		pos="e,17762,3756 17762,3842.7 17762,3842.7 17762,3766 17762,3766"];
	Layer3_Device3_Stage5_Attention -> Layer3_Device3_Stage5_Accumulate	[pos="e,18751,3536.5 18751,3676.1 18751,3676.1 18751,3546.5 18751,3546.5"];
	Layer3_Device3_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18546,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage5_Accumulate -> Layer3_Device3_Stage6_Accumulate	[pos="e,18546,3343.5 18546,3483.1 18546,3483.1 18546,3353.5 18546,3353.5"];
	Layer3_Device3_Stage6_RecvKV -> Layer3_Device3_Stage6_Attention	[pos="e,19012,3536.6 18337,3663 18667,3663 19012,3663 19012,3663 19012,3663 19012,3546.6 19012,3546.6"];
	Layer3_Device3_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17337,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage6_RecvKV -> Layer3_Device3_Stage7_RecvKV	[label="Ring transfer",
		lp="17660,3606.4",
		pos="e,17542,3563 17542,3649.7 17542,3649.7 17542,3573 17542,3573"];
	Layer3_Device3_Stage6_Attention -> Layer3_Device3_Stage6_Accumulate	[pos="e,18778,3316 18956,3483.4 18956,3430.2 18956,3316 18956,3316 18956,3316 18788,3316 18788,3316"];
	Layer3_Device3_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18323,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage6_Accumulate -> Layer3_Device3_Stage7_Accumulate	[pos="e,18434,3150.5 18434,3290.1 18434,3290.1 18434,3160.5 18434,3160.5"];
	Layer3_Device3_Stage7_RecvKV -> Layer3_Device3_Stage7_Attention	[pos="e,16333,3343.6 16464,3470 16384,3470 16333,3470 16333,3470 16333,3470 16333,3353.6 16333,3353.6"];
	Layer3_Device3_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17337,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage7_RecvKV -> Layer3_Device3_Stage8_RecvKV	[label="Ring transfer",
		lp="17385,3413.4",
		pos="e,17337,3370 17337,3456.7 17337,3456.7 17337,3380 17337,3380"];
	Layer3_Device3_Stage7_Attention -> Layer3_Device3_Stage7_Accumulate	[pos="e,18174,3150.4 16277,3290.1 16277,3253.4 16277,3192 16277,3192 16277,3192 18174,3192 18174,3192 18174,3192 18174,3160.4 18174,3160.4"];
	Layer3_Device3_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="18323,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage7_Accumulate -> Layer3_Device3_Stage8_Accumulate	[pos="e,18323,2957.5 18323,3097.1 18323,3097.1 18323,2967.5 18323,2967.5"];
	Layer3_Device3_Stage8_RecvKV -> Layer3_Device3_Stage8_Attention	[pos="e,18695,3150.6 17927,3277 18294,3277 18695,3277 18695,3277 18695,3277 18695,3160.6 18695,3160.6"];
	Layer3_Device3_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17114,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage8_RecvKV -> Layer3_Device3_Stage9_RecvKV	[label="Ring transfer",
		lp="17290,3220.4",
		pos="e,17226,3177 17226,3263.7 17226,3263.7 17226,3187 17226,3187"];
	Layer3_Device3_Stage8_Attention -> Layer3_Device3_Stage8_Accumulate	[pos="e,18555,2930 18844,3097.4 18844,3044.2 18844,2930 18844,2930 18844,2930 18565,2930 18565,2930"];
	Layer3_Device3_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16208,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage8_Accumulate -> Layer3_Device3_Stage9_Accumulate	[pos="e,16317,2764.4 18214,2904.1 18214,2886.3 18214,2866 18214,2866 18214,2866 16317,2866 16317,2866 16317,2866 16317,2774.4 16317,2774.4"];
	Layer3_Device3_Stage9_RecvKV -> Layer3_Device3_Stage9_Attention	[pos="e,16016,2957.6 16241,3084 16109,3084 16016,3084 16016,3084 16016,3084 16016,2967.6 16016,2967.6"];
	Layer3_Device3_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17114,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage9_RecvKV -> Layer3_Device3_Stage10_RecvKV	[label="Ring transfer",
		lp="17162,3027.4",
		pos="e,17114,2984 17114,3070.7 17114,3070.7 17114,2994 17114,2994"];
	Layer3_Device3_Stage9_Attention -> Layer3_Device3_Stage9_Accumulate	[pos="e,16056,2764.5 16056,2904.1 16056,2904.1 16056,2774.5 16056,2774.5"];
	Layer3_Device3_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="16208,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage9_Accumulate -> Layer3_Device3_Stage10_Accumulate	[pos="e,16208,2571.5 16208,2711.1 16208,2711.1 16208,2581.5 16208,2581.5"];
	Layer3_Device3_Stage10_RecvKV -> Layer3_Device3_Stage10_Attention	[pos="e,15796,2764.6 16240,2891 15996,2891 15796,2891 15796,2891 15796,2891 15796,2774.6 15796,2774.6"];
	Layer3_Device3_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="17417,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage10_RecvKV -> Layer3_Device3_Stage11_RecvKV	[label="Ring transfer",
		lp="17262,2834.4",
		pos="e,17266,2791 17266,2877.7 17266,2877.7 17266,2801 17266,2801"];
	Layer3_Device3_Stage10_Attention -> Layer3_Device3_Stage10_Accumulate	[pos="e,15976,2544 15804,2711.4 15804,2658.2 15804,2544 15804,2544 15804,2544 15966,2544 15966,2544"];
	Layer3_Device3_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17082,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage10_Accumulate -> Layer3_Device3_Stage11_Accumulate	[pos="e,16956,2378.6 16440,2544 16659,2544 16956,2544 16956,2544 16956,2544 16956,2388.6 16956,2388.6"];
	Layer3_Device3_Stage11_RecvKV -> Layer3_Device3_Stage11_Attention	[pos="e,17216,2571.6 17216,2684.7 17216,2684.7 17216,2581.6 17216,2581.6"];
	Layer3_Device3_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18502,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage11_RecvKV -> Layer3_Device3_Stage12_RecvKV	[label="Ring transfer",
		lp="17992,2641.4",
		pos="e,17960,2598 17960,2684.7 17960,2684.7 17960,2608 17960,2608"];
	Layer3_Device3_Stage11_Attention -> Layer3_Device3_Stage11_Accumulate	[pos="e,17188,2378.5 17188,2518.1 17188,2518.1 17188,2388.5 17188,2388.5"];
	Layer3_Device3_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17200,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage11_Accumulate -> Layer3_Device3_Stage12_Accumulate	[pos="e,17141,2185.5 17141,2325.1 17141,2325.1 17141,2195.5 17141,2195.5"];
	Layer3_Device3_Stage12_RecvKV -> Layer3_Device3_Stage12_Attention	[pos="e,17708,2378.6 17708,2491.7 17708,2491.7 17708,2388.6 17708,2388.6"];
	Layer3_Device3_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18812,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage12_RecvKV -> Layer3_Device3_Stage13_RecvKV	[label="Ring transfer",
		lp="18685,2448.4",
		pos="e,18657,2405 18657,2491.7 18657,2491.7 18657,2415 18657,2415"];
	Layer3_Device3_Stage12_Attention -> Layer3_Device3_Stage12_Accumulate	[pos="e,17402,2185.5 17402,2325.1 17402,2325.1 17402,2195.5 17402,2195.5"];
	Layer3_Device3_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17721,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage12_Accumulate -> Layer3_Device3_Stage13_Accumulate	[pos="e,17489,1965 17432,2158 17449,2158 17460,2158 17460,2158 17460,2158 17460,1965 17460,1965 17460,1965 17479,1965 17479,1965"];
	Layer3_Device3_Stage13_RecvKV -> Layer3_Device3_Stage13_Attention	[pos="e,19559,2185.8 19559,2351.8 19559,2351.8 19559,2195.8 19559,2195.8"];
	Layer3_Device3_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18409,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage13_RecvKV -> Layer3_Device3_Stage14_RecvKV	[label="Ring transfer",
		lp="18593,2255.4",
		pos="e,18610,2212 18610,2298.7 18610,2298.7 18610,2222 18610,2222"];
	Layer3_Device3_Stage13_Attention -> Layer3_Device3_Stage13_Accumulate	[pos="e,17721,1992.8 19618,2132.2 19618,2096.3 19618,2037 19618,2037 19618,2037 17721,2037 17721,2037 17721,2037 17721,2002.8 17721,2002.8"];
	Layer3_Device3_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="17721,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage13_Accumulate -> Layer3_Device3_Stage14_Accumulate	[pos="e,17721,1839.9 17721,1939.3 17721,1939.3 17721,1849.9 17721,1849.9"];
	Layer3_Device3_Stage14_RecvKV -> Layer3_Device3_Stage14_Attention	[pos="e,17123,1992.6 17536,2119 17307,2119 17123,2119 17123,2119 17123,2119 17123,2002.6 17123,2002.6"];
	Layer3_Device3_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="18930,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device3_Stage14_RecvKV -> Layer3_Device3_Stage15_RecvKV	[label="Ring transfer",
		lp="18717,2062.4",
		pos="e,18670,2019 18670,2105.7 18670,2105.7 18670,2029 18670,2029"];
	Layer3_Device3_Stage14_Attention -> Layer3_Device3_Stage14_Accumulate	[pos="e,17489,1804 17200,1939.2 17200,1893.2 17200,1804 17200,1804 17200,1804 17479,1804 17479,1804"];
	Layer3_Device3_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="19184,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device3_Stage14_Accumulate -> Layer3_Device3_Stage15_Accumulate	[pos="e,18952,1687 17721,1786.6 17721,1749.5 17721,1687 17721,1687 17721,1687 18942,1687 18942,1687"];
	Layer3_Device3_Stage15_RecvKV -> Layer3_Device3_Stage15_Attention	[pos="e,19184,1840 19184,1912.6 19184,1912.6 19184,1850 19184,1850"];
	Layer3_Device3_Stage15_Attention -> Layer3_Device3_Stage15_Accumulate	[pos="e,19184,1714.1 19184,1786.7 19184,1786.7 19184,1724.1 19184,1724.1"];
	Layer3_Device3_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19336,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device3_Stage15_Accumulate -> Layer3_Device3_ConcatHeads	[pos="e,19263,1588.1 19263,1660.7 19263,1660.7 19263,1598.1 19263,1598.1"];
	Layer3_Device3_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="19548,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device3_ConcatHeads -> Layer3_Device3_OutputProj	[pos="e,19448,1462.1 19448,1534.7 19448,1534.7 19448,1472.1 19448,1472.1"];
	Layer3_Device3_OutputProj -> Layer3_Device3_Residual1	[pos="e,19715,1336.1 19715,1408.7 19715,1408.7 19715,1346.1 19715,1346.1"];
	Layer3_Device3_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="20056,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device3_Residual1 -> Layer3_Device3_LayerNorm2	[pos="e,20056,1210.1 20056,1282.7 20056,1282.7 20056,1220.1 20056,1220.1"];
	Layer3_Device3_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="20455,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device3_Residual1 -> Layer3_Device3_Residual2	[pos="e,20405,527.98 20405,1282.5 20405,1282.5 20405,537.98 20405,537.98"];
	Layer3_Device3_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19812,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device3_LayerNorm2 -> Layer3_Device3_GateProj	[pos="e,19940,1084.1 19940,1156.7 19940,1156.7 19940,1094.1 19940,1094.1"];
	Layer3_Device3_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="20125,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device3_LayerNorm2 -> Layer3_Device3_UpProj	[pos="e,20153,995.2 20153,1156.6 20153,1156.6 20153,1005.2 20153,1005.2"];
	Layer3_Device3_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19812,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device3_GateProj -> Layer3_Device3_Activation	[pos="e,19744,905.95 19744,1030.8 19744,1030.8 19744,915.95 19744,915.95"];
	Layer3_Device3_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="19957,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device3_UpProj -> Layer3_Device3_ElemMul	[pos="e,20194,780.2 20194,941.61 20194,941.61 20194,790.2 20194,790.2"];
	Layer3_Device3_Activation -> Layer3_Device3_ElemMul	[pos="e,19812,780.09 19812,852.69 19812,852.69 19812,790.09 19812,790.09"];
	Layer3_Device3_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="20053,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device3_ElemMul -> Layer3_Device3_DownProj	[pos="e,20053,654.09 20053,726.69 20053,726.69 20053,664.09 20053,664.09"];
	Layer3_Device3_DownProj -> Layer3_Device3_Residual2	[pos="e,20170,528.09 20170,600.69 20170,600.69 20170,538.09 20170,538.09"];
	Layer3_Device3_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 3 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="23816,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device3_Residual2 -> Layer3_Device3_Output	[pos="e,23724,400.31 20842,501 21707,501 23724,501 23724,501 23724,501 23724,410.31 23724,410.31"];
	Layer3_Device3_Output -> Sequence_Aggregate	[pos="e,40245,200 23816,326.71 23816,278.16 23816,200 23816,200 23816,200 40235,200 40235,200"];
	Layer3_Device4_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24255,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device4_Input -> Layer3_Device4_LayerNorm1	[pos="e,24468,5153 24559,5283.8 24559,5256.3 24559,5153 24559,5153 24559,5153 24478,5153 24478,5153"];
	Layer3_Device4_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="24888,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device4_Input -> Layer3_Device4_Residual1	[pos="e,24977,1336 24977,5256 24977,5256 24977,1346 24977,1346"];
	Layer3_Device4_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23852,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device4_LayerNorm1 -> Layer3_Device4_QKVProj	[pos="e,24255,5054.1 24255,5126.7 24255,5126.7 24255,5064.1 24255,5064.1"];
	Layer3_Device4_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22606,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage0_RecvKV	[label="Local K,V",
		lp="23106,4957.4",
		pos="e,23364,4914.2 23364,5000.7 23364,5000.7 23364,4924.2 23364,4924.2"];
	Layer3_Device4_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23396,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage0_Attention	[label=Q_local,
		lp="23668,4860.9",
		pos="e,23577,4694.8 23577,5000.7 23577,5000.7 23577,4704.8 23577,4704.8"];
	Layer3_Device4_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21066,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage1_Attention	[label=Q_local,
		lp="21130,4764.4",
		pos="e,21066,4501.6 23202,5006 22379,5006 21066,5006 21066,5006 21066,5006 21066,4511.6 21066,4511.6"];
	Layer3_Device4_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="21655,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage2_Attention	[label=Q_local,
		lp="23808,4667.9",
		pos="e,21847,4308.6 23672,5000.6 23672,4903.6 23672,4577 23672,4577 23672,4577 21847,4577 21847,4577 21847,4577 21847,4318.6 21847,4318.6"];
	Layer3_Device4_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23670,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage3_Attention	[label=Q_local,
		lp="23948,4571.4",
		pos="e,23843,4115.7 23843,5000.8 23843,5000.8 23843,4125.7 23843,4125.7"];
	Layer3_Device4_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20920,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage4_Attention	[label=Q_local,
		lp="20750,4474.9",
		pos="e,20820,3922.5 23202,5018 22311,5018 20820,5018 20820,5018 20820,5018 20820,3932.5 20820,3932.5"];
	Layer3_Device4_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23788,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage5_Attention	[label=Q_local,
		lp="24041,4378.4",
		pos="e,23960,3729.7 23960,5000.6 23960,5000.6 23960,3739.7 23960,3739.7"];
	Layer3_Device4_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23808,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage6_Attention	[label=Q_local,
		lp="24162,4281.9",
		pos="e,24030,3536.6 24030,5000.7 24030,5000.7 24030,3546.6 24030,3546.6"];
	Layer3_Device4_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20869,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage7_Attention	[label=Q_local,
		lp="20611,4185.4",
		pos="e,20663,3343.6 23202,5036 22270,5036 20663,5036 20663,5036 20663,5036 20663,3353.6 20663,3353.6"];
	Layer3_Device4_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="23680,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage8_Attention	[label=Q_local,
		lp="24280,4088.9",
		pos="e,23912,3123 24091,5000.8 24091,4770.7 24091,3123 24091,3123 24091,3123 23922,3123 23922,3123"];
	Layer3_Device4_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20741,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage9_Attention	[label=Q_local,
		lp="20500,3992.4",
		pos="e,20574,2957.6 23202,5042 22247,5042 20574,5042 20574,5042 20574,5042 20574,2967.6 20574,2967.6"];
	Layer3_Device4_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="20866,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage10_Attention	[label=Q_local,
		lp="20368,3895.9",
		pos="e,20634,2737 23202,5048 22221,5048 20476,5048 20476,5048 20476,5048 20476,2737 20476,2737 20476,2737 20624,2737 20624,2737"];
	Layer3_Device4_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="22166,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage11_Attention	[label=Q_local,
		lp="24400,3799.4",
		pos="e,22243,2571.7 24193,5000.9 24193,4739.4 24193,2631 24193,2631 24193,2631 22243,2631 22243,2631 22243,2631 22243,2581.7 22243,2581.7"];
	Layer3_Device4_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="22435,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage12_Attention	[label=Q_local,
		lp="24538,3702.9",
		pos="e,22426,2378.5 24244,5000.9 24244,4739.2 24244,2629 24244,2629 24244,2629 22426,2629 22426,2629 22426,2629 22426,2388.5 22426,2388.5"];
	Layer3_Device4_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="24450,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage13_Attention	[label=Q_local,
		lp="24678,3606.4",
		pos="e,24623,2185.8 24502,5014 24574,5014 24623,5014 24623,5014 24623,5014 24623,2195.8 24623,2195.8"];
	Layer3_Device4_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="22032,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage14_Attention	[label=Q_local,
		lp="20228,3509.9",
		pos="e,21800,1965 24142,5000.7 24142,4752 24142,2852 24142,2852 24142,2852 21647,2852 21647,2852 21647,2852 21647,1965 21647,1965 21647,\
1965 21790,1965 21790,1965"];
	Layer3_Device4_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="24165,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_QKVProj -> Layer3_Device4_Stage15_Attention	[label=Q_local,
		lp="24796,3413.4",
		pos="e,24397,1822 24502,5027 24637,5027 24739,5027 24739,5027 24739,5027 24739,1822 24739,1822 24739,1822 24407,1822 24407,1822"];
	Layer3_Device4_Stage0_RecvKV -> Layer3_Device4_Stage0_Attention	[pos="e,23345,4694.8 23345,4860.8 23345,4860.8 23345,4704.8 23345,4704.8"];
	Layer3_Device4_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22187,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage0_RecvKV -> Layer3_Device4_Stage1_RecvKV	[label="Ring transfer",
		lp="22444,4764.4",
		pos="e,22396,4721 22396,4807.7 22396,4807.7 22396,4731 22396,4731"];
	Layer3_Device4_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="21587,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage0_Attention -> Layer3_Device4_Stage0_Accumulate	[pos="e,21587,4501.4 23396,4641.1 23396,4615.2 23396,4580 23396,4580 23396,4580 21587,4580 21587,4580 21587,4580 21587,4511.4 21587,4511.4"];
	Layer3_Device4_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21134,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage0_Accumulate -> Layer3_Device4_Stage1_Accumulate	[pos="e,21360,4308.5 21360,4448.1 21360,4448.1 21360,4318.5 21360,4318.5"];
	Layer3_Device4_Stage1_RecvKV -> Layer3_Device4_Stage1_Attention	[pos="e,21282,4501.8 21282,4667.8 21282,4667.8 21282,4511.8 21282,4511.8"];
	Layer3_Device4_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22796,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage1_RecvKV -> Layer3_Device4_Stage2_RecvKV	[label="Ring transfer",
		lp="22494,4571.4",
		pos="e,22492,4528 22492,4614.7 22492,4614.7 22492,4538 22492,4538"];
	Layer3_Device4_Stage1_Attention -> Layer3_Device4_Stage1_Accumulate	[pos="e,21100,4308.5 21100,4448.1 21100,4448.1 21100,4318.5 21100,4318.5"];
	Layer3_Device4_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21252,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage1_Accumulate -> Layer3_Device4_Stage2_Accumulate	[pos="e,21193,4115.5 21193,4255.1 21193,4255.1 21193,4125.5 21193,4125.5"];
	Layer3_Device4_Stage2_RecvKV -> Layer3_Device4_Stage2_Attention	[pos="e,21881,4308.8 21881,4474.8 21881,4474.8 21881,4318.8 21881,4318.8"];
	Layer3_Device4_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22864,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage2_RecvKV -> Layer3_Device4_Stage3_RecvKV	[label="Ring transfer",
		lp="22878,4378.4",
		pos="e,22830,4335 22830,4421.7 22830,4421.7 22830,4345 22830,4345"];
	Layer3_Device4_Stage2_Attention -> Layer3_Device4_Stage2_Accumulate	[pos="e,21454,4115.5 21454,4255.1 21454,4255.1 21454,4125.5 21454,4125.5"];
	Layer3_Device4_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21441,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage2_Accumulate -> Layer3_Device4_Stage3_Accumulate	[pos="e,21346,3922.5 21346,4062.1 21346,4062.1 21346,3932.5 21346,3932.5"];
	Layer3_Device4_Stage3_RecvKV -> Layer3_Device4_Stage3_Attention	[pos="e,23611,4115.8 23611,4281.8 23611,4281.8 23611,4125.8 23611,4125.8"];
	Layer3_Device4_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="22461,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage3_RecvKV -> Layer3_Device4_Stage4_RecvKV	[label="Ring transfer",
		lp="22710,4185.4",
		pos="e,22662,4142 22662,4228.7 22662,4228.7 22662,4152 22662,4152"];
	Layer3_Device4_Stage3_Attention -> Layer3_Device4_Stage3_Accumulate	[pos="e,21607,3922.7 23504,4062.3 23504,4024.3 23504,3959 23504,3959 23504,3959 21607,3959 21607,3959 21607,3959 21607,3932.7 21607,3932.7"];
	Layer3_Device4_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21370,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage3_Accumulate -> Layer3_Device4_Stage4_Accumulate	[pos="e,21406,3729.5 21406,3869.1 21406,3869.1 21406,3739.5 21406,3739.5"];
	Layer3_Device4_Stage4_RecvKV -> Layer3_Device4_Stage4_Attention	[pos="e,21086,3922.6 21588,4049 21316,4049 21086,4049 21086,4049 21086,4049 21086,3932.6 21086,3932.6"];
	Layer3_Device4_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22650,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage4_RecvKV -> Layer3_Device4_Stage5_RecvKV	[label="Ring transfer",
		lp="22603,3992.4",
		pos="e,22556,3949 22556,4035.7 22556,4035.7 22556,3959 22556,3959"];
	Layer3_Device4_Stage4_Attention -> Layer3_Device4_Stage4_Accumulate	[pos="e,21145,3729.5 21145,3869.1 21145,3869.1 21145,3739.5 21145,3739.5"];
	Layer3_Device4_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="23287,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage4_Accumulate -> Layer3_Device4_Stage5_Accumulate	[pos="e,23277,3536.7 21380,3676.3 21380,3638.3 21380,3573 21380,3573 21380,3573 23277,3573 23277,3573 23277,3573 23277,3546.7 23277,3546.7"];
	Layer3_Device4_Stage5_RecvKV -> Layer3_Device4_Stage5_Attention	[pos="e,23563,3729.8 23563,3895.8 23563,3895.8 23563,3739.8 23563,3739.8"];
	Layer3_Device4_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22579,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage5_RecvKV -> Layer3_Device4_Stage6_RecvKV	[label="Ring transfer",
		lp="22662,3799.4",
		pos="e,22614,3756 22614,3842.7 22614,3842.7 22614,3766 22614,3766"];
	Layer3_Device4_Stage5_Attention -> Layer3_Device4_Stage5_Accumulate	[pos="e,23509,3536.6 23556,3702 23528,3702 23509,3702 23509,3702 23509,3702 23509,3546.6 23509,3546.6"];
	Layer3_Device4_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="23287,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage5_Accumulate -> Layer3_Device4_Stage6_Accumulate	[pos="e,23287,3343.5 23287,3483.1 23287,3483.1 23287,3353.5 23287,3353.5"];
	Layer3_Device4_Stage6_RecvKV -> Layer3_Device4_Stage6_Attention	[pos="e,23798,3536.6 23169,3663 23480,3663 23798,3663 23798,3663 23798,3663 23798,3546.6 23798,3546.6"];
	Layer3_Device4_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22078,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage6_RecvKV -> Layer3_Device4_Stage7_RecvKV	[label="Ring transfer",
		lp="22469,3606.4",
		pos="e,22328,3563 22328,3649.7 22328,3649.7 22328,3573 22328,3573"];
	Layer3_Device4_Stage6_Attention -> Layer3_Device4_Stage6_Accumulate	[pos="e,23519,3316 23744,3483.4 23744,3430.2 23744,3316 23744,3316 23744,3316 23529,3316 23529,3316"];
	Layer3_Device4_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="23159,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage6_Accumulate -> Layer3_Device4_Stage7_Accumulate	[pos="e,23223,3150.5 23223,3290.1 23223,3290.1 23223,3160.5 23223,3160.5"];
	Layer3_Device4_Stage7_RecvKV -> Layer3_Device4_Stage7_Attention	[pos="e,20894,3343.6 21205,3470 21028,3470 20894,3470 20894,3470 20894,3470 20894,3353.6 20894,3353.6"];
	Layer3_Device4_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22078,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage7_RecvKV -> Layer3_Device4_Stage8_RecvKV	[label="Ring transfer",
		lp="22126,3413.4",
		pos="e,22078,3370 22078,3456.7 22078,3456.7 22078,3380 22078,3380"];
	Layer3_Device4_Stage7_Attention -> Layer3_Device4_Stage7_Accumulate	[pos="e,22963,3150.5 21065,3290.1 21065,3270.7 21065,3248 21065,3248 21065,3248 22963,3248 22963,3248 22963,3248 22963,3160.5 22963,3160.5"];
	Layer3_Device4_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="23159,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage7_Accumulate -> Layer3_Device4_Stage8_Accumulate	[pos="e,23159,2957.5 23159,3097.1 23159,3097.1 23159,2967.5 23159,2967.5"];
	Layer3_Device4_Stage8_RecvKV -> Layer3_Device4_Stage8_Attention	[pos="e,23484,3150.6 22668,3277 23053,3277 23484,3277 23484,3277 23484,3277 23484,3160.6 23484,3160.6"];
	Layer3_Device4_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21950,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage8_RecvKV -> Layer3_Device4_Stage9_RecvKV	[label="Ring transfer",
		lp="22002,3220.4",
		pos="e,22014,3177 22014,3263.7 22014,3263.7 22014,3187 22014,3187"];
	Layer3_Device4_Stage8_Attention -> Layer3_Device4_Stage8_Accumulate	[pos="e,23391,2930 23482,3097.4 23482,3044.2 23482,2930 23482,2930 23482,2930 23401,2930 23401,2930"];
	Layer3_Device4_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21387,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage8_Accumulate -> Layer3_Device4_Stage9_Accumulate	[pos="e,21464,2764.5 23159,2904 23159,2881.8 23159,2854 23159,2854 23159,2854 21464,2854 21464,2854 21464,2854 21464,2774.5 21464,2774.5"];
	Layer3_Device4_Stage9_RecvKV -> Layer3_Device4_Stage9_Attention	[pos="e,20805,2957.6 21077,3084 20920,3084 20805,3084 20805,3084 20805,3084 20805,2967.6 20805,2967.6"];
	Layer3_Device4_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="21950,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage9_RecvKV -> Layer3_Device4_Stage10_RecvKV	[label="Ring transfer",
		lp="21998,3027.4",
		pos="e,21950,2984 21950,3070.7 21950,3070.7 21950,2994 21950,2994"];
	Layer3_Device4_Stage9_Attention -> Layer3_Device4_Stage9_Accumulate	[pos="e,21310,2764.7 20804,2904.1 20804,2865.1 20804,2797 20804,2797 20804,2797 21310,2797 21310,2797 21310,2797 21310,2774.7 21310,2774.7"];
	Layer3_Device4_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21387,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage9_Accumulate -> Layer3_Device4_Stage10_Accumulate	[pos="e,21387,2571.5 21387,2711.1 21387,2711.1 21387,2581.5 21387,2581.5"];
	Layer3_Device4_Stage10_RecvKV -> Layer3_Device4_Stage10_Attention	[pos="e,21064,2764.8 21064,2930.8 21064,2930.8 21064,2774.8 21064,2774.8"];
	Layer3_Device4_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="22596,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage10_RecvKV -> Layer3_Device4_Stage11_RecvKV	[label="Ring transfer",
		lp="22356,2834.4",
		pos="e,22273,2791 22273,2877.7 22273,2877.7 22273,2801 22273,2801"];
	Layer3_Device4_Stage10_Attention -> Layer3_Device4_Stage10_Accumulate	[pos="e,21155,2544 20970,2711.4 20970,2658.2 20970,2544 20970,2544 20970,2544 21145,2544 21145,2544"];
	Layer3_Device4_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="21914,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage10_Accumulate -> Layer3_Device4_Stage11_Accumulate	[pos="e,21808,2378.6 21619,2544 21717,2544 21808,2544 21808,2544 21808,2544 21808,2388.6 21808,2388.6"];
	Layer3_Device4_Stage11_RecvKV -> Layer3_Device4_Stage11_Attention	[pos="e,22089,2571.6 22089,2684.7 22089,2684.7 22089,2581.6 22089,2581.6"];
	Layer3_Device4_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23375,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage11_RecvKV -> Layer3_Device4_Stage12_RecvKV	[label="Ring transfer",
		lp="23033,2641.4",
		pos="e,22986,2598 22986,2684.7 22986,2684.7 22986,2608 22986,2608"];
	Layer3_Device4_Stage11_Attention -> Layer3_Device4_Stage11_Accumulate	[pos="e,22040,2378.5 22040,2518.1 22040,2518.1 22040,2388.5 22040,2388.5"];
	Layer3_Device4_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22032,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage11_Accumulate -> Layer3_Device4_Stage12_Accumulate	[pos="e,21973,2185.5 21973,2325.1 21973,2325.1 21973,2195.5 21973,2195.5"];
	Layer3_Device4_Stage12_RecvKV -> Layer3_Device4_Stage12_Attention	[pos="e,22561,2378.6 22561,2491.7 22561,2491.7 22561,2388.6 22561,2388.6"];
	Layer3_Device4_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23644,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage12_RecvKV -> Layer3_Device4_Stage13_RecvKV	[label="Ring transfer",
		lp="23557,2448.4",
		pos="e,23510,2405 23510,2491.7 23510,2491.7 23510,2415 23510,2415"];
	Layer3_Device4_Stage12_Attention -> Layer3_Device4_Stage12_Accumulate	[pos="e,22234,2185.5 22234,2325.1 22234,2325.1 22234,2195.5 22234,2195.5"];
	Layer3_Device4_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22553,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage12_Accumulate -> Layer3_Device4_Stage13_Accumulate	[pos="e,22321,1965 22264,2158 22281,2158 22292,2158 22292,2158 22292,2158 22292,1965 22292,1965 22292,1965 22311,1965 22311,1965"];
	Layer3_Device4_Stage13_RecvKV -> Layer3_Device4_Stage13_Attention	[pos="e,24391,2185.8 24391,2351.8 24391,2351.8 24391,2195.8 24391,2195.8"];
	Layer3_Device4_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23241,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage13_RecvKV -> Layer3_Device4_Stage14_RecvKV	[label="Ring transfer",
		lp="23558,2255.4",
		pos="e,23442,2212 23442,2298.7 23442,2298.7 23442,2222 23442,2222"];
	Layer3_Device4_Stage13_Attention -> Layer3_Device4_Stage13_Accumulate	[pos="e,22553,1992.6 24450,2132.2 24450,2097.4 24450,2041 24450,2041 24450,2041 22553,2041 22553,2041 22553,2041 22553,2002.6 22553,2002.6"];
	Layer3_Device4_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="22553,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage13_Accumulate -> Layer3_Device4_Stage14_Accumulate	[pos="e,22553,1839.9 22553,1939.3 22553,1939.3 22553,1849.9 22553,1849.9"];
	Layer3_Device4_Stage14_RecvKV -> Layer3_Device4_Stage14_Attention	[pos="e,22032,1992.6 22368,2119 22178,2119 22032,2119 22032,2119 22032,2119 22032,2002.6 22032,2002.6"];
	Layer3_Device4_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="23762,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device4_Stage14_RecvKV -> Layer3_Device4_Stage15_RecvKV	[label="Ring transfer",
		lp="23549,2062.4",
		pos="e,23502,2019 23502,2105.7 23502,2105.7 23502,2029 23502,2029"];
	Layer3_Device4_Stage14_Attention -> Layer3_Device4_Stage14_Accumulate	[pos="e,22321,1804 22032,1939.2 22032,1893.2 22032,1804 22032,1804 22032,1804 22311,1804 22311,1804"];
	Layer3_Device4_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="24165,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device4_Stage14_Accumulate -> Layer3_Device4_Stage15_Accumulate	[pos="e,23933,1687 22553,1786.6 22553,1749.5 22553,1687 22553,1687 22553,1687 23923,1687 23923,1687"];
	Layer3_Device4_Stage15_RecvKV -> Layer3_Device4_Stage15_Attention	[pos="e,24165,1840 24165,1912.6 24165,1912.6 24165,1850 24165,1850"];
	Layer3_Device4_Stage15_Attention -> Layer3_Device4_Stage15_Accumulate	[pos="e,24165,1714.1 24165,1786.7 24165,1786.7 24165,1724.1 24165,1724.1"];
	Layer3_Device4_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24571,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device4_Stage15_Accumulate -> Layer3_Device4_ConcatHeads	[pos="e,24371,1588.1 24371,1660.7 24371,1660.7 24371,1598.1 24371,1598.1"];
	Layer3_Device4_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24581,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device4_ConcatHeads -> Layer3_Device4_OutputProj	[pos="e,24581,1462.1 24581,1534.7 24581,1534.7 24581,1472.1 24581,1472.1"];
	Layer3_Device4_OutputProj -> Layer3_Device4_Residual1	[pos="e,24648,1336.1 24648,1408.7 24648,1408.7 24648,1346.1 24648,1346.1"];
	Layer3_Device4_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="24888,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device4_Residual1 -> Layer3_Device4_LayerNorm2	[pos="e,24888,1210.1 24888,1282.7 24888,1282.7 24888,1220.1 24888,1220.1"];
	Layer3_Device4_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="25405,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device4_Residual1 -> Layer3_Device4_Residual2	[pos="e,25390,528.28 25275,1309 25341,1309 25390,1309 25390,1309 25390,1309 25390,538.28 25390,538.28"];
	Layer3_Device4_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24670,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device4_LayerNorm2 -> Layer3_Device4_GateProj	[pos="e,24785,1084.1 24785,1156.7 24785,1156.7 24785,1094.1 24785,1094.1"];
	Layer3_Device4_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24983,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device4_LayerNorm2 -> Layer3_Device4_UpProj	[pos="e,24998,995.2 24998,1156.6 24998,1156.6 24998,1005.2 24998,1005.2"];
	Layer3_Device4_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24670,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device4_GateProj -> Layer3_Device4_Activation	[pos="e,24602,905.95 24602,1030.8 24602,1030.8 24602,915.95 24602,915.95"];
	Layer3_Device4_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="24907,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device4_UpProj -> Layer3_Device4_ElemMul	[pos="e,25052,780.2 25052,941.61 25052,941.61 25052,790.2 25052,790.2"];
	Layer3_Device4_Activation -> Layer3_Device4_ElemMul	[pos="e,24696,780.09 24696,852.69 24696,852.69 24696,790.09 24696,790.09"];
	Layer3_Device4_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="25050,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device4_ElemMul -> Layer3_Device4_DownProj	[pos="e,25050,654.09 25050,726.69 25050,726.69 25050,664.09 25050,664.09"];
	Layer3_Device4_DownProj -> Layer3_Device4_Residual2	[pos="e,25144,528.09 25144,600.69 25144,600.69 25144,538.09 25144,538.09"];
	Layer3_Device4_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 4 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="28841,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device4_Residual2 -> Layer3_Device4_Output	[pos="e,28653,393.73 25792,501 26652,501 28653,501 28653,501 28653,501 28653,403.73 28653,403.73"];
	Layer3_Device4_Output -> Sequence_Aggregate	[pos="e,40270,214 28841,326.86 28841,282.3 28841,214 28841,214 28841,214 40260,214 40260,214"];
	Layer3_Device5_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29202,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device5_Input -> Layer3_Device5_LayerNorm1	[pos="e,29348,5179.9 29348,5266.8 29348,5266.8 29348,5189.9 29348,5189.9"];
	Layer3_Device5_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="29720,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device5_Input -> Layer3_Device5_Residual1	[pos="e,29756,1336.2 29756,5259.5 29756,5259.5 29756,1346.2 29756,1346.2"];
	Layer3_Device5_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28676,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device5_LayerNorm1 -> Layer3_Device5_QKVProj	[pos="e,29157,5054.1 29157,5126.7 29157,5126.7 29157,5064.1 29157,5064.1"];
	Layer3_Device5_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27438,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage0_RecvKV	[label="Local K,V",
		lp="27938,4957.4",
		pos="e,28192,4914.2 28192,5000.7 28192,5000.7 28192,4924.2 28192,4924.2"];
	Layer3_Device5_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28228,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage0_Attention	[label=Q_local,
		lp="28500,4860.9",
		pos="e,28409,4694.8 28409,5000.7 28409,5000.7 28409,4704.8 28409,4704.8"];
	Layer3_Device5_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25898,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage1_Attention	[label=Q_local,
		lp="25962,4764.4",
		pos="e,25883,4501.7 28026,5007 27201,5007 25883,5007 25883,5007 25883,5007 25883,4511.7 25883,4511.7"];
	Layer3_Device5_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26487,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage2_Attention	[label=Q_local,
		lp="28640,4667.9",
		pos="e,26679,4308.6 28504,5000.6 28504,4904.5 28504,4583 28504,4583 28504,4583 26679,4583 26679,4583 26679,4583 26679,4318.6 26679,4318.6"];
	Layer3_Device5_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28502,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage3_Attention	[label=Q_local,
		lp="28780,4571.4",
		pos="e,28675,4115.7 28675,5000.8 28675,5000.8 28675,4125.7 28675,4125.7"];
	Layer3_Device5_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25752,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage4_Attention	[label=Q_local,
		lp="25582,4474.9",
		pos="e,25656,3922.6 28026,5020 27139,5020 25656,5020 25656,5020 25656,5020 25656,3932.6 25656,3932.6"];
	Layer3_Device5_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28620,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage5_Attention	[label=Q_local,
		lp="28866,4378.4",
		pos="e,28792,3729.7 28792,5000.6 28792,5000.6 28792,3739.7 28792,3739.7"];
	Layer3_Device5_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28678,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage6_Attention	[label=Q_local,
		lp="28994,4281.9",
		pos="e,28880,3536.6 28880,5000.7 28880,5000.7 28880,3546.6 28880,3546.6"];
	Layer3_Device5_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25739,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage7_Attention	[label=Q_local,
		lp="25450,4185.4",
		pos="e,25514,3343.5 28026,5034 27101,5034 25514,5034 25514,5034 25514,5034 25514,3353.5 25514,3353.5"];
	Layer3_Device5_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28596,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage8_Attention	[label=Q_local,
		lp="29112,4088.9",
		pos="e,28828,3123 28960,5000.8 28960,4770.7 28960,3123 28960,3123 28960,3123 28838,3123 28838,3123"];
	Layer3_Device5_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25657,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage9_Attention	[label=Q_local,
		lp="25338,3992.4",
		pos="e,25466,2957.6 28026,5040 27088,5040 25466,5040 25466,5040 25466,5040 25466,2967.6 25466,2967.6"];
	Layer3_Device5_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="25722,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage10_Attention	[label=Q_local,
		lp="25200,3895.9",
		pos="e,25490,2737 28026,5047 27059,5047 25354,5047 25354,5047 25354,5047 25354,2737 25354,2737 25354,2737 25480,2737 25480,2737"];
	Layer3_Device5_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="27031,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage11_Attention	[label=Q_local,
		lp="29232,3799.4",
		pos="e,27108,2571.4 29060,5000.7 29060,4737.1 29060,2612 29060,2612 29060,2612 27108,2612 27108,2612 27108,2612 27108,2581.4 27108,2581.4"];
	Layer3_Device5_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="27267,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage12_Attention	[label=Q_local,
		lp="29370,3702.9",
		pos="e,27291,2378.5 29110,5000.7 29110,4736.8 29110,2609 29110,2609 29110,2609 27291,2609 27291,2609 27291,2609 27291,2388.5 27291,2388.5"];
	Layer3_Device5_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="29282,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage13_Attention	[label=Q_local,
		lp="29510,3606.4",
		pos="e,29405,2185.8 29326,5014 29374,5014 29405,5014 29405,5014 29405,5014 29405,2195.8 29405,2195.8"];
	Layer3_Device5_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="26864,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage14_Attention	[label=Q_local,
		lp="25060,3509.9",
		pos="e,26632,1965 29010,5000.8 29010,4752.9 29010,2859 29010,2859 29010,2859 26494,2859 26494,2859 26494,2859 26494,1965 26494,1965 26494,\
1965 26622,1965 26622,1965"];
	Layer3_Device5_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="28997,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_QKVProj -> Layer3_Device5_Stage15_Attention	[label=Q_local,
		lp="29628,3413.4",
		pos="e,29229,1822 29326,5027 29465,5027 29571,5027 29571,5027 29571,5027 29571,1822 29571,1822 29571,1822 29239,1822 29239,1822"];
	Layer3_Device5_Stage0_RecvKV -> Layer3_Device5_Stage0_Attention	[pos="e,28177,4694.8 28177,4860.8 28177,4860.8 28177,4704.8 28177,4704.8"];
	Layer3_Device5_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27019,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage0_RecvKV -> Layer3_Device5_Stage1_RecvKV	[label="Ring transfer",
		lp="27348,4764.4",
		pos="e,27228,4721 27228,4807.7 27228,4807.7 27228,4731 27228,4731"];
	Layer3_Device5_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="26419,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage0_Attention -> Layer3_Device5_Stage0_Accumulate	[pos="e,26419,4501.6 28228,4641.2 28228,4617 28228,4585 28228,4585 28228,4585 26419,4585 26419,4585 26419,4585 26419,4511.6 26419,4511.6"];
	Layer3_Device5_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="25966,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage0_Accumulate -> Layer3_Device5_Stage1_Accumulate	[pos="e,26192,4308.5 26192,4448.1 26192,4448.1 26192,4318.5 26192,4318.5"];
	Layer3_Device5_Stage1_RecvKV -> Layer3_Device5_Stage1_Attention	[pos="e,26114,4501.8 26114,4667.8 26114,4667.8 26114,4511.8 26114,4511.8"];
	Layer3_Device5_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27628,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage1_RecvKV -> Layer3_Device5_Stage2_RecvKV	[label="Ring transfer",
		lp="27336,4571.4",
		pos="e,27324,4528 27324,4614.7 27324,4614.7 27324,4538 27324,4538"];
	Layer3_Device5_Stage1_Attention -> Layer3_Device5_Stage1_Accumulate	[pos="e,25932,4308.5 25932,4448.1 25932,4448.1 25932,4318.5 25932,4318.5"];
	Layer3_Device5_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26084,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage1_Accumulate -> Layer3_Device5_Stage2_Accumulate	[pos="e,26025,4115.5 26025,4255.1 26025,4255.1 26025,4125.5 26025,4125.5"];
	Layer3_Device5_Stage2_RecvKV -> Layer3_Device5_Stage2_Attention	[pos="e,26713,4308.8 26713,4474.8 26713,4474.8 26713,4318.8 26713,4318.8"];
	Layer3_Device5_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27696,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage2_RecvKV -> Layer3_Device5_Stage3_RecvKV	[label="Ring transfer",
		lp="27710,4378.4",
		pos="e,27662,4335 27662,4421.7 27662,4421.7 27662,4345 27662,4345"];
	Layer3_Device5_Stage2_Attention -> Layer3_Device5_Stage2_Accumulate	[pos="e,26286,4115.5 26286,4255.1 26286,4255.1 26286,4125.5 26286,4125.5"];
	Layer3_Device5_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26273,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage2_Accumulate -> Layer3_Device5_Stage3_Accumulate	[pos="e,26178,3922.5 26178,4062.1 26178,4062.1 26178,3932.5 26178,3932.5"];
	Layer3_Device5_Stage3_RecvKV -> Layer3_Device5_Stage3_Attention	[pos="e,28443,4115.8 28443,4281.8 28443,4281.8 28443,4125.8 28443,4125.8"];
	Layer3_Device5_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27293,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage3_RecvKV -> Layer3_Device5_Stage4_RecvKV	[label="Ring transfer",
		lp="27477,4185.4",
		pos="e,27494,4142 27494,4228.7 27494,4228.7 27494,4152 27494,4152"];
	Layer3_Device5_Stage3_Attention -> Layer3_Device5_Stage3_Accumulate	[pos="e,26439,3922.4 28336,4062.1 28336,4025.4 28336,3964 28336,3964 28336,3964 26439,3964 26439,3964 26439,3964 26439,3932.4 26439,3932.4"];
	Layer3_Device5_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26202,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage3_Accumulate -> Layer3_Device5_Stage4_Accumulate	[pos="e,26238,3729.5 26238,3869.1 26238,3869.1 26238,3739.5 26238,3739.5"];
	Layer3_Device5_Stage4_RecvKV -> Layer3_Device5_Stage4_Attention	[pos="e,25918,3922.6 26420,4049 26148,4049 25918,4049 25918,4049 25918,4049 25918,3932.6 25918,3932.6"];
	Layer3_Device5_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="27482,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage4_RecvKV -> Layer3_Device5_Stage5_RecvKV	[label="Ring transfer",
		lp="27435,3992.4",
		pos="e,27388,3949 27388,4035.7 27388,4035.7 27388,3959 27388,3959"];
	Layer3_Device5_Stage4_Attention -> Layer3_Device5_Stage4_Accumulate	[pos="e,25977,3729.5 25977,3869.1 25977,3869.1 25977,3739.5 25977,3739.5"];
	Layer3_Device5_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28157,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage4_Accumulate -> Layer3_Device5_Stage5_Accumulate	[pos="e,28128,3536.7 26231,3676.2 26231,3653.3 26231,3624 26231,3624 26231,3624 28128,3624 28128,3624 28128,3624 28128,3546.7 28128,3546.7"];
	Layer3_Device5_Stage5_RecvKV -> Layer3_Device5_Stage5_Attention	[pos="e,28395,3729.8 28395,3895.8 28395,3895.8 28395,3739.8 28395,3739.8"];
	Layer3_Device5_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27411,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage5_RecvKV -> Layer3_Device5_Stage6_RecvKV	[label="Ring transfer",
		lp="27494,3799.4",
		pos="e,27446,3756 27446,3842.7 27446,3842.7 27446,3766 27446,3766"];
	Layer3_Device5_Stage5_Attention -> Layer3_Device5_Stage5_Accumulate	[pos="e,28360,3536.6 28388,3702 28371,3702 28360,3702 28360,3702 28360,3702 28360,3546.6 28360,3546.6"];
	Layer3_Device5_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28157,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage5_Accumulate -> Layer3_Device5_Stage6_Accumulate	[pos="e,28157,3343.5 28157,3483.1 28157,3483.1 28157,3353.5 28157,3353.5"];
	Layer3_Device5_Stage6_RecvKV -> Layer3_Device5_Stage6_Attention	[pos="e,28649,3536.6 28001,3663 28320,3663 28649,3663 28649,3663 28649,3663 28649,3546.6 28649,3546.6"];
	Layer3_Device5_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26948,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage6_RecvKV -> Layer3_Device5_Stage7_RecvKV	[label="Ring transfer",
		lp="27227,3606.4",
		pos="e,27180,3563 27180,3649.7 27180,3649.7 27180,3573 27180,3573"];
	Layer3_Device5_Stage6_Attention -> Layer3_Device5_Stage6_Accumulate	[pos="e,28389,3308 28637,3483.1 28637,3428.2 28637,3308 28637,3308 28637,3308 28399,3308 28399,3308"];
	Layer3_Device5_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28075,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage6_Accumulate -> Layer3_Device5_Stage7_Accumulate	[pos="e,28116,3150.5 28116,3290.1 28116,3290.1 28116,3160.5 28116,3160.5"];
	Layer3_Device5_Stage7_RecvKV -> Layer3_Device5_Stage7_Attention	[pos="e,25746,3343.6 26074,3470 25888,3470 25746,3470 25746,3470 25746,3470 25746,3353.6 25746,3353.6"];
	Layer3_Device5_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26948,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage7_RecvKV -> Layer3_Device5_Stage8_RecvKV	[label="Ring transfer",
		lp="26996,3413.4",
		pos="e,26948,3370 26948,3456.7 26948,3456.7 26948,3380 26948,3380"];
	Layer3_Device5_Stage7_Attention -> Layer3_Device5_Stage7_Accumulate	[pos="e,27856,3150.7 25958,3290.2 25958,3274.6 25958,3258 25958,3258 25958,3258 27856,3258 27856,3258 27856,3258 27856,3160.7 27856,3160.7"];
	Layer3_Device5_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28075,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage7_Accumulate -> Layer3_Device5_Stage8_Accumulate	[pos="e,28075,2957.5 28075,3097.1 28075,3097.1 28075,2967.5 28075,2967.5"];
	Layer3_Device5_Stage8_RecvKV -> Layer3_Device5_Stage8_Attention	[pos="e,28376,3150.6 27538,3277 27931,3277 28376,3277 28376,3277 28376,3277 28376,3160.6 28376,3160.6"];
	Layer3_Device5_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26866,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage8_RecvKV -> Layer3_Device5_Stage9_RecvKV	[label="Ring transfer",
		lp="26955,3220.4",
		pos="e,26907,3177 26907,3263.7 26907,3263.7 26907,3187 26907,3187"];
	Layer3_Device5_Stage8_Attention -> Layer3_Device5_Stage8_Accumulate	[pos="e,28307,2930 28368,3097.4 28368,3044.2 28368,2930 28368,2930 28368,2930 28317,2930 28317,2930"];
	Layer3_Device5_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26243,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage8_Accumulate -> Layer3_Device5_Stage9_Accumulate	[pos="e,26320,2764.6 28075,2904.1 28075,2884.4 28075,2861 28075,2861 28075,2861 26320,2861 26320,2861 26320,2861 26320,2774.6 26320,2774.6"];
	Layer3_Device5_Stage9_RecvKV -> Layer3_Device5_Stage9_Attention	[pos="e,25698,2957.6 25993,3084 25824,3084 25698,3084 25698,3084 25698,3084 25698,2967.6 25698,2967.6"];
	Layer3_Device5_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="26866,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage9_RecvKV -> Layer3_Device5_Stage10_RecvKV	[label="Ring transfer",
		lp="26914,3027.4",
		pos="e,26866,2984 26866,3070.7 26866,3070.7 26866,2994 26866,2994"];
	Layer3_Device5_Stage9_Attention -> Layer3_Device5_Stage9_Accumulate	[pos="e,26166,2764.5 25690,2904.3 25690,2864.2 25690,2793 25690,2793 25690,2793 26166,2793 26166,2793 26166,2793 26166,2774.5 26166,2774.5"];
	Layer3_Device5_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26243,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage9_Accumulate -> Layer3_Device5_Stage10_Accumulate	[pos="e,26243,2571.5 26243,2711.1 26243,2711.1 26243,2581.5 26243,2581.5"];
	Layer3_Device5_Stage10_RecvKV -> Layer3_Device5_Stage10_Attention	[pos="e,25950,2764.8 25950,2930.8 25950,2930.8 25950,2774.8 25950,2774.8"];
	Layer3_Device5_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="27452,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage10_RecvKV -> Layer3_Device5_Stage11_RecvKV	[label="Ring transfer",
		lp="27139,2834.4",
		pos="e,27159,2791 27159,2877.7 27159,2877.7 27159,2801 27159,2801"];
	Layer3_Device5_Stage10_Attention -> Layer3_Device5_Stage10_Accumulate	[pos="e,26011,2544 25872,2711.4 25872,2658.2 25872,2544 25872,2544 25872,2544 26001,2544 26001,2544"];
	Layer3_Device5_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26746,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage10_Accumulate -> Layer3_Device5_Stage11_Accumulate	[pos="e,26523,2378.6 26475,2544 26504,2544 26523,2544 26523,2544 26523,2544 26523,2388.6 26523,2388.6"];
	Layer3_Device5_Stage11_RecvKV -> Layer3_Device5_Stage11_Attention	[pos="e,26954,2571.6 26954,2684.7 26954,2684.7 26954,2581.6 26954,2581.6"];
	Layer3_Device5_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28240,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage11_RecvKV -> Layer3_Device5_Stage12_RecvKV	[label="Ring transfer",
		lp="27654,2641.4",
		pos="e,27846,2598 27846,2684.7 27846,2684.7 27846,2608 27846,2608"];
	Layer3_Device5_Stage11_Attention -> Layer3_Device5_Stage11_Accumulate	[pos="e,26888,2378.5 26888,2518.1 26888,2518.1 26888,2388.5 26888,2388.5"];
	Layer3_Device5_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="26864,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage11_Accumulate -> Layer3_Device5_Stage12_Accumulate	[pos="e,26805,2185.5 26805,2325.1 26805,2325.1 26805,2195.5 26805,2195.5"];
	Layer3_Device5_Stage12_RecvKV -> Layer3_Device5_Stage12_Attention	[pos="e,27409,2378.6 27409,2491.7 27409,2491.7 27409,2388.6 27409,2388.6"];
	Layer3_Device5_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28476,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage12_RecvKV -> Layer3_Device5_Stage13_RecvKV	[label="Ring transfer",
		lp="28406,2448.4",
		pos="e,28358,2405 28358,2491.7 28358,2491.7 28358,2415 28358,2415"];
	Layer3_Device5_Stage12_Attention -> Layer3_Device5_Stage12_Accumulate	[pos="e,27066,2185.5 27066,2325.1 27066,2325.1 27066,2195.5 27066,2195.5"];
	Layer3_Device5_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27385,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage12_Accumulate -> Layer3_Device5_Stage13_Accumulate	[pos="e,27153,1965 27096,2158 27113,2158 27124,2158 27124,2158 27124,2158 27124,1965 27124,1965 27124,1965 27143,1965 27143,1965"];
	Layer3_Device5_Stage13_RecvKV -> Layer3_Device5_Stage13_Attention	[pos="e,29223,2185.8 29223,2351.8 29223,2351.8 29223,2195.8 29223,2195.8"];
	Layer3_Device5_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28073,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage13_RecvKV -> Layer3_Device5_Stage14_RecvKV	[label="Ring transfer",
		lp="28322,2255.4",
		pos="e,28274,2212 28274,2298.7 28274,2298.7 28274,2222 28274,2222"];
	Layer3_Device5_Stage13_Attention -> Layer3_Device5_Stage13_Accumulate	[pos="e,27385,1992.6 29282,2132.1 29282,2098.8 29282,2046 29282,2046 29282,2046 27385,2046 27385,2046 27385,2046 27385,2002.6 27385,2002.6"];
	Layer3_Device5_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="27385,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage13_Accumulate -> Layer3_Device5_Stage14_Accumulate	[pos="e,27385,1839.9 27385,1939.3 27385,1939.3 27385,1849.9 27385,1849.9"];
	Layer3_Device5_Stage14_RecvKV -> Layer3_Device5_Stage14_Attention	[pos="e,26864,1992.6 27200,2119 27010,2119 26864,2119 26864,2119 26864,2119 26864,2002.6 26864,2002.6"];
	Layer3_Device5_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="28594,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device5_Stage14_RecvKV -> Layer3_Device5_Stage15_RecvKV	[label="Ring transfer",
		lp="28381,2062.4",
		pos="e,28334,2019 28334,2105.7 28334,2105.7 28334,2029 28334,2029"];
	Layer3_Device5_Stage14_Attention -> Layer3_Device5_Stage14_Accumulate	[pos="e,27153,1804 26864,1939.2 26864,1893.2 26864,1804 26864,1804 26864,1804 27143,1804 27143,1804"];
	Layer3_Device5_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="28997,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device5_Stage14_Accumulate -> Layer3_Device5_Stage15_Accumulate	[pos="e,28765,1687 27385,1786.6 27385,1749.5 27385,1687 27385,1687 27385,1687 28755,1687 28755,1687"];
	Layer3_Device5_Stage15_RecvKV -> Layer3_Device5_Stage15_Attention	[pos="e,28997,1840 28997,1912.6 28997,1912.6 28997,1850 28997,1850"];
	Layer3_Device5_Stage15_Attention -> Layer3_Device5_Stage15_Accumulate	[pos="e,28997,1714.1 28997,1786.7 28997,1786.7 28997,1724.1 28997,1724.1"];
	Layer3_Device5_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29403,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device5_Stage15_Accumulate -> Layer3_Device5_ConcatHeads	[pos="e,29203,1588.1 29203,1660.7 29203,1660.7 29203,1598.1 29203,1598.1"];
	Layer3_Device5_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29414,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device5_ConcatHeads -> Layer3_Device5_OutputProj	[pos="e,29414,1462.1 29414,1534.7 29414,1534.7 29414,1472.1 29414,1472.1"];
	Layer3_Device5_OutputProj -> Layer3_Device5_Residual1	[pos="e,29480,1336.1 29480,1408.7 29480,1408.7 29480,1346.1 29480,1346.1"];
	Layer3_Device5_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29720,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device5_Residual1 -> Layer3_Device5_LayerNorm2	[pos="e,29720,1210.1 29720,1282.7 29720,1282.7 29720,1220.1 29720,1220.1"];
	Layer3_Device5_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="30119,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device5_Residual1 -> Layer3_Device5_Residual2	[pos="e,30069,527.98 30069,1282.5 30069,1282.5 30069,537.98 30069,537.98"];
	Layer3_Device5_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29476,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device5_LayerNorm2 -> Layer3_Device5_GateProj	[pos="e,29604,1084.1 29604,1156.7 29604,1156.7 29604,1094.1 29604,1094.1"];
	Layer3_Device5_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29789,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device5_LayerNorm2 -> Layer3_Device5_UpProj	[pos="e,29817,995.2 29817,1156.6 29817,1156.6 29817,1005.2 29817,1005.2"];
	Layer3_Device5_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29476,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device5_GateProj -> Layer3_Device5_Activation	[pos="e,29408,905.95 29408,1030.8 29408,1030.8 29408,915.95 29408,915.95"];
	Layer3_Device5_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="29621,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device5_UpProj -> Layer3_Device5_ElemMul	[pos="e,29858,780.2 29858,941.61 29858,941.61 29858,790.2 29858,790.2"];
	Layer3_Device5_Activation -> Layer3_Device5_ElemMul	[pos="e,29476,780.09 29476,852.69 29476,852.69 29476,790.09 29476,790.09"];
	Layer3_Device5_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="29717,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device5_ElemMul -> Layer3_Device5_DownProj	[pos="e,29717,654.09 29717,726.69 29717,726.69 29717,664.09 29717,664.09"];
	Layer3_Device5_DownProj -> Layer3_Device5_Residual2	[pos="e,29834,528.09 29834,600.69 29834,600.69 29834,538.09 29834,538.09"];
	Layer3_Device5_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 5 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34249,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device5_Residual2 -> Layer3_Device5_Output	[pos="e,33974,379.83 30506,501 31483,501 33974,501 33974,501 33974,501 33974,389.83 33974,389.83"];
	Layer3_Device5_Output -> Sequence_Aggregate	[pos="e,40293,227 34249,326.88 34249,286.19 34249,227 34249,227 34249,227 40283,227 40283,227"];
	Layer3_Device6_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33825,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device6_Input -> Layer3_Device6_LayerNorm1	[pos="e,34038,5153 34135,5280.1 34135,5248.1 34135,5153 34135,5153 34135,5153 34048,5153 34048,5153"];
	Layer3_Device6_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="34523,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device6_Input -> Layer3_Device6_Residual1	[pos="e,34576,1336.1 34576,5257.8 34576,5257.8 34576,1346.1 34576,1346.1"];
	Layer3_Device6_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33497,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device6_LayerNorm1 -> Layer3_Device6_QKVProj	[pos="e,33825,5054.1 33825,5126.7 33825,5126.7 33825,5064.1 33825,5064.1"];
	Layer3_Device6_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32251,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage0_RecvKV	[label="Local K,V",
		lp="32752,4957.4",
		pos="e,33009,4914.2 33009,5000.7 33009,5000.7 33009,4924.2 33009,4924.2"];
	Layer3_Device6_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33004,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage0_Attention	[label=Q_local,
		lp="33313,4860.9",
		pos="e,33203,4694.8 33203,5000.7 33203,5000.7 33203,4704.8 33203,4704.8"];
	Layer3_Device6_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30711,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage1_Attention	[label=Q_local,
		lp="30775,4764.4",
		pos="e,30831,4501.7 32847,5007 32058,5007 30831,5007 30831,5007 30831,5007 30831,4511.7 30831,4511.7"];
	Layer3_Device6_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31300,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage2_Attention	[label=Q_local,
		lp="33453,4667.9",
		pos="e,31492,4308.7 33298,5000.6 33298,4898.6 33298,4541 33298,4541 33298,4541 31492,4541 31492,4541 31492,4541 31492,4318.7 31492,4318.7"];
	Layer3_Device6_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33315,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage3_Attention	[label=Q_local,
		lp="33593,4571.4",
		pos="e,33488,4115.7 33488,5000.8 33488,5000.8 33488,4125.7 33488,4125.7"];
	Layer3_Device6_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30565,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage4_Attention	[label=Q_local,
		lp="30395,4474.9",
		pos="e,30406,3922.6 32847,5020 31941,5020 30406,5020 30406,5020 30406,5020 30406,3932.6 30406,3932.6"];
	Layer3_Device6_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33433,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage5_Attention	[label=Q_local,
		lp="33686,4378.4",
		pos="e,33606,3729.7 33606,5000.6 33606,5000.6 33606,3739.7 33606,3739.7"];
	Layer3_Device6_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33526,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage6_Attention	[label=Q_local,
		lp="33807,4281.9",
		pos="e,33711,3536.6 33711,5000.7 33711,5000.7 33711,3546.6 33711,3546.6"];
	Layer3_Device6_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30587,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage7_Attention	[label=Q_local,
		lp="30266,4185.4",
		pos="e,30355,3325 32847,5034 31916,5034 30311,5034 30311,5034 30311,5034 30311,3325 30311,3325 30311,3325 30345,3325 30345,3325"];
	Layer3_Device6_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33302,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage8_Attention	[label=Q_local,
		lp="33925,4088.9",
		pos="e,33534,3123 33808,5000.8 33808,4770.7 33808,3123 33808,3123 33808,3123 33544,3123 33544,3123"];
	Layer3_Device6_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30363,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage9_Attention	[label=Q_local,
		lp="30150,3992.4",
		pos="e,30288,2957.6 32847,5040 31909,5040 30288,5040 30288,5040 30288,5040 30288,2967.6 30288,2967.6"];
	Layer3_Device6_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="30568,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage10_Attention	[label=Q_local,
		lp="30013,3895.9",
		pos="e,30336,2737 32847,5047 31866,5047 30119,5047 30119,5047 30119,5047 30119,2737 30119,2737 30119,2737 30326,2737 30326,2737"];
	Layer3_Device6_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31831,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage11_Attention	[label=Q_local,
		lp="34034,3799.4",
		pos="e,31908,2571.5 33859,5000.8 33859,4738.8 33859,2626 33859,2626 33859,2626 31908,2626 31908,2626 31908,2626 31908,2581.5 31908,2581.5"];
	Layer3_Device6_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="32070,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage12_Attention	[label=Q_local,
		lp="34173,3702.9",
		pos="e,32091,2378.5 33909,5000.8 33909,4738.6 33909,2624 33909,2624 33909,2624 32091,2624 32091,2624 32091,2624 32091,2388.5 32091,2388.5"];
	Layer3_Device6_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="34085,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage13_Attention	[label=Q_local,
		lp="34313,3606.4",
		pos="e,34258,2185.8 34147,5012 34213,5012 34258,5012 34258,5012 34258,5012 34258,2195.8 34258,2195.8"];
	Layer3_Device6_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="31667,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage14_Attention	[label=Q_local,
		lp="29892,3509.9",
		pos="e,31744,1992.6 34147,5024 34266,5024 34353,5024 34353,5024 34353,5024 34353,2101 34353,2101 34353,2101 31744,2101 31744,2101 31744,\
2101 31744,2002.6 31744,2002.6"];
	Layer3_Device6_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="33800,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_QKVProj -> Layer3_Device6_Stage15_Attention	[label=Q_local,
		lp="34431,3413.4",
		pos="e,34032,1822 34147,5030 34285,5030 34390,5030 34390,5030 34390,5030 34390,1822 34390,1822 34390,1822 34042,1822 34042,1822"];
	Layer3_Device6_Stage0_RecvKV -> Layer3_Device6_Stage0_Attention	[pos="e,32972,4694.7 32972,4857.6 32972,4857.6 32972,4704.7 32972,4704.7"];
	Layer3_Device6_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="31795,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage0_RecvKV -> Layer3_Device6_Stage1_RecvKV	[label="Ring transfer",
		lp="32152,4764.4",
		pos="e,32023,4721 32023,4807.7 32023,4807.7 32023,4731 32023,4731"];
	Layer3_Device6_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="31232,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage0_Attention -> Layer3_Device6_Stage0_Accumulate	[pos="e,31232,4501.4 33004,4641.1 33004,4604.4 33004,4543 33004,4543 33004,4543 31232,4543 31232,4543 31232,4543 31232,4511.4 31232,4511.4"];
	Layer3_Device6_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30779,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage0_Accumulate -> Layer3_Device6_Stage1_Accumulate	[pos="e,31006,4308.5 31006,4448.1 31006,4448.1 31006,4318.5 31006,4318.5"];
	Layer3_Device6_Stage1_RecvKV -> Layer3_Device6_Stage1_Attention	[pos="e,30909,4501.8 30909,4667.8 30909,4667.8 30909,4511.8 30909,4511.8"];
	Layer3_Device6_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32441,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage1_RecvKV -> Layer3_Device6_Stage2_RecvKV	[label="Ring transfer",
		lp="32112,4571.4",
		pos="e,32118,4528 32118,4614.7 32118,4614.7 32118,4538 32118,4538"];
	Layer3_Device6_Stage1_Attention -> Layer3_Device6_Stage1_Accumulate	[pos="e,30745,4308.5 30745,4448.1 30745,4448.1 30745,4318.5 30745,4318.5"];
	Layer3_Device6_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="30897,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage1_Accumulate -> Layer3_Device6_Stage2_Accumulate	[pos="e,30838,4115.5 30838,4255.1 30838,4255.1 30838,4125.5 30838,4125.5"];
	Layer3_Device6_Stage2_RecvKV -> Layer3_Device6_Stage2_Attention	[pos="e,31526,4308.8 31526,4474.8 31526,4474.8 31526,4318.8 31526,4318.8"];
	Layer3_Device6_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32509,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage2_RecvKV -> Layer3_Device6_Stage3_RecvKV	[label="Ring transfer",
		lp="32523,4378.4",
		pos="e,32475,4335 32475,4421.7 32475,4421.7 32475,4345 32475,4345"];
	Layer3_Device6_Stage2_Attention -> Layer3_Device6_Stage2_Accumulate	[pos="e,31098,4115.5 31098,4255.1 31098,4255.1 31098,4125.5 31098,4125.5"];
	Layer3_Device6_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31086,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage2_Accumulate -> Layer3_Device6_Stage3_Accumulate	[pos="e,30992,3922.5 30992,4062.1 30992,4062.1 30992,3932.5 30992,3932.5"];
	Layer3_Device6_Stage3_RecvKV -> Layer3_Device6_Stage3_Attention	[pos="e,33256,4115.8 33256,4281.8 33256,4281.8 33256,4125.8 33256,4125.8"];
	Layer3_Device6_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32106,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage3_RecvKV -> Layer3_Device6_Stage4_RecvKV	[label="Ring transfer",
		lp="32355,4185.4",
		pos="e,32308,4142 32308,4228.7 32308,4228.7 32308,4152 32308,4152"];
	Layer3_Device6_Stage3_Attention -> Layer3_Device6_Stage3_Accumulate	[pos="e,31252,3922.7 33149,4062.3 33149,4027 33149,3969 33149,3969 33149,3969 31252,3969 31252,3969 31252,3969 31252,3932.7 31252,3932.7"];
	Layer3_Device6_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31015,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage3_Accumulate -> Layer3_Device6_Stage4_Accumulate	[pos="e,31050,3729.5 31050,3869.1 31050,3869.1 31050,3739.5 31050,3739.5"];
	Layer3_Device6_Stage4_RecvKV -> Layer3_Device6_Stage4_Attention	[pos="e,30731,3922.6 31233,4049 30961,4049 30731,4049 30731,4049 30731,4049 30731,3932.6 30731,3932.6"];
	Layer3_Device6_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32295,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage4_RecvKV -> Layer3_Device6_Stage5_RecvKV	[label="Ring transfer",
		lp="32248,3992.4",
		pos="e,32200,3949 32200,4035.7 32200,4035.7 32200,3959 32200,3959"];
	Layer3_Device6_Stage4_Attention -> Layer3_Device6_Stage4_Accumulate	[pos="e,30790,3729.5 30790,3869.1 30790,3869.1 30790,3739.5 30790,3739.5"];
	Layer3_Device6_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="33005,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage4_Accumulate -> Layer3_Device6_Stage5_Accumulate	[pos="e,32959,3536.7 31061,3676.2 31061,3658.7 31061,3639 31061,3639 31061,3639 32959,3639 32959,3639 32959,3639 32959,3546.7 32959,3546.7"];
	Layer3_Device6_Stage5_RecvKV -> Layer3_Device6_Stage5_Attention	[pos="e,33208,3729.8 33208,3895.8 33208,3895.8 33208,3739.8 33208,3739.8"];
	Layer3_Device6_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="32224,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage5_RecvKV -> Layer3_Device6_Stage6_RecvKV	[label="Ring transfer",
		lp="32307,3799.4",
		pos="e,32260,3756 32260,3842.7 32260,3842.7 32260,3766 32260,3766"];
	Layer3_Device6_Stage5_Attention -> Layer3_Device6_Stage5_Accumulate	[pos="e,33219,3536.5 33219,3676.1 33219,3676.1 33219,3546.5 33219,3546.5"];
	Layer3_Device6_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="33005,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage5_Accumulate -> Layer3_Device6_Stage6_Accumulate	[pos="e,33005,3343.5 33005,3483.1 33005,3483.1 33005,3353.5 33005,3353.5"];
	Layer3_Device6_Stage6_RecvKV -> Layer3_Device6_Stage6_Attention	[pos="e,33480,3536.6 32814,3663 33140,3663 33480,3663 33480,3663 33480,3663 33480,3546.6 33480,3546.6"];
	Layer3_Device6_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31796,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage6_RecvKV -> Layer3_Device6_Stage7_RecvKV	[label="Ring transfer",
		lp="32058,3606.4",
		pos="e,32010,3563 32010,3649.7 32010,3649.7 32010,3573 32010,3573"];
	Layer3_Device6_Stage6_Attention -> Layer3_Device6_Stage6_Accumulate	[pos="e,33237,3308 33414,3483.1 33414,3428.2 33414,3308 33414,3308 33414,3308 33247,3308 33247,3308"];
	Layer3_Device6_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32781,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage6_Accumulate -> Layer3_Device6_Stage7_Accumulate	[pos="e,32893,3150.5 32893,3290.1 32893,3290.1 32893,3160.5 32893,3160.5"];
	Layer3_Device6_Stage7_RecvKV -> Layer3_Device6_Stage7_Attention	[pos="e,30801,3343.6 30923,3470 30848,3470 30801,3470 30801,3470 30801,3470 30801,3353.6 30801,3353.6"];
	Layer3_Device6_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31796,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage7_RecvKV -> Layer3_Device6_Stage8_RecvKV	[label="Ring transfer",
		lp="31844,3413.4",
		pos="e,31796,3370 31796,3456.7 31796,3456.7 31796,3380 31796,3380"];
	Layer3_Device6_Stage7_Attention -> Layer3_Device6_Stage7_Accumulate	[pos="e,32633,3150.7 30735,3290.3 30735,3252.3 30735,3187 30735,3187 30735,3187 32633,3187 32633,3187 32633,3187 32633,3160.7 32633,3160.7"];
	Layer3_Device6_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32781,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage7_Accumulate -> Layer3_Device6_Stage8_Accumulate	[pos="e,32781,2957.5 32781,3097.1 32781,3097.1 32781,2967.5 32781,2967.5"];
	Layer3_Device6_Stage8_RecvKV -> Layer3_Device6_Stage8_Attention	[pos="e,33154,3150.6 32386,3277 32752,3277 33154,3277 33154,3277 33154,3277 33154,3160.6 33154,3160.6"];
	Layer3_Device6_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31572,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage8_RecvKV -> Layer3_Device6_Stage9_RecvKV	[label="Ring transfer",
		lp="31732,3220.4",
		pos="e,31684,3177 31684,3263.7 31684,3263.7 31684,3187 31684,3187"];
	Layer3_Device6_Stage8_Attention -> Layer3_Device6_Stage8_Accumulate	[pos="e,33013,2930 33144,3097.4 33144,3044.2 33144,2930 33144,2930 33144,2930 33023,2930 33023,2930"];
	Layer3_Device6_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31089,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage8_Accumulate -> Layer3_Device6_Stage9_Accumulate	[pos="e,31166,2764.6 32781,2904.2 32781,2875.2 32781,2833 32781,2833 32781,2833 31166,2833 31166,2833 31166,2833 31166,2774.6 31166,2774.6"];
	Layer3_Device6_Stage9_RecvKV -> Layer3_Device6_Stage9_Attention	[pos="e,30475,2957.6 30699,3084 30567,3084 30475,3084 30475,3084 30475,3084 30475,2967.6 30475,2967.6"];
	Layer3_Device6_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="31572,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage9_RecvKV -> Layer3_Device6_Stage10_RecvKV	[label="Ring transfer",
		lp="31620,3027.4",
		pos="e,31572,2984 31572,3070.7 31572,3070.7 31572,2994 31572,2994"];
	Layer3_Device6_Stage9_Attention -> Layer3_Device6_Stage9_Accumulate	[pos="e,31012,2764.4 30466,2904.3 30466,2867.9 30466,2807 30466,2807 30466,2807 31012,2807 31012,2807 31012,2807 31012,2774.4 31012,2774.4"];
	Layer3_Device6_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31089,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage9_Accumulate -> Layer3_Device6_Stage10_Accumulate	[pos="e,31089,2571.5 31089,2711.1 31089,2711.1 31089,2581.5 31089,2581.5"];
	Layer3_Device6_Stage10_RecvKV -> Layer3_Device6_Stage10_Attention	[pos="e,30726,2764.8 30726,2930.8 30726,2930.8 30726,2774.8 30726,2774.8"];
	Layer3_Device6_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32298,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage10_RecvKV -> Layer3_Device6_Stage11_RecvKV	[label="Ring transfer",
		lp="31889,2834.4",
		pos="e,31935,2791 31935,2877.7 31935,2877.7 31935,2801 31935,2801"];
	Layer3_Device6_Stage10_Attention -> Layer3_Device6_Stage10_Accumulate	[pos="e,30857,2544 30652,2711.4 30652,2658.2 30652,2544 30652,2544 30652,2544 30847,2544 30847,2544"];
	Layer3_Device6_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31549,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage10_Accumulate -> Layer3_Device6_Stage11_Accumulate	[pos="e,31349,2378.6 31321,2544 31338,2544 31349,2544 31349,2544 31349,2544 31349,2388.6 31349,2388.6"];
	Layer3_Device6_Stage11_RecvKV -> Layer3_Device6_Stage11_Attention	[pos="e,31754,2571.6 31754,2684.7 31754,2684.7 31754,2581.6 31754,2581.6"];
	Layer3_Device6_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33040,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage11_RecvKV -> Layer3_Device6_Stage12_RecvKV	[label="Ring transfer",
		lp="32775,2641.4",
		pos="e,32669,2598 32669,2684.7 32669,2684.7 32669,2608 32669,2608"];
	Layer3_Device6_Stage11_Attention -> Layer3_Device6_Stage11_Accumulate	[pos="e,31690,2378.5 31690,2518.1 31690,2518.1 31690,2388.5 31690,2388.5"];
	Layer3_Device6_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="31667,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage11_Accumulate -> Layer3_Device6_Stage12_Accumulate	[pos="e,31608,2185.5 31608,2325.1 31608,2325.1 31608,2195.5 31608,2195.5"];
	Layer3_Device6_Stage12_RecvKV -> Layer3_Device6_Stage12_Attention	[pos="e,32211,2378.6 32211,2491.7 32211,2491.7 32211,2388.6 32211,2388.6"];
	Layer3_Device6_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33279,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage12_RecvKV -> Layer3_Device6_Stage13_RecvKV	[label="Ring transfer",
		lp="33171,2448.4",
		pos="e,33160,2405 33160,2491.7 33160,2491.7 33160,2415 33160,2415"];
	Layer3_Device6_Stage12_Attention -> Layer3_Device6_Stage12_Accumulate	[pos="e,31868,2185.5 31868,2325.1 31868,2325.1 31868,2195.5 31868,2195.5"];
	Layer3_Device6_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32188,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage12_Accumulate -> Layer3_Device6_Stage13_Accumulate	[pos="e,31956,1965 31899,2158 31916,2158 31927,2158 31927,2158 31927,2158 31927,1965 31927,1965 31927,1965 31946,1965 31946,1965"];
	Layer3_Device6_Stage13_RecvKV -> Layer3_Device6_Stage13_Attention	[pos="e,34026,2185.8 34026,2351.8 34026,2351.8 34026,2195.8 34026,2195.8"];
	Layer3_Device6_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="32876,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage13_RecvKV -> Layer3_Device6_Stage14_RecvKV	[label="Ring transfer",
		lp="33193,2255.4",
		pos="e,33078,2212 33078,2298.7 33078,2298.7 33078,2222 33078,2222"];
	Layer3_Device6_Stage13_Attention -> Layer3_Device6_Stage13_Accumulate	[pos="e,32188,1992.5 34085,2132.3 34085,2100 34085,2050 34085,2050 34085,2050 32188,2050 32188,2050 32188,2050 32188,2002.5 32188,2002.5"];
	Layer3_Device6_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="32188,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage13_Accumulate -> Layer3_Device6_Stage14_Accumulate	[pos="e,32188,1839.9 32188,1939.3 32188,1939.3 32188,1849.9 32188,1849.9"];
	Layer3_Device6_Stage14_RecvKV -> Layer3_Device6_Stage14_Attention	[pos="e,31590,1992.6 32003,2119 31774,2119 31590,2119 31590,2119 31590,2119 31590,2002.6 31590,2002.6"];
	Layer3_Device6_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="33397,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device6_Stage14_RecvKV -> Layer3_Device6_Stage15_RecvKV	[label="Ring transfer",
		lp="33184,2062.4",
		pos="e,33136,2019 33136,2105.7 33136,2105.7 33136,2029 33136,2029"];
	Layer3_Device6_Stage14_Attention -> Layer3_Device6_Stage14_Accumulate	[pos="e,31956,1804 31667,1939.2 31667,1893.2 31667,1804 31667,1804 31667,1804 31946,1804 31946,1804"];
	Layer3_Device6_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="33800,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device6_Stage14_Accumulate -> Layer3_Device6_Stage15_Accumulate	[pos="e,33568,1687 32188,1786.6 32188,1749.5 32188,1687 32188,1687 32188,1687 33558,1687 33558,1687"];
	Layer3_Device6_Stage15_RecvKV -> Layer3_Device6_Stage15_Attention	[pos="e,33800,1840 33800,1912.6 33800,1912.6 33800,1850 33800,1850"];
	Layer3_Device6_Stage15_Attention -> Layer3_Device6_Stage15_Accumulate	[pos="e,33800,1714.1 33800,1786.7 33800,1786.7 33800,1724.1 33800,1724.1"];
	Layer3_Device6_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="33804,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device6_Stage15_Accumulate -> Layer3_Device6_ConcatHeads	[pos="e,33804,1588.1 33804,1660.7 33804,1660.7 33804,1598.1 33804,1598.1"];
	Layer3_Device6_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34213,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device6_ConcatHeads -> Layer3_Device6_OutputProj	[pos="e,34015,1462.1 34015,1534.7 34015,1534.7 34015,1472.1 34015,1472.1"];
	Layer3_Device6_OutputProj -> Layer3_Device6_Residual1	[pos="e,34281,1336.1 34281,1408.7 34281,1408.7 34281,1346.1 34281,1346.1"];
	Layer3_Device6_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34523,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device6_Residual1 -> Layer3_Device6_LayerNorm2	[pos="e,34523,1210.1 34523,1282.7 34523,1282.7 34523,1220.1 34523,1220.1"];
	Layer3_Device6_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="35083,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device6_Residual1 -> Layer3_Device6_Residual2	[pos="e,35040,528.28 34910,1309 34984,1309 35040,1309 35040,1309 35040,1309 35040,538.28 35040,538.28"];
	Layer3_Device6_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="34366,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device6_LayerNorm2 -> Layer3_Device6_GateProj	[pos="e,34450,1084.1 34450,1156.7 34450,1156.7 34450,1094.1 34450,1094.1"];
	Layer3_Device6_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="34679,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device6_LayerNorm2 -> Layer3_Device6_UpProj	[pos="e,34664,995.2 34664,1156.6 34664,1156.6 34664,1005.2 34664,1005.2"];
	Layer3_Device6_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="34366,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device6_GateProj -> Layer3_Device6_Activation	[pos="e,34298,905.95 34298,1030.8 34298,1030.8 34298,915.95 34298,915.95"];
	Layer3_Device6_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="34585,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device6_UpProj -> Layer3_Device6_ElemMul	[pos="e,34748,780.2 34748,941.61 34748,941.61 34748,790.2 34748,790.2"];
	Layer3_Device6_Activation -> Layer3_Device6_ElemMul	[pos="e,34383,780.09 34383,852.69 34383,852.69 34383,790.09 34383,790.09"];
	Layer3_Device6_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="34681,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device6_ElemMul -> Layer3_Device6_DownProj	[pos="e,34681,654.09 34681,726.69 34681,726.69 34681,664.09 34681,664.09"];
	Layer3_Device6_DownProj -> Layer3_Device6_Residual2	[pos="e,34798,528.09 34798,600.69 34798,600.69 34798,538.09 34798,538.09"];
	Layer3_Device6_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 6 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="37550,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device6_Residual2 -> Layer3_Device6_Output	[pos="e,37257,373.39 35470,501 36095,501 37257,501 37257,501 37257,501 37257,383.39 37257,383.39"];
	Layer3_Device6_Output -> Sequence_Aggregate	[pos="e,40316,240 37550,326.76 37550,290.14 37550,240 37550,240 37550,240 40306,240 40306,240"];
	Layer3_Device7_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38689,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device7_Input -> Layer3_Device7_LayerNorm1	[pos="e,38902,5153 38982,5283.8 38982,5256.3 38982,5153 38982,5153 38982,5153 38912,5153 38912,5153"];
	Layer3_Device7_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="39311,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device7_Input -> Layer3_Device7_Residual1	[pos="e,39399,1336 39399,5256 39399,5256 39399,1346 39399,1346"];
	Layer3_Device7_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38297,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device7_LayerNorm1 -> Layer3_Device7_QKVProj	[pos="e,38689,5054.1 38689,5126.7 38689,5126.7 38689,5064.1 38689,5064.1"];
	Layer3_Device7_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37051,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage0_RecvKV	[label="Local K,V",
		lp="37552,4957.4",
		pos="e,37809,4914.2 37809,5000.7 37809,5000.7 37809,4924.2 37809,4924.2"];
	Layer3_Device7_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="37804,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage0_Attention	[label=Q_local,
		lp="38113,4860.9",
		pos="e,38003,4694.8 38003,5000.7 38003,5000.7 38003,4704.8 38003,4704.8"];
	Layer3_Device7_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35511,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage1_Attention	[label=Q_local,
		lp="35575,4764.4",
		pos="e,35580,4501.6 37647,5006 36844,5006 35580,5006 35580,5006 35580,5006 35580,4511.6 35580,4511.6"];
	Layer3_Device7_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36100,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage2_Attention	[label=Q_local,
		lp="38253,4667.9",
		pos="e,36292,4308.7 38098,5000.8 38098,4899.9 38098,4546 38098,4546 38098,4546 36292,4546 36292,4546 36292,4546 36292,4318.7 36292,4318.7"];
	Layer3_Device7_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38115,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage3_Attention	[label=Q_local,
		lp="38393,4571.4",
		pos="e,38288,4115.7 38288,5000.8 38288,5000.8 38288,4125.7 38288,4125.7"];
	Layer3_Device7_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35365,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage4_Attention	[label=Q_local,
		lp="35195,4474.9",
		pos="e,35206,3922.5 37647,5018 36741,5018 35206,5018 35206,5018 35206,5018 35206,3932.5 35206,3932.5"];
	Layer3_Device7_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38233,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage5_Attention	[label=Q_local,
		lp="38473,4378.4",
		pos="e,38406,3729.7 38406,5000.6 38406,5000.6 38406,3739.7 38406,3739.7"];
	Layer3_Device7_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38300,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage6_Attention	[label=Q_local,
		lp="38607,4281.9",
		pos="e,38498,3536.6 38498,5000.7 38498,5000.7 38498,3546.6 38498,3546.6"];
	Layer3_Device7_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35361,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage7_Attention	[label=Q_local,
		lp="35052,4185.4",
		pos="e,35129,3325 37647,5036 36706,5036 35074,5036 35074,5036 35074,5036 35074,3325 35074,3325 35074,3325 35119,3325 35119,3325"];
	Layer3_Device7_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38128,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage8_Attention	[label=Q_local,
		lp="38725,4088.9",
		pos="e,38360,3123 38571,5000.8 38571,4770.7 38571,3123 38571,3123 38571,3123 38370,3123 38370,3123"];
	Layer3_Device7_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35189,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage9_Attention	[label=Q_local,
		lp="34942,3992.4",
		pos="e,35020,2957.6 37647,5042 36691,5042 35020,5042 35020,5042 35020,5042 35020,2967.6 35020,2967.6"];
	Layer3_Device7_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="35315,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage10_Attention	[label=Q_local,
		lp="34813,3895.9",
		pos="e,35083,2737 37647,5048 36670,5048 34934,5048 34934,5048 34934,5048 34934,2737 34934,2737 34934,2737 35073,2737 35073,2737"];
	Layer3_Device7_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36598,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage11_Attention	[label=Q_local,
		lp="38829,3799.4",
		pos="e,36675,2571.5 38649,5000.7 38649,4739 38649,2646 38649,2646 38649,2646 36675,2646 36675,2646 36675,2646 36675,2581.5 36675,2581.5"];
	Layer3_Device7_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36858,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage12_Attention	[label=Q_local,
		lp="38961,3702.9",
		pos="e,36858,2378.5 38688,5000.6 38688,4738.7 38688,2643 38688,2643 38688,2643 36858,2643 36858,2643 36858,2643 36858,2388.5 36858,2388.5"];
	Layer3_Device7_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38873,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage13_Attention	[label=Q_local,
		lp="39101,3606.4",
		pos="e,39046,2185.8 38947,5014 39007,5014 39046,5014 39046,5014 39046,5014 39046,2195.8 39046,2195.8"];
	Layer3_Device7_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="36455,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage14_Attention	[label=Q_local,
		lp="34695,3509.9",
		pos="e,36223,1965 38610,5000.6 38610,4751.4 38610,2847 38610,2847 38610,2847 36086,2847 36086,2847 36086,2847 36086,1965 36086,1965 36086,\
1965 36213,1965 36213,1965"];
	Layer3_Device7_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="38439,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_QKVProj -> Layer3_Device7_Stage15_Attention	[label=Q_local,
		lp="39219,3413.4",
		pos="e,38671,1822 38947,5027 39070,5027 39161,5027 39161,5027 39161,5027 39161,1822 39161,1822 39161,1822 38681,1822 38681,1822"];
	Layer3_Device7_Stage0_RecvKV -> Layer3_Device7_Stage0_Attention	[pos="e,37772,4694.7 37772,4857.6 37772,4857.6 37772,4704.7 37772,4704.7"];
	Layer3_Device7_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36595,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage0_RecvKV -> Layer3_Device7_Stage1_RecvKV	[label="Ring transfer",
		lp="36871,4764.4",
		pos="e,36823,4721 36823,4807.7 36823,4807.7 36823,4731 36823,4731"];
	Layer3_Device7_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="36032,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage0_Attention -> Layer3_Device7_Stage0_Accumulate	[pos="e,36032,4501.7 37804,4641.3 37804,4606 37804,4548 37804,4548 37804,4548 36032,4548 36032,4548 36032,4548 36032,4511.7 36032,4511.7"];
	Layer3_Device7_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35579,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage0_Accumulate -> Layer3_Device7_Stage1_Accumulate	[pos="e,35806,4308.5 35806,4448.1 35806,4448.1 35806,4318.5 35806,4318.5"];
	Layer3_Device7_Stage1_RecvKV -> Layer3_Device7_Stage1_Attention	[pos="e,35709,4501.8 35709,4667.8 35709,4667.8 35709,4511.8 35709,4511.8"];
	Layer3_Device7_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37241,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage1_RecvKV -> Layer3_Device7_Stage2_RecvKV	[label="Ring transfer",
		lp="36966,4571.4",
		pos="e,36918,4528 36918,4614.7 36918,4614.7 36918,4538 36918,4538"];
	Layer3_Device7_Stage1_Attention -> Layer3_Device7_Stage1_Accumulate	[pos="e,35545,4308.5 35545,4448.1 35545,4448.1 35545,4318.5 35545,4318.5"];
	Layer3_Device7_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35697,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage1_Accumulate -> Layer3_Device7_Stage2_Accumulate	[pos="e,35638,4115.5 35638,4255.1 35638,4255.1 35638,4125.5 35638,4125.5"];
	Layer3_Device7_Stage2_RecvKV -> Layer3_Device7_Stage2_Attention	[pos="e,36326,4308.8 36326,4474.8 36326,4474.8 36326,4318.8 36326,4318.8"];
	Layer3_Device7_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37309,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage2_RecvKV -> Layer3_Device7_Stage3_RecvKV	[label="Ring transfer",
		lp="37323,4378.4",
		pos="e,37275,4335 37275,4421.7 37275,4421.7 37275,4345 37275,4345"];
	Layer3_Device7_Stage2_Attention -> Layer3_Device7_Stage2_Accumulate	[pos="e,35898,4115.5 35898,4255.1 35898,4255.1 35898,4125.5 35898,4125.5"];
	Layer3_Device7_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35886,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage2_Accumulate -> Layer3_Device7_Stage3_Accumulate	[pos="e,35792,3922.5 35792,4062.1 35792,4062.1 35792,3932.5 35792,3932.5"];
	Layer3_Device7_Stage3_RecvKV -> Layer3_Device7_Stage3_Attention	[pos="e,38056,4115.8 38056,4281.8 38056,4281.8 38056,4125.8 38056,4125.8"];
	Layer3_Device7_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36906,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage3_RecvKV -> Layer3_Device7_Stage4_RecvKV	[label="Ring transfer",
		lp="37223,4185.4",
		pos="e,37108,4142 37108,4228.7 37108,4228.7 37108,4152 37108,4152"];
	Layer3_Device7_Stage3_Attention -> Layer3_Device7_Stage3_Accumulate	[pos="e,36052,3922.8 37949,4062.3 37949,4028.3 37949,3974 37949,3974 37949,3974 36052,3974 36052,3974 36052,3974 36052,3932.8 36052,3932.8"];
	Layer3_Device7_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35815,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage3_Accumulate -> Layer3_Device7_Stage4_Accumulate	[pos="e,35850,3729.5 35850,3869.1 35850,3869.1 35850,3739.5 35850,3739.5"];
	Layer3_Device7_Stage4_RecvKV -> Layer3_Device7_Stage4_Attention	[pos="e,35531,3922.6 36033,4049 35761,4049 35531,4049 35531,4049 35531,4049 35531,3932.6 35531,3932.6"];
	Layer3_Device7_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37095,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage4_RecvKV -> Layer3_Device7_Stage5_RecvKV	[label="Ring transfer",
		lp="37049,3992.4",
		pos="e,37000,3949 37000,4035.7 37000,4035.7 37000,3959 37000,3959"];
	Layer3_Device7_Stage4_Attention -> Layer3_Device7_Stage4_Accumulate	[pos="e,35590,3729.5 35590,3869.1 35590,3869.1 35590,3739.5 35590,3739.5"];
	Layer3_Device7_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37779,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage4_Accumulate -> Layer3_Device7_Stage5_Accumulate	[pos="e,37746,3536.6 35848,3676.2 35848,3655.1 35848,3629 35848,3629 35848,3629 37746,3629 37746,3629 37746,3629 37746,3546.6 37746,3546.6"];
	Layer3_Device7_Stage5_RecvKV -> Layer3_Device7_Stage5_Attention	[pos="e,38008,3729.8 38008,3895.8 38008,3895.8 38008,3739.8 38008,3739.8"];
	Layer3_Device7_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="37024,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage5_RecvKV -> Layer3_Device7_Stage6_RecvKV	[label="Ring transfer",
		lp="37107,3799.4",
		pos="e,37060,3756 37060,3842.7 37060,3842.7 37060,3766 37060,3766"];
	Layer3_Device7_Stage5_Attention -> Layer3_Device7_Stage5_Accumulate	[pos="e,38006,3536.5 38006,3676.1 38006,3676.1 38006,3546.5 38006,3546.5"];
	Layer3_Device7_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37779,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage5_Accumulate -> Layer3_Device7_Stage6_Accumulate	[pos="e,37779,3343.5 37779,3483.1 37779,3483.1 37779,3353.5 37779,3353.5"];
	Layer3_Device7_Stage6_RecvKV -> Layer3_Device7_Stage6_Attention	[pos="e,38266,3536.6 37614,3663 37935,3663 38266,3663 38266,3663 38266,3663 38266,3546.6 38266,3546.6"];
	Layer3_Device7_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="36570,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage6_RecvKV -> Layer3_Device7_Stage7_RecvKV	[label="Ring transfer",
		lp="36779,3606.4",
		pos="e,36797,3563 36797,3649.7 36797,3649.7 36797,3573 36797,3573"];
	Layer3_Device7_Stage6_Attention -> Layer3_Device7_Stage6_Accumulate	[pos="e,38011,3316 38214,3483.4 38214,3430.2 38214,3316 38214,3316 38214,3316 38021,3316 38021,3316"];
	Layer3_Device7_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37607,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage6_Accumulate -> Layer3_Device7_Stage7_Accumulate	[pos="e,37693,3150.5 37693,3290.1 37693,3290.1 37693,3160.5 37693,3160.5"];
	Layer3_Device7_Stage7_RecvKV -> Layer3_Device7_Stage7_Attention	[pos="e,35588,3343.6 35696,3470 35630,3470 35588,3470 35588,3470 35588,3470 35588,3353.6 35588,3353.6"];
	Layer3_Device7_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36570,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage7_RecvKV -> Layer3_Device7_Stage8_RecvKV	[label="Ring transfer",
		lp="36618,3413.4",
		pos="e,36570,3370 36570,3456.7 36570,3456.7 36570,3380 36570,3380"];
	Layer3_Device7_Stage7_Attention -> Layer3_Device7_Stage7_Accumulate	[pos="e,37433,3150.4 35535,3290.2 35535,3264 35535,3228 35535,3228 35535,3228 37433,3228 37433,3228 37433,3228 37433,3160.4 37433,3160.4"];
	Layer3_Device7_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="37607,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage7_Accumulate -> Layer3_Device7_Stage8_Accumulate	[pos="e,37607,2957.5 37607,3097.1 37607,3097.1 37607,2967.5 37607,2967.5"];
	Layer3_Device7_Stage8_RecvKV -> Layer3_Device7_Stage8_Attention	[pos="e,37954,3150.6 37160,3277 37536,3277 37954,3277 37954,3277 37954,3277 37954,3160.6 37954,3160.6"];
	Layer3_Device7_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36398,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage8_RecvKV -> Layer3_Device7_Stage9_RecvKV	[label="Ring transfer",
		lp="36492,3220.4",
		pos="e,36484,3177 36484,3263.7 36484,3263.7 36484,3187 36484,3187"];
	Layer3_Device7_Stage8_Attention -> Layer3_Device7_Stage8_Accumulate	[pos="e,37839,2930 37931,3097.4 37931,3044.2 37931,2930 37931,2930 37931,2930 37849,2930 37849,2930"];
	Layer3_Device7_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35836,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage8_Accumulate -> Layer3_Device7_Stage9_Accumulate	[pos="e,35913,2764.5 37607,2904.1 37607,2880.2 37607,2849 37607,2849 37607,2849 35913,2849 35913,2849 35913,2849 35913,2774.5 35913,2774.5"];
	Layer3_Device7_Stage9_RecvKV -> Layer3_Device7_Stage9_Attention	[pos="e,35275,2957.6 35525,3084 35379,3084 35275,3084 35275,3084 35275,3084 35275,2967.6 35275,2967.6"];
	Layer3_Device7_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="36398,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage9_RecvKV -> Layer3_Device7_Stage10_RecvKV	[label="Ring transfer",
		lp="36446,3027.4",
		pos="e,36398,2984 36398,3070.7 36398,3070.7 36398,2994 36398,2994"];
	Layer3_Device7_Stage9_Attention -> Layer3_Device7_Stage9_Accumulate	[pos="e,35759,2764.6 35252,2904.1 35252,2860 35252,2777 35252,2777 35252,2777 35759,2777 35759,2777 35759,2777 35759,2774.6 35759,2774.6"];
	Layer3_Device7_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="35836,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage9_Accumulate -> Layer3_Device7_Stage10_Accumulate	[pos="e,35836,2571.5 35836,2711.1 35836,2711.1 35836,2581.5 35836,2581.5"];
	Layer3_Device7_Stage10_RecvKV -> Layer3_Device7_Stage10_Attention	[pos="e,35512,2764.8 35512,2930.8 35512,2930.8 35512,2774.8 35512,2774.8"];
	Layer3_Device7_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37045,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage10_RecvKV -> Layer3_Device7_Stage11_RecvKV	[label="Ring transfer",
		lp="36715,2834.4",
		pos="e,36722,2791 36722,2877.7 36722,2877.7 36722,2801 36722,2801"];
	Layer3_Device7_Stage10_Attention -> Layer3_Device7_Stage10_Accumulate	[pos="e,35604,2544 35508,2711.4 35508,2658.2 35508,2544 35508,2544 35508,2544 35594,2544 35594,2544"];
	Layer3_Device7_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36337,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage10_Accumulate -> Layer3_Device7_Stage11_Accumulate	[pos="e,36115,2378.6 36068,2544 36096,2544 36115,2544 36115,2544 36115,2544 36115,2388.6 36115,2388.6"];
	Layer3_Device7_Stage11_RecvKV -> Layer3_Device7_Stage11_Attention	[pos="e,36521,2571.6 36521,2684.7 36521,2684.7 36521,2581.6 36521,2581.6"];
	Layer3_Device7_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37807,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage11_RecvKV -> Layer3_Device7_Stage12_RecvKV	[label="Ring transfer",
		lp="37228,2641.4",
		pos="e,37426,2598 37426,2684.7 37426,2684.7 37426,2608 37426,2608"];
	Layer3_Device7_Stage11_Attention -> Layer3_Device7_Stage11_Accumulate	[pos="e,36468,2378.5 36468,2518.1 36468,2518.1 36468,2388.5 36468,2388.5"];
	Layer3_Device7_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36455,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage11_Accumulate -> Layer3_Device7_Stage12_Accumulate	[pos="e,36396,2185.5 36396,2325.1 36396,2325.1 36396,2195.5 36396,2195.5"];
	Layer3_Device7_Stage12_RecvKV -> Layer3_Device7_Stage12_Attention	[pos="e,36988,2378.6 36988,2491.7 36988,2491.7 36988,2388.6 36988,2388.6"];
	Layer3_Device7_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="38067,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage12_RecvKV -> Layer3_Device7_Stage13_RecvKV	[label="Ring transfer",
		lp="38092,2448.4",
		pos="e,37937,2405 37937,2491.7 37937,2491.7 37937,2415 37937,2415"];
	Layer3_Device7_Stage12_Attention -> Layer3_Device7_Stage12_Accumulate	[pos="e,36656,2185.5 36656,2325.1 36656,2325.1 36656,2195.5 36656,2195.5"];
	Layer3_Device7_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36976,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage12_Accumulate -> Layer3_Device7_Stage13_Accumulate	[pos="e,36744,1965 36687,2158 36704,2158 36715,2158 36715,2158 36715,2158 36715,1965 36715,1965 36715,1965 36734,1965 36734,1965"];
	Layer3_Device7_Stage13_RecvKV -> Layer3_Device7_Stage13_Attention	[pos="e,38814,2185.8 38814,2351.8 38814,2351.8 38814,2195.8 38814,2195.8"];
	Layer3_Device7_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="37664,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage13_RecvKV -> Layer3_Device7_Stage14_RecvKV	[label="Ring transfer",
		lp="37848,2255.4",
		pos="e,37866,2212 37866,2298.7 37866,2298.7 37866,2222 37866,2222"];
	Layer3_Device7_Stage13_Attention -> Layer3_Device7_Stage13_Accumulate	[pos="e,36976,1992.6 38873,2132.1 38873,2101.4 38873,2055 38873,2055 38873,2055 36976,2055 36976,2055 36976,2055 36976,2002.6 36976,2002.6"];
	Layer3_Device7_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="36976,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage13_Accumulate -> Layer3_Device7_Stage14_Accumulate	[pos="e,36976,1839.9 36976,1939.3 36976,1939.3 36976,1849.9 36976,1849.9"];
	Layer3_Device7_Stage14_RecvKV -> Layer3_Device7_Stage14_Attention	[pos="e,36455,1992.6 36791,2119 36601,2119 36455,2119 36455,2119 36455,2119 36455,2002.6 36455,2002.6"];
	Layer3_Device7_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="38185,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device7_Stage14_RecvKV -> Layer3_Device7_Stage15_RecvKV	[label="Ring transfer",
		lp="37972,2062.4",
		pos="e,37924,2019 37924,2105.7 37924,2105.7 37924,2029 37924,2029"];
	Layer3_Device7_Stage14_Attention -> Layer3_Device7_Stage14_Accumulate	[pos="e,36744,1804 36455,1939.2 36455,1893.2 36455,1804 36455,1804 36455,1804 36734,1804 36734,1804"];
	Layer3_Device7_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="38439,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device7_Stage14_Accumulate -> Layer3_Device7_Stage15_Accumulate	[pos="e,38207,1687 36976,1786.6 36976,1749.5 36976,1687 36976,1687 36976,1687 38197,1687 38197,1687"];
	Layer3_Device7_Stage15_RecvKV -> Layer3_Device7_Stage15_Attention	[pos="e,38439,1840 38439,1912.6 38439,1912.6 38439,1850 38439,1850"];
	Layer3_Device7_Stage15_Attention -> Layer3_Device7_Stage15_Accumulate	[pos="e,38439,1714.1 38439,1786.7 38439,1786.7 38439,1724.1 38439,1724.1"];
	Layer3_Device7_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="38591,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device7_Stage15_Accumulate -> Layer3_Device7_ConcatHeads	[pos="e,38518,1588.1 38518,1660.7 38518,1660.7 38518,1598.1 38518,1598.1"];
	Layer3_Device7_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39004,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device7_ConcatHeads -> Layer3_Device7_OutputProj	[pos="e,38804,1462.1 38804,1534.7 38804,1534.7 38804,1472.1 38804,1472.1"];
	Layer3_Device7_OutputProj -> Layer3_Device7_Residual1	[pos="e,39071,1336.1 39071,1408.7 39071,1408.7 39071,1346.1 39071,1346.1"];
	Layer3_Device7_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39311,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device7_Residual1 -> Layer3_Device7_LayerNorm2	[pos="e,39311,1210.1 39311,1282.7 39311,1282.7 39311,1220.1 39311,1220.1"];
	Layer3_Device7_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="39710,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device7_Residual1 -> Layer3_Device7_Residual2	[pos="e,39660,527.98 39660,1282.5 39660,1282.5 39660,537.98 39660,537.98"];
	Layer3_Device7_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="39067,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device7_LayerNorm2 -> Layer3_Device7_GateProj	[pos="e,39195,1084.1 39195,1156.7 39195,1156.7 39195,1094.1 39195,1094.1"];
	Layer3_Device7_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="39380,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device7_LayerNorm2 -> Layer3_Device7_UpProj	[pos="e,39408,995.2 39408,1156.6 39408,1156.6 39408,1005.2 39408,1005.2"];
	Layer3_Device7_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="39067,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device7_GateProj -> Layer3_Device7_Activation	[pos="e,38998,905.95 38998,1030.8 38998,1030.8 38998,915.95 38998,915.95"];
	Layer3_Device7_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="39212,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device7_UpProj -> Layer3_Device7_ElemMul	[pos="e,39448,780.2 39448,941.61 39448,941.61 39448,790.2 39448,790.2"];
	Layer3_Device7_Activation -> Layer3_Device7_ElemMul	[pos="e,39067,780.09 39067,852.69 39067,852.69 39067,790.09 39067,790.09"];
	Layer3_Device7_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="39308,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device7_ElemMul -> Layer3_Device7_DownProj	[pos="e,39308,654.09 39308,726.69 39308,726.69 39308,664.09 39308,664.09"];
	Layer3_Device7_DownProj -> Layer3_Device7_Residual2	[pos="e,39425,528.09 39425,600.69 39425,600.69 39425,538.09 39425,538.09"];
	Layer3_Device7_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 7 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="40175,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device7_Residual2 -> Layer3_Device7_Output	[pos="e,39985,393.95 39985,474.67 39985,474.67 39985,403.95 39985,403.95"];
	Layer3_Device7_Output -> Sequence_Aggregate	[pos="e,40314,238.96 40314,330.95 40314,330.95 40314,248.96 40314,248.96"];
	Layer3_Device8_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="43256,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device8_Input -> Layer3_Device8_LayerNorm1	[pos="e,43328,5179.9 43328,5258.6 43328,5258.6 43328,5189.9 43328,5189.9"];
	Layer3_Device8_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="42205,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device8_Input -> Layer3_Device8_Residual1	[pos="e,42592,1309 43790,5290 43838,5290 43872,5290 43872,5290 43872,5290 43872,1309 43872,1309 43872,1309 42602,1309 42602,1309"];
	Layer3_Device8_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43013,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device8_LayerNorm1 -> Layer3_Device8_QKVProj	[pos="e,43256,5054.1 43256,5126.7 43256,5126.7 43256,5064.1 43256,5064.1"];
	Layer3_Device8_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41812,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage0_RecvKV	[label="Local K,V",
		lp="42312,4957.4",
		pos="e,42548,4914.2 42548,5000.7 42548,5000.7 42548,4924.2 42548,4924.2"];
	Layer3_Device8_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42602,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage0_Attention	[label=Q_local,
		lp="42874,4860.9",
		pos="e,42783,4694.8 42783,5000.7 42783,5000.7 42783,4704.8 42783,4704.8"];
	Layer3_Device8_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40272,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage1_Attention	[label=Q_local,
		lp="40336,4764.4",
		pos="e,40310,4501.7 42363,5007 41564,5007 40310,5007 40310,5007 40310,5007 40310,4511.7 40310,4511.7"];
	Layer3_Device8_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40861,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage2_Attention	[label=Q_local,
		lp="43014,4667.9",
		pos="e,41053,4308.5 42878,5000.6 42878,4905.2 42878,4588 42878,4588 42878,4588 41053,4588 41053,4588 41053,4588 41053,4318.5 41053,4318.5"];
	Layer3_Device8_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42876,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage3_Attention	[label=Q_local,
		lp="43154,4571.4",
		pos="e,43049,4115.7 43049,5000.8 43049,5000.8 43049,4125.7 43049,4125.7"];
	Layer3_Device8_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40153,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage4_Attention	[label=Q_local,
		lp="39956,4474.9",
		pos="e,39981,3922.6 42363,5020 41473,5020 39981,5020 39981,5020 39981,5020 39981,3932.6 39981,3932.6"];
	Layer3_Device8_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42994,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage5_Attention	[label=Q_local,
		lp="43247,4378.4",
		pos="e,43166,3729.7 43166,5000.6 43166,5000.6 43166,3739.7 43166,3739.7"];
	Layer3_Device8_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43043,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage6_Attention	[label=Q_local,
		lp="43368,4281.9",
		pos="e,43250,3536.6 43250,5000.7 43250,5000.7 43250,3546.6 43250,3546.6"];
	Layer3_Device8_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40104,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage7_Attention	[label=Q_local,
		lp="39826,4185.4",
		pos="e,39897,3343.5 42363,5034 41450,5034 39897,5034 39897,5034 39897,5034 39897,3353.5 39897,3353.5"];
	Layer3_Device8_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42955,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage8_Attention	[label=Q_local,
		lp="43486,4088.9",
		pos="e,43187,3123 43310,5000.8 43310,4770.7 43310,3123 43310,3123 43310,3123 43197,3123 43197,3123"];
	Layer3_Device8_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40016,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage9_Attention	[label=Q_local,
		lp="39730,3992.4",
		pos="e,39828,2957.6 42363,5040 41432,5040 39828,5040 39828,5040 39828,5040 39828,2967.6 39828,2967.6"];
	Layer3_Device8_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40145,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage10_Attention	[label=Q_local,
		lp="39601,3895.9",
		pos="e,39913,2737 42363,5047 41412,5047 39752,5047 39752,5047 39752,5047 39752,2737 39752,2737 39752,2737 39903,2737 39903,2737"];
	Layer3_Device8_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41289,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage11_Attention	[label=Q_local,
		lp="43583,3799.4",
		pos="e,41366,2571.4 43346,5000.8 43346,4740.5 43346,2658 43346,2658 43346,2658 41366,2658 41366,2658 41366,2658 41366,2581.4 41366,2581.4"];
	Layer3_Device8_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="41619,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage12_Attention	[label=Q_local,
		lp="43722,3702.9",
		pos="e,41549,2378.6 43382,5000.7 43382,4739.9 43382,2653 43382,2653 43382,2653 41549,2653 41549,2653 41549,2653 41549,2388.6 41549,2388.6"];
	Layer3_Device8_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="43634,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage13_Attention	[label=Q_local,
		lp="43862,3606.4",
		pos="e,43769,2185.8 43663,5014 43727,5014 43769,5014 43769,5014 43769,5014 43769,2195.8 43769,2195.8"];
	Layer3_Device8_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="40956,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage14_Attention	[label=Q_local,
		lp="39483,3509.9",
		pos="e,40796,1992.7 43540,5000.9 43540,4727.5 43540,2433 43540,2433 43540,2433 40796,2433 40796,2433 40796,2433 40796,2002.7 40796,2002.7"];
	Layer3_Device8_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="42686,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_QKVProj -> Layer3_Device8_Stage15_Attention	[label=Q_local,
		lp="43980,3413.4",
		pos="e,42918,1822 43663,5027 43782,5027 43869,5027 43869,5027 43869,5027 43869,1822 43869,1822 43869,1822 42928,1822 42928,1822"];
	Layer3_Device8_Stage0_RecvKV -> Layer3_Device8_Stage0_Attention	[pos="e,42551,4694.8 42551,4860.8 42551,4860.8 42551,4704.8 42551,4704.8"];
	Layer3_Device8_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41393,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage0_RecvKV -> Layer3_Device8_Stage1_RecvKV	[label="Ring transfer",
		lp="41732,4764.4",
		pos="e,41602,4721 41602,4807.7 41602,4807.7 41602,4731 41602,4731"];
	Layer3_Device8_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="40793,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage0_Attention -> Layer3_Device8_Stage0_Accumulate	[pos="e,40793,4501.5 42602,4641 42602,4618.8 42602,4591 42602,4591 42602,4591 40793,4591 40793,4591 40793,4591 40793,4511.5 40793,4511.5"];
	Layer3_Device8_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40340,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage0_Accumulate -> Layer3_Device8_Stage1_Accumulate	[pos="e,40566,4308.5 40566,4448.1 40566,4448.1 40566,4318.5 40566,4318.5"];
	Layer3_Device8_Stage1_RecvKV -> Layer3_Device8_Stage1_Attention	[pos="e,40488,4501.8 40488,4667.8 40488,4667.8 40488,4511.8 40488,4511.8"];
	Layer3_Device8_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="42002,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage1_RecvKV -> Layer3_Device8_Stage2_RecvKV	[label="Ring transfer",
		lp="41710,4571.4",
		pos="e,41698,4528 41698,4614.7 41698,4614.7 41698,4538 41698,4538"];
	Layer3_Device8_Stage1_Attention -> Layer3_Device8_Stage1_Accumulate	[pos="e,40306,4308.5 40306,4448.1 40306,4448.1 40306,4318.5 40306,4318.5"];
	Layer3_Device8_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40458,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage1_Accumulate -> Layer3_Device8_Stage2_Accumulate	[pos="e,40399,4115.5 40399,4255.1 40399,4255.1 40399,4125.5 40399,4125.5"];
	Layer3_Device8_Stage2_RecvKV -> Layer3_Device8_Stage2_Attention	[pos="e,41087,4308.8 41087,4474.8 41087,4474.8 41087,4318.8 41087,4318.8"];
	Layer3_Device8_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="42070,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage2_RecvKV -> Layer3_Device8_Stage3_RecvKV	[label="Ring transfer",
		lp="42084,4378.4",
		pos="e,42036,4335 42036,4421.7 42036,4421.7 42036,4345 42036,4345"];
	Layer3_Device8_Stage2_Attention -> Layer3_Device8_Stage2_Accumulate	[pos="e,40660,4115.5 40660,4255.1 40660,4255.1 40660,4125.5 40660,4125.5"];
	Layer3_Device8_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40674,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage2_Accumulate -> Layer3_Device8_Stage3_Accumulate	[pos="e,40566,3922.5 40566,4062.1 40566,4062.1 40566,3932.5 40566,3932.5"];
	Layer3_Device8_Stage3_RecvKV -> Layer3_Device8_Stage3_Attention	[pos="e,42817,4115.8 42817,4281.8 42817,4281.8 42817,4125.8 42817,4125.8"];
	Layer3_Device8_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41667,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage3_RecvKV -> Layer3_Device8_Stage4_RecvKV	[label="Ring transfer",
		lp="41917,4185.4",
		pos="e,41868,4142 41868,4228.7 41868,4228.7 41868,4152 41868,4152"];
	Layer3_Device8_Stage3_Attention -> Layer3_Device8_Stage3_Accumulate	[pos="e,40826,3922.6 42724,4062.3 42724,4022.9 42724,3954 42724,3954 42724,3954 40826,3954 40826,3954 40826,3954 40826,3932.6 40826,3932.6"];
	Layer3_Device8_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40576,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage3_Accumulate -> Layer3_Device8_Stage4_Accumulate	[pos="e,40625,3729.5 40625,3869.1 40625,3869.1 40625,3739.5 40625,3739.5"];
	Layer3_Device8_Stage4_RecvKV -> Layer3_Device8_Stage4_Attention	[pos="e,40306,3922.6 40794,4049 40528,4049 40306,4049 40306,4049 40306,4049 40306,3932.6 40306,3932.6"];
	Layer3_Device8_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41883,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage4_RecvKV -> Layer3_Device8_Stage5_RecvKV	[label="Ring transfer",
		lp="41823,3992.4",
		pos="e,41775,3949 41775,4035.7 41775,4035.7 41775,3959 41775,3959"];
	Layer3_Device8_Stage4_Attention -> Layer3_Device8_Stage4_Accumulate	[pos="e,40364,3729.5 40364,3869.1 40364,3869.1 40364,3739.5 40364,3739.5"];
	Layer3_Device8_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42522,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage4_Accumulate -> Layer3_Device8_Stage5_Accumulate	[pos="e,42498,3536.6 40600,3676.4 40600,3651.8 40600,3619 40600,3619 40600,3619 42498,3619 42498,3619 42498,3619 42498,3546.6 42498,3546.6"];
	Layer3_Device8_Stage5_RecvKV -> Layer3_Device8_Stage5_Attention	[pos="e,42783,3729.8 42783,3895.8 42783,3895.8 42783,3739.8 42783,3739.8"];
	Layer3_Device8_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41785,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage5_RecvKV -> Layer3_Device8_Stage6_RecvKV	[label="Ring transfer",
		lp="41882,3799.4",
		pos="e,41834,3756 41834,3842.7 41834,3842.7 41834,3766 41834,3766"];
	Layer3_Device8_Stage5_Attention -> Layer3_Device8_Stage5_Accumulate	[pos="e,42729,3536.6 42762,3702 42742,3702 42729,3702 42729,3702 42729,3702 42729,3546.6 42729,3546.6"];
	Layer3_Device8_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42522,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage5_Accumulate -> Layer3_Device8_Stage6_Accumulate	[pos="e,42522,3343.5 42522,3483.1 42522,3483.1 42522,3353.5 42522,3353.5"];
	Layer3_Device8_Stage6_RecvKV -> Layer3_Device8_Stage6_Attention	[pos="e,43018,3536.6 42375,3663 42692,3663 43018,3663 43018,3663 43018,3663 43018,3546.6 43018,3546.6"];
	Layer3_Device8_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41313,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage6_RecvKV -> Layer3_Device8_Stage7_RecvKV	[label="Ring transfer",
		lp="41597,3606.4",
		pos="e,41549,3563 41549,3649.7 41549,3649.7 41549,3573 41549,3573"];
	Layer3_Device8_Stage6_Attention -> Layer3_Device8_Stage6_Accumulate	[pos="e,42754,3316 42999,3483.4 42999,3430.2 42999,3316 42999,3316 42999,3316 42764,3316 42764,3316"];
	Layer3_Device8_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42434,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage6_Accumulate -> Layer3_Device8_Stage7_Accumulate	[pos="e,42478,3150.5 42478,3290.1 42478,3290.1 42478,3160.5 42478,3160.5"];
	Layer3_Device8_Stage7_RecvKV -> Layer3_Device8_Stage7_Attention	[pos="e,40128,3343.6 40440,3470 40262,3470 40128,3470 40128,3470 40128,3470 40128,3353.6 40128,3353.6"];
	Layer3_Device8_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="41313,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage7_RecvKV -> Layer3_Device8_Stage8_RecvKV	[label="Ring transfer",
		lp="41361,3413.4",
		pos="e,41313,3370 41313,3456.7 41313,3456.7 41313,3380 41313,3380"];
	Layer3_Device8_Stage7_Attention -> Layer3_Device8_Stage7_Accumulate	[pos="e,42218,3150.7 40320,3290.2 40320,3272.7 40320,3253 40320,3253 40320,3253 42218,3253 42218,3253 42218,3253 42218,3160.7 42218,3160.7"];
	Layer3_Device8_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42434,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage7_Accumulate -> Layer3_Device8_Stage8_Accumulate	[pos="e,42434,2957.5 42434,3097.1 42434,3097.1 42434,2967.5 42434,2967.5"];
	Layer3_Device8_Stage8_RecvKV -> Layer3_Device8_Stage8_Attention	[pos="e,42738,3150.6 41903,3277 42295,3277 42738,3277 42738,3277 42738,3277 42738,3160.6 42738,3160.6"];
	Layer3_Device8_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41225,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage8_RecvKV -> Layer3_Device8_Stage9_RecvKV	[label="Ring transfer",
		lp="41302,3220.4",
		pos="e,41269,3177 41269,3263.7 41269,3263.7 41269,3187 41269,3187"];
	Layer3_Device8_Stage8_Attention -> Layer3_Device8_Stage8_Accumulate	[pos="e,42666,2930 42759,3097.4 42759,3044.2 42759,2930 42759,2930 42759,2930 42676,2930 42676,2930"];
	Layer3_Device8_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40666,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage8_Accumulate -> Layer3_Device8_Stage9_Accumulate	[pos="e,40743,2764.7 42434,2904.4 42434,2878.8 42434,2844 42434,2844 42434,2844 40743,2844 40743,2844 40743,2844 40743,2774.7 40743,2774.7"];
	Layer3_Device8_Stage9_RecvKV -> Layer3_Device8_Stage9_Attention	[pos="e,40060,2957.6 40352,3084 40184,3084 40060,3084 40060,3084 40060,3084 40060,2967.6 40060,2967.6"];
	Layer3_Device8_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41225,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage9_RecvKV -> Layer3_Device8_Stage10_RecvKV	[label="Ring transfer",
		lp="41273,3027.4",
		pos="e,41225,2984 41225,3070.7 41225,3070.7 41225,2994 41225,2994"];
	Layer3_Device8_Stage9_Attention -> Layer3_Device8_Stage9_Accumulate	[pos="e,40589,2764.6 40080,2904.1 40080,2865.8 40080,2800 40080,2800 40080,2800 40589,2800 40589,2800 40589,2800 40589,2774.6 40589,2774.6"];
	Layer3_Device8_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="40666,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage9_Accumulate -> Layer3_Device8_Stage10_Accumulate	[pos="e,40666,2571.5 40666,2711.1 40666,2711.1 40666,2581.5 40666,2581.5"];
	Layer3_Device8_Stage10_RecvKV -> Layer3_Device8_Stage10_Attention	[pos="e,40341,2764.8 40341,2930.8 40341,2930.8 40341,2774.8 40341,2774.8"];
	Layer3_Device8_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="41875,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage10_RecvKV -> Layer3_Device8_Stage11_RecvKV	[label="Ring transfer",
		lp="41542,2834.4",
		pos="e,41550,2791 41550,2877.7 41550,2877.7 41550,2801 41550,2801"];
	Layer3_Device8_Stage10_Attention -> Layer3_Device8_Stage10_Accumulate	[pos="e,40434,2544 40236,2711.4 40236,2658.2 40236,2544 40236,2544 40236,2544 40424,2544 40424,2544"];
	Layer3_Device8_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41098,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage10_Accumulate -> Layer3_Device8_Stage11_Accumulate	[pos="e,40882,2378.5 40882,2518.1 40882,2518.1 40882,2388.5 40882,2388.5"];
	Layer3_Device8_Stage11_RecvKV -> Layer3_Device8_Stage11_Attention	[pos="e,41212,2571.6 41212,2684.7 41212,2684.7 41212,2581.6 41212,2581.6"];
	Layer3_Device8_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42498,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage11_RecvKV -> Layer3_Device8_Stage12_RecvKV	[label="Ring transfer",
		lp="42118,2641.4",
		pos="e,42186,2598 42186,2684.7 42186,2684.7 42186,2608 42186,2608"];
	Layer3_Device8_Stage11_Attention -> Layer3_Device8_Stage11_Accumulate	[pos="e,41194,2378.5 41194,2518.1 41194,2518.1 41194,2388.5 41194,2388.5"];
	Layer3_Device8_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41216,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage11_Accumulate -> Layer3_Device8_Stage12_Accumulate	[pos="e,41157,2185.5 41157,2325.1 41157,2325.1 41157,2195.5 41157,2195.5"];
	Layer3_Device8_Stage12_RecvKV -> Layer3_Device8_Stage12_Attention	[pos="e,41714,2378.6 41714,2491.7 41714,2491.7 41714,2388.6 41714,2388.6"];
	Layer3_Device8_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42828,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage12_RecvKV -> Layer3_Device8_Stage13_RecvKV	[label="Ring transfer",
		lp="42711,2448.4",
		pos="e,42663,2405 42663,2491.7 42663,2491.7 42663,2415 42663,2415"];
	Layer3_Device8_Stage12_Attention -> Layer3_Device8_Stage12_Accumulate	[pos="e,41418,2185.5 41418,2325.1 41418,2325.1 41418,2195.5 41418,2195.5"];
	Layer3_Device8_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41477,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage12_Accumulate -> Layer3_Device8_Stage13_Accumulate	[pos="e,41346,1992.5 41346,2132.1 41346,2132.1 41346,2002.5 41346,2002.5"];
	Layer3_Device8_Stage13_RecvKV -> Layer3_Device8_Stage13_Attention	[pos="e,43575,2185.8 43575,2351.8 43575,2351.8 43575,2195.8 43575,2195.8"];
	Layer3_Device8_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42425,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage13_RecvKV -> Layer3_Device8_Stage14_RecvKV	[label="Ring transfer",
		lp="42609,2255.4",
		pos="e,42626,2212 42626,2298.7 42626,2298.7 42626,2222 42626,2222"];
	Layer3_Device8_Stage13_Attention -> Layer3_Device8_Stage13_Accumulate	[pos="e,41607,1992.6 43504,2132.4 43504,2103.1 43504,2060 43504,2060 43504,2060 41607,2060 41607,2060 41607,2060 41607,2002.6 41607,2002.6"];
	Layer3_Device8_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="41477,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage13_Accumulate -> Layer3_Device8_Stage14_Accumulate	[pos="e,41477,1839.9 41477,1939.3 41477,1939.3 41477,1849.9 41477,1849.9"];
	Layer3_Device8_Stage14_RecvKV -> Layer3_Device8_Stage14_Attention	[pos="e,41086,1992.6 41551,2119 41297,2119 41086,2119 41086,2119 41086,2119 41086,2002.6 41086,2002.6"];
	Layer3_Device8_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="42686,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device8_Stage14_RecvKV -> Layer3_Device8_Stage15_RecvKV	[label="Ring transfer",
		lp="42603,2062.4",
		pos="e,42556,2019 42556,2105.7 42556,2105.7 42556,2029 42556,2029"];
	Layer3_Device8_Stage14_Attention -> Layer3_Device8_Stage14_Accumulate	[pos="e,41245,1804 41118,1939.2 41118,1893.2 41118,1804 41118,1804 41118,1804 41235,1804 41235,1804"];
	Layer3_Device8_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="42205,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device8_Stage14_Accumulate -> Layer3_Device8_Stage15_Accumulate	[pos="e,41973,1687 41700,1786.6 41700,1749.5 41700,1687 41700,1687 41700,1687 41963,1687 41963,1687"];
	Layer3_Device8_Stage15_RecvKV -> Layer3_Device8_Stage15_Attention	[pos="e,42686,1840 42686,1912.6 42686,1912.6 42686,1850 42686,1850"];
	Layer3_Device8_Stage15_Attention -> Layer3_Device8_Stage15_Accumulate	[pos="e,42437,1687 42523,1786.6 42523,1749.5 42523,1687 42523,1687 42523,1687 42447,1687 42447,1687"];
	Layer3_Device8_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="42205,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device8_Stage15_Accumulate -> Layer3_Device8_ConcatHeads	[pos="e,42205,1588.1 42205,1660.7 42205,1660.7 42205,1598.1 42205,1598.1"];
	Layer3_Device8_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="42205,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device8_ConcatHeads -> Layer3_Device8_OutputProj	[pos="e,42205,1462.1 42205,1534.7 42205,1534.7 42205,1472.1 42205,1472.1"];
	Layer3_Device8_OutputProj -> Layer3_Device8_Residual1	[pos="e,42205,1336.1 42205,1408.7 42205,1408.7 42205,1346.1 42205,1346.1"];
	Layer3_Device8_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="41904,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device8_Residual1 -> Layer3_Device8_LayerNorm2	[pos="e,41968,1210.1 41968,1282.7 41968,1282.7 41968,1220.1 41968,1220.1"];
	Layer3_Device8_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="41649,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device8_Residual1 -> Layer3_Device8_Residual2	[pos="e,42036,492 42370,1282.7 42370,1141.8 42370,492 42370,492 42370,492 42046,492 42046,492"];
	Layer3_Device8_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="41611,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device8_LayerNorm2 -> Layer3_Device8_GateProj	[pos="e,41764,1084.1 41764,1156.7 41764,1156.7 41764,1094.1 41764,1094.1"];
	Layer3_Device8_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="41924,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device8_LayerNorm2 -> Layer3_Device8_UpProj	[pos="e,41976,995.2 41976,1156.6 41976,1156.6 41976,1005.2 41976,1005.2"];
	Layer3_Device8_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="41611,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device8_GateProj -> Layer3_Device8_Activation	[pos="e,41542,905.95 41542,1030.8 41542,1030.8 41542,915.95 41542,915.95"];
	Layer3_Device8_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="41649,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device8_UpProj -> Layer3_Device8_ElemMul	[pos="e,41948,780.2 41948,941.61 41948,941.61 41948,790.2 41948,790.2"];
	Layer3_Device8_Activation -> Layer3_Device8_ElemMul	[pos="e,41611,780.09 41611,852.69 41611,852.69 41611,790.09 41611,790.09"];
	Layer3_Device8_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="41649,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device8_ElemMul -> Layer3_Device8_DownProj	[pos="e,41649,654.09 41649,726.69 41649,726.69 41649,664.09 41649,664.09"];
	Layer3_Device8_DownProj -> Layer3_Device8_Residual2	[pos="e,41649,528.09 41649,600.69 41649,600.69 41649,538.09 41649,538.09"];
	Layer3_Device8_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 8 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="41350,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device8_Residual2 -> Layer3_Device8_Output	[pos="e,41457,399.66 41457,474.67 41457,474.67 41457,409.66 41457,409.66"];
	Layer3_Device8_Output -> Sequence_Aggregate	[pos="e,41061,201.04 41061,353.7 41061,353.7 41061,211.04 41061,211.04"];
	Layer3_Device9_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="48296,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device9_Input -> Layer3_Device9_LayerNorm1	[pos="e,48480,5180.1 48480,5274.4 48480,5274.4 48480,5190.1 48480,5190.1"];
	Layer3_Device9_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="46727,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device9_Input -> Layer3_Device9_Residual1	[pos="e,47114,1309 48748,5252.9 48748,4848.5 48748,1309 48748,1309 48748,1309 47124,1309 47124,1309"];
	Layer3_Device9_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47846,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device9_LayerNorm1 -> Layer3_Device9_QKVProj	[pos="e,48289,5054.1 48289,5126.7 48289,5126.7 48289,5064.1 48289,5064.1"];
	Layer3_Device9_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46600,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage0_RecvKV	[label="Local K,V",
		lp="47100,4957.4",
		pos="e,47358,4914.2 47358,5000.7 47358,5000.7 47358,4924.2 47358,4924.2"];
	Layer3_Device9_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47390,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage0_Attention	[label=Q_local,
		lp="47662,4860.9",
		pos="e,47571,4694.8 47571,5000.7 47571,5000.7 47571,4704.8 47571,4704.8"];
	Layer3_Device9_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45060,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage1_Attention	[label=Q_local,
		lp="45124,4764.4",
		pos="e,45092,4501.7 47196,5007 46382,5007 45092,5007 45092,5007 45092,5007 45092,4511.7 45092,4511.7"];
	Layer3_Device9_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45649,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage2_Attention	[label=Q_local,
		lp="47802,4667.9",
		pos="e,45841,4308.4 47666,5000.9 47666,4906.6 47666,4593 47666,4593 47666,4593 45841,4593 45841,4593 45841,4593 45841,4318.4 45841,4318.4"];
	Layer3_Device9_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47664,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage3_Attention	[label=Q_local,
		lp="47942,4571.4",
		pos="e,47837,4115.7 47837,5000.8 47837,5000.8 47837,4125.7 47837,4125.7"];
	Layer3_Device9_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44914,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage4_Attention	[label=Q_local,
		lp="44744,4474.9",
		pos="e,44756,3922.6 47196,5020 46290,5020 44756,5020 44756,5020 44756,5020 44756,3932.6 44756,3932.6"];
	Layer3_Device9_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47782,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage5_Attention	[label=Q_local,
		lp="48035,4378.4",
		pos="e,47954,3729.7 47954,5000.6 47954,5000.6 47954,3739.7 47954,3739.7"];
	Layer3_Device9_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47817,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage6_Attention	[label=Q_local,
		lp="48156,4281.9",
		pos="e,48031,3536.6 48031,5000.7 48031,5000.7 48031,3546.6 48031,3546.6"];
	Layer3_Device9_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44878,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage7_Attention	[label=Q_local,
		lp="44602,4185.4",
		pos="e,44664,3343.5 47196,5034 46266,5034 44664,5034 44664,5034 44664,5034 44664,3353.5 44664,3353.5"];
	Layer3_Device9_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47688,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage8_Attention	[label=Q_local,
		lp="48274,4088.9",
		pos="e,47920,3123 48090,5000.8 48090,4770.7 48090,3123 48090,3123 48090,3123 47930,3123 47930,3123"];
	Layer3_Device9_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44749,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage9_Attention	[label=Q_local,
		lp="44493,3992.4",
		pos="e,44604,2957.6 47196,5040 46250,5040 44604,5040 44604,5040 44604,5040 44604,2967.6 44604,2967.6"];
	Layer3_Device9_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="44964,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage10_Attention	[label=Q_local,
		lp="44362,3895.9",
		pos="e,44732,2737 47196,5047 46223,5047 44501,5047 44501,5047 44501,5047 44501,2737 44501,2737 44501,2737 44722,2737 44722,2737"];
	Layer3_Device9_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46085,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage11_Attention	[label=Q_local,
		lp="48403,3799.4",
		pos="e,46162,2571.5 48131,5000.6 48131,4740.4 48131,2675 48131,2675 48131,2675 46162,2675 46162,2675 46162,2675 46162,2581.5 46162,2581.5"];
	Layer3_Device9_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="46407,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage12_Attention	[label=Q_local,
		lp="48510,3702.9",
		pos="e,46345,2378.5 48172,5000.5 48172,4739.8 48172,2670 48172,2670 48172,2670 46345,2670 46345,2670 46345,2670 46345,2388.5 46345,2388.5"];
	Layer3_Device9_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="48422,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage13_Attention	[label=Q_local,
		lp="48650,3606.4",
		pos="e,48595,2185.8 48496,5014 48556,5014 48595,5014 48595,5014 48595,5014 48595,2195.8 48595,2195.8"];
	Layer3_Device9_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="45744,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage14_Attention	[label=Q_local,
		lp="44244,3509.9",
		pos="e,45584,1992.7 48355,5000.5 48355,4727.9 48355,2477 48355,2477 48355,2477 45584,2477 45584,2477 45584,2477 45584,2002.7 45584,2002.7"];
	Layer3_Device9_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="47474,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_QKVProj -> Layer3_Device9_Stage15_Attention	[label=Q_local,
		lp="48768,3413.4",
		pos="e,47706,1822 48496,5027 48614,5027 48701,5027 48701,5027 48701,5027 48701,1822 48701,1822 48701,1822 47716,1822 47716,1822"];
	Layer3_Device9_Stage0_RecvKV -> Layer3_Device9_Stage0_Attention	[pos="e,47339,4694.8 47339,4860.8 47339,4860.8 47339,4704.8 47339,4704.8"];
	Layer3_Device9_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46181,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage0_RecvKV -> Layer3_Device9_Stage1_RecvKV	[label="Ring transfer",
		lp="46520,4764.4",
		pos="e,46390,4721 46390,4807.7 46390,4807.7 46390,4731 46390,4731"];
	Layer3_Device9_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="45581,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage0_Attention -> Layer3_Device9_Stage0_Accumulate	[pos="e,45581,4501.5 47390,4641.1 47390,4620.7 47390,4596 47390,4596 47390,4596 45581,4596 45581,4596 45581,4596 45581,4511.5 45581,4511.5"];
	Layer3_Device9_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45128,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage0_Accumulate -> Layer3_Device9_Stage1_Accumulate	[pos="e,45354,4308.5 45354,4448.1 45354,4448.1 45354,4318.5 45354,4318.5"];
	Layer3_Device9_Stage1_RecvKV -> Layer3_Device9_Stage1_Attention	[pos="e,45276,4501.8 45276,4667.8 45276,4667.8 45276,4511.8 45276,4511.8"];
	Layer3_Device9_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46790,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage1_RecvKV -> Layer3_Device9_Stage2_RecvKV	[label="Ring transfer",
		lp="46498,4571.4",
		pos="e,46486,4528 46486,4614.7 46486,4614.7 46486,4538 46486,4538"];
	Layer3_Device9_Stage1_Attention -> Layer3_Device9_Stage1_Accumulate	[pos="e,45094,4308.5 45094,4448.1 45094,4448.1 45094,4318.5 45094,4318.5"];
	Layer3_Device9_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45246,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage1_Accumulate -> Layer3_Device9_Stage2_Accumulate	[pos="e,45187,4115.5 45187,4255.1 45187,4255.1 45187,4125.5 45187,4125.5"];
	Layer3_Device9_Stage2_RecvKV -> Layer3_Device9_Stage2_Attention	[pos="e,45875,4308.8 45875,4474.8 45875,4474.8 45875,4318.8 45875,4318.8"];
	Layer3_Device9_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46858,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage2_RecvKV -> Layer3_Device9_Stage3_RecvKV	[label="Ring transfer",
		lp="46872,4378.4",
		pos="e,46824,4335 46824,4421.7 46824,4421.7 46824,4345 46824,4345"];
	Layer3_Device9_Stage2_Attention -> Layer3_Device9_Stage2_Accumulate	[pos="e,45448,4115.5 45448,4255.1 45448,4255.1 45448,4125.5 45448,4125.5"];
	Layer3_Device9_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45435,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage2_Accumulate -> Layer3_Device9_Stage3_Accumulate	[pos="e,45340,3922.5 45340,4062.1 45340,4062.1 45340,3932.5 45340,3932.5"];
	Layer3_Device9_Stage3_RecvKV -> Layer3_Device9_Stage3_Attention	[pos="e,47605,4115.8 47605,4281.8 47605,4281.8 47605,4125.8 47605,4125.8"];
	Layer3_Device9_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46455,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage3_RecvKV -> Layer3_Device9_Stage4_RecvKV	[label="Ring transfer",
		lp="46772,4185.4",
		pos="e,46656,4142 46656,4228.7 46656,4228.7 46656,4152 46656,4152"];
	Layer3_Device9_Stage3_Attention -> Layer3_Device9_Stage3_Accumulate	[pos="e,45601,3922.7 47498,4062.3 47498,4029.8 47498,3979 47498,3979 47498,3979 45601,3979 45601,3979 45601,3979 45601,3932.7 45601,3932.7"];
	Layer3_Device9_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45364,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage3_Accumulate -> Layer3_Device9_Stage4_Accumulate	[pos="e,45400,3729.5 45400,3869.1 45400,3869.1 45400,3739.5 45400,3739.5"];
	Layer3_Device9_Stage4_RecvKV -> Layer3_Device9_Stage4_Attention	[pos="e,45080,3922.6 45582,4049 45310,4049 45080,4049 45080,4049 45080,4049 45080,3932.6 45080,3932.6"];
	Layer3_Device9_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46644,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage4_RecvKV -> Layer3_Device9_Stage5_RecvKV	[label="Ring transfer",
		lp="46598,3992.4",
		pos="e,46550,3949 46550,4035.7 46550,4035.7 46550,3959 46550,3959"];
	Layer3_Device9_Stage4_Attention -> Layer3_Device9_Stage4_Accumulate	[pos="e,45139,3729.5 45139,3869.1 45139,3869.1 45139,3739.5 45139,3739.5"];
	Layer3_Device9_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47296,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage4_Accumulate -> Layer3_Device9_Stage5_Accumulate	[pos="e,47279,3536.8 45381,3676.2 45381,3645.1 45381,3598 45381,3598 45381,3598 47279,3598 47279,3598 47279,3598 47279,3546.8 47279,3546.8"];
	Layer3_Device9_Stage5_RecvKV -> Layer3_Device9_Stage5_Attention	[pos="e,47557,3729.8 47557,3895.8 47557,3895.8 47557,3739.8 47557,3739.8"];
	Layer3_Device9_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46573,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage5_RecvKV -> Layer3_Device9_Stage6_RecvKV	[label="Ring transfer",
		lp="46656,3799.4",
		pos="e,46608,3756 46608,3842.7 46608,3842.7 46608,3766 46608,3766"];
	Layer3_Device9_Stage5_Attention -> Layer3_Device9_Stage5_Accumulate	[pos="e,47510,3536.6 47550,3702 47526,3702 47510,3702 47510,3702 47510,3702 47510,3546.6 47510,3546.6"];
	Layer3_Device9_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47296,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage5_Accumulate -> Layer3_Device9_Stage6_Accumulate	[pos="e,47296,3343.5 47296,3483.1 47296,3483.1 47296,3353.5 47296,3353.5"];
	Layer3_Device9_Stage6_RecvKV -> Layer3_Device9_Stage6_Attention	[pos="e,47800,3536.6 47163,3663 47477,3663 47800,3663 47800,3663 47800,3663 47800,3546.6 47800,3546.6"];
	Layer3_Device9_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46087,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage6_RecvKV -> Layer3_Device9_Stage7_RecvKV	[label="Ring transfer",
		lp="46312,3606.4",
		pos="e,46330,3563 46330,3649.7 46330,3649.7 46330,3573 46330,3573"];
	Layer3_Device9_Stage6_Attention -> Layer3_Device9_Stage6_Accumulate	[pos="e,47528,3316 47752,3483.4 47752,3430.2 47752,3316 47752,3316 47752,3316 47538,3316 47538,3316"];
	Layer3_Device9_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47167,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage6_Accumulate -> Layer3_Device9_Stage7_Accumulate	[pos="e,47232,3150.5 47232,3290.1 47232,3290.1 47232,3160.5 47232,3160.5"];
	Layer3_Device9_Stage7_RecvKV -> Layer3_Device9_Stage7_Attention	[pos="e,44896,3343.6 45214,3470 45033,3470 44896,3470 44896,3470 44896,3470 44896,3353.6 44896,3353.6"];
	Layer3_Device9_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="46087,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage7_RecvKV -> Layer3_Device9_Stage8_RecvKV	[label="Ring transfer",
		lp="46135,3413.4",
		pos="e,46087,3370 46087,3456.7 46087,3456.7 46087,3380 46087,3380"];
	Layer3_Device9_Stage7_Attention -> Layer3_Device9_Stage7_Accumulate	[pos="e,46971,3150.7 45074,3290.2 45074,3267.3 45074,3238 45074,3238 45074,3238 46971,3238 46971,3238 46971,3238 46971,3160.7 46971,3160.7"];
	Layer3_Device9_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="47167,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage7_Accumulate -> Layer3_Device9_Stage8_Accumulate	[pos="e,47167,2957.5 47167,3097.1 47167,3097.1 47167,2967.5 47167,2967.5"];
	Layer3_Device9_Stage8_RecvKV -> Layer3_Device9_Stage8_Attention	[pos="e,47492,3150.6 46677,3277 47061,3277 47492,3277 47492,3277 47492,3277 47492,3160.6 47492,3160.6"];
	Layer3_Device9_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="45958,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage8_RecvKV -> Layer3_Device9_Stage9_RecvKV	[label="Ring transfer",
		lp="46076,3220.4",
		pos="e,46022,3177 46022,3263.7 46022,3263.7 46022,3187 46022,3187"];
	Layer3_Device9_Stage8_Attention -> Layer3_Device9_Stage8_Accumulate	[pos="e,47399,2930 47535,3097.4 47535,3044.2 47535,2930 47535,2930 47535,2930 47409,2930 47409,2930"];
	Layer3_Device9_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45485,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage8_Accumulate -> Layer3_Device9_Stage9_Accumulate	[pos="e,45562,2764.5 47167,2904.4 47167,2873.9 47167,2828 47167,2828 47167,2828 45562,2828 45562,2828 45562,2828 45562,2774.5 45562,2774.5"];
	Layer3_Device9_Stage9_RecvKV -> Layer3_Device9_Stage9_Attention	[pos="e,44814,2957.6 45085,3084 44928,3084 44814,3084 44814,3084 44814,3084 44814,2967.6 44814,2967.6"];
	Layer3_Device9_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="45958,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage9_RecvKV -> Layer3_Device9_Stage10_RecvKV	[label="Ring transfer",
		lp="46006,3027.4",
		pos="e,45958,2984 45958,3070.7 45958,3070.7 45958,2994 45958,2994"];
	Layer3_Device9_Stage9_Attention -> Layer3_Device9_Stage9_Accumulate	[pos="e,45408,2764.8 44856,2904.2 44856,2868.3 44856,2809 44856,2809 44856,2809 45408,2809 45408,2809 45408,2809 45408,2774.8 45408,2774.8"];
	Layer3_Device9_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45485,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage9_Accumulate -> Layer3_Device9_Stage10_Accumulate	[pos="e,45485,2571.5 45485,2711.1 45485,2711.1 45485,2581.5 45485,2581.5"];
	Layer3_Device9_Stage10_RecvKV -> Layer3_Device9_Stage10_Attention	[pos="e,45117,2764.8 45117,2930.8 45117,2930.8 45117,2774.8 45117,2774.8"];
	Layer3_Device9_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="46694,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage10_RecvKV -> Layer3_Device9_Stage11_RecvKV	[label="Ring transfer",
		lp="46275,2834.4",
		pos="e,46326,2791 46326,2877.7 46326,2877.7 46326,2801 46326,2801"];
	Layer3_Device9_Stage10_Attention -> Layer3_Device9_Stage10_Accumulate	[pos="e,45253,2544 44964,2711.4 44964,2658.2 44964,2544 44964,2544 44964,2544 45243,2544 45243,2544"];
	Layer3_Device9_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="45886,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage10_Accumulate -> Layer3_Device9_Stage11_Accumulate	[pos="e,45686,2378.5 45686,2518.1 45686,2518.1 45686,2388.5 45686,2388.5"];
	Layer3_Device9_Stage11_RecvKV -> Layer3_Device9_Stage11_Attention	[pos="e,46008,2571.6 46008,2684.7 46008,2684.7 46008,2581.6 46008,2581.6"];
	Layer3_Device9_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47294,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage11_RecvKV -> Layer3_Device9_Stage12_RecvKV	[label="Ring transfer",
		lp="46937,2641.4",
		pos="e,46994,2598 46994,2684.7 46994,2684.7 46994,2608 46994,2608"];
	Layer3_Device9_Stage11_Attention -> Layer3_Device9_Stage11_Accumulate	[pos="e,45986,2378.5 45986,2518.1 45986,2518.1 45986,2388.5 45986,2388.5"];
	Layer3_Device9_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46004,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage11_Accumulate -> Layer3_Device9_Stage12_Accumulate	[pos="e,45945,2185.5 45945,2325.1 45945,2325.1 45945,2195.5 45945,2195.5"];
	Layer3_Device9_Stage12_RecvKV -> Layer3_Device9_Stage12_Attention	[pos="e,46506,2378.6 46506,2491.7 46506,2491.7 46506,2388.6 46506,2388.6"];
	Layer3_Device9_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47616,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage12_RecvKV -> Layer3_Device9_Stage13_RecvKV	[label="Ring transfer",
		lp="47550,2448.4",
		pos="e,47455,2405 47455,2491.7 47455,2491.7 47455,2415 47455,2415"];
	Layer3_Device9_Stage12_Attention -> Layer3_Device9_Stage12_Accumulate	[pos="e,46206,2185.5 46206,2325.1 46206,2325.1 46206,2195.5 46206,2195.5"];
	Layer3_Device9_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46265,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage12_Accumulate -> Layer3_Device9_Stage13_Accumulate	[pos="e,46134,1992.5 46134,2132.1 46134,2132.1 46134,2002.5 46134,2002.5"];
	Layer3_Device9_Stage13_RecvKV -> Layer3_Device9_Stage13_Attention	[pos="e,48363,2185.8 48363,2351.8 48363,2351.8 48363,2195.8 48363,2195.8"];
	Layer3_Device9_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47213,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage13_RecvKV -> Layer3_Device9_Stage14_RecvKV	[label="Ring transfer",
		lp="47397,2255.4",
		pos="e,47414,2212 47414,2298.7 47414,2298.7 47414,2222 47414,2222"];
	Layer3_Device9_Stage13_Attention -> Layer3_Device9_Stage13_Accumulate	[pos="e,46395,1992.5 48292,2132.2 48292,2104.1 48292,2064 48292,2064 48292,2064 46395,2064 46395,2064 46395,2064 46395,2002.5 46395,2002.5"];
	Layer3_Device9_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46265,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage13_Accumulate -> Layer3_Device9_Stage14_Accumulate	[pos="e,46265,1839.9 46265,1939.3 46265,1939.3 46265,1849.9 46265,1849.9"];
	Layer3_Device9_Stage14_RecvKV -> Layer3_Device9_Stage14_Attention	[pos="e,45874,1992.6 46339,2119 46085,2119 45874,2119 45874,2119 45874,2119 45874,2002.6 45874,2002.6"];
	Layer3_Device9_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="47474,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device9_Stage14_RecvKV -> Layer3_Device9_Stage15_RecvKV	[label="Ring transfer",
		lp="47391,2062.4",
		pos="e,47344,2019 47344,2105.7 47344,2105.7 47344,2029 47344,2029"];
	Layer3_Device9_Stage14_Attention -> Layer3_Device9_Stage14_Accumulate	[pos="e,46033,1804 45942,1939.2 45942,1893.2 45942,1804 45942,1804 45942,1804 46023,1804 46023,1804"];
	Layer3_Device9_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="46727,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device9_Stage14_Accumulate -> Layer3_Device9_Stage15_Accumulate	[pos="e,46525,1714.2 46497,1804 46514,1804 46525,1804 46525,1804 46525,1804 46525,1724.2 46525,1724.2"];
	Layer3_Device9_Stage15_RecvKV -> Layer3_Device9_Stage15_Attention	[pos="e,47474,1840 47474,1912.6 47474,1912.6 47474,1850 47474,1850"];
	Layer3_Device9_Stage15_Attention -> Layer3_Device9_Stage15_Accumulate	[pos="e,46756,1714 47242,1822 47032,1822 46756,1822 46756,1822 46756,1822 46756,1724 46756,1724"];
	Layer3_Device9_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="46727,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device9_Stage15_Accumulate -> Layer3_Device9_ConcatHeads	[pos="e,46727,1588.1 46727,1660.7 46727,1660.7 46727,1598.1 46727,1598.1"];
	Layer3_Device9_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="46727,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device9_ConcatHeads -> Layer3_Device9_OutputProj	[pos="e,46727,1462.1 46727,1534.7 46727,1534.7 46727,1472.1 46727,1472.1"];
	Layer3_Device9_OutputProj -> Layer3_Device9_Residual1	[pos="e,46727,1336.1 46727,1408.7 46727,1408.7 46727,1346.1 46727,1346.1"];
	Layer3_Device9_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="46426,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device9_Residual1 -> Layer3_Device9_LayerNorm2	[pos="e,46490,1210.1 46490,1282.7 46490,1282.7 46490,1220.1 46490,1220.1"];
	Layer3_Device9_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="46171,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device9_Residual1 -> Layer3_Device9_Residual2	[pos="e,46558,492 46892,1282.7 46892,1141.8 46892,492 46892,492 46892,492 46568,492 46568,492"];
	Layer3_Device9_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="46133,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device9_LayerNorm2 -> Layer3_Device9_GateProj	[pos="e,46286,1084.1 46286,1156.7 46286,1156.7 46286,1094.1 46286,1094.1"];
	Layer3_Device9_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="46446,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device9_LayerNorm2 -> Layer3_Device9_UpProj	[pos="e,46498,995.2 46498,1156.6 46498,1156.6 46498,1005.2 46498,1005.2"];
	Layer3_Device9_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="46133,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device9_GateProj -> Layer3_Device9_Activation	[pos="e,46064,905.95 46064,1030.8 46064,1030.8 46064,915.95 46064,915.95"];
	Layer3_Device9_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="46171,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device9_UpProj -> Layer3_Device9_ElemMul	[pos="e,46470,780.2 46470,941.61 46470,941.61 46470,790.2 46470,790.2"];
	Layer3_Device9_Activation -> Layer3_Device9_ElemMul	[pos="e,46133,780.09 46133,852.69 46133,852.69 46133,790.09 46133,790.09"];
	Layer3_Device9_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="46171,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device9_ElemMul -> Layer3_Device9_DownProj	[pos="e,46171,654.09 46171,726.69 46171,726.69 46171,664.09 46171,664.09"];
	Layer3_Device9_DownProj -> Layer3_Device9_Residual2	[pos="e,46171,528.09 46171,600.69 46171,600.69 46171,538.09 46171,538.09"];
	Layer3_Device9_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 9 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="44904,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device9_Residual2 -> Layer3_Device9_Output	[pos="e,45200,371.45 45784,510 45514,510 45200,510 45200,510 45200,510 45200,381.45 45200,381.45"];
	Layer3_Device9_Output -> Sequence_Aggregate	[pos="e,41049,240 44904,326.76 44904,290.14 44904,240 44904,240 44904,240 41059,240 41059,240"];
	Layer3_Device10_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53047,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device10_Input -> Layer3_Device10_LayerNorm1	[pos="e,53260,5153 53425,5270.1 53425,5231.9 53425,5153 53425,5153 53425,5153 53270,5153 53270,5153"];
	Layer3_Device10_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="51546,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device10_Input -> Layer3_Device10_Residual1	[pos="e,51933,1309 53566,5255.4 53566,4863.2 53566,1309 53566,1309 53566,1309 51943,1309 51943,1309"];
	Layer3_Device10_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52652,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device10_LayerNorm1 -> Layer3_Device10_QKVProj	[pos="e,53047,5054.1 53047,5126.7 53047,5126.7 53047,5064.1 53047,5064.1"];
	Layer3_Device10_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51406,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage0_RecvKV	[label="Local K,V",
		lp="51906,4957.4",
		pos="e,52164,4914.2 52164,5000.7 52164,5000.7 52164,4924.2 52164,4924.2"];
	Layer3_Device10_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52196,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage0_Attention	[label=Q_local,
		lp="52468,4860.9",
		pos="e,52377,4694.8 52377,5000.7 52377,5000.7 52377,4704.8 52377,4704.8"];
	Layer3_Device10_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49866,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage1_Attention	[label=Q_local,
		lp="49930,4764.4",
		pos="e,49853,4501.7 52002,5007 51175,5007 49853,5007 49853,5007 49853,5007 49853,4511.7 49853,4511.7"];
	Layer3_Device10_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50455,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage2_Attention	[label=Q_local,
		lp="52608,4667.9",
		pos="e,50647,4308.7 52472,5000.6 52472,4906.8 52472,4599 52472,4599 52472,4599 50647,4599 50647,4599 50647,4599 50647,4318.7 50647,4318.7"];
	Layer3_Device10_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52470,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage3_Attention	[label=Q_local,
		lp="52748,4571.4",
		pos="e,52643,4115.7 52643,5000.8 52643,5000.8 52643,4125.7 52643,4125.7"];
	Layer3_Device10_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49720,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage4_Attention	[label=Q_local,
		lp="49550,4474.9",
		pos="e,49578,3922.6 52002,5020 51100,5020 49578,5020 49578,5020 49578,5020 49578,3932.6 49578,3932.6"];
	Layer3_Device10_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52588,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage5_Attention	[label=Q_local,
		lp="52841,4378.4",
		pos="e,52760,3729.7 52760,5000.6 52760,5000.6 52760,3739.7 52760,3739.7"];
	Layer3_Device10_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52630,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage6_Attention	[label=Q_local,
		lp="52962,4281.9",
		pos="e,52840,3536.6 52840,5000.7 52840,5000.7 52840,3546.6 52840,3546.6"];
	Layer3_Device10_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49691,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage7_Attention	[label=Q_local,
		lp="49409,4185.4",
		pos="e,49474,3343.5 52002,5034 51073,5034 49474,5034 49474,5034 49474,5034 49474,3353.5 49474,3353.5"];
	Layer3_Device10_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52409,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage8_Attention	[label=Q_local,
		lp="53080,4088.9",
		pos="e,52641,3123 52894,5000.8 52894,4770.7 52894,3123 52894,3123 52894,3123 52651,3123 52651,3123"];
	Layer3_Device10_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49470,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage9_Attention	[label=Q_local,
		lp="49303,3992.4",
		pos="e,49400,2957.6 52002,5040 51054,5040 49400,5040 49400,5040 49400,5040 49400,2967.6 49400,2967.6"];
	Layer3_Device10_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="49557,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage10_Attention	[label=Q_local,
		lp="49168,3895.9",
		pos="e,49325,2737 52002,5047 51000,5047 49189,5047 49189,5047 49189,5047 49189,2737 49189,2737 49189,2737 49315,2737 49315,2737"];
	Layer3_Device10_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50896,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage11_Attention	[label=Q_local,
		lp="53194,3799.4",
		pos="e,50973,2571.5 52959,5000.5 52959,4740 52959,2672 52959,2672 52959,2672 50973,2672 50973,2672 50973,2672 50973,2581.5 50973,2581.5"];
	Layer3_Device10_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="51226,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage12_Attention	[label=Q_local,
		lp="53329,3702.9",
		pos="e,51156,2378.6 52992,5000.9 52992,4741.3 52992,2665 52992,2665 52992,2665 51156,2665 51156,2665 51156,2665 51156,2388.6 51156,2388.6"];
	Layer3_Device10_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="53241,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage13_Attention	[label=Q_local,
		lp="53469,3606.4",
		pos="e,53366,2185.8 53302,5014 53341,5014 53366,5014 53366,5014 53366,5014 53366,2195.8 53366,2195.8"];
	Layer3_Device10_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="50563,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage14_Attention	[label=Q_local,
		lp="49032,3509.9",
		pos="e,50420,1992.7 52927,5000.7 52927,4741.2 52927,2682 52927,2682 52927,2682 50420,2682 50420,2682 50420,2682 50420,2002.7 50420,2002.7"];
	Layer3_Device10_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="52293,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_QKVProj -> Layer3_Device10_Stage15_Attention	[label=Q_local,
		lp="53587,3413.4",
		pos="e,52525,1822 53302,5027 53427,5027 53519,5027 53519,5027 53519,5027 53519,1822 53519,1822 53519,1822 52535,1822 52535,1822"];
	Layer3_Device10_Stage0_RecvKV -> Layer3_Device10_Stage0_Attention	[pos="e,52145,4694.8 52145,4860.8 52145,4860.8 52145,4704.8 52145,4704.8"];
	Layer3_Device10_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="50987,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage0_RecvKV -> Layer3_Device10_Stage1_RecvKV	[label="Ring transfer",
		lp="51244,4764.4",
		pos="e,51196,4721 51196,4807.7 51196,4807.7 51196,4731 51196,4731"];
	Layer3_Device10_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="50387,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage0_Attention -> Layer3_Device10_Stage0_Accumulate	[pos="e,50387,4501.6 52196,4641.1 52196,4622.5 52196,4601 52196,4601 52196,4601 50387,4601 50387,4601 50387,4601 50387,4511.6 50387,4511.6"];
	Layer3_Device10_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="49934,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage0_Accumulate -> Layer3_Device10_Stage1_Accumulate	[pos="e,50160,4308.5 50160,4448.1 50160,4448.1 50160,4318.5 50160,4318.5"];
	Layer3_Device10_Stage1_RecvKV -> Layer3_Device10_Stage1_Attention	[pos="e,50082,4501.8 50082,4667.8 50082,4667.8 50082,4511.8 50082,4511.8"];
	Layer3_Device10_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51596,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage1_RecvKV -> Layer3_Device10_Stage2_RecvKV	[label="Ring transfer",
		lp="51294,4571.4",
		pos="e,51292,4528 51292,4614.7 51292,4614.7 51292,4538 51292,4538"];
	Layer3_Device10_Stage1_Attention -> Layer3_Device10_Stage1_Accumulate	[pos="e,49900,4308.5 49900,4448.1 49900,4448.1 49900,4318.5 49900,4318.5"];
	Layer3_Device10_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50052,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage1_Accumulate -> Layer3_Device10_Stage2_Accumulate	[pos="e,49993,4115.5 49993,4255.1 49993,4255.1 49993,4125.5 49993,4125.5"];
	Layer3_Device10_Stage2_RecvKV -> Layer3_Device10_Stage2_Attention	[pos="e,50681,4308.8 50681,4474.8 50681,4474.8 50681,4318.8 50681,4318.8"];
	Layer3_Device10_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51664,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage2_RecvKV -> Layer3_Device10_Stage3_RecvKV	[label="Ring transfer",
		lp="51678,4378.4",
		pos="e,51630,4335 51630,4421.7 51630,4421.7 51630,4345 51630,4345"];
	Layer3_Device10_Stage2_Attention -> Layer3_Device10_Stage2_Accumulate	[pos="e,50254,4115.5 50254,4255.1 50254,4255.1 50254,4125.5 50254,4125.5"];
	Layer3_Device10_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50241,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage2_Accumulate -> Layer3_Device10_Stage3_Accumulate	[pos="e,50146,3922.5 50146,4062.1 50146,4062.1 50146,3932.5 50146,3932.5"];
	Layer3_Device10_Stage3_RecvKV -> Layer3_Device10_Stage3_Attention	[pos="e,52411,4115.8 52411,4281.8 52411,4281.8 52411,4125.8 52411,4125.8"];
	Layer3_Device10_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51261,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage3_RecvKV -> Layer3_Device10_Stage4_RecvKV	[label="Ring transfer",
		lp="51445,4185.4",
		pos="e,51462,4142 51462,4228.7 51462,4228.7 51462,4152 51462,4152"];
	Layer3_Device10_Stage3_Attention -> Layer3_Device10_Stage3_Accumulate	[pos="e,50407,3922.8 52304,4062.2 52304,4031.1 52304,3984 52304,3984 52304,3984 50407,3984 50407,3984 50407,3984 50407,3932.8 50407,3932.8"];
	Layer3_Device10_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50170,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage3_Accumulate -> Layer3_Device10_Stage4_Accumulate	[pos="e,50206,3729.5 50206,3869.1 50206,3869.1 50206,3739.5 50206,3739.5"];
	Layer3_Device10_Stage4_RecvKV -> Layer3_Device10_Stage4_Attention	[pos="e,49886,3922.6 50388,4049 50116,4049 49886,4049 49886,4049 49886,4049 49886,3932.6 49886,3932.6"];
	Layer3_Device10_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51450,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage4_RecvKV -> Layer3_Device10_Stage5_RecvKV	[label="Ring transfer",
		lp="51403,3992.4",
		pos="e,51356,3949 51356,4035.7 51356,4035.7 51356,3959 51356,3959"];
	Layer3_Device10_Stage4_Attention -> Layer3_Device10_Stage4_Accumulate	[pos="e,49945,3729.5 49945,3869.1 49945,3869.1 49945,3739.5 49945,3739.5"];
	Layer3_Device10_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52109,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage4_Accumulate -> Layer3_Device10_Stage5_Accumulate	[pos="e,52088,3536.8 50191,3676.4 50191,3646.8 50191,3603 50191,3603 50191,3603 52088,3603 52088,3603 52088,3603 52088,3546.8 52088,3546.8"];
	Layer3_Device10_Stage5_RecvKV -> Layer3_Device10_Stage5_Attention	[pos="e,52363,3729.8 52363,3895.8 52363,3895.8 52363,3739.8 52363,3739.8"];
	Layer3_Device10_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="51379,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage5_RecvKV -> Layer3_Device10_Stage6_RecvKV	[label="Ring transfer",
		lp="51463,3799.4",
		pos="e,51414,3756 51414,3842.7 51414,3842.7 51414,3766 51414,3766"];
	Layer3_Device10_Stage5_Attention -> Layer3_Device10_Stage5_Accumulate	[pos="e,52341,3509 52378,3676.4 52378,3623.2 52378,3509 52378,3509 52378,3509 52351,3509 52351,3509"];
	Layer3_Device10_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="52109,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage5_Accumulate -> Layer3_Device10_Stage6_Accumulate	[pos="e,52109,3343.5 52109,3483.1 52109,3483.1 52109,3353.5 52109,3353.5"];
	Layer3_Device10_Stage6_RecvKV -> Layer3_Device10_Stage6_Attention	[pos="e,52609,3536.6 51969,3663 52285,3663 52609,3663 52609,3663 52609,3663 52609,3546.6 52609,3546.6"];
	Layer3_Device10_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="50900,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage6_RecvKV -> Layer3_Device10_Stage7_RecvKV	[label="Ring transfer",
		lp="51148,3606.4",
		pos="e,51140,3563 51140,3649.7 51140,3649.7 51140,3573 51140,3573"];
	Layer3_Device10_Stage6_Attention -> Layer3_Device10_Stage6_Accumulate	[pos="e,52341,3308 52520,3483.1 52520,3428.2 52520,3308 52520,3308 52520,3308 52351,3308 52351,3308"];
	Layer3_Device10_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51888,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage6_Accumulate -> Layer3_Device10_Stage7_Accumulate	[pos="e,51998,3150.5 51998,3290.1 51998,3290.1 51998,3160.5 51998,3160.5"];
	Layer3_Device10_Stage7_RecvKV -> Layer3_Device10_Stage7_Attention	[pos="e,49706,3343.6 50027,3470 49844,3470 49706,3470 49706,3470 49706,3470 49706,3353.6 49706,3353.6"];
	Layer3_Device10_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="50900,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage7_RecvKV -> Layer3_Device10_Stage8_RecvKV	[label="Ring transfer",
		lp="50948,3413.4",
		pos="e,50900,3370 50900,3456.7 50900,3456.7 50900,3380 50900,3380"];
	Layer3_Device10_Stage7_Attention -> Layer3_Device10_Stage7_Accumulate	[pos="e,51738,3150.7 49841,3290.3 49841,3255 49841,3197 49841,3197 49841,3197 51738,3197 51738,3197 51738,3197 51738,3160.7 51738,3160.7"];
	Layer3_Device10_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51888,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage7_Accumulate -> Layer3_Device10_Stage8_Accumulate	[pos="e,51888,2957.5 51888,3097.1 51888,3097.1 51888,2967.5 51888,2967.5"];
	Layer3_Device10_Stage8_RecvKV -> Layer3_Device10_Stage8_Attention	[pos="e,52259,3150.6 51490,3277 51857,3277 52259,3277 52259,3277 52259,3277 52259,3160.6 52259,3160.6"];
	Layer3_Device10_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="50679,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage8_RecvKV -> Layer3_Device10_Stage9_RecvKV	[label="Ring transfer",
		lp="50727,3220.4",
		pos="e,50790,3177 50790,3263.7 50790,3263.7 50790,3187 50790,3187"];
	Layer3_Device10_Stage8_Attention -> Layer3_Device10_Stage8_Accumulate	[pos="e,52120,2930 52192,3097.4 52192,3044.2 52192,2930 52192,2930 52192,2930 52130,2930 52130,2930"];
	Layer3_Device10_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50078,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage8_Accumulate -> Layer3_Device10_Stage9_Accumulate	[pos="e,50155,2764.4 51888,2904.3 51888,2882.8 51888,2856 51888,2856 51888,2856 50155,2856 50155,2856 50155,2856 50155,2774.4 50155,2774.4"];
	Layer3_Device10_Stage9_RecvKV -> Layer3_Device10_Stage9_Attention	[pos="e,49580,2957.6 49806,3084 49674,3084 49580,3084 49580,3084 49580,3084 49580,2967.6 49580,2967.6"];
	Layer3_Device10_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="50679,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage9_RecvKV -> Layer3_Device10_Stage10_RecvKV	[label="Ring transfer",
		lp="50727,3027.4",
		pos="e,50679,2984 50679,3070.7 50679,3070.7 50679,2994 50679,2994"];
	Layer3_Device10_Stage9_Attention -> Layer3_Device10_Stage9_Accumulate	[pos="e,50001,2764.4 49514,2904.4 49514,2864.8 49514,2795 49514,2795 49514,2795 50001,2795 50001,2795 50001,2795 50001,2774.4 50001,2774.4"];
	Layer3_Device10_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50078,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage9_Accumulate -> Layer3_Device10_Stage10_Accumulate	[pos="e,50078,2571.5 50078,2711.1 50078,2711.1 50078,2581.5 50078,2581.5"];
	Layer3_Device10_Stage10_RecvKV -> Layer3_Device10_Stage10_Attention	[pos="e,49774,2764.8 49774,2930.8 49774,2930.8 49774,2774.8 49774,2774.8"];
	Layer3_Device10_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="51287,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage10_RecvKV -> Layer3_Device10_Stage11_RecvKV	[label="Ring transfer",
		lp="50996,2834.4",
		pos="e,50983,2791 50983,2877.7 50983,2877.7 50983,2801 50983,2801"];
	Layer3_Device10_Stage10_Attention -> Layer3_Device10_Stage10_Accumulate	[pos="e,49846,2544 49593,2711.4 49593,2658.2 49593,2544 49593,2544 49593,2544 49836,2544 49836,2544"];
	Layer3_Device10_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50705,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage10_Accumulate -> Layer3_Device10_Stage11_Accumulate	[pos="e,50569,2378.6 50310,2544 50437,2544 50569,2544 50569,2544 50569,2544 50569,2388.6 50569,2388.6"];
	Layer3_Device10_Stage11_RecvKV -> Layer3_Device10_Stage11_Attention	[pos="e,50819,2571.6 50819,2684.7 50819,2684.7 50819,2581.6 50819,2581.6"];
	Layer3_Device10_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52105,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage11_RecvKV -> Layer3_Device10_Stage12_RecvKV	[label="Ring transfer",
		lp="51744,2641.4",
		pos="e,51696,2598 51696,2684.7 51696,2684.7 51696,2608 51696,2608"];
	Layer3_Device10_Stage11_Attention -> Layer3_Device10_Stage11_Accumulate	[pos="e,50800,2378.5 50800,2518.1 50800,2518.1 50800,2388.5 50800,2388.5"];
	Layer3_Device10_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="50823,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage11_Accumulate -> Layer3_Device10_Stage12_Accumulate	[pos="e,50764,2185.5 50764,2325.1 50764,2325.1 50764,2195.5 50764,2195.5"];
	Layer3_Device10_Stage12_RecvKV -> Layer3_Device10_Stage12_Attention	[pos="e,51321,2378.6 51321,2491.7 51321,2491.7 51321,2388.6 51321,2388.6"];
	Layer3_Device10_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52435,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage12_RecvKV -> Layer3_Device10_Stage13_RecvKV	[label="Ring transfer",
		lp="52318,2448.4",
		pos="e,52270,2405 52270,2491.7 52270,2491.7 52270,2415 52270,2415"];
	Layer3_Device10_Stage12_Attention -> Layer3_Device10_Stage12_Accumulate	[pos="e,51024,2185.5 51024,2325.1 51024,2325.1 51024,2195.5 51024,2195.5"];
	Layer3_Device10_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51084,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage12_Accumulate -> Layer3_Device10_Stage13_Accumulate	[pos="e,50954,1992.5 50954,2132.1 50954,2132.1 50954,2002.5 50954,2002.5"];
	Layer3_Device10_Stage13_RecvKV -> Layer3_Device10_Stage13_Attention	[pos="e,53182,2185.8 53182,2351.8 53182,2351.8 53182,2195.8 53182,2195.8"];
	Layer3_Device10_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52032,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage13_RecvKV -> Layer3_Device10_Stage14_RecvKV	[label="Ring transfer",
		lp="52281,2255.4",
		pos="e,52234,2212 52234,2298.7 52234,2298.7 52234,2222 52234,2222"];
	Layer3_Device10_Stage13_Attention -> Layer3_Device10_Stage13_Accumulate	[pos="e,51214,1992.5 53111,2131.9 53111,2105.4 53111,2069 53111,2069 53111,2069 51214,2069 51214,2069 51214,2069 51214,2002.5 51214,2002.5"];
	Layer3_Device10_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51084,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage13_Accumulate -> Layer3_Device10_Stage14_Accumulate	[pos="e,51084,1839.9 51084,1939.3 51084,1939.3 51084,1849.9 51084,1849.9"];
	Layer3_Device10_Stage14_RecvKV -> Layer3_Device10_Stage14_Attention	[pos="e,50693,1992.6 51158,2119 50904,2119 50693,2119 50693,2119 50693,2119 50693,2002.6 50693,2002.6"];
	Layer3_Device10_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="52293,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device10_Stage14_RecvKV -> Layer3_Device10_Stage15_RecvKV	[label="Ring transfer",
		lp="52210,2062.4",
		pos="e,52162,2019 52162,2105.7 52162,2105.7 52162,2029 52162,2029"];
	Layer3_Device10_Stage14_Attention -> Layer3_Device10_Stage14_Accumulate	[pos="e,50852,1804 50761,1939.2 50761,1893.2 50761,1804 50761,1804 50761,1804 50842,1804 50842,1804"];
	Layer3_Device10_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="51546,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device10_Stage14_Accumulate -> Layer3_Device10_Stage15_Accumulate	[pos="e,51344,1714.2 51316,1804 51333,1804 51344,1804 51344,1804 51344,1804 51344,1724.2 51344,1724.2"];
	Layer3_Device10_Stage15_RecvKV -> Layer3_Device10_Stage15_Attention	[pos="e,52293,1840 52293,1912.6 52293,1912.6 52293,1850 52293,1850"];
	Layer3_Device10_Stage15_Attention -> Layer3_Device10_Stage15_Accumulate	[pos="e,51575,1714 52061,1822 51851,1822 51575,1822 51575,1822 51575,1822 51575,1724 51575,1724"];
	Layer3_Device10_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="51546,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device10_Stage15_Accumulate -> Layer3_Device10_ConcatHeads	[pos="e,51546,1588.1 51546,1660.7 51546,1660.7 51546,1598.1 51546,1598.1"];
	Layer3_Device10_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="51546,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device10_ConcatHeads -> Layer3_Device10_OutputProj	[pos="e,51546,1462.1 51546,1534.7 51546,1534.7 51546,1472.1 51546,1472.1"];
	Layer3_Device10_OutputProj -> Layer3_Device10_Residual1	[pos="e,51546,1336.1 51546,1408.7 51546,1408.7 51546,1346.1 51546,1346.1"];
	Layer3_Device10_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="51245,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device10_Residual1 -> Layer3_Device10_LayerNorm2	[pos="e,51309,1210.1 51309,1282.7 51309,1282.7 51309,1220.1 51309,1220.1"];
	Layer3_Device10_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="50990,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device10_Residual1 -> Layer3_Device10_Residual2	[pos="e,51377,501 51711,1282.5 51711,1142.2 51711,501 51711,501 51711,501 51387,501 51387,501"];
	Layer3_Device10_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="50952,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device10_LayerNorm2 -> Layer3_Device10_GateProj	[pos="e,51104,1084.1 51104,1156.7 51104,1156.7 51104,1094.1 51104,1094.1"];
	Layer3_Device10_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="51265,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device10_LayerNorm2 -> Layer3_Device10_UpProj	[pos="e,51318,995.2 51318,1156.6 51318,1156.6 51318,1005.2 51318,1005.2"];
	Layer3_Device10_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="50952,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device10_GateProj -> Layer3_Device10_Activation	[pos="e,50884,905.95 50884,1030.8 50884,1030.8 50884,915.95 50884,915.95"];
	Layer3_Device10_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="50990,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device10_UpProj -> Layer3_Device10_ElemMul	[pos="e,51289,780.2 51289,941.61 51289,941.61 51289,790.2 51289,790.2"];
	Layer3_Device10_Activation -> Layer3_Device10_ElemMul	[pos="e,50952,780.09 50952,852.69 50952,852.69 50952,790.09 50952,790.09"];
	Layer3_Device10_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="50990,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device10_ElemMul -> Layer3_Device10_DownProj	[pos="e,50990,654.09 50990,726.69 50990,726.69 50990,664.09 50990,664.09"];
	Layer3_Device10_DownProj -> Layer3_Device10_Residual2	[pos="e,50990,528.09 50990,600.69 50990,600.69 50990,538.09 50990,538.09"];
	Layer3_Device10_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 10 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="49097,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device10_Residual2 -> Layer3_Device10_Output	[pos="e,49362,382.57 50603,510 50122,510 49362,510 49362,510 49362,510 49362,392.57 49362,392.57"];
	Layer3_Device10_Output -> Sequence_Aggregate	[pos="e,41025,227 49097,326.88 49097,286.19 49097,227 49097,227 49097,227 41035,227 41035,227"];
	Layer3_Device11_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="57936,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device11_Input -> Layer3_Device11_LayerNorm1	[pos="e,58120,5180 58120,5274.1 58120,5274.1 58120,5190 58120,5190"];
	Layer3_Device11_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="56378,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device11_Input -> Layer3_Device11_Residual1	[pos="e,56765,1309 58564,5259.5 58564,4888.9 58564,1309 58564,1309 58564,1309 56775,1309 56775,1309"];
	Layer3_Device11_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57475,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device11_LayerNorm1 -> Layer3_Device11_QKVProj	[pos="e,57924,5054.1 57924,5126.7 57924,5126.7 57924,5064.1 57924,5064.1"];
	Layer3_Device11_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56229,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage0_RecvKV	[label="Local K,V",
		lp="56730,4957.4",
		pos="e,56987,4914.2 56987,5000.7 56987,5000.7 56987,4924.2 56987,4924.2"];
	Layer3_Device11_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57019,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage0_Attention	[label=Q_local,
		lp="57291,4860.9",
		pos="e,57200,4694.8 57200,5000.7 57200,5000.7 57200,4704.8 57200,4704.8"];
	Layer3_Device11_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54689,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage1_Attention	[label=Q_local,
		lp="54753,4764.4",
		pos="e,54864,4501.7 56825,5007 56052,5007 54864,5007 54864,5007 54864,5007 54864,4511.7 54864,4511.7"];
	Layer3_Device11_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55278,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage2_Attention	[label=Q_local,
		lp="57431,4667.9",
		pos="e,55470,4308.5 57295,5000.6 57295,4907.5 57295,4604 57295,4604 57295,4604 55470,4604 55470,4604 55470,4604 55470,4318.5 55470,4318.5"];
	Layer3_Device11_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57293,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage3_Attention	[label=Q_local,
		lp="57571,4571.4",
		pos="e,57466,4115.7 57466,5000.8 57466,5000.8 57466,4125.7 57466,4125.7"];
	Layer3_Device11_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54543,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage4_Attention	[label=Q_local,
		lp="54373,4474.9",
		pos="e,54429,3922.6 56825,5020 55931,5020 54429,5020 54429,5020 54429,5020 54429,3932.6 54429,3932.6"];
	Layer3_Device11_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57411,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage5_Attention	[label=Q_local,
		lp="57664,4378.4",
		pos="e,57584,3729.7 57584,5000.6 57584,5000.6 57584,3739.7 57584,3739.7"];
	Layer3_Device11_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57502,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage6_Attention	[label=Q_local,
		lp="57785,4281.9",
		pos="e,57688,3536.6 57688,5000.7 57688,5000.7 57688,3546.6 57688,3546.6"];
	Layer3_Device11_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54563,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage7_Attention	[label=Q_local,
		lp="54235,4185.4",
		pos="e,54331,3325 56825,5034 55897,5034 54300,5034 54300,5034 54300,5034 54300,3325 54300,3325 54300,3325 54321,3325 54321,3325"];
	Layer3_Device11_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57290,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage8_Attention	[label=Q_local,
		lp="57903,4088.9",
		pos="e,57522,3123 57779,5000.8 57779,4770.7 57779,3123 57779,3123 57779,3123 57532,3123 57532,3123"];
	Layer3_Device11_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54351,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage9_Attention	[label=Q_local,
		lp="54124,3992.4",
		pos="e,54289,2957.6 56825,5040 55894,5040 54289,5040 54289,5040 54289,5040 54289,2967.6 54289,2967.6"];
	Layer3_Device11_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="54588,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage10_Attention	[label=Q_local,
		lp="53991,3895.9",
		pos="e,54356,2737 56825,5047 55845,5047 54101,5047 54101,5047 54101,5047 54101,2737 54101,2737 54101,2737 54346,2737 54346,2737"];
	Layer3_Device11_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55787,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage11_Attention	[label=Q_local,
		lp="58023,3799.4",
		pos="e,55864,2571.7 57825,5000.6 57825,4737.8 57825,2636 57825,2636 57825,2636 55864,2636 55864,2636 55864,2636 55864,2581.7 55864,2581.7"];
	Layer3_Device11_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="56058,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage12_Attention	[label=Q_local,
		lp="58161,3702.9",
		pos="e,56047,2378.4 57870,5000.5 57870,4737.6 57870,2634 57870,2634 57870,2634 56047,2634 56047,2634 56047,2634 56047,2388.4 56047,2388.4"];
	Layer3_Device11_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="58073,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage13_Attention	[label=Q_local,
		lp="58301,3606.4",
		pos="e,58246,2185.8 58125,5014 58197,5014 58246,5014 58246,5014 58246,5014 58246,2195.8 58246,2195.8"];
	Layer3_Device11_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="55395,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage14_Attention	[label=Q_local,
		lp="53851,3509.9",
		pos="e,55234,1992.6 58020,5000.8 58020,4728.5 58020,2462 58020,2462 58020,2462 55234,2462 55234,2462 55234,2462 55234,2002.6 55234,2002.6"];
	Layer3_Device11_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="57125,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_QKVProj -> Layer3_Device11_Stage15_Attention	[label=Q_local,
		lp="58419,3413.4",
		pos="e,57357,1822 58125,5027 58296,5027 58434,5027 58434,5027 58434,5027 58434,1822 58434,1822 58434,1822 57367,1822 57367,1822"];
	Layer3_Device11_Stage0_RecvKV -> Layer3_Device11_Stage0_Attention	[pos="e,56968,4694.8 56968,4860.8 56968,4860.8 56968,4704.8 56968,4704.8"];
	Layer3_Device11_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="55810,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage0_RecvKV -> Layer3_Device11_Stage1_RecvKV	[label="Ring transfer",
		lp="56139,4764.4",
		pos="e,56020,4721 56020,4807.7 56020,4807.7 56020,4731 56020,4731"];
	Layer3_Device11_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="55210,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage0_Attention -> Layer3_Device11_Stage0_Accumulate	[pos="e,55210,4501.4 57019,4641.3 57019,4624.5 57019,4606 57019,4606 57019,4606 55210,4606 55210,4606 55210,4606 55210,4511.4 55210,4511.4"];
	Layer3_Device11_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="54757,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage0_Accumulate -> Layer3_Device11_Stage1_Accumulate	[pos="e,54984,4308.5 54984,4448.1 54984,4448.1 54984,4318.5 54984,4318.5"];
	Layer3_Device11_Stage1_RecvKV -> Layer3_Device11_Stage1_Attention	[pos="e,54905,4501.8 54905,4667.8 54905,4667.8 54905,4511.8 54905,4511.8"];
	Layer3_Device11_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56419,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage1_RecvKV -> Layer3_Device11_Stage2_RecvKV	[label="Ring transfer",
		lp="56117,4571.4",
		pos="e,56114,4528 56114,4614.7 56114,4614.7 56114,4538 56114,4538"];
	Layer3_Device11_Stage1_Attention -> Layer3_Device11_Stage1_Accumulate	[pos="e,54723,4308.5 54723,4448.1 54723,4448.1 54723,4318.5 54723,4318.5"];
	Layer3_Device11_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="54875,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage1_Accumulate -> Layer3_Device11_Stage2_Accumulate	[pos="e,54816,4115.5 54816,4255.1 54816,4255.1 54816,4125.5 54816,4125.5"];
	Layer3_Device11_Stage2_RecvKV -> Layer3_Device11_Stage2_Attention	[pos="e,55504,4308.8 55504,4474.8 55504,4474.8 55504,4318.8 55504,4318.8"];
	Layer3_Device11_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56487,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage2_RecvKV -> Layer3_Device11_Stage3_RecvKV	[label="Ring transfer",
		lp="56501,4378.4",
		pos="e,56453,4335 56453,4421.7 56453,4421.7 56453,4345 56453,4345"];
	Layer3_Device11_Stage2_Attention -> Layer3_Device11_Stage2_Accumulate	[pos="e,55076,4115.5 55076,4255.1 55076,4255.1 55076,4125.5 55076,4125.5"];
	Layer3_Device11_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55064,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage2_Accumulate -> Layer3_Device11_Stage3_Accumulate	[pos="e,54970,3922.5 54970,4062.1 54970,4062.1 54970,3932.5 54970,3932.5"];
	Layer3_Device11_Stage3_RecvKV -> Layer3_Device11_Stage3_Attention	[pos="e,57234,4115.8 57234,4281.8 57234,4281.8 57234,4125.8 57234,4125.8"];
	Layer3_Device11_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56084,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage3_RecvKV -> Layer3_Device11_Stage4_RecvKV	[label="Ring transfer",
		lp="56401,4185.4",
		pos="e,56286,4142 56286,4228.7 56286,4228.7 56286,4152 56286,4152"];
	Layer3_Device11_Stage3_Attention -> Layer3_Device11_Stage3_Accumulate	[pos="e,55230,3922.8 57127,4062.4 57127,4032.8 57127,3989 57127,3989 57127,3989 55230,3989 55230,3989 55230,3989 55230,3932.8 55230,3932.8"];
	Layer3_Device11_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="54993,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage3_Accumulate -> Layer3_Device11_Stage4_Accumulate	[pos="e,55028,3729.5 55028,3869.1 55028,3869.1 55028,3739.5 55028,3739.5"];
	Layer3_Device11_Stage4_RecvKV -> Layer3_Device11_Stage4_Attention	[pos="e,54709,3922.6 55211,4049 54939,4049 54709,4049 54709,4049 54709,4049 54709,3932.6 54709,3932.6"];
	Layer3_Device11_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56273,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage4_RecvKV -> Layer3_Device11_Stage5_RecvKV	[label="Ring transfer",
		lp="56226,3992.4",
		pos="e,56178,3949 56178,4035.7 56178,4035.7 56178,3959 56178,3959"];
	Layer3_Device11_Stage4_Attention -> Layer3_Device11_Stage4_Accumulate	[pos="e,54768,3729.5 54768,3869.1 54768,3869.1 54768,3739.5 54768,3739.5"];
	Layer3_Device11_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56981,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage4_Accumulate -> Layer3_Device11_Stage5_Accumulate	[pos="e,56936,3536.5 55038,3676.1 55038,3656.7 55038,3634 55038,3634 55038,3634 56936,3634 56936,3634 56936,3634 56936,3546.5 56936,3546.5"];
	Layer3_Device11_Stage5_RecvKV -> Layer3_Device11_Stage5_Attention	[pos="e,57186,3729.8 57186,3895.8 57186,3895.8 57186,3739.8 57186,3739.8"];
	Layer3_Device11_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="56202,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage5_RecvKV -> Layer3_Device11_Stage6_RecvKV	[label="Ring transfer",
		lp="56285,3799.4",
		pos="e,56238,3756 56238,3842.7 56238,3842.7 56238,3766 56238,3766"];
	Layer3_Device11_Stage5_Attention -> Layer3_Device11_Stage5_Accumulate	[pos="e,57196,3536.5 57196,3676.1 57196,3676.1 57196,3546.5 57196,3546.5"];
	Layer3_Device11_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56981,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage5_Accumulate -> Layer3_Device11_Stage6_Accumulate	[pos="e,56981,3343.5 56981,3483.1 56981,3483.1 56981,3353.5 56981,3353.5"];
	Layer3_Device11_Stage6_RecvKV -> Layer3_Device11_Stage6_Attention	[pos="e,57456,3536.6 56792,3663 57118,3663 57456,3663 57456,3663 57456,3663 57456,3546.6 57456,3546.6"];
	Layer3_Device11_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="55772,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage6_RecvKV -> Layer3_Device11_Stage7_RecvKV	[label="Ring transfer",
		lp="55969,3606.4",
		pos="e,55987,3563 55987,3649.7 55987,3649.7 55987,3573 55987,3573"];
	Layer3_Device11_Stage6_Attention -> Layer3_Device11_Stage6_Accumulate	[pos="e,57213,3316 57396,3483.4 57396,3430.2 57396,3316 57396,3316 57396,3316 57223,3316 57223,3316"];
	Layer3_Device11_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56769,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage6_Accumulate -> Layer3_Device11_Stage7_Accumulate	[pos="e,56875,3150.5 56875,3290.1 56875,3290.1 56875,3160.5 56875,3160.5"];
	Layer3_Device11_Stage7_RecvKV -> Layer3_Device11_Stage7_Attention	[pos="e,54778,3343.6 54898,3470 54825,3470 54778,3470 54778,3470 54778,3470 54778,3353.6 54778,3353.6"];
	Layer3_Device11_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="55772,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage7_RecvKV -> Layer3_Device11_Stage8_RecvKV	[label="Ring transfer",
		lp="55820,3413.4",
		pos="e,55772,3370 55772,3456.7 55772,3456.7 55772,3380 55772,3380"];
	Layer3_Device11_Stage7_Attention -> Layer3_Device11_Stage7_Accumulate	[pos="e,56615,3150.7 54717,3290.3 54717,3257.8 54717,3207 54717,3207 54717,3207 56615,3207 56615,3207 56615,3207 56615,3160.7 56615,3160.7"];
	Layer3_Device11_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56769,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage7_Accumulate -> Layer3_Device11_Stage8_Accumulate	[pos="e,56769,2957.5 56769,3097.1 56769,3097.1 56769,2967.5 56769,2967.5"];
	Layer3_Device11_Stage8_RecvKV -> Layer3_Device11_Stage8_Attention	[pos="e,57136,3150.6 56362,3277 56731,3277 57136,3277 57136,3277 57136,3277 57136,3160.6 57136,3160.6"];
	Layer3_Device11_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="55560,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage8_RecvKV -> Layer3_Device11_Stage9_RecvKV	[label="Ring transfer",
		lp="55714,3220.4",
		pos="e,55666,3177 55666,3263.7 55666,3263.7 55666,3187 55666,3187"];
	Layer3_Device11_Stage8_Attention -> Layer3_Device11_Stage8_Accumulate	[pos="e,57001,2930 57148,3097.4 57148,3044.2 57148,2930 57148,2930 57148,2930 57011,2930 57011,2930"];
	Layer3_Device11_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55109,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage8_Accumulate -> Layer3_Device11_Stage9_Accumulate	[pos="e,55186,2764.7 56769,2904.3 56769,2871.8 56769,2821 56769,2821 56769,2821 55186,2821 55186,2821 55186,2821 55186,2774.7 55186,2774.7"];
	Layer3_Device11_Stage9_RecvKV -> Layer3_Device11_Stage9_Attention	[pos="e,54457,2957.6 54687,3084 54552,3084 54457,3084 54457,3084 54457,3084 54457,2967.6 54457,2967.6"];
	Layer3_Device11_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="55560,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage9_RecvKV -> Layer3_Device11_Stage10_RecvKV	[label="Ring transfer",
		lp="55608,3027.4",
		pos="e,55560,2984 55560,3070.7 55560,3070.7 55560,2994 55560,2994"];
	Layer3_Device11_Stage9_Attention -> Layer3_Device11_Stage9_Accumulate	[pos="e,55032,2764.7 54470,2904.2 54470,2869.2 54470,2812 54470,2812 54470,2812 55032,2812 55032,2812 55032,2812 55032,2774.7 55032,2774.7"];
	Layer3_Device11_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55109,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage9_Accumulate -> Layer3_Device11_Stage10_Accumulate	[pos="e,55109,2571.5 55109,2711.1 55109,2711.1 55109,2581.5 55109,2581.5"];
	Layer3_Device11_Stage10_RecvKV -> Layer3_Device11_Stage10_Attention	[pos="e,54730,2764.6 54730,2877.7 54730,2877.7 54730,2774.6 54730,2774.6"];
	Layer3_Device11_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56318,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage10_RecvKV -> Layer3_Device11_Stage11_RecvKV	[label="Ring transfer",
		lp="55877,2834.4",
		pos="e,55939,2791 55939,2877.7 55939,2877.7 55939,2801 55939,2801"];
	Layer3_Device11_Stage10_Attention -> Layer3_Device11_Stage10_Accumulate	[pos="e,54877,2544 54588,2711.4 54588,2658.2 54588,2544 54588,2544 54588,2544 54867,2544 54867,2544"];
	Layer3_Device11_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55537,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage10_Accumulate -> Layer3_Device11_Stage11_Accumulate	[pos="e,55323,2378.5 55323,2518.1 55323,2518.1 55323,2388.5 55323,2388.5"];
	Layer3_Device11_Stage11_RecvKV -> Layer3_Device11_Stage11_Attention	[pos="e,55710,2571.6 55710,2684.7 55710,2684.7 55710,2581.6 55710,2581.6"];
	Layer3_Device11_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56996,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage11_RecvKV -> Layer3_Device11_Stage12_RecvKV	[label="Ring transfer",
		lp="56429,2641.4",
		pos="e,56657,2598 56657,2684.7 56657,2684.7 56657,2608 56657,2608"];
	Layer3_Device11_Stage11_Attention -> Layer3_Device11_Stage11_Accumulate	[pos="e,55662,2378.5 55662,2518.1 55662,2518.1 55662,2388.5 55662,2388.5"];
	Layer3_Device11_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55655,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage11_Accumulate -> Layer3_Device11_Stage12_Accumulate	[pos="e,55596,2185.5 55596,2325.1 55596,2325.1 55596,2195.5 55596,2195.5"];
	Layer3_Device11_Stage12_RecvKV -> Layer3_Device11_Stage12_Attention	[pos="e,56183,2378.6 56183,2491.7 56183,2491.7 56183,2388.6 56183,2388.6"];
	Layer3_Device11_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57267,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage12_RecvKV -> Layer3_Device11_Stage13_RecvKV	[label="Ring transfer",
		lp="57179,2448.4",
		pos="e,57132,2405 57132,2491.7 57132,2491.7 57132,2415 57132,2415"];
	Layer3_Device11_Stage12_Attention -> Layer3_Device11_Stage12_Accumulate	[pos="e,55856,2185.5 55856,2325.1 55856,2325.1 55856,2195.5 55856,2195.5"];
	Layer3_Device11_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55916,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage12_Accumulate -> Layer3_Device11_Stage13_Accumulate	[pos="e,55786,1992.5 55786,2132.1 55786,2132.1 55786,2002.5 55786,2002.5"];
	Layer3_Device11_Stage13_RecvKV -> Layer3_Device11_Stage13_Attention	[pos="e,58014,2185.8 58014,2351.8 58014,2351.8 58014,2195.8 58014,2195.8"];
	Layer3_Device11_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="56864,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage13_RecvKV -> Layer3_Device11_Stage14_RecvKV	[label="Ring transfer",
		lp="57048,2255.4",
		pos="e,57066,2212 57066,2298.7 57066,2298.7 57066,2222 57066,2222"];
	Layer3_Device11_Stage13_Attention -> Layer3_Device11_Stage13_Accumulate	[pos="e,56046,1992.7 57943,2132.2 57943,2107 57943,2073 57943,2073 57943,2073 56046,2073 56046,2073 56046,2073 56046,2002.7 56046,2002.7"];
	Layer3_Device11_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="55916,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage13_Accumulate -> Layer3_Device11_Stage14_Accumulate	[pos="e,55916,1839.9 55916,1939.3 55916,1939.3 55916,1849.9 55916,1849.9"];
	Layer3_Device11_Stage14_RecvKV -> Layer3_Device11_Stage14_Attention	[pos="e,55525,1992.6 55990,2119 55736,2119 55525,2119 55525,2119 55525,2119 55525,2002.6 55525,2002.6"];
	Layer3_Device11_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="57125,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device11_Stage14_RecvKV -> Layer3_Device11_Stage15_RecvKV	[label="Ring transfer",
		lp="57042,2062.4",
		pos="e,56994,2019 56994,2105.7 56994,2105.7 56994,2029 56994,2029"];
	Layer3_Device11_Stage14_Attention -> Layer3_Device11_Stage14_Accumulate	[pos="e,55684,1804 55593,1939.2 55593,1893.2 55593,1804 55593,1804 55593,1804 55674,1804 55674,1804"];
	Layer3_Device11_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="56378,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device11_Stage14_Accumulate -> Layer3_Device11_Stage15_Accumulate	[pos="e,56176,1714.2 56148,1804 56165,1804 56176,1804 56176,1804 56176,1804 56176,1724.2 56176,1724.2"];
	Layer3_Device11_Stage15_RecvKV -> Layer3_Device11_Stage15_Attention	[pos="e,57125,1840 57125,1912.6 57125,1912.6 57125,1850 57125,1850"];
	Layer3_Device11_Stage15_Attention -> Layer3_Device11_Stage15_Accumulate	[pos="e,56407,1714 56893,1822 56683,1822 56407,1822 56407,1822 56407,1822 56407,1724 56407,1724"];
	Layer3_Device11_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="56378,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device11_Stage15_Accumulate -> Layer3_Device11_ConcatHeads	[pos="e,56378,1588.1 56378,1660.7 56378,1660.7 56378,1598.1 56378,1598.1"];
	Layer3_Device11_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="56378,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device11_ConcatHeads -> Layer3_Device11_OutputProj	[pos="e,56378,1462.1 56378,1534.7 56378,1534.7 56378,1472.1 56378,1472.1"];
	Layer3_Device11_OutputProj -> Layer3_Device11_Residual1	[pos="e,56378,1336.1 56378,1408.7 56378,1408.7 56378,1346.1 56378,1346.1"];
	Layer3_Device11_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="56077,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device11_Residual1 -> Layer3_Device11_LayerNorm2	[pos="e,56141,1210.1 56141,1282.7 56141,1282.7 56141,1220.1 56141,1220.1"];
	Layer3_Device11_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="55822,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device11_Residual1 -> Layer3_Device11_Residual2	[pos="e,56209,501 56543,1282.5 56543,1142.2 56543,501 56543,501 56543,501 56219,501 56219,501"];
	Layer3_Device11_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="55784,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device11_LayerNorm2 -> Layer3_Device11_GateProj	[pos="e,55936,1084.1 55936,1156.7 55936,1156.7 55936,1094.1 55936,1094.1"];
	Layer3_Device11_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="56097,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device11_LayerNorm2 -> Layer3_Device11_UpProj	[pos="e,56150,995.2 56150,1156.6 56150,1156.6 56150,1005.2 56150,1005.2"];
	Layer3_Device11_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="55784,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device11_GateProj -> Layer3_Device11_Activation	[pos="e,55716,905.95 55716,1030.8 55716,1030.8 55716,915.95 55716,915.95"];
	Layer3_Device11_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="55822,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device11_UpProj -> Layer3_Device11_ElemMul	[pos="e,56121,780.2 56121,941.61 56121,941.61 56121,790.2 56121,790.2"];
	Layer3_Device11_Activation -> Layer3_Device11_ElemMul	[pos="e,55784,780.09 55784,852.69 55784,852.69 55784,790.09 55784,790.09"];
	Layer3_Device11_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="55822,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device11_ElemMul -> Layer3_Device11_DownProj	[pos="e,55822,654.09 55822,726.69 55822,726.69 55822,664.09 55822,664.09"];
	Layer3_Device11_DownProj -> Layer3_Device11_Residual2	[pos="e,55822,528.09 55822,600.69 55822,600.69 55822,538.09 55822,538.09"];
	Layer3_Device11_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 11 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="53312,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device11_Residual2 -> Layer3_Device11_Output	[pos="e,53613,364 55442,474.59 55442,434.6 55442,364 55442,364 55442,364 53623,364 53623,364"];
	Layer3_Device11_Output -> Sequence_Aggregate	[pos="e,41002,214 53312,326.86 53312,282.3 53312,214 53312,214 53312,214 41012,214 41012,214"];
	Layer3_Device12_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="62876,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device12_Input -> Layer3_Device12_LayerNorm1	[pos="e,62986,5180.4 62986,5262 62986,5262 62986,5190.4 62986,5190.4"];
	Layer3_Device12_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="61210,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device12_Input -> Layer3_Device12_Residual1	[pos="e,61597,1309 63369,5260.6 63369,4896.3 63369,1309 63369,1309 63369,1309 61607,1309 61607,1309"];
	Layer3_Device12_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62307,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device12_LayerNorm1 -> Layer3_Device12_QKVProj	[pos="e,62810,5054.1 62810,5126.7 62810,5126.7 62810,5064.1 62810,5064.1"];
	Layer3_Device12_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61061,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage0_RecvKV	[label="Local K,V",
		lp="61562,4957.4",
		pos="e,61819,4914.2 61819,5000.7 61819,5000.7 61819,4924.2 61819,4924.2"];
	Layer3_Device12_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61814,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage0_Attention	[label=Q_local,
		lp="62123,4860.9",
		pos="e,62013,4694.8 62013,5000.7 62013,5000.7 62013,4704.8 62013,4704.8"];
	Layer3_Device12_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59521,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage1_Attention	[label=Q_local,
		lp="59585,4764.4",
		pos="e,59506,4501.7 61657,5007 60830,5007 59506,5007 59506,5007 59506,5007 59506,4511.7 59506,4511.7"];
	Layer3_Device12_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60110,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage2_Attention	[label=Q_local,
		lp="62263,4667.9",
		pos="e,60302,4308.7 62108,5000.8 62108,4900.4 62108,4551 62108,4551 62108,4551 60302,4551 60302,4551 60302,4551 60302,4318.7 60302,4318.7"];
	Layer3_Device12_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62125,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage3_Attention	[label=Q_local,
		lp="62403,4571.4",
		pos="e,62298,4115.7 62298,5000.8 62298,5000.8 62298,4125.7 62298,4125.7"];
	Layer3_Device12_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59375,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage4_Attention	[label=Q_local,
		lp="59205,4474.9",
		pos="e,59284,3922.6 61657,5020 60769,5020 59284,5020 59284,5020 59284,5020 59284,3932.6 59284,3932.6"];
	Layer3_Device12_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62243,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage5_Attention	[label=Q_local,
		lp="62496,4378.4",
		pos="e,62416,3729.7 62416,5000.6 62416,5000.6 62416,3739.7 62416,3739.7"];
	Layer3_Device12_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62290,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage6_Attention	[label=Q_local,
		lp="62617,4281.9",
		pos="e,62498,3536.6 62498,5000.7 62498,5000.7 62498,3546.6 62498,3546.6"];
	Layer3_Device12_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59351,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage7_Attention	[label=Q_local,
		lp="59064,4185.4",
		pos="e,59135,3343.5 61657,5034 60729,5034 59135,5034 59135,5034 59135,5034 59135,3353.5 59135,3353.5"];
	Layer3_Device12_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62115,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage8_Attention	[label=Q_local,
		lp="62735,4088.9",
		pos="e,62347,3123 62555,5000.8 62555,4770.7 62555,3123 62555,3123 62555,3123 62357,3123 62357,3123"];
	Layer3_Device12_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59176,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage9_Attention	[label=Q_local,
		lp="58963,3992.4",
		pos="e,59092,2957.6 61657,5040 60718,5040 59092,5040 59092,5040 59092,5040 59092,2967.6 59092,2967.6"];
	Layer3_Device12_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="59221,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage10_Attention	[label=Q_local,
		lp="58823,3895.9",
		pos="e,58989,2737 61657,5047 60659,5047 58862,5047 58862,5047 58862,5047 58862,2737 58862,2737 58862,2737 58979,2737 58979,2737"];
	Layer3_Device12_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60560,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage11_Attention	[label=Q_local,
		lp="62851,3799.4",
		pos="e,60637,2571.7 62622,5000.9 62622,4741.6 62622,2667 62622,2667 62622,2667 60637,2667 60637,2667 60637,2667 60637,2581.7 60637,2581.7"];
	Layer3_Device12_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60890,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage12_Attention	[label=Q_local,
		lp="62993,3702.9",
		pos="e,60820,2378.4 62655,5000.9 62655,4741.1 62655,2663 62655,2663 62655,2663 60820,2663 60820,2663 60820,2663 60820,2388.4 60820,2388.4"];
	Layer3_Device12_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="62905,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage13_Attention	[label=Q_local,
		lp="63133,3606.4",
		pos="e,63054,2185.8 62957,5016 63016,5016 63054,5016 63054,5016 63054,5016 63054,2195.8 63054,2195.8"];
	Layer3_Device12_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="60227,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage14_Attention	[label=Q_local,
		lp="58683,3509.9",
		pos="e,60084,1992.6 62588,5000.6 62588,4741 62588,2680 62588,2680 62588,2680 60084,2680 60084,2680 60084,2680 60084,2002.6 60084,2002.6"];
	Layer3_Device12_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="61957,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_QKVProj -> Layer3_Device12_Stage15_Attention	[label=Q_local,
		lp="63251,3413.4",
		pos="e,62189,1822 62957,5031 63121,5031 63253,5031 63253,5031 63253,5031 63253,1822 63253,1822 63253,1822 62199,1822 62199,1822"];
	Layer3_Device12_Stage0_RecvKV -> Layer3_Device12_Stage0_Attention	[pos="e,61782,4694.7 61782,4857.6 61782,4857.6 61782,4704.7 61782,4704.7"];
	Layer3_Device12_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="60605,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage0_RecvKV -> Layer3_Device12_Stage1_RecvKV	[label="Ring transfer",
		lp="60962,4764.4",
		pos="e,60833,4721 60833,4807.7 60833,4807.7 60833,4731 60833,4731"];
	Layer3_Device12_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="60042,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage0_Attention -> Layer3_Device12_Stage0_Accumulate	[pos="e,60042,4501.7 61814,4641.2 61814,4607.5 61814,4554 61814,4554 61814,4554 60042,4554 60042,4554 60042,4554 60042,4511.7 60042,4511.7"];
	Layer3_Device12_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="59589,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage0_Accumulate -> Layer3_Device12_Stage1_Accumulate	[pos="e,59816,4308.5 59816,4448.1 59816,4448.1 59816,4318.5 59816,4318.5"];
	Layer3_Device12_Stage1_RecvKV -> Layer3_Device12_Stage1_Attention	[pos="e,59719,4501.8 59719,4667.8 59719,4667.8 59719,4511.8 59719,4511.8"];
	Layer3_Device12_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61251,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage1_RecvKV -> Layer3_Device12_Stage2_RecvKV	[label="Ring transfer",
		lp="60912,4571.4",
		pos="e,60928,4528 60928,4614.7 60928,4614.7 60928,4538 60928,4538"];
	Layer3_Device12_Stage1_Attention -> Layer3_Device12_Stage1_Accumulate	[pos="e,59555,4308.5 59555,4448.1 59555,4448.1 59555,4318.5 59555,4318.5"];
	Layer3_Device12_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="59707,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage1_Accumulate -> Layer3_Device12_Stage2_Accumulate	[pos="e,59648,4115.5 59648,4255.1 59648,4255.1 59648,4125.5 59648,4125.5"];
	Layer3_Device12_Stage2_RecvKV -> Layer3_Device12_Stage2_Attention	[pos="e,60336,4308.8 60336,4474.8 60336,4474.8 60336,4318.8 60336,4318.8"];
	Layer3_Device12_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61319,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage2_RecvKV -> Layer3_Device12_Stage3_RecvKV	[label="Ring transfer",
		lp="61333,4378.4",
		pos="e,61285,4335 61285,4421.7 61285,4421.7 61285,4345 61285,4345"];
	Layer3_Device12_Stage2_Attention -> Layer3_Device12_Stage2_Accumulate	[pos="e,59908,4115.5 59908,4255.1 59908,4255.1 59908,4125.5 59908,4125.5"];
	Layer3_Device12_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="59896,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage2_Accumulate -> Layer3_Device12_Stage3_Accumulate	[pos="e,59802,3922.5 59802,4062.1 59802,4062.1 59802,3932.5 59802,3932.5"];
	Layer3_Device12_Stage3_RecvKV -> Layer3_Device12_Stage3_Attention	[pos="e,62066,4115.8 62066,4281.8 62066,4281.8 62066,4125.8 62066,4125.8"];
	Layer3_Device12_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60916,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage3_RecvKV -> Layer3_Device12_Stage4_RecvKV	[label="Ring transfer",
		lp="61165,4185.4",
		pos="e,61118,4142 61118,4228.7 61118,4228.7 61118,4152 61118,4152"];
	Layer3_Device12_Stage3_Attention -> Layer3_Device12_Stage3_Accumulate	[pos="e,60062,3922.5 61959,4062.2 61959,4034.1 61959,3994 61959,3994 61959,3994 60062,3994 60062,3994 60062,3994 60062,3932.5 60062,3932.5"];
	Layer3_Device12_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="59825,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage3_Accumulate -> Layer3_Device12_Stage4_Accumulate	[pos="e,59860,3729.5 59860,3869.1 59860,3869.1 59860,3739.5 59860,3739.5"];
	Layer3_Device12_Stage4_RecvKV -> Layer3_Device12_Stage4_Attention	[pos="e,59541,3922.6 60043,4049 59771,4049 59541,4049 59541,4049 59541,4049 59541,3932.6 59541,3932.6"];
	Layer3_Device12_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61105,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage4_RecvKV -> Layer3_Device12_Stage5_RecvKV	[label="Ring transfer",
		lp="61058,3992.4",
		pos="e,61010,3949 61010,4035.7 61010,4035.7 61010,3959 61010,3959"];
	Layer3_Device12_Stage4_Attention -> Layer3_Device12_Stage4_Accumulate	[pos="e,59600,3729.5 59600,3869.1 59600,3869.1 59600,3739.5 59600,3739.5"];
	Layer3_Device12_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61769,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage4_Accumulate -> Layer3_Device12_Stage5_Accumulate	[pos="e,61746,3536.5 59848,3676.2 59848,3648.1 59848,3608 59848,3608 59848,3608 61746,3608 61746,3608 61746,3608 61746,3546.5 61746,3546.5"];
	Layer3_Device12_Stage5_RecvKV -> Layer3_Device12_Stage5_Attention	[pos="e,62018,3729.8 62018,3895.8 62018,3895.8 62018,3739.8 62018,3739.8"];
	Layer3_Device12_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="61034,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage5_RecvKV -> Layer3_Device12_Stage6_RecvKV	[label="Ring transfer",
		lp="61117,3799.4",
		pos="e,61070,3756 61070,3842.7 61070,3842.7 61070,3766 61070,3766"];
	Layer3_Device12_Stage5_Attention -> Layer3_Device12_Stage5_Accumulate	[pos="e,61977,3536.6 62011,3702 61990,3702 61977,3702 61977,3702 61977,3702 61977,3546.6 61977,3546.6"];
	Layer3_Device12_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61769,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage5_Accumulate -> Layer3_Device12_Stage6_Accumulate	[pos="e,61769,3343.5 61769,3483.1 61769,3483.1 61769,3353.5 61769,3353.5"];
	Layer3_Device12_Stage6_RecvKV -> Layer3_Device12_Stage6_Attention	[pos="e,62266,3536.6 61624,3663 61941,3663 62266,3663 62266,3663 62266,3663 62266,3546.6 62266,3546.6"];
	Layer3_Device12_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60560,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage6_RecvKV -> Layer3_Device12_Stage7_RecvKV	[label="Ring transfer",
		lp="60779,3606.4",
		pos="e,60797,3563 60797,3649.7 60797,3649.7 60797,3573 60797,3573"];
	Layer3_Device12_Stage6_Attention -> Layer3_Device12_Stage6_Accumulate	[pos="e,62001,3316 62202,3483.4 62202,3430.2 62202,3316 62202,3316 62202,3316 62011,3316 62011,3316"];
	Layer3_Device12_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61594,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage6_Accumulate -> Layer3_Device12_Stage7_Accumulate	[pos="e,61682,3150.5 61682,3290.1 61682,3290.1 61682,3160.5 61682,3160.5"];
	Layer3_Device12_Stage7_RecvKV -> Layer3_Device12_Stage7_Attention	[pos="e,59363,3343.6 59687,3470 59503,3470 59363,3470 59363,3470 59363,3470 59363,3353.6 59363,3353.6"];
	Layer3_Device12_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60560,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage7_RecvKV -> Layer3_Device12_Stage8_RecvKV	[label="Ring transfer",
		lp="60608,3413.4",
		pos="e,60560,3370 60560,3456.7 60560,3456.7 60560,3380 60560,3380"];
	Layer3_Device12_Stage7_Attention -> Layer3_Device12_Stage7_Accumulate	[pos="e,61421,3150.5 59524,3290.2 59524,3262.1 59524,3222 59524,3222 59524,3222 61421,3222 61421,3222 61421,3222 61421,3160.5 61421,3160.5"];
	Layer3_Device12_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61594,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage7_Accumulate -> Layer3_Device12_Stage8_Accumulate	[pos="e,61594,2957.5 61594,3097.1 61594,3097.1 61594,2967.5 61594,2967.5"];
	Layer3_Device12_Stage8_RecvKV -> Layer3_Device12_Stage8_Attention	[pos="e,61942,3150.6 61150,3277 61526,3277 61942,3277 61942,3277 61942,3277 61942,3160.6 61942,3160.6"];
	Layer3_Device12_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="60385,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage8_RecvKV -> Layer3_Device12_Stage9_RecvKV	[label="Ring transfer",
		lp="60491,3220.4",
		pos="e,60472,3177 60472,3263.7 60472,3263.7 60472,3187 60472,3187"];
	Layer3_Device12_Stage8_Attention -> Layer3_Device12_Stage8_Accumulate	[pos="e,61826,2930 62115,3097.4 62115,3044.2 62115,2930 62115,2930 62115,2930 61836,2930 61836,2930"];
	Layer3_Device12_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="59742,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage8_Accumulate -> Layer3_Device12_Stage9_Accumulate	[pos="e,59742,2764.4 61594,2904.1 61594,2885.1 61594,2863 61594,2863 61594,2863 59742,2863 59742,2863 59742,2863 59742,2774.4 59742,2774.4"];
	Layer3_Device12_Stage9_RecvKV -> Layer3_Device12_Stage9_Attention	[pos="e,59264,2957.6 59512,3084 59367,3084 59264,3084 59264,3084 59264,3084 59264,2967.6 59264,2967.6"];
	Layer3_Device12_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="60385,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage9_RecvKV -> Layer3_Device12_Stage10_RecvKV	[label="Ring transfer",
		lp="60433,3027.4",
		pos="e,60385,2984 60385,3070.7 60385,3070.7 60385,2994 60385,2994"];
	Layer3_Device12_Stage9_Attention -> Layer3_Device12_Stage9_Accumulate	[pos="e,59510,2737 59408,2930 59438,2930 59459,2930 59459,2930 59459,2930 59459,2737 59459,2737 59459,2737 59500,2737 59500,2737"];
	Layer3_Device12_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="59742,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage9_Accumulate -> Layer3_Device12_Stage10_Accumulate	[pos="e,59742,2571.5 59742,2711.1 59742,2711.1 59742,2581.5 59742,2581.5"];
	Layer3_Device12_Stage10_RecvKV -> Layer3_Device12_Stage10_Attention	[pos="e,59430,2764.6 59511,2891 59461,2891 59430,2891 59430,2891 59430,2891 59430,2774.6 59430,2774.6"];
	Layer3_Device12_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="60951,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage10_RecvKV -> Layer3_Device12_Stage11_RecvKV	[label="Ring transfer",
		lp="60644,2834.4",
		pos="e,60668,2791 60668,2877.7 60668,2877.7 60668,2801 60668,2801"];
	Layer3_Device12_Stage10_Attention -> Layer3_Device12_Stage10_Accumulate	[pos="e,59510,2544 59221,2711.4 59221,2658.2 59221,2544 59221,2544 59221,2544 59500,2544 59500,2544"];
	Layer3_Device12_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60369,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage10_Accumulate -> Layer3_Device12_Stage11_Accumulate	[pos="e,60233,2378.6 59974,2544 60101,2544 60233,2544 60233,2544 60233,2544 60233,2388.6 60233,2388.6"];
	Layer3_Device12_Stage11_RecvKV -> Layer3_Device12_Stage11_Attention	[pos="e,60483,2571.6 60483,2684.7 60483,2684.7 60483,2581.6 60483,2581.6"];
	Layer3_Device12_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61769,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage11_RecvKV -> Layer3_Device12_Stage12_RecvKV	[label="Ring transfer",
		lp="61408,2641.4",
		pos="e,61360,2598 61360,2684.7 61360,2684.7 61360,2608 61360,2608"];
	Layer3_Device12_Stage11_Attention -> Layer3_Device12_Stage11_Accumulate	[pos="e,60464,2378.5 60464,2518.1 60464,2518.1 60464,2388.5 60464,2388.5"];
	Layer3_Device12_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60487,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage11_Accumulate -> Layer3_Device12_Stage12_Accumulate	[pos="e,60428,2185.5 60428,2325.1 60428,2325.1 60428,2195.5 60428,2195.5"];
	Layer3_Device12_Stage12_RecvKV -> Layer3_Device12_Stage12_Attention	[pos="e,60985,2378.6 60985,2491.7 60985,2491.7 60985,2388.6 60985,2388.6"];
	Layer3_Device12_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="62099,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage12_RecvKV -> Layer3_Device12_Stage13_RecvKV	[label="Ring transfer",
		lp="61982,2448.4",
		pos="e,61934,2405 61934,2491.7 61934,2491.7 61934,2415 61934,2415"];
	Layer3_Device12_Stage12_Attention -> Layer3_Device12_Stage12_Accumulate	[pos="e,60688,2185.5 60688,2325.1 60688,2325.1 60688,2195.5 60688,2195.5"];
	Layer3_Device12_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60748,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage12_Accumulate -> Layer3_Device12_Stage13_Accumulate	[pos="e,60618,1992.5 60618,2132.1 60618,2132.1 60618,2002.5 60618,2002.5"];
	Layer3_Device12_Stage13_RecvKV -> Layer3_Device12_Stage13_Attention	[pos="e,62846,2185.8 62846,2351.8 62846,2351.8 62846,2195.8 62846,2195.8"];
	Layer3_Device12_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61696,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage13_RecvKV -> Layer3_Device12_Stage14_RecvKV	[label="Ring transfer",
		lp="61945,2255.4",
		pos="e,61898,2212 61898,2298.7 61898,2298.7 61898,2222 61898,2222"];
	Layer3_Device12_Stage13_Attention -> Layer3_Device12_Stage13_Accumulate	[pos="e,60878,1992.5 62775,2132 62775,2108.4 62775,2078 62775,2078 62775,2078 60878,2078 60878,2078 60878,2078 60878,2002.5 60878,2002.5"];
	Layer3_Device12_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="60748,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage13_Accumulate -> Layer3_Device12_Stage14_Accumulate	[pos="e,60748,1839.9 60748,1939.3 60748,1939.3 60748,1849.9 60748,1849.9"];
	Layer3_Device12_Stage14_RecvKV -> Layer3_Device12_Stage14_Attention	[pos="e,60357,1992.6 60822,2119 60568,2119 60357,2119 60357,2119 60357,2119 60357,2002.6 60357,2002.6"];
	Layer3_Device12_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="61957,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device12_Stage14_RecvKV -> Layer3_Device12_Stage15_RecvKV	[label="Ring transfer",
		lp="61874,2062.4",
		pos="e,61826,2019 61826,2105.7 61826,2105.7 61826,2029 61826,2029"];
	Layer3_Device12_Stage14_Attention -> Layer3_Device12_Stage14_Accumulate	[pos="e,60516,1804 60425,1939.2 60425,1893.2 60425,1804 60425,1804 60425,1804 60506,1804 60506,1804"];
	Layer3_Device12_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="61210,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device12_Stage14_Accumulate -> Layer3_Device12_Stage15_Accumulate	[pos="e,61008,1714 60980,1813 60997,1813 61008,1813 61008,1813 61008,1813 61008,1724 61008,1724"];
	Layer3_Device12_Stage15_RecvKV -> Layer3_Device12_Stage15_Attention	[pos="e,61957,1840 61957,1912.6 61957,1912.6 61957,1850 61957,1850"];
	Layer3_Device12_Stage15_Attention -> Layer3_Device12_Stage15_Accumulate	[pos="e,61442,1687 61971,1786.6 61971,1749.5 61971,1687 61971,1687 61971,1687 61452,1687 61452,1687"];
	Layer3_Device12_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="61210,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device12_Stage15_Accumulate -> Layer3_Device12_ConcatHeads	[pos="e,61210,1588.1 61210,1660.7 61210,1660.7 61210,1598.1 61210,1598.1"];
	Layer3_Device12_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="61210,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device12_ConcatHeads -> Layer3_Device12_OutputProj	[pos="e,61210,1462.1 61210,1534.7 61210,1534.7 61210,1472.1 61210,1472.1"];
	Layer3_Device12_OutputProj -> Layer3_Device12_Residual1	[pos="e,61210,1336.1 61210,1408.7 61210,1408.7 61210,1346.1 61210,1346.1"];
	Layer3_Device12_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="60909,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device12_Residual1 -> Layer3_Device12_LayerNorm2	[pos="e,60973,1210.1 60973,1282.7 60973,1282.7 60973,1220.1 60973,1220.1"];
	Layer3_Device12_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="60654,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device12_Residual1 -> Layer3_Device12_Residual2	[pos="e,61041,492 61322,1282.7 61322,1141.8 61322,492 61322,492 61322,492 61051,492 61051,492"];
	Layer3_Device12_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="60616,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device12_LayerNorm2 -> Layer3_Device12_GateProj	[pos="e,60768,1084.1 60768,1156.7 60768,1156.7 60768,1094.1 60768,1094.1"];
	Layer3_Device12_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="60929,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device12_LayerNorm2 -> Layer3_Device12_UpProj	[pos="e,60982,995.2 60982,1156.6 60982,1156.6 60982,1005.2 60982,1005.2"];
	Layer3_Device12_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="60616,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device12_GateProj -> Layer3_Device12_Activation	[pos="e,60548,905.95 60548,1030.8 60548,1030.8 60548,915.95 60548,915.95"];
	Layer3_Device12_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="60654,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device12_UpProj -> Layer3_Device12_ElemMul	[pos="e,60953,780.2 60953,941.61 60953,941.61 60953,790.2 60953,790.2"];
	Layer3_Device12_Activation -> Layer3_Device12_ElemMul	[pos="e,60616,780.09 60616,852.69 60616,852.69 60616,790.09 60616,790.09"];
	Layer3_Device12_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="60654,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device12_ElemMul -> Layer3_Device12_DownProj	[pos="e,60654,654.09 60654,726.69 60654,726.69 60654,664.09 60654,664.09"];
	Layer3_Device12_DownProj -> Layer3_Device12_Residual2	[pos="e,60654,528.09 60654,600.69 60654,600.69 60654,538.09 60654,538.09"];
	Layer3_Device12_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 12 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="55749,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device12_Residual2 -> Layer3_Device12_Output	[pos="e,56050,364 60654,474.59 60654,434.6 60654,364 60654,364 60654,364 56060,364 56060,364"];
	Layer3_Device12_Output -> Sequence_Aggregate	[pos="e,40977,200 55749,326.71 55749,278.16 55749,200 55749,200 55749,200 40987,200 40987,200"];
	Layer3_Device13_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="67481,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device13_Input -> Layer3_Device13_LayerNorm1	[pos="e,67672,5180.1 67870,5290 67765,5290 67672,5290 67672,5290 67672,5290 67672,5190.1 67672,5190.1"];
	Layer3_Device13_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="66057,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device13_Input -> Layer3_Device13_Residual1	[pos="e,66444,1309 68309,5256.9 68309,4872.3 68309,1309 68309,1309 68309,1309 66454,1309 66454,1309"];
	Layer3_Device13_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67143,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device13_LayerNorm1 -> Layer3_Device13_QKVProj	[pos="e,67481,5054.1 67481,5126.7 67481,5126.7 67481,5064.1 67481,5064.1"];
	Layer3_Device13_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65897,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage0_RecvKV	[label="Local K,V",
		lp="66398,4957.4",
		pos="e,66655,4914.2 66655,5000.7 66655,5000.7 66655,4924.2 66655,4924.2"];
	Layer3_Device13_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="66650,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage0_Attention	[label=Q_local,
		lp="66959,4860.9",
		pos="e,66849,4694.8 66849,5000.7 66849,5000.7 66849,4704.8 66849,4704.8"];
	Layer3_Device13_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64357,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage1_Attention	[label=Q_local,
		lp="64421,4764.4",
		pos="e,64473,4501.7 66493,5008 65703,5008 64473,5008 64473,5008 64473,5008 64473,4511.7 64473,4511.7"];
	Layer3_Device13_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64957,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage2_Attention	[label=Q_local,
		lp="67099,4667.9",
		pos="e,65138,4308.7 66944,5000.7 66944,4901 66944,4556 66944,4556 66944,4556 65138,4556 65138,4556 65138,4556 65138,4318.7 65138,4318.7"];
	Layer3_Device13_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="66972,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage3_Attention	[label=Q_local,
		lp="67239,4571.4",
		pos="e,67145,4115.7 67145,5000.8 67145,5000.8 67145,4125.7 67145,4125.7"];
	Layer3_Device13_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64208,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage4_Attention	[label=Q_local,
		lp="64041,4474.9",
		pos="e,64110,3922.7 66493,5023 65602,5023 64110,5023 64110,5023 64110,5023 64110,3932.7 64110,3932.7"];
	Layer3_Device13_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67090,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage5_Attention	[label=Q_local,
		lp="67319,4378.4",
		pos="e,67262,3729.7 67262,5000.6 67262,5000.6 67262,3739.7 67262,3739.7"];
	Layer3_Device13_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67137,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage6_Attention	[label=Q_local,
		lp="67464,4281.9",
		pos="e,67345,3536.6 67345,5000.7 67345,5000.7 67345,3546.6 67345,3546.6"];
	Layer3_Device13_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64198,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage7_Attention	[label=Q_local,
		lp="63903,4185.4",
		pos="e,63972,3343.6 66493,5038 65565,5038 63972,5038 63972,5038 63972,5038 63972,3353.6 63972,3353.6"];
	Layer3_Device13_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="66921,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage8_Attention	[label=Q_local,
		lp="67582,4088.9",
		pos="e,67153,3123 67412,5000.8 67412,4770.7 67412,3123 67412,3123 67412,3123 67163,3123 67163,3123"];
	Layer3_Device13_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="63982,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage9_Attention	[label=Q_local,
		lp="63788,3992.4",
		pos="e,63963,2957.6 66493,5046 65563,5046 63963,5046 63963,5046 63963,5046 63963,2967.6 63963,2967.6"];
	Layer3_Device13_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="64210,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage10_Attention	[label=Q_local,
		lp="63656,3895.9",
		pos="e,64385,2764.6 67455,5000.6 67455,4752.7 67455,2873 67455,2873 67455,2873 64385,2873 64385,2873 64385,2873 64385,2774.6 64385,2774.6"];
	Layer3_Device13_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65498,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage11_Attention	[label=Q_local,
		lp="67702,3799.4",
		pos="e,65575,2571.8 67541,5000.7 67541,4737.7 67541,2617 67541,2617 67541,2617 65575,2617 65575,2617 65575,2617 65575,2581.8 65575,2581.8"];
	Layer3_Device13_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65737,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage12_Attention	[label=Q_local,
		lp="67840,3702.9",
		pos="e,65758,2378.5 67584,5000.7 67584,4737.4 67584,2614 67584,2614 67584,2614 65758,2614 65758,2614 65758,2614 65758,2388.5 65758,2388.5"];
	Layer3_Device13_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="67752,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage13_Attention	[label=Q_local,
		lp="67980,3606.4",
		pos="e,67927,2185.8 67793,5014 67873,5014 67927,5014 67927,5014 67927,5014 67927,2195.8 67927,2195.8"];
	Layer3_Device13_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="65074,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage14_Attention	[label=Q_local,
		lp="63515,3509.9",
		pos="e,64974,1992.5 67498,5000.7 67498,4750.4 67498,2823 67498,2823 67498,2823 64974,2823 64974,2823 64974,2823 64974,2002.5 64974,2002.5"];
	Layer3_Device13_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="66804,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_QKVProj -> Layer3_Device13_Stage15_Attention	[label=Q_local,
		lp="68098,3413.4",
		pos="e,67036,1822 67793,5027 67986,5027 68146,5027 68146,5027 68146,5027 68146,1822 68146,1822 68146,1822 67046,1822 67046,1822"];
	Layer3_Device13_Stage0_RecvKV -> Layer3_Device13_Stage0_Attention	[pos="e,66618,4694.7 66618,4857.6 66618,4857.6 66618,4704.7 66618,4704.7"];
	Layer3_Device13_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65441,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage0_RecvKV -> Layer3_Device13_Stage1_RecvKV	[label="Ring transfer",
		lp="65717,4764.4",
		pos="e,65669,4721 65669,4807.7 65669,4807.7 65669,4731 65669,4731"];
	Layer3_Device13_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="64878,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage0_Attention -> Layer3_Device13_Stage0_Accumulate	[pos="e,64878,4501.5 66650,4641.3 66650,4609 66650,4559 66650,4559 66650,4559 64878,4559 64878,4559 64878,4559 64878,4511.5 64878,4511.5"];
	Layer3_Device13_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64436,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage0_Accumulate -> Layer3_Device13_Stage1_Accumulate	[pos="e,64657,4308.5 64657,4448.1 64657,4448.1 64657,4318.5 64657,4318.5"];
	Layer3_Device13_Stage1_RecvKV -> Layer3_Device13_Stage1_Attention	[pos="e,64555,4501.8 64555,4667.8 64555,4667.8 64555,4511.8 64555,4511.8"];
	Layer3_Device13_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66087,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage1_RecvKV -> Layer3_Device13_Stage2_RecvKV	[label="Ring transfer",
		lp="65758,4571.4",
		pos="e,65764,4528 65764,4614.7 65764,4614.7 65764,4538 65764,4538"];
	Layer3_Device13_Stage1_Attention -> Layer3_Device13_Stage1_Accumulate	[pos="e,64396,4308.5 64396,4448.1 64396,4448.1 64396,4318.5 64396,4318.5"];
	Layer3_Device13_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64554,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage1_Accumulate -> Layer3_Device13_Stage2_Accumulate	[pos="e,64495,4115.5 64495,4255.1 64495,4255.1 64495,4125.5 64495,4125.5"];
	Layer3_Device13_Stage2_RecvKV -> Layer3_Device13_Stage2_Attention	[pos="e,65178,4308.8 65178,4474.8 65178,4474.8 65178,4318.8 65178,4318.8"];
	Layer3_Device13_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66166,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage2_RecvKV -> Layer3_Device13_Stage3_RecvKV	[label="Ring transfer",
		lp="66169,4378.4",
		pos="e,66126,4335 66126,4421.7 66126,4421.7 66126,4345 66126,4345"];
	Layer3_Device13_Stage2_Attention -> Layer3_Device13_Stage2_Accumulate	[pos="e,64756,4115.5 64756,4255.1 64756,4255.1 64756,4125.5 64756,4125.5"];
	Layer3_Device13_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64729,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage2_Accumulate -> Layer3_Device13_Stage3_Accumulate	[pos="e,64642,3922.5 64642,4062.1 64642,4062.1 64642,3932.5 64642,3932.5"];
	Layer3_Device13_Stage3_RecvKV -> Layer3_Device13_Stage3_Attention	[pos="e,66913,4115.8 66913,4281.8 66913,4281.8 66913,4125.8 66913,4125.8"];
	Layer3_Device13_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65763,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage3_RecvKV -> Layer3_Device13_Stage4_RecvKV	[label="Ring transfer",
		lp="65947,4185.4",
		pos="e,65964,4142 65964,4228.7 65964,4228.7 65964,4152 65964,4152"];
	Layer3_Device13_Stage3_Attention -> Layer3_Device13_Stage3_Accumulate	[pos="e,64902,3922.4 66799,4062.2 66799,4036 66799,4000 66799,4000 66799,4000 64902,4000 64902,4000 64902,4000 64902,3932.4 64902,3932.4"];
	Layer3_Device13_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64672,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage3_Accumulate -> Layer3_Device13_Stage4_Accumulate	[pos="e,64700,3729.5 64700,3869.1 64700,3869.1 64700,3739.5 64700,3739.5"];
	Layer3_Device13_Stage4_RecvKV -> Layer3_Device13_Stage4_Attention	[pos="e,64381,3922.6 64890,4049 64615,4049 64381,4049 64381,4049 64381,4049 64381,3932.6 64381,3932.6"];
	Layer3_Device13_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65938,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage4_RecvKV -> Layer3_Device13_Stage5_RecvKV	[label="Ring transfer",
		lp="65871,3992.4",
		pos="e,65850,3949 65850,4035.7 65850,4035.7 65850,3959 65850,3959"];
	Layer3_Device13_Stage4_Attention -> Layer3_Device13_Stage4_Accumulate	[pos="e,64440,3702 64434,3869.4 64434,3816.2 64434,3702 64434,3702 64434,3702 64435,3702 64435,3702"];
	Layer3_Device13_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66616,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage4_Accumulate -> Layer3_Device13_Stage5_Accumulate	[pos="e,66593,3536.4 64695,3676.2 64695,3650 64695,3614 64695,3614 64695,3614 66593,3614 66593,3614 66593,3614 66593,3546.4 66593,3546.4"];
	Layer3_Device13_Stage5_RecvKV -> Layer3_Device13_Stage5_Attention	[pos="e,66858,3711 66853,3895.8 66853,3890.2 66853,3711 66853,3711 66853,3711 66853,3711 66853,3711"];
	Layer3_Device13_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65881,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage5_RecvKV -> Layer3_Device13_Stage6_RecvKV	[label="Ring transfer",
		lp="65958,3799.4",
		pos="e,65910,3756 65910,3842.7 65910,3842.7 65910,3766 65910,3766"];
	Layer3_Device13_Stage5_Attention -> Layer3_Device13_Stage5_Accumulate	[pos="e,66824,3536.7 66858,3694 66837,3694 66824,3694 66824,3694 66824,3694 66824,3546.7 66824,3546.7"];
	Layer3_Device13_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66616,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage5_Accumulate -> Layer3_Device13_Stage6_Accumulate	[pos="e,66616,3343.5 66616,3483.1 66616,3483.1 66616,3353.5 66616,3353.5"];
	Layer3_Device13_Stage6_RecvKV -> Layer3_Device13_Stage6_Attention	[pos="e,67114,3536.6 66471,3663 66788,3663 67114,3663 67114,3663 67114,3663 67114,3546.6 67114,3546.6"];
	Layer3_Device13_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65407,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage6_RecvKV -> Layer3_Device13_Stage7_RecvKV	[label="Ring transfer",
		lp="65692,3606.4",
		pos="e,65644,3563 65644,3649.7 65644,3649.7 65644,3573 65644,3573"];
	Layer3_Device13_Stage6_Attention -> Layer3_Device13_Stage6_Accumulate	[pos="e,66848,3308 67029,3483.1 67029,3428.2 67029,3308 67029,3308 67029,3308 66858,3308 66858,3308"];
	Layer3_Device13_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66400,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage6_Accumulate -> Layer3_Device13_Stage7_Accumulate	[pos="e,66508,3150.5 66508,3290.1 66508,3290.1 66508,3160.5 66508,3160.5"];
	Layer3_Device13_Stage7_RecvKV -> Layer3_Device13_Stage7_Attention	[pos="e,64203,3343.6 64534,3470 64346,3470 64203,3470 64203,3470 64203,3470 64203,3353.6 64203,3353.6"];
	Layer3_Device13_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65407,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage7_RecvKV -> Layer3_Device13_Stage8_RecvKV	[label="Ring transfer",
		lp="65455,3413.4",
		pos="e,65407,3370 65407,3456.7 65407,3456.7 65407,3380 65407,3380"];
	Layer3_Device13_Stage7_Attention -> Layer3_Device13_Stage7_Accumulate	[pos="e,66248,3150.8 64350,3290.3 64350,3256.3 64350,3202 64350,3202 64350,3202 66248,3202 66248,3202 66248,3202 66248,3160.8 66248,3160.8"];
	Layer3_Device13_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66400,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage7_Accumulate -> Layer3_Device13_Stage8_Accumulate	[pos="e,66400,2957.5 66400,3097.1 66400,3097.1 66400,2967.5 66400,2967.5"];
	Layer3_Device13_Stage8_RecvKV -> Layer3_Device13_Stage8_Attention	[pos="e,66768,3150.6 65997,3277 66365,3277 66768,3277 66768,3277 66768,3277 66768,3160.6 66768,3160.6"];
	Layer3_Device13_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="65191,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage8_RecvKV -> Layer3_Device13_Stage9_RecvKV	[label="Ring transfer",
		lp="65424,3220.4",
		pos="e,65299,3177 65299,3263.7 65299,3263.7 65299,3187 65299,3187"];
	Layer3_Device13_Stage8_Attention -> Layer3_Device13_Stage8_Accumulate	[pos="e,66632,2930 66775,3097.4 66775,3044.2 66775,2930 66775,2930 66775,2930 66642,2930 66642,2930"];
	Layer3_Device13_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64731,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage8_Accumulate -> Layer3_Device13_Stage9_Accumulate	[pos="e,64808,2764.8 66400,2904.2 66400,2873.1 66400,2826 66400,2826 66400,2826 64808,2826 64808,2826 64808,2826 64808,2774.8 64808,2774.8"];
	Layer3_Device13_Stage9_RecvKV -> Layer3_Device13_Stage9_Attention	[pos="e,64090,2957.6 64317,3084 64184,3084 64090,3084 64090,3084 64090,3084 64090,2967.6 64090,2967.6"];
	Layer3_Device13_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65191,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage9_RecvKV -> Layer3_Device13_Stage10_RecvKV	[label="Ring transfer",
		lp="65239,3027.4",
		pos="e,65191,2984 65191,3070.7 65191,3070.7 65191,2994 65191,2994"];
	Layer3_Device13_Stage9_Attention -> Layer3_Device13_Stage9_Accumulate	[pos="e,64654,2764.6 64096,2904.1 64096,2860 64096,2777 64096,2777 64096,2777 64654,2777 64654,2777 64654,2777 64654,2774.6 64654,2774.6"];
	Layer3_Device13_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="64731,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage9_Accumulate -> Layer3_Device13_Stage10_Accumulate	[pos="e,64731,2571.5 64731,2711.1 64731,2711.1 64731,2581.5 64731,2581.5"];
	Layer3_Device13_Stage10_RecvKV -> Layer3_Device13_Stage10_Attention	[pos="e,64328,2764.8 64328,2930.8 64328,2930.8 64328,2774.8 64328,2774.8"];
	Layer3_Device13_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="65940,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage10_RecvKV -> Layer3_Device13_Stage11_RecvKV	[label="Ring transfer",
		lp="65670,2834.4",
		pos="e,65566,2791 65566,2877.7 65566,2877.7 65566,2801 65566,2801"];
	Layer3_Device13_Stage10_Attention -> Layer3_Device13_Stage10_Accumulate	[pos="e,64499,2544 64210,2711.4 64210,2658.2 64210,2544 64210,2544 64210,2544 64489,2544 64489,2544"];
	Layer3_Device13_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65216,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage10_Accumulate -> Layer3_Device13_Stage11_Accumulate	[pos="e,65002,2378.6 64963,2544 64987,2544 65002,2544 65002,2544 65002,2544 65002,2388.6 65002,2388.6"];
	Layer3_Device13_Stage11_RecvKV -> Layer3_Device13_Stage11_Attention	[pos="e,65421,2571.6 65421,2684.7 65421,2684.7 65421,2581.6 65421,2581.6"];
	Layer3_Device13_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66707,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage11_RecvKV -> Layer3_Device13_Stage12_RecvKV	[label="Ring transfer",
		lp="66331,2641.4",
		pos="e,66324,2598 66324,2684.7 66324,2684.7 66324,2608 66324,2608"];
	Layer3_Device13_Stage11_Attention -> Layer3_Device13_Stage11_Accumulate	[pos="e,65357,2378.5 65357,2518.1 65357,2518.1 65357,2388.5 65357,2388.5"];
	Layer3_Device13_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65334,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage11_Accumulate -> Layer3_Device13_Stage12_Accumulate	[pos="e,65275,2185.5 65275,2325.1 65275,2325.1 65275,2195.5 65275,2195.5"];
	Layer3_Device13_Stage12_RecvKV -> Layer3_Device13_Stage12_Attention	[pos="e,65878,2378.6 65878,2491.7 65878,2491.7 65878,2388.6 65878,2388.6"];
	Layer3_Device13_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66946,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage12_RecvKV -> Layer3_Device13_Stage13_RecvKV	[label="Ring transfer",
		lp="66874,2448.4",
		pos="e,66826,2405 66826,2491.7 66826,2491.7 66826,2415 66826,2415"];
	Layer3_Device13_Stage12_Attention -> Layer3_Device13_Stage12_Accumulate	[pos="e,65536,2185.5 65536,2325.1 65536,2325.1 65536,2195.5 65536,2195.5"];
	Layer3_Device13_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65595,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage12_Accumulate -> Layer3_Device13_Stage13_Accumulate	[pos="e,65464,1992.5 65464,2132.1 65464,2132.1 65464,2002.5 65464,2002.5"];
	Layer3_Device13_Stage13_RecvKV -> Layer3_Device13_Stage13_Attention	[pos="e,67693,2185.8 67693,2351.8 67693,2351.8 67693,2195.8 67693,2195.8"];
	Layer3_Device13_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66543,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage13_RecvKV -> Layer3_Device13_Stage14_RecvKV	[label="Ring transfer",
		lp="66727,2255.4",
		pos="e,66744,2212 66744,2298.7 66744,2298.7 66744,2222 66744,2222"];
	Layer3_Device13_Stage13_Attention -> Layer3_Device13_Stage13_Accumulate	[pos="e,65725,1992.5 67622,2132.4 67622,2110.5 67622,2083 67622,2083 67622,2083 65725,2083 65725,2083 65725,2083 65725,2002.5 65725,2002.5"];
	Layer3_Device13_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="65595,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage13_Accumulate -> Layer3_Device13_Stage14_Accumulate	[pos="e,65595,1839.9 65595,1939.3 65595,1939.3 65595,1849.9 65595,1849.9"];
	Layer3_Device13_Stage14_RecvKV -> Layer3_Device13_Stage14_Attention	[pos="e,65204,1992.6 65669,2119 65415,2119 65204,2119 65204,2119 65204,2119 65204,2002.6 65204,2002.6"];
	Layer3_Device13_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="66804,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device13_Stage14_RecvKV -> Layer3_Device13_Stage15_RecvKV	[label="Ring transfer",
		lp="66721,2062.4",
		pos="e,66674,2019 66674,2105.7 66674,2105.7 66674,2029 66674,2029"];
	Layer3_Device13_Stage14_Attention -> Layer3_Device13_Stage14_Accumulate	[pos="e,65363,1804 65272,1939.2 65272,1893.2 65272,1804 65272,1804 65272,1804 65353,1804 65353,1804"];
	Layer3_Device13_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="66057,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device13_Stage14_Accumulate -> Layer3_Device13_Stage15_Accumulate	[pos="e,65855,1714.2 65827,1804 65844,1804 65855,1804 65855,1804 65855,1804 65855,1724.2 65855,1724.2"];
	Layer3_Device13_Stage15_RecvKV -> Layer3_Device13_Stage15_Attention	[pos="e,66804,1840 66804,1912.6 66804,1912.6 66804,1850 66804,1850"];
	Layer3_Device13_Stage15_Attention -> Layer3_Device13_Stage15_Accumulate	[pos="e,66086,1714 66572,1822 66362,1822 66086,1822 66086,1822 66086,1822 66086,1724 66086,1724"];
	Layer3_Device13_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="66057,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device13_Stage15_Accumulate -> Layer3_Device13_ConcatHeads	[pos="e,66057,1588.1 66057,1660.7 66057,1660.7 66057,1598.1 66057,1598.1"];
	Layer3_Device13_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="66057,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device13_ConcatHeads -> Layer3_Device13_OutputProj	[pos="e,66057,1462.1 66057,1534.7 66057,1534.7 66057,1472.1 66057,1472.1"];
	Layer3_Device13_OutputProj -> Layer3_Device13_Residual1	[pos="e,66057,1336.1 66057,1408.7 66057,1408.7 66057,1346.1 66057,1346.1"];
	Layer3_Device13_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="65756,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device13_Residual1 -> Layer3_Device13_LayerNorm2	[pos="e,65820,1210.1 65820,1282.7 65820,1282.7 65820,1220.1 65820,1220.1"];
	Layer3_Device13_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="65501,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device13_Residual1 -> Layer3_Device13_Residual2	[pos="e,65888,492 66191,1282.7 66191,1141.8 66191,492 66191,492 66191,492 65898,492 65898,492"];
	Layer3_Device13_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="65463,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device13_LayerNorm2 -> Layer3_Device13_GateProj	[pos="e,65616,1084.1 65616,1156.7 65616,1156.7 65616,1094.1 65616,1094.1"];
	Layer3_Device13_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="65776,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device13_LayerNorm2 -> Layer3_Device13_UpProj	[pos="e,65828,995.2 65828,1156.6 65828,1156.6 65828,1005.2 65828,1005.2"];
	Layer3_Device13_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="65463,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device13_GateProj -> Layer3_Device13_Activation	[pos="e,65394,905.95 65394,1030.8 65394,1030.8 65394,915.95 65394,915.95"];
	Layer3_Device13_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="65501,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device13_UpProj -> Layer3_Device13_ElemMul	[pos="e,65800,780.2 65800,941.61 65800,941.61 65800,790.2 65800,790.2"];
	Layer3_Device13_Activation -> Layer3_Device13_ElemMul	[pos="e,65463,780.09 65463,852.69 65463,852.69 65463,790.09 65463,790.09"];
	Layer3_Device13_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="65501,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device13_ElemMul -> Layer3_Device13_DownProj	[pos="e,65501,654.09 65501,726.69 65501,726.69 65501,664.09 65501,664.09"];
	Layer3_Device13_DownProj -> Layer3_Device13_Residual2	[pos="e,65501,528.09 65501,600.69 65501,600.69 65501,538.09 65501,538.09"];
	Layer3_Device13_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 13 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="61792,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device13_Residual2 -> Layer3_Device13_Output	[pos="e,61848,401.54 65114,510 64175,510 61848,510 61848,510 61848,510 61848,411.54 61848,411.54"];
	Layer3_Device13_Output -> Sequence_Aggregate	[pos="e,40954,187 61792,326.8 61792,274.72 61792,187 61792,187 61792,187 40964,187 40964,187"];
	Layer3_Device14_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="72502,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device14_Input -> Layer3_Device14_LayerNorm1	[pos="e,72700,5180.1 72771,5290 72729,5290 72700,5290 72700,5290 72700,5290 72700,5190.1 72700,5190.1"];
	Layer3_Device14_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="71082,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device14_Input -> Layer3_Device14_Residual1	[pos="e,71469,1309 73204,5256.6 73204,4870.5 73204,1309 73204,1309 73204,1309 71479,1309 71479,1309"];
	Layer3_Device14_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72014,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device14_LayerNorm1 -> Layer3_Device14_QKVProj	[pos="e,72476,5054.1 72476,5126.7 72476,5126.7 72476,5064.1 72476,5064.1"];
	Layer3_Device14_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70768,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage0_RecvKV	[label="Local K,V",
		lp="71268,4957.4",
		pos="e,71526,4914.2 71526,5000.7 71526,5000.7 71526,4924.2 71526,4924.2"];
	Layer3_Device14_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71558,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage0_Attention	[label=Q_local,
		lp="71830,4860.9",
		pos="e,71739,4694.8 71739,5000.7 71739,5000.7 71739,4704.8 71739,4704.8"];
	Layer3_Device14_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69228,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage1_Attention	[label=Q_local,
		lp="69292,4764.4",
		pos="e,69265,4501.7 71364,5007 70552,5007 69265,5007 69265,5007 69265,5007 69265,4511.7 69265,4511.7"];
	Layer3_Device14_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69839,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage2_Attention	[label=Q_local,
		lp="71970,4667.9",
		pos="e,70009,4308.7 71834,5000.6 71834,4908.3 71834,4609 71834,4609 71834,4609 70009,4609 70009,4609 70009,4609 70009,4318.7 70009,4318.7"];
	Layer3_Device14_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71854,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage3_Attention	[label=Q_local,
		lp="72110,4571.4",
		pos="e,72027,4115.7 72027,5000.8 72027,5000.8 72027,4125.7 72027,4125.7"];
	Layer3_Device14_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69060,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage4_Attention	[label=Q_local,
		lp="68912,4474.9",
		pos="e,68982,3922.6 71364,5020 70474,5020 68982,5020 68982,5020 68982,5020 68982,3932.6 68982,3932.6"];
	Layer3_Device14_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71972,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage5_Attention	[label=Q_local,
		lp="72214,4378.4",
		pos="e,72144,3729.7 72144,5000.6 72144,5000.6 72144,3739.7 72144,3739.7"];
	Layer3_Device14_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71997,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage6_Attention	[label=Q_local,
		lp="72346,4281.9",
		pos="e,72216,3536.6 72216,5000.7 72216,5000.7 72216,3546.6 72216,3546.6"];
	Layer3_Device14_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69058,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage7_Attention	[label=Q_local,
		lp="68766,4185.4",
		pos="e,68826,3325 71364,5034 70416,5034 68762,5034 68762,5034 68762,5034 68762,3325 68762,3325 68762,3325 68816,3325 68816,3325"];
	Layer3_Device14_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71805,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage8_Attention	[label=Q_local,
		lp="72464,4088.9",
		pos="e,72037,3123 72266,5000.8 72266,4770.7 72266,3123 72266,3123 72266,3123 72047,3123 72047,3123"];
	Layer3_Device14_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="68866,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage9_Attention	[label=Q_local,
		lp="68648,3992.4",
		pos="e,68698,2957.6 71364,5040 70399,5040 68698,5040 68698,5040 68698,5040 68698,2967.6 68698,2967.6"];
	Layer3_Device14_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69057,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage10_Attention	[label=Q_local,
		lp="68508,3895.9",
		pos="e,68825,2737 71364,5047 70373,5047 68596,5047 68596,5047 68596,5047 68596,2737 68596,2737 68596,2737 68815,2737 68815,2737"];
	Layer3_Device14_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70289,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage11_Attention	[label=Q_local,
		lp="72590,3799.4",
		pos="e,70366,2571.6 72342,5000.8 72342,4740.7 72342,2660 72342,2660 72342,2660 70366,2660 70366,2660 70366,2660 70366,2581.6 70366,2581.6"];
	Layer3_Device14_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="70619,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage12_Attention	[label=Q_local,
		lp="72722,3702.9",
		pos="e,70549,2378.5 72380,5000.8 72380,4740.1 72380,2655 72380,2655 72380,2655 70549,2655 70549,2655 70549,2655 70549,2388.5 70549,2388.5"];
	Layer3_Device14_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="72634,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage13_Attention	[label=Q_local,
		lp="72862,3606.4",
		pos="e,72759,2185.8 72664,5016 72722,5016 72759,5016 72759,5016 72759,5016 72759,2195.8 72759,2195.8"];
	Layer3_Device14_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="69956,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage14_Attention	[label=Q_local,
		lp="68362,3509.9",
		pos="e,69838,1992.9 72304,5000.9 72304,4751.9 72304,2835 72304,2835 72304,2835 69838,2835 69838,2835 69838,2835 69838,2002.9 69838,2002.9"];
	Layer3_Device14_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="71686,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_QKVProj -> Layer3_Device14_Stage15_Attention	[label=Q_local,
		lp="72980,3413.4",
		pos="e,71918,1822 72664,5031 72865,5031 73035,5031 73035,5031 73035,5031 73035,1822 73035,1822 73035,1822 71928,1822 71928,1822"];
	Layer3_Device14_Stage0_RecvKV -> Layer3_Device14_Stage0_Attention	[pos="e,71507,4694.8 71507,4860.8 71507,4860.8 71507,4704.8 71507,4704.8"];
	Layer3_Device14_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70349,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage0_RecvKV -> Layer3_Device14_Stage1_RecvKV	[label="Ring transfer",
		lp="70606,4764.4",
		pos="e,70558,4721 70558,4807.7 70558,4807.7 70558,4731 70558,4731"];
	Layer3_Device14_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="69749,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage0_Attention -> Layer3_Device14_Stage0_Accumulate	[pos="e,69749,4501.7 71558,4641.3 71558,4626.9 71558,4612 71558,4612 71558,4612 69749,4612 69749,4612 69749,4612 69749,4511.7 69749,4511.7"];
	Layer3_Device14_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69318,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage0_Accumulate -> Layer3_Device14_Stage1_Accumulate	[pos="e,69534,4308.5 69534,4448.1 69534,4448.1 69534,4318.5 69534,4318.5"];
	Layer3_Device14_Stage1_RecvKV -> Layer3_Device14_Stage1_Attention	[pos="e,69444,4501.8 69444,4667.8 69444,4667.8 69444,4511.8 69444,4511.8"];
	Layer3_Device14_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70958,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage1_RecvKV -> Layer3_Device14_Stage2_RecvKV	[label="Ring transfer",
		lp="70656,4571.4",
		pos="e,70654,4528 70654,4614.7 70654,4614.7 70654,4538 70654,4538"];
	Layer3_Device14_Stage1_Attention -> Layer3_Device14_Stage1_Accumulate	[pos="e,69273,4308.5 69273,4448.1 69273,4448.1 69273,4318.5 69273,4318.5"];
	Layer3_Device14_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69436,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage1_Accumulate -> Layer3_Device14_Stage2_Accumulate	[pos="e,69377,4115.5 69377,4255.1 69377,4255.1 69377,4125.5 69377,4125.5"];
	Layer3_Device14_Stage2_RecvKV -> Layer3_Device14_Stage2_Attention	[pos="e,70054,4308.8 70054,4474.8 70054,4474.8 70054,4318.8 70054,4318.8"];
	Layer3_Device14_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71048,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage2_RecvKV -> Layer3_Device14_Stage3_RecvKV	[label="Ring transfer",
		lp="71051,4378.4",
		pos="e,71003,4335 71003,4421.7 71003,4421.7 71003,4345 71003,4345"];
	Layer3_Device14_Stage2_Attention -> Layer3_Device14_Stage2_Accumulate	[pos="e,69638,4115.5 69638,4255.1 69638,4255.1 69638,4125.5 69638,4125.5"];
	Layer3_Device14_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69581,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage2_Accumulate -> Layer3_Device14_Stage3_Accumulate	[pos="e,69508,3922.5 69508,4062.1 69508,4062.1 69508,3932.5 69508,3932.5"];
	Layer3_Device14_Stage3_RecvKV -> Layer3_Device14_Stage3_Attention	[pos="e,71795,4115.8 71795,4281.8 71795,4281.8 71795,4125.8 71795,4125.8"];
	Layer3_Device14_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70645,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage3_RecvKV -> Layer3_Device14_Stage4_RecvKV	[label="Ring transfer",
		lp="70829,4185.4",
		pos="e,70846,4142 70846,4228.7 70846,4228.7 70846,4152 70846,4152"];
	Layer3_Device14_Stage3_Attention -> Layer3_Device14_Stage3_Accumulate	[pos="e,69769,3922.7 71666,4062.2 71666,4044.7 71666,4025 71666,4025 71666,4025 69769,4025 69769,4025 69769,4025 69769,3932.7 69769,3932.7"];
	Layer3_Device14_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69554,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage3_Accumulate -> Layer3_Device14_Stage4_Accumulate	[pos="e,69568,3729.5 69568,3869.1 69568,3869.1 69568,3739.5 69568,3739.5"];
	Layer3_Device14_Stage4_RecvKV -> Layer3_Device14_Stage4_Attention	[pos="e,69248,3922.6 69772,4049 69489,4049 69248,4049 69248,4049 69248,4049 69248,3932.6 69248,3932.6"];
	Layer3_Device14_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70790,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage4_RecvKV -> Layer3_Device14_Stage5_RecvKV	[label="Ring transfer",
		lp="70765,3992.4",
		pos="e,70718,3949 70718,4035.7 70718,4035.7 70718,3959 70718,3959"];
	Layer3_Device14_Stage4_Attention -> Layer3_Device14_Stage4_Accumulate	[pos="e,69336,3729.6 69292,3895 69318,3895 69336,3895 69336,3895 69336,3895 69336,3739.6 69336,3739.6"];
	Layer3_Device14_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71476,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage4_Accumulate -> Layer3_Device14_Stage5_Accumulate	[pos="e,71464,3536.7 69566,3676.3 69566,3643.8 69566,3593 69566,3593 69566,3593 71464,3593 71464,3593 71464,3593 71464,3546.7 71464,3546.7"];
	Layer3_Device14_Stage5_RecvKV -> Layer3_Device14_Stage5_Attention	[pos="e,71740,3711 71691,3895.8 71691,3890.2 71691,3711 71691,3711 71691,3711 71730,3711 71730,3711"];
	Layer3_Device14_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70763,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage5_RecvKV -> Layer3_Device14_Stage6_RecvKV	[label="Ring transfer",
		lp="70824,3799.4",
		pos="e,70776,3756 70776,3842.7 70776,3842.7 70776,3766 70776,3766"];
	Layer3_Device14_Stage5_Attention -> Layer3_Device14_Stage5_Accumulate	[pos="e,71699,3536.7 71740,3694 71716,3694 71699,3694 71699,3694 71699,3694 71699,3546.7 71699,3546.7"];
	Layer3_Device14_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71476,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage5_Accumulate -> Layer3_Device14_Stage6_Accumulate	[pos="e,71476,3343.5 71476,3483.1 71476,3483.1 71476,3353.5 71476,3353.5"];
	Layer3_Device14_Stage6_RecvKV -> Layer3_Device14_Stage6_Attention	[pos="e,71984,3536.6 71353,3663 71665,3663 71984,3663 71984,3663 71984,3663 71984,3546.6 71984,3546.6"];
	Layer3_Device14_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70267,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage6_RecvKV -> Layer3_Device14_Stage7_RecvKV	[label="Ring transfer",
		lp="70563,3606.4",
		pos="e,70515,3563 70515,3649.7 70515,3649.7 70515,3573 70515,3573"];
	Layer3_Device14_Stage6_Attention -> Layer3_Device14_Stage6_Accumulate	[pos="e,71708,3308 71901,3483.1 71901,3428.2 71901,3308 71901,3308 71901,3308 71718,3308 71718,3308"];
	Layer3_Device14_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71284,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage6_Accumulate -> Layer3_Device14_Stage7_Accumulate	[pos="e,71380,3150.5 71380,3290.1 71380,3290.1 71380,3160.5 71380,3160.5"];
	Layer3_Device14_Stage7_RecvKV -> Layer3_Device14_Stage7_Attention	[pos="e,69059,3343.6 69393,3470 69204,3470 69059,3470 69059,3470 69059,3470 69059,3353.6 69059,3353.6"];
	Layer3_Device14_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70267,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage7_RecvKV -> Layer3_Device14_Stage8_RecvKV	[label="Ring transfer",
		lp="70315,3413.4",
		pos="e,70267,3370 70267,3456.7 70267,3456.7 70267,3380 70267,3380"];
	Layer3_Device14_Stage7_Attention -> Layer3_Device14_Stage7_Accumulate	[pos="e,71120,3150.8 69222,3290.4 69222,3260.8 69222,3217 69222,3217 69222,3217 71120,3217 71120,3217 71120,3217 71120,3160.8 71120,3160.8"];
	Layer3_Device14_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71284,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage7_Accumulate -> Layer3_Device14_Stage8_Accumulate	[pos="e,71284,2957.5 71284,3097.1 71284,3097.1 71284,2967.5 71284,2967.5"];
	Layer3_Device14_Stage8_RecvKV -> Layer3_Device14_Stage8_Attention	[pos="e,71640,3150.6 70857,3277 71229,3277 71640,3277 71640,3277 71640,3277 71640,3160.6 71640,3160.6"];
	Layer3_Device14_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="70075,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage8_RecvKV -> Layer3_Device14_Stage9_RecvKV	[label="Ring transfer",
		lp="70219,3220.4",
		pos="e,70171,3177 70171,3263.7 70171,3263.7 70171,3187 70171,3187"];
	Layer3_Device14_Stage8_Attention -> Layer3_Device14_Stage8_Accumulate	[pos="e,71516,2930 71640,3097.4 71640,3044.2 71640,2930 71640,2930 71640,2930 71526,2930 71526,2930"];
	Layer3_Device14_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69578,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage8_Accumulate -> Layer3_Device14_Stage9_Accumulate	[pos="e,69655,2764.5 71284,2904.2 71284,2876.5 71284,2837 71284,2837 71284,2837 69655,2837 69655,2837 69655,2837 69655,2774.5 69655,2774.5"];
	Layer3_Device14_Stage9_RecvKV -> Layer3_Device14_Stage9_Attention	[pos="e,68962,2957.6 69201,3084 69062,3084 68962,3084 68962,3084 68962,3084 68962,2967.6 68962,2967.6"];
	Layer3_Device14_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70075,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage9_RecvKV -> Layer3_Device14_Stage10_RecvKV	[label="Ring transfer",
		lp="70123,3027.4",
		pos="e,70075,2984 70075,3070.7 70075,3070.7 70075,2994 70075,2994"];
	Layer3_Device14_Stage9_Attention -> Layer3_Device14_Stage9_Accumulate	[pos="e,69501,2764.4 68962,2904.2 68962,2867.2 68962,2805 68962,2805 68962,2805 69501,2805 69501,2805 69501,2805 69501,2774.4 69501,2774.4"];
	Layer3_Device14_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="69578,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage9_Accumulate -> Layer3_Device14_Stage10_Accumulate	[pos="e,69578,2571.5 69578,2711.1 69578,2711.1 69578,2581.5 69578,2581.5"];
	Layer3_Device14_Stage10_RecvKV -> Layer3_Device14_Stage10_Attention	[pos="e,69222,2764.8 69222,2930.8 69222,2930.8 69222,2774.8 69222,2774.8"];
	Layer3_Device14_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="70787,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage10_RecvKV -> Layer3_Device14_Stage11_RecvKV	[label="Ring transfer",
		lp="70392,2834.4",
		pos="e,70431,2791 70431,2877.7 70431,2877.7 70431,2801 70431,2801"];
	Layer3_Device14_Stage10_Attention -> Layer3_Device14_Stage10_Accumulate	[pos="e,69346,2544 69057,2711.4 69057,2658.2 69057,2544 69057,2544 69057,2544 69336,2544 69336,2544"];
	Layer3_Device14_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70098,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage10_Accumulate -> Layer3_Device14_Stage11_Accumulate	[pos="e,69866,2351 69767,2518.4 69767,2465.2 69767,2351 69767,2351 69767,2351 69856,2351 69856,2351"];
	Layer3_Device14_Stage11_RecvKV -> Layer3_Device14_Stage11_Attention	[pos="e,70212,2571.6 70212,2684.7 70212,2684.7 70212,2581.6 70212,2581.6"];
	Layer3_Device14_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71498,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage11_RecvKV -> Layer3_Device14_Stage12_RecvKV	[label="Ring transfer",
		lp="71191,2641.4",
		pos="e,71142,2598 71142,2684.7 71142,2684.7 71142,2608 71142,2608"];
	Layer3_Device14_Stage11_Attention -> Layer3_Device14_Stage11_Accumulate	[pos="e,70194,2378.5 70194,2518.1 70194,2518.1 70194,2388.5 70194,2388.5"];
	Layer3_Device14_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70216,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage11_Accumulate -> Layer3_Device14_Stage12_Accumulate	[pos="e,70157,2185.5 70157,2325.1 70157,2325.1 70157,2195.5 70157,2195.5"];
	Layer3_Device14_Stage12_RecvKV -> Layer3_Device14_Stage12_Attention	[pos="e,70714,2378.6 70714,2491.7 70714,2491.7 70714,2388.6 70714,2388.6"];
	Layer3_Device14_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71828,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage12_RecvKV -> Layer3_Device14_Stage13_RecvKV	[label="Ring transfer",
		lp="71711,2448.4",
		pos="e,71663,2405 71663,2491.7 71663,2491.7 71663,2415 71663,2415"];
	Layer3_Device14_Stage12_Attention -> Layer3_Device14_Stage12_Accumulate	[pos="e,70418,2185.5 70418,2325.1 70418,2325.1 70418,2195.5 70418,2195.5"];
	Layer3_Device14_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70477,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage12_Accumulate -> Layer3_Device14_Stage13_Accumulate	[pos="e,70346,1992.5 70346,2132.1 70346,2132.1 70346,2002.5 70346,2002.5"];
	Layer3_Device14_Stage13_RecvKV -> Layer3_Device14_Stage13_Attention	[pos="e,72575,2185.8 72575,2351.8 72575,2351.8 72575,2195.8 72575,2195.8"];
	Layer3_Device14_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71425,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage13_RecvKV -> Layer3_Device14_Stage14_RecvKV	[label="Ring transfer",
		lp="71609,2255.4",
		pos="e,71626,2212 71626,2298.7 71626,2298.7 71626,2222 71626,2222"];
	Layer3_Device14_Stage13_Attention -> Layer3_Device14_Stage13_Accumulate	[pos="e,70607,1992.5 72504,2132.1 72504,2111.7 72504,2087 72504,2087 72504,2087 70607,2087 70607,2087 70607,2087 70607,2002.5 70607,2002.5"];
	Layer3_Device14_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="70477,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage13_Accumulate -> Layer3_Device14_Stage14_Accumulate	[pos="e,70477,1839.9 70477,1939.3 70477,1939.3 70477,1849.9 70477,1849.9"];
	Layer3_Device14_Stage14_RecvKV -> Layer3_Device14_Stage14_Attention	[pos="e,70086,1992.6 70551,2119 70297,2119 70086,2119 70086,2119 70086,2119 70086,2002.6 70086,2002.6"];
	Layer3_Device14_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="71686,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device14_Stage14_RecvKV -> Layer3_Device14_Stage15_RecvKV	[label="Ring transfer",
		lp="71603,2062.4",
		pos="e,71556,2019 71556,2105.7 71556,2105.7 71556,2029 71556,2029"];
	Layer3_Device14_Stage14_Attention -> Layer3_Device14_Stage14_Accumulate	[pos="e,70245,1804 70152,1939.2 70152,1893.2 70152,1804 70152,1804 70152,1804 70235,1804 70235,1804"];
	Layer3_Device14_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="71082,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device14_Stage14_Accumulate -> Layer3_Device14_Stage15_Accumulate	[pos="e,70850,1687 70702,1786.6 70702,1749.5 70702,1687 70702,1687 70702,1687 70840,1687 70840,1687"];
	Layer3_Device14_Stage15_RecvKV -> Layer3_Device14_Stage15_Attention	[pos="e,71686,1840 71686,1912.6 71686,1912.6 71686,1850 71686,1850"];
	Layer3_Device14_Stage15_Attention -> Layer3_Device14_Stage15_Accumulate	[pos="e,71314,1687 71462,1786.6 71462,1749.5 71462,1687 71462,1687 71462,1687 71324,1687 71324,1687"];
	Layer3_Device14_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="71082,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device14_Stage15_Accumulate -> Layer3_Device14_ConcatHeads	[pos="e,71082,1588.1 71082,1660.7 71082,1660.7 71082,1598.1 71082,1598.1"];
	Layer3_Device14_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="71082,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device14_ConcatHeads -> Layer3_Device14_OutputProj	[pos="e,71082,1462.1 71082,1534.7 71082,1534.7 71082,1472.1 71082,1472.1"];
	Layer3_Device14_OutputProj -> Layer3_Device14_Residual1	[pos="e,71082,1336.1 71082,1408.7 71082,1408.7 71082,1346.1 71082,1346.1"];
	Layer3_Device14_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="70781,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device14_Residual1 -> Layer3_Device14_LayerNorm2	[pos="e,70845,1210.1 70845,1282.7 70845,1282.7 70845,1220.1 70845,1220.1"];
	Layer3_Device14_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="70526,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device14_Residual1 -> Layer3_Device14_Residual2	[pos="e,70913,492 71112,1282.7 71112,1141.8 71112,492 71112,492 71112,492 70923,492 70923,492"];
	Layer3_Device14_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="70488,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device14_LayerNorm2 -> Layer3_Device14_GateProj	[pos="e,70640,1084.1 70640,1156.7 70640,1156.7 70640,1094.1 70640,1094.1"];
	Layer3_Device14_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="70801,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device14_LayerNorm2 -> Layer3_Device14_UpProj	[pos="e,70854,995.2 70854,1156.6 70854,1156.6 70854,1005.2 70854,1005.2"];
	Layer3_Device14_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="70488,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device14_GateProj -> Layer3_Device14_Activation	[pos="e,70420,905.95 70420,1030.8 70420,1030.8 70420,915.95 70420,915.95"];
	Layer3_Device14_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="70526,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device14_UpProj -> Layer3_Device14_ElemMul	[pos="e,70825,780.2 70825,941.61 70825,941.61 70825,790.2 70825,790.2"];
	Layer3_Device14_Activation -> Layer3_Device14_ElemMul	[pos="e,70488,780.09 70488,852.69 70488,852.69 70488,790.09 70488,790.09"];
	Layer3_Device14_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="70526,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device14_ElemMul -> Layer3_Device14_DownProj	[pos="e,70526,654.09 70526,726.69 70526,726.69 70526,664.09 70526,664.09"];
	Layer3_Device14_DownProj -> Layer3_Device14_Residual2	[pos="e,70526,528.09 70526,600.69 70526,600.69 70526,538.09 70526,538.09"];
	Layer3_Device14_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 14 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="66683,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device14_Residual2 -> Layer3_Device14_Output	[pos="e,66778,400.04 70139,510 69182,510 66778,510 66778,510 66778,510 66778,410.04 66778,410.04"];
	Layer3_Device14_Output -> Sequence_Aggregate	[pos="e,40931,174 66683,326.81 66683,271.31 66683,174 66683,174 66683,174 40941,174 40941,174"];
	Layer3_Device15_LayerNorm1	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm1\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="77274,5153.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device15_Input -> Layer3_Device15_LayerNorm1	[pos="e,77487,5153 77698,5269.5 77698,5231.1 77698,5153 77698,5153 77698,5153 77497,5153 77497,5153"];
	Layer3_Device15_Residual1	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 1\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="75821,1309.4",
		shape=rectangle,
		width=10.736];
	Layer3_Device15_Input -> Layer3_Device15_Residual1	[pos="e,76208,1309 78083,5256.6 78083,4870.5 78083,1309 78083,1309 78083,1309 76218,1309 76218,1309"];
	Layer3_Device15_QKVProj	[fillcolor=lightcoral,
		height=0.73611,
		label="QKV Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: Q:[batch_size=1024, seq_len=625, heads=16, d_k=512], \
K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76896,5027.4",
		shape=rectangle,
		width=18.042];
	Layer3_Device15_LayerNorm1 -> Layer3_Device15_QKVProj	[pos="e,77274,5054.1 77274,5126.7 77274,5126.7 77274,5064.1 77274,5064.1"];
	Layer3_Device15_Stage0_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 0\nInput: K,V from Device 15\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75650,4860.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage0_RecvKV	[label="Local K,V",
		lp="76150,4957.4",
		pos="e,76408,4914.2 76408,5000.7 76408,5000.7 76408,4924.2 76408,4924.2"];
	Layer3_Device15_Stage0_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 0\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76403,4667.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage0_Attention	[label=Q_local,
		lp="76712,4860.9",
		pos="e,76602,4694.8 76602,5000.7 76602,5000.7 76602,4704.8 76602,4704.8"];
	Layer3_Device15_Stage1_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 1\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74110,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage1_Attention	[label=Q_local,
		lp="74174,4764.4",
		pos="e,74201,4501.7 76246,5008 75449,5008 74201,5008 74201,5008 74201,5008 74201,4511.7 74201,4511.7"];
	Layer3_Device15_Stage2_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 2\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74721,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage2_Attention	[label=Q_local,
		lp="76852,4667.9",
		pos="e,74891,4308.5 76697,5000.7 76697,4901.8 76697,4562 76697,4562 76697,4562 74891,4562 74891,4562 74891,4562 74891,4318.5 74891,4318.5"];
	Layer3_Device15_Stage3_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 3\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76736,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage3_Attention	[label=Q_local,
		lp="76992,4571.4",
		pos="e,76909,4115.7 76909,5000.8 76909,5000.8 76909,4125.7 76909,4125.7"];
	Layer3_Device15_Stage4_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 4\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73942,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage4_Attention	[label=Q_local,
		lp="73794,4474.9",
		pos="e,73854,3922.7 76246,5023 75353,5023 73854,5023 73854,5023 73854,5023 73854,3932.7 73854,3932.7"];
	Layer3_Device15_Stage5_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 5\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76854,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage5_Attention	[label=Q_local,
		lp="77098,4378.4",
		pos="e,77026,3729.7 77026,5000.6 77026,5000.6 77026,3739.7 77026,3739.7"];
	Layer3_Device15_Stage6_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 6\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76878,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage6_Attention	[label=Q_local,
		lp="77228,4281.9",
		pos="e,77098,3536.6 77098,5000.7 77098,5000.7 77098,3546.6 77098,3546.6"];
	Layer3_Device15_Stage7_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 7\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73939,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage7_Attention	[label=Q_local,
		lp="73648,4185.4",
		pos="e,73707,3325 76246,5038 75297,5038 73642,5038 73642,5038 73642,5038 73642,3325 73642,3325 73642,3325 73697,3325 73697,3325"];
	Layer3_Device15_Stage8_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 8\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76683,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage8_Attention	[label=Q_local,
		lp="77346,4088.9",
		pos="e,76915,3123 77167,5000.8 77167,4770.7 77167,3123 77167,3123 77167,3123 76925,3123 76925,3123"];
	Layer3_Device15_Stage9_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 9\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="73744,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage9_Attention	[label=Q_local,
		lp="73530,3992.4",
		pos="e,73578,2957.6 76246,5046 75280,5046 73578,5046 73578,5046 73578,5046 73578,2967.6 73578,2967.6"];
	Layer3_Device15_Stage10_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 10\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74106,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage10_Attention	[label=Q_local,
		lp="73390,3895.9",
		pos="e,74236,2764.6 77224,5000.5 77224,4752.4 77224,2870 77224,2870 77224,2870 74236,2870 74236,2870 74236,2870 74236,2774.6 74236,2774.6"];
	Layer3_Device15_Stage11_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 11\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75267,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage11_Attention	[label=Q_local,
		lp="77474,3799.4",
		pos="e,75344,2571.6 77281,5000.6 77281,4736.5 77281,2607 77281,2607 77281,2607 75344,2607 75344,2607 75344,2607 75344,2581.6 75344,2581.6"];
	Layer3_Device15_Stage12_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 12\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="75502,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage12_Attention	[label=Q_local,
		lp="77605,3702.9",
		pos="e,75527,2378.5 77338,5000.6 77338,4736.3 77338,2605 77338,2605 77338,2605 75527,2605 75527,2605 75527,2605 75527,2388.5 75527,2388.5"];
	Layer3_Device15_Stage13_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 13\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="77517,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage13_Attention	[label=Q_local,
		lp="77745,3606.4",
		pos="e,77639,2185.8 77546,5018 77603,5018 77639,5018 77639,5018 77639,5018 77639,2195.8 77639,2195.8"];
	Layer3_Device15_Stage14_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 14\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="74839,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage14_Attention	[label=Q_local,
		lp="73244,3509.9",
		pos="e,74678,1992.5 77471,5000.6 77471,4726.9 77471,2448 77471,2448 77471,2448 74678,2448 74678,2448 74678,2448 74678,2002.5 74678,2002.5"];
	Layer3_Device15_Stage15_Attention	[fillcolor=lightpink,
		height=0.73611,
		label="Attention Computation Stage 15\nInput: Q_local, K_received, V_received\nOutput: [batch_size=1024, seq_len=625, heads=16, d_k=512]",
		pos="76569,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_QKVProj -> Layer3_Device15_Stage15_Attention	[label=Q_local,
		lp="77863,3413.4",
		pos="e,76801,1813 77546,5036 77746,5036 77916,5036 77916,5036 77916,5036 77916,1813 77916,1813 77916,1813 76811,1813 76811,1813"];
	Layer3_Device15_Stage0_RecvKV -> Layer3_Device15_Stage0_Attention	[pos="e,76371,4694.7 76371,4857.6 76371,4857.6 76371,4704.7 76371,4704.7"];
	Layer3_Device15_Stage1_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 1\nInput: K,V from Device 14\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75194,4667.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage0_RecvKV -> Layer3_Device15_Stage1_RecvKV	[label="Ring transfer",
		lp="75551,4764.4",
		pos="e,75422,4721 75422,4807.7 75422,4807.7 75422,4731 75422,4731"];
	Layer3_Device15_Stage0_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Initialize Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, \
d_k=512]",
		pos="74631,4474.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage0_Attention -> Layer3_Device15_Stage0_Accumulate	[pos="e,74631,4501.6 76403,4641.1 76403,4610.4 76403,4564 76403,4564 76403,4564 74631,4564 74631,4564 74631,4564 74631,4511.6 74631,4511.6"];
	Layer3_Device15_Stage1_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74200,4281.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage0_Accumulate -> Layer3_Device15_Stage1_Accumulate	[pos="e,74416,4308.5 74416,4448.1 74416,4448.1 74416,4318.5 74416,4318.5"];
	Layer3_Device15_Stage1_RecvKV -> Layer3_Device15_Stage1_Attention	[pos="e,74308,4501.8 74308,4667.8 74308,4667.8 74308,4511.8 74308,4511.8"];
	Layer3_Device15_Stage2_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 2\nInput: K,V from Device 13\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75840,4474.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage1_RecvKV -> Layer3_Device15_Stage2_RecvKV	[label="Ring transfer",
		lp="75511,4571.4",
		pos="e,75517,4528 75517,4614.7 75517,4614.7 75517,4538 75517,4538"];
	Layer3_Device15_Stage1_Attention -> Layer3_Device15_Stage1_Accumulate	[pos="e,74155,4308.5 74155,4448.1 74155,4448.1 74155,4318.5 74155,4318.5"];
	Layer3_Device15_Stage2_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74318,4088.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage1_Accumulate -> Layer3_Device15_Stage2_Accumulate	[pos="e,74259,4115.5 74259,4255.1 74259,4255.1 74259,4125.5 74259,4125.5"];
	Layer3_Device15_Stage2_RecvKV -> Layer3_Device15_Stage2_Attention	[pos="e,74936,4308.8 74936,4474.8 74936,4474.8 74936,4318.8 74936,4318.8"];
	Layer3_Device15_Stage3_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 3\nInput: K,V from Device 12\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75930,4281.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage2_RecvKV -> Layer3_Device15_Stage3_RecvKV	[label="Ring transfer",
		lp="75933,4378.4",
		pos="e,75885,4335 75885,4421.7 75885,4421.7 75885,4345 75885,4345"];
	Layer3_Device15_Stage2_Attention -> Layer3_Device15_Stage2_Accumulate	[pos="e,74520,4115.5 74520,4255.1 74520,4255.1 74520,4125.5 74520,4125.5"];
	Layer3_Device15_Stage3_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74463,3895.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage2_Accumulate -> Layer3_Device15_Stage3_Accumulate	[pos="e,74390,3922.5 74390,4062.1 74390,4062.1 74390,3932.5 74390,3932.5"];
	Layer3_Device15_Stage3_RecvKV -> Layer3_Device15_Stage3_Attention	[pos="e,76677,4115.8 76677,4281.8 76677,4281.8 76677,4125.8 76677,4125.8"];
	Layer3_Device15_Stage4_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 4\nInput: K,V from Device 11\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75527,4088.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage3_RecvKV -> Layer3_Device15_Stage4_RecvKV	[label="Ring transfer",
		lp="75711,4185.4",
		pos="e,75728,4142 75728,4228.7 75728,4228.7 75728,4152 75728,4152"];
	Layer3_Device15_Stage3_Attention -> Layer3_Device15_Stage3_Accumulate	[pos="e,74651,3922.7 76548,4062.2 76548,4046.6 76548,4030 76548,4030 76548,4030 74651,4030 74651,4030 74651,4030 74651,3932.7 74651,3932.7"];
	Layer3_Device15_Stage4_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74436,3702.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage3_Accumulate -> Layer3_Device15_Stage4_Accumulate	[pos="e,74450,3729.5 74450,3869.1 74450,3869.1 74450,3739.5 74450,3739.5"];
	Layer3_Device15_Stage4_RecvKV -> Layer3_Device15_Stage4_Attention	[pos="e,74130,3922.6 74654,4049 74371,4049 74130,4049 74130,4049 74130,4049 74130,3932.6 74130,3932.6"];
	Layer3_Device15_Stage5_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 5\nInput: K,V from Device 10\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75672,3895.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage4_RecvKV -> Layer3_Device15_Stage5_RecvKV	[label="Ring transfer",
		lp="75647,3992.4",
		pos="e,75600,3949 75600,4035.7 75600,4035.7 75600,3959 75600,3959"];
	Layer3_Device15_Stage4_Attention -> Layer3_Device15_Stage4_Accumulate	[pos="e,74218,3729.6 74174,3895 74200,3895 74218,3895 74218,3895 74218,3895 74218,3739.6 74218,3739.6"];
	Layer3_Device15_Stage5_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76357,3509.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage4_Accumulate -> Layer3_Device15_Stage5_Accumulate	[pos="e,76345,3536.8 74448,3676.3 74448,3642.3 74448,3588 74448,3588 74448,3588 76345,3588 76345,3588 76345,3588 76345,3546.8 76345,3546.8"];
	Layer3_Device15_Stage5_RecvKV -> Layer3_Device15_Stage5_Attention	[pos="e,76622,3702 76577,3895.8 76577,3889.9 76577,3702 76577,3702 76577,3702 76612,3702 76612,3702"];
	Layer3_Device15_Stage6_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 6\nInput: K,V from Device 9\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75645,3702.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage5_RecvKV -> Layer3_Device15_Stage6_RecvKV	[label="Ring transfer",
		lp="75706,3799.4",
		pos="e,75658,3756 75658,3842.7 75658,3842.7 75658,3766 75658,3766"];
	Layer3_Device15_Stage5_Attention -> Layer3_Device15_Stage5_Accumulate	[pos="e,76589,3509 76634,3676.4 76634,3623.2 76634,3509 76634,3509 76634,3509 76599,3509 76599,3509"];
	Layer3_Device15_Stage6_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76357,3316.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage5_Accumulate -> Layer3_Device15_Stage6_Accumulate	[pos="e,76357,3343.5 76357,3483.1 76357,3483.1 76357,3353.5 76357,3353.5"];
	Layer3_Device15_Stage6_RecvKV -> Layer3_Device15_Stage6_Attention	[pos="e,76866,3536.6 76235,3663 76547,3663 76866,3663 76866,3663 76866,3663 76866,3546.6 76866,3546.6"];
	Layer3_Device15_Stage7_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 7\nInput: K,V from Device 8\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75148,3509.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage6_RecvKV -> Layer3_Device15_Stage7_RecvKV	[label="Ring transfer",
		lp="75379,3606.4",
		pos="e,75396,3563 75396,3649.7 75396,3649.7 75396,3573 75396,3573"];
	Layer3_Device15_Stage6_Attention -> Layer3_Device15_Stage6_Accumulate	[pos="e,76589,3316 76780,3483.4 76780,3430.2 76780,3316 76780,3316 76780,3316 76599,3316 76599,3316"];
	Layer3_Device15_Stage7_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76162,3123.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage6_Accumulate -> Layer3_Device15_Stage7_Accumulate	[pos="e,76260,3150.5 76260,3290.1 76260,3290.1 76260,3160.5 76260,3160.5"];
	Layer3_Device15_Stage7_RecvKV -> Layer3_Device15_Stage7_Attention	[pos="e,73940,3343.6 74274,3470 74085,3470 73940,3470 73940,3470 73940,3470 73940,3353.6 73940,3353.6"];
	Layer3_Device15_Stage8_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 8\nInput: K,V from Device 7\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="75148,3316.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage7_RecvKV -> Layer3_Device15_Stage8_RecvKV	[label="Ring transfer",
		lp="75196,3413.4",
		pos="e,75148,3370 75148,3456.7 75148,3456.7 75148,3380 75148,3380"];
	Layer3_Device15_Stage7_Attention -> Layer3_Device15_Stage7_Accumulate	[pos="e,75999,3150.8 74102,3290.2 74102,3259.1 74102,3212 74102,3212 74102,3212 75999,3212 75999,3212 75999,3212 75999,3160.8 75999,3160.8"];
	Layer3_Device15_Stage8_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="76162,2930.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage7_Accumulate -> Layer3_Device15_Stage8_Accumulate	[pos="e,76162,2957.5 76162,3097.1 76162,3097.1 76162,2967.5 76162,2967.5"];
	Layer3_Device15_Stage8_RecvKV -> Layer3_Device15_Stage8_Attention	[pos="e,76520,3150.6 75738,3277 76110,3277 76520,3277 76520,3277 76520,3277 76520,3160.6 76520,3160.6"];
	Layer3_Device15_Stage9_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 9\nInput: K,V from Device 6\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, seq_\
len=625, heads=16, d_k=512]",
		pos="74953,3123.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage8_RecvKV -> Layer3_Device15_Stage9_RecvKV	[label="Ring transfer",
		lp="75114,3220.4",
		pos="e,75050,3177 75050,3263.7 75050,3263.7 75050,3187 75050,3187"];
	Layer3_Device15_Stage8_Attention -> Layer3_Device15_Stage8_Accumulate	[pos="e,76394,2930 76604,3097.4 76604,3044.2 76604,2930 76604,2930 76604,2930 76404,2930 76404,2930"];
	Layer3_Device15_Stage9_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74627,2737.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage8_Accumulate -> Layer3_Device15_Stage9_Accumulate	[pos="e,74704,2764.5 76162,2904.4 76162,2871.3 76162,2819 76162,2819 76162,2819 74704,2819 74704,2819 74704,2819 74704,2774.5 74704,2774.5"];
	Layer3_Device15_Stage9_RecvKV -> Layer3_Device15_Stage9_Attention	[pos="e,73842,2957.6 74080,3084 73941,3084 73842,3084 73842,3084 73842,3084 73842,2967.6 73842,2967.6"];
	Layer3_Device15_Stage10_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 10\nInput: K,V from Device 5\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="74953,2930.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage9_RecvKV -> Layer3_Device15_Stage10_RecvKV	[label="Ring transfer",
		lp="75001,3027.4",
		pos="e,74953,2984 74953,3070.7 74953,3070.7 74953,2994 74953,2994"];
	Layer3_Device15_Stage9_Attention -> Layer3_Device15_Stage9_Accumulate	[pos="e,74550,2764.6 73925,2904.1 73925,2860 73925,2777 73925,2777 73925,2777 74550,2777 74550,2777 74550,2777 74550,2774.6 74550,2774.6"];
	Layer3_Device15_Stage10_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74627,2544.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage9_Accumulate -> Layer3_Device15_Stage10_Accumulate	[pos="e,74627,2571.5 74627,2711.1 74627,2711.1 74627,2581.5 74627,2581.5"];
	Layer3_Device15_Stage10_RecvKV -> Layer3_Device15_Stage10_Attention	[pos="e,74135,2764.6 74135,2877.7 74135,2877.7 74135,2774.6 74135,2774.6"];
	Layer3_Device15_Stage11_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 11\nInput: K,V from Device 4\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="75836,2737.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage10_RecvKV -> Layer3_Device15_Stage11_RecvKV	[label="Ring transfer",
		lp="75270,2834.4",
		pos="e,75394,2791 75394,2877.7 75394,2877.7 75394,2801 75394,2801"];
	Layer3_Device15_Stage10_Attention -> Layer3_Device15_Stage10_Accumulate	[pos="e,74395,2544 74106,2711.4 74106,2658.2 74106,2544 74106,2544 74106,2544 74385,2544 74385,2544"];
	Layer3_Device15_Stage11_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="74981,2351.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage10_Accumulate -> Layer3_Device15_Stage11_Accumulate	[pos="e,74804,2378.5 74804,2518.1 74804,2518.1 74804,2388.5 74804,2388.5"];
	Layer3_Device15_Stage11_RecvKV -> Layer3_Device15_Stage11_Attention	[pos="e,75190,2571.6 75190,2684.7 75190,2684.7 75190,2581.6 75190,2581.6"];
	Layer3_Device15_Stage12_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 12\nInput: K,V from Device 3\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76476,2544.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage11_RecvKV -> Layer3_Device15_Stage12_RecvKV	[label="Ring transfer",
		lp="76204,2641.4",
		pos="e,76156,2598 76156,2684.7 76156,2684.7 76156,2608 76156,2608"];
	Layer3_Device15_Stage11_Attention -> Layer3_Device15_Stage11_Accumulate	[pos="e,75124,2378.5 75124,2518.1 75124,2518.1 75124,2388.5 75124,2388.5"];
	Layer3_Device15_Stage12_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75099,2158.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage11_Accumulate -> Layer3_Device15_Stage12_Accumulate	[pos="e,75040,2185.5 75040,2325.1 75040,2325.1 75040,2195.5 75040,2195.5"];
	Layer3_Device15_Stage12_RecvKV -> Layer3_Device15_Stage12_Attention	[pos="e,75645,2378.6 75645,2491.7 75645,2491.7 75645,2388.6 75645,2388.6"];
	Layer3_Device15_Stage13_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 13\nInput: K,V from Device 2\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76711,2351.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage12_RecvKV -> Layer3_Device15_Stage13_RecvKV	[label="Ring transfer",
		lp="76641,2448.4",
		pos="e,76594,2405 76594,2491.7 76594,2491.7 76594,2415 76594,2415"];
	Layer3_Device15_Stage12_Attention -> Layer3_Device15_Stage12_Accumulate	[pos="e,75300,2185.5 75300,2325.1 75300,2325.1 75300,2195.5 75300,2195.5"];
	Layer3_Device15_Stage13_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75360,1965.9",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage12_Accumulate -> Layer3_Device15_Stage13_Accumulate	[pos="e,75230,1992.5 75230,2132.1 75230,2132.1 75230,2002.5 75230,2002.5"];
	Layer3_Device15_Stage13_RecvKV -> Layer3_Device15_Stage13_Attention	[pos="e,77458,2185.8 77458,2351.8 77458,2351.8 77458,2195.8 77458,2195.8"];
	Layer3_Device15_Stage14_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 14\nInput: K,V from Device 1\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76308,2158.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage13_RecvKV -> Layer3_Device15_Stage14_RecvKV	[label="Ring transfer",
		lp="76557,2255.4",
		pos="e,76510,2212 76510,2298.7 76510,2298.7 76510,2222 76510,2222"];
	Layer3_Device15_Stage13_Attention -> Layer3_Device15_Stage13_Accumulate	[pos="e,75490,1992.6 77387,2132.1 77387,2113.5 77387,2092 77387,2092 77387,2092 75490,2092 75490,2092 75490,2092 75490,2002.6 75490,2002.6"];
	Layer3_Device15_Stage14_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75360,1813.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage13_Accumulate -> Layer3_Device15_Stage14_Accumulate	[pos="e,75360,1839.9 75360,1939.3 75360,1939.3 75360,1849.9 75360,1849.9"];
	Layer3_Device15_Stage14_RecvKV -> Layer3_Device15_Stage14_Attention	[pos="e,74969,1992.6 75434,2119 75180,2119 74969,2119 74969,2119 74969,2119 74969,2002.6 74969,2002.6"];
	Layer3_Device15_Stage15_RecvKV	[fillcolor=lightyellow,
		height=1.4722,
		label="Receive KV Stage 15\nInput: K,V from Device 0\nOutput: K:[batch_size=1024, seq_len=625, heads=16, d_k=512], V:[batch_size=1024, \
seq_len=625, heads=16, d_k=512]",
		pos="76569,1965.9",
		shape=parallelogram,
		style=dashed,
		width=25.544];
	Layer3_Device15_Stage14_RecvKV -> Layer3_Device15_Stage15_RecvKV	[label="Ring transfer",
		lp="76486,2062.4",
		pos="e,76438,2019 76438,2105.7 76438,2105.7 76438,2029 76438,2029"];
	Layer3_Device15_Stage14_Attention -> Layer3_Device15_Stage14_Accumulate	[pos="e,75128,1804 75037,1939.2 75037,1893.2 75037,1804 75037,1804 75037,1804 75118,1804 75118,1804"];
	Layer3_Device15_Stage15_Accumulate	[fillcolor=lightcyan,
		height=0.73611,
		label="Add to Accumulator\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, heads=16, d_\
k=512]",
		pos="75821,1687.4",
		shape=rectangle,
		width=6.4306];
	Layer3_Device15_Stage14_Accumulate -> Layer3_Device15_Stage15_Accumulate	[pos="e,75620,1714.2 75592,1804 75609,1804 75620,1804 75620,1804 75620,1804 75620,1724.2 75620,1724.2"];
	Layer3_Device15_Stage15_RecvKV -> Layer3_Device15_Stage15_Attention	[pos="e,76569,1840 76569,1912.6 76569,1912.6 76569,1850 76569,1850"];
	Layer3_Device15_Stage15_Attention -> Layer3_Device15_Stage15_Accumulate	[pos="e,75851,1714 76337,1822 76128,1822 75851,1822 75851,1822 75851,1822 75851,1724 75851,1724"];
	Layer3_Device15_ConcatHeads	[fillcolor=lightblue,
		height=0.73611,
		label="Concatenate Heads\nInput: [batch_size=1024, seq_len=625, heads=16, d_k=512]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="75821,1561.4",
		shape=rectangle,
		width=6.2639];
	Layer3_Device15_Stage15_Accumulate -> Layer3_Device15_ConcatHeads	[pos="e,75821,1588.1 75821,1660.7 75821,1660.7 75821,1598.1 75821,1598.1"];
	Layer3_Device15_OutputProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Output Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="75821,1435.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device15_ConcatHeads -> Layer3_Device15_OutputProj	[pos="e,75821,1462.1 75821,1534.7 75821,1534.7 75821,1472.1 75821,1472.1"];
	Layer3_Device15_OutputProj -> Layer3_Device15_Residual1	[pos="e,75821,1336.1 75821,1408.7 75821,1408.7 75821,1346.1 75821,1346.1"];
	Layer3_Device15_LayerNorm2	[fillcolor=lightgreen,
		height=0.73611,
		label="LayerNorm2\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="75520,1183.4",
		shape=rectangle,
		width=5.9167];
	Layer3_Device15_Residual1 -> Layer3_Device15_LayerNorm2	[pos="e,75584,1210.1 75584,1282.7 75584,1282.7 75584,1220.1 75584,1220.1"];
	Layer3_Device15_Residual2	[fillcolor=lightgray,
		height=0.73611,
		label="Residual Add 2\nInput: [batch_size=1024, seq_len=625, d_model=8192], [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_\
size=1024, seq_len=625, d_model=8192]",
		pos="75299,501.41",
		shape=rectangle,
		width=10.736];
	Layer3_Device15_Residual1 -> Layer3_Device15_Residual2	[pos="e,75686,501 75987,1282.5 75987,1142.2 75987,501 75987,501 75987,501 75696,501 75696,501"];
	Layer3_Device15_GateProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Gate Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="75228,1057.4",
		shape=rectangle,
		width=6.25];
	Layer3_Device15_LayerNorm2 -> Layer3_Device15_GateProj	[pos="e,75380,1084.1 75380,1156.7 75380,1156.7 75380,1094.1 75380,1094.1"];
	Layer3_Device15_UpProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Up Projection\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="75541,968.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device15_LayerNorm2 -> Layer3_Device15_UpProj	[pos="e,75593,995.2 75593,1156.6 75593,1156.6 75593,1005.2 75593,1005.2"];
	Layer3_Device15_Activation	[fillcolor=lightgreen,
		height=0.73611,
		label="GELU Activation\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="75228,879.41",
		shape=rectangle,
		width=6.25];
	Layer3_Device15_GateProj -> Layer3_Device15_Activation	[pos="e,75160,905.95 75160,1030.8 75160,1030.8 75160,915.95 75160,915.95"];
	Layer3_Device15_ElemMul	[fillcolor=lightgreen,
		height=0.73611,
		label="Element-wise Multiply\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768], [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [\
batch_size=1024, seq_len=625, ffn_hidden=32768]",
		pos="75299,753.41",
		shape=rectangle,
		width=11.403];
	Layer3_Device15_UpProj -> Layer3_Device15_ElemMul	[pos="e,75581,780.2 75581,941.61 75581,941.61 75581,790.2 75581,790.2"];
	Layer3_Device15_Activation -> Layer3_Device15_ElemMul	[pos="e,75228,780.09 75228,852.69 75228,852.69 75228,790.09 75228,790.09"];
	Layer3_Device15_DownProj	[fillcolor=lightcoral,
		height=0.73611,
		label="Down Projection\nInput: [batch_size=1024, seq_len=625, ffn_hidden=32768]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="75299,627.41",
		shape=rectangle,
		width=6.0833];
	Layer3_Device15_ElemMul -> Layer3_Device15_DownProj	[pos="e,75299,654.09 75299,726.69 75299,726.69 75299,664.09 75299,664.09"];
	Layer3_Device15_DownProj -> Layer3_Device15_Residual2	[pos="e,75299,528.09 75299,600.69 75299,600.69 75299,538.09 75299,538.09"];
	Layer3_Device15_Output	[fillcolor=lightgray,
		height=1.041,
		label="Layer 3 Device 15 Output\nInput: [batch_size=1024, seq_len=625, d_model=8192]\nOutput: [batch_size=1024, seq_len=625, d_model=8192]",
		pos="71499,364.43",
		shape=ellipse,
		width=8.3674];
	Layer3_Device15_Residual2 -> Layer3_Device15_Output	[pos="e,71634,398.19 74912,510 73971,510 71634,510 71634,510 71634,510 71634,408.19 71634,408.19"];
	Layer3_Device15_Output -> Sequence_Aggregate	[pos="e,40908,161 71499,326.64 71499,267.81 71499,161 71499,161 71499,161 40918,161 40918,161"];
	Output_Total	[fillcolor=lightblue,
		height=1.041,
		label="Final Output\nInput: [batch_size=1024, seq_len=10000, d_model=8192]\nOutput: [batch_size=1024, seq_len=10000, d_model=8192]",
		pos="40613,37.477",
		shape=ellipse,
		width=8.721];
	Sequence_Aggregate -> Output_Total	[pos="e,40613,75.03 40613,147.88 40613,147.88 40613,85.03 40613,85.03"];
}
